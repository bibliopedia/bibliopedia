<SectLabel_title> Heterogeneous Transfer Learning for Image Clustering via the Social Web +L+ </SectLabel_title> <SectLabel_author> Qiang Yang +L+ </SectLabel_author> <SectLabel_affiliation> Hong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kong +L+ </SectLabel_affiliation> <SectLabel_email> qyang@cs.ust.hk +L+ </SectLabel_email> <SectLabel_author> Yuqiang Chen	Gui-Rong Xue	Wenyuan Dai	Yong Yu +L+ </SectLabel_author> <SectLabel_affiliation> Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China +L+ </SectLabel_affiliation> <SectLabel_email> {yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cn +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we present a new learning +L+ scenario, heterogeneous transfer learn- +L+ ing, which improves learning performance +L+ when the data can be in different feature +L+ spaces and where no correspondence be- +L+ tween data instances in these spaces is pro- +L+ vided. In the past, we have classified Chi- +L+ nese text documents using English train- +L+ ing data under the heterogeneous trans- +L+ fer learning framework. In this paper, +L+ we present image clustering as an exam- +L+ ple to illustrate how unsupervised learning +L+ can be improved by transferring knowl- +L+ edge from auxiliary heterogeneous data +L+ obtained from the social Web. Image +L+ clustering is useful for image sense dis- +L+ ambiguation in query-based image search, +L+ but its quality is often low due to image- +L+ data sparsity problem. We extend PLSA +L+ to help transfer the knowledge from social +L+ Web data, which have mixed feature repre- +L+ sentations. Experiments on image-object +L+ clustering and scene clustering tasks show +L+ that our approach in heterogeneous trans- +L+ fer learning based on the auxiliary data is +L+ indeed effective and promising. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Traditional machine learning relies on the avail- +L+ ability of a large amount of data to train a model, +L+ which is then applied to test data in the same +L+ feature space. However, labeled data are often +L+ scarce and expensive to obtain. Various machine +L+ learning strategies have been proposed to address +L+ this problem, including semi-supervised learning +L+ (Zhu, 2007), domain adaptation (Wu and Diet- +L+ terich, 2004; Blitzer et al., 2006; Blitzer et al., +L+ 2007; Arnold et al., 2007; Chan and Ng, 2007; +L+ Daume, 2007; Jiang and Zhai, 2007; Reichart +L+ and Rappoport, 2007; Andreevskaia and Bergler, +L+ 2008), multi-task learning (Caruana, 1997; Re- +L+ ichart et al., 2008; Arnold et al., 2008), self-taught +L+ learning (Raina et al., 2007), etc. A commonality +L+ among these methods is that they all require the +L+ training data and test data to be in the same fea- +L+ ture space. In addition, most of them are designed +L+ for supervised learning. However, in practice, we +L+ often face the problem where the labeled data are +L+ scarce in their own feature space, whereas there +L+ may be a large amount of labeled heterogeneous +L+ data in another feature space. In such situations, it +L+ would be desirable to transfer the knowledge from +L+ heterogeneous data to domains where we have rel- +L+ atively little training data available. +L+ To learn from heterogeneous data, researchers +L+ have previously proposed multi-view learning +L+ (Blum and Mitchell, 1998; Nigam and Ghani, +L+ 2000) in which each instance has multiple views in +L+ different feature spaces. Different from previous +L+ works, we focus on the problem of heterogeneous +L+ transfer learning, which is designed for situation +L+ when the training data are in one feature space +L+ (such as text), and the test data are in another (such +L+ as images), and there may be no correspondence +L+ between instances in these spaces. The type of +L+ heterogeneous data can be very different, as in the +L+ case of text and image. To consider how hetero- +L+ geneous transfer learning relates to other types of +L+ learning, Figure 1 presents an intuitive illustration +L+ of four learning strategies, including traditional +L+ machine learning, transfer learning across differ- +L+ ent distributions, multi-view learning and hetero- +L+ geneous transfer learning. As we can see, an +L+ important distinguishing feature of heterogeneous +L+ transfer learning, as compared to other types of +L+ learning, is that more constraints on the problem +L+ are relaxed, such that data instances do not need to +L+ correspond anymore. This allows, for example, a +L+ collection of Chinese text documents to be classi- +L+ fied using another collection of English text as the +L+ </SectLabel_bodyText> <SectLabel_page> 1 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1–9, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> training data (c.f. (Ling et al., 2008) and Section +L+ 2.1). +L+ In this paper, we will give an illustrative exam- +L+ ple of heterogeneous transfer learning to demon- +L+ strate how the task of image clustering can ben- +L+ efit from learning from the heterogeneous social +L+ Web data. A major motivation of our work is +L+ Web-based image search, where users submit tex- +L+ tual queries and browse through the returned result +L+ pages. One problem is that the user queries are of- +L+ ten ambiguous. An ambiguous keyword such as +L+ “Apple” might retrieve images of Apple comput- +L+ ers and mobile phones, or images of fruits. Im- +L+ age clustering is an effective method for improv- +L+ ing the accessibility of image search result. Loeff +L+ et al. (2006) addressed the image clustering prob- +L+ lem with a focus on image sense discrimination. +L+ In their approach, images associated with textual +L+ features are used for clustering, so that the text +L+ and images are clustered at the same time. Specif- +L+ ically, spectral clustering is applied to the distance +L+ matrix built from a multimodal feature set associ- +L+ ated with the images to get a better feature repre- +L+ sentation. This new representation contains both +L+ image and text information, with which the per- +L+ formance of image clustering is shown to be im- +L+ proved. A problem with this approach is that when +L+ images contained in the Web search results are +L+ very scarce and when the textual data associated +L+ with the images are very few, clustering on the im- +L+ ages and their associated text may not be very ef- +L+ fective. +L+ Different from these previous works, in this pa- +L+ per, we address the image clustering problem as +L+ a heterogeneous transfer learning problem. We +L+ aim to leverage heterogeneous auxiliary data, so- +L+ cial annotations, etc. to enhance image cluster- +L+ ing performance. We observe that the World Wide +L+ Web has many annotated images in Web sites such +L+ as Flickr (http : / /www. flickr . com), which +L+ can be used as auxiliary information source for +L+ our clustering task. In this work, our objective +L+ is to cluster a small collection of images that we +L+ are interested in, where these images are not suf- +L+ ficient for traditional clustering algorithms to per- +L+ form well due to data sparsity and the low level of +L+ image features. We investigate how to utilize the +L+ readily available socially annotated image data on +L+ the Web to improve image clustering. Although +L+ these auxiliary data may be irrelevant to the im- +L+ ages to be clustered and cannot be directly used +L+ to solve the data sparsity problem, we show that +L+ they can still be used to estimate a good latentfea- +L+ ture representation, which can be used to improve +L+ image clustering. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Related Works +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 2.1 Heterogeneous Transfer Learning +L+ Between Languages +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this section, we summarize our previous work +L+ on cross-language classification as an example of +L+ heterogeneous transfer learning. This example +L+ is related to our image clustering problem be- +L+ cause they both rely on data from different feature +L+ spaces. +L+ As the World Wide Web in China grows rapidly, +L+ it has become an increasingly important prob- +L+ lem to be able to accurately classify Chinese Web +L+ pages. However, because the labeled Chinese Web +L+ pages are still not sufficient, we often find it diffi- +L+ cult to achieve high accuracy by applying tradi- +L+ tional machine learning algorithms to the Chinese +L+ Web pages directly. Would it be possible to make +L+ the best use of the relatively abundant labeled En- +L+ glish Web pages for classifying the Chinese Web +L+ pages? +L+ To answer this question, in (Ling et al., 2008), +L+ we developed a novel approach for classifying the +L+ Web pages in Chinese using the training docu- +L+ ments in English. In this subsection, we give a +L+ brief summary of this work. The problem to be +L+ solved is: we are given a collection of labeled +L+ English documents and a large number of unla- +L+ beled Chinese documents. The English and Chi- +L+ nese texts are not aligned. Our objective is to clas- +L+ sify the Chinese documents into the same label +L+ space as the English data. +L+ Our key observation is that even though the data +L+ use different text features, they may still share +L+ many of the same semantic information. What we +L+ need to do is to uncover this latent semantic in- +L+ formation by finding out what is common among +L+ them. We did this in (Ling et al., 2008) by us- +L+ ing the information bottleneck theory (Tishby et +L+ al., 1999). In our work, we first translated the +L+ Chinese document into English automatically us- +L+ ing some available translation software, such as +L+ Google translate. Then, we encoded the training +L+ text as well as the translated target text together, +L+ in terms of the information theory. We allowed all +L+ the information to be put through a ‘bottleneck’ +L+ and be represented by a limited number of code- +L+ </SectLabel_bodyText> <SectLabel_page> 2 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering of +L+ </SectLabel_figureCaption> <SectLabel_bodyText> image apple and banana as the example. +L+ words (i.e. labels in the classification problem). +L+ Finally, information bottleneck was used to main- +L+ tain most of the common information between the +L+ two data sources, and discard the remaining irrel- +L+ evant information. In this way, we can approxi- +L+ mate the ideal situation where similar training and +L+ translated test pages shared in the common part are +L+ encoded into the same codewords, and are thus as- +L+ signed the correct labels. In (Ling et al., 2008), we +L+ experimentally showed that heterogeneous trans- +L+ fer learning can indeed improve the performance +L+ of cross-language text classification as compared +L+ to directly training learning models (e.g., Naive +L+ Bayes or SVM) and testing on the translated texts. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Other Works in Transfer Learning +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the past, several other works made use of trans- +L+ fer learning for cross-feature-space learning. Wu +L+ and Oard (2008) proposed to handle the cross- +L+ language learning problem by translating the data +L+ into a same language and applying kNN on the +L+ latent topic space for classification. Most learning +L+ algorithms for dealing with cross-language hetero- +L+ geneous data require a translator to convert the +L+ data to the same feature space. For those data that +L+ are in different feature spaces where no transla- +L+ tor is available, Davis and Domingos (2008) pro- +L+ posed a Markov-logic-based transfer learning al- +L+ gorithm, which is called deep transfer, for trans- +L+ ferring knowledge between biological domains +L+ and Web domains. Dai et al. (2008a) proposed +L+ a novel learning paradigm, known as translated +L+ learning, to deal with the problem of learning het- +L+ erogeneous data that belong to quite different fea- +L+ ture spaces by using a risk minimization frame- +L+ work. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.3 Relation to PLSA +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our work makes use of PLSA. Probabilistic la- +L+ tent semantic analysis (PLSA) is a widely used +L+ probabilistic model (Hofmann, 1999), and could +L+ be considered as a probabilistic implementation of +L+ latent semantic analysis (LSA) (Deerwester et al., +L+ 1990). An extension to PLSA was proposed in +L+ (Cohn and Hofmann, 2000), which incorporated +L+ the hyperlink connectivity in the PLSA model by +L+ using a joint probabilistic model for connectivity +L+ and content. Moreover, PLSA has shown a lot +L+ of applications ranging from text clustering (Hof- +L+ mann, 2001) to image analysis (Sivic et al., 2005). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.4 Relation to Clustering +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Compared to many previous works on image clus- +L+ tering, we note that traditional image cluster- +L+ ing is generally based on techniques such as K- +L+ means (MacQueen, 1967) and hierarchical clus- +L+ tering (Kaufman and Rousseeuw, 1990). How- +L+ ever, when the data are sparse, traditional clus- +L+ tering algorithms may have difficulties in obtain- +L+ ing high-quality image clusters. Recently, several +L+ researchers have investigated how to leverage the +L+ auxiliary information to improve target clustering +L+ </SectLabel_bodyText> <SectLabel_page> 3 +L+ </SectLabel_page> <SectLabel_equation> P(zIv)	P(fIz) +L+ </SectLabel_equation> <SectLabel_bodyText> performance, such as supervised clustering (Fin- +L+ ley and Joachims, 2005), semi-supervised cluster- +L+ ing (Basu et al., 2004), self-taught clustering (Dai +L+ et al., 2008b), etc. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Image Clustering with Annotated +L+ Auxiliary Data +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we present our annotation-based +L+ probabilistic latent semantic analysis algorithm +L+ (aPLSA), which extends the traditional PLSA +L+ model by incorporating annotated auxiliary im- +L+ age data. Intuitively, our algorithm aPLSA per- +L+ forms PLSA analysis on the target images, which +L+ are converted to an image instance-to-feature co- +L+ occurrence matrix. At the same time, PLSA is +L+ also applied to the annotated image data from so- +L+ cial Web, which is converted into a text-to-image- +L+ feature co-occurrence matrix. In order to unify +L+ those two separate PLSA models, these two steps +L+ are done simultaneously with common latent vari- +L+ ables used as a bridge linking them. Through +L+ these common latent variables, which are now +L+ constrained by both target image data and auxil- +L+ iary annotation data, a better clustering result is +L+ expected for the target data. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Probabilistic Latent Semantic Analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Let F = { fi}!Fi be an image feature space, and +L+ V = {vi}iv11 be the image data set. Each image +L+ vi E V is represented by a bag-of-features {f I f E +L+ vi A f E F}. +L+ Based on the image data set V, we can esti- +L+ mate an image instance-to-feature co-occurrence +L+ matrix AIVI x I FI E RIVIx IFI, where each element +L+ AiT (1 G i G IV I and 1 G j G IF I) in the matrix +L+ A is the frequency of the feature fT appearing in +L+ the instance vi. +L+ Let W = {wi } I WI i��be a text feature space. The +L+ annotated image data allow us to obtain the co- +L+ occurrence information between images v and text +L+ features w E W. An example of annotated im- +L+ age data is the Flickr (http : / /www. f l ickr . +L+ com), which is a social Web site containing a large +L+ number of annotated images. +L+ By extracting image features from the annotated +L+ images v, we can estimate a text-to-image fea- +L+ ture co-occurrence matrix B I WIxIFI E RIW IxIFI, +L+ where each element BiT (1 G i G I W I and +L+ 1 G j G IFI) in the matrix B is the frequency +L+ of the text feature wi and the image feature fT oc- +L+ curring together in the annotated image data set. +L+ Figure 2: Graphical model representation of PLSA +L+ model. +L+ Let Z = {ziffl be the latent variable set in our +L+ aPLSA model. In clustering, each latent variable +L+ zi E Z corresponds to a certain cluster. +L+ Our objective is to estimate a clustering func- +L+ tion g : V H Z with the help of the two co- +L+ occurrence matrices A and B as defined above. +L+ To formally introduce the aPLSA model, we +L+ start from the probabilistic latent semantic anal- +L+ ysis (PLSA) (Hofmann, 1999) model. PLSA is +L+ a probabilistic implementation of latent seman- +L+ tic analysis (LSA) (Deerwester et al., 1990). In +L+ our image clustering task, PLSA decomposes the +L+ instance-feature co-occurrence matrix A under the +L+ assumption of conditional independence of image +L+ instances V and image features F, given the latent +L+ variables Z. +L+ </SectLabel_bodyText> <SectLabel_equation> P(f Iv) = 1: P(f I z)P(zI v).	(1) +L+ ZEZ +L+ </SectLabel_equation> <SectLabel_bodyText> The graphical model representation of PLSA is +L+ shown in Figure 2. +L+ Based on the PLSA model, the log-likelihood can +L+ be defined as: +L+ </SectLabel_bodyText> <SectLabel_equation> 1: L = +L+ i 1:AiT +L+ T ET, AiT, log P(fT I vi)	(2) +L+ </SectLabel_equation> <SectLabel_bodyText> where AIVI x IFI E RIVI xIFI is the image instance- +L+ feature co-occurrence matrix. The term : A�j +L+ j, Azj/ +L+ in Equation (2) is a normalization term ensuring +L+ each image is giving the same weight in the log- +L+ likelihood. +L+ Using EM algorithm (Dempster et al., 1977), +L+ which locally maximizes the log-likelihood of +L+ the PLSA model (Equation (2)), the probabilities +L+ P(f I z) and P(zI v) can be estimated. Then, the +L+ clustering function is derived as +L+ </SectLabel_bodyText> <SectLabel_equation> g(v) = argmax P(zIv).	(3) +L+ ZEZ +L+ </SectLabel_equation> <SectLabel_bodyText> Due to space limitation, we omit the details for the +L+ PLSA model, which can be found in (Hofmann, +L+ 1999). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 aPLSA: Annotation-based PLSA +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this section, we consider how to incorporate +L+ a large number of socially annotated images in a +L+ </SectLabel_bodyText> <SectLabel_equation> V	Z	F +L+ </SectLabel_equation> <SectLabel_page> 4 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 3: Graphical model representation of +L+ aPLSA model. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> unified PLSA model for the purpose of utilizing +L+ the correlation between text features and image +L+ features. In the auxiliary data, each image has cer- +L+ tain textual tags that are attached by users. The +L+ correlation between text features and image fea- +L+ tures can be formulated as follows. +L+ </SectLabel_bodyText> <SectLabel_equation> P(f Iw) = X P(f I z)P(zI w). (4) +L+ ZEZ +L+ </SectLabel_equation> <SectLabel_bodyText> It is clear that Equations (1) and (4) share a same +L+ term P(f I z). So we design a new PLSA model by +L+ joining the probabilistic model in Equation (1) and +L+ the probabilistic model in Equation (4) into a uni- +L+ fied model, as shown in Figure 3. In Figure 3, the +L+ latent variables Z depend not only on the corre- +L+ lation between image instances V and image fea- +L+ tures F, but also the correlation between text fea- +L+ tures W and image features F. Therefore, the aux- +L+ iliary socially-annotated image data can be used +L+ to help the target image clustering performance by +L+ estimating good set of latent variables Z. +L+ Based on the graphical model representation in +L+ Figure 3, we derive the log-likelihood objective +L+ function, in a similar way as in (Cohn and Hof- +L+ mann, 2000), as follows +L+ text-to-image occurrence matrix B. In this case, +L+ the aPLSA model degenerates to the traditional +L+ PLSA model. Therefore, aPLSA is an extension +L+ to the PLSA model. +L+ Now, the objective is to maximize the log- +L+ likelihood L of the aPLSA model in Equation (5). +L+ Then we apply the EM algorithm (Dempster et +L+ al., 1977) to estimate the conditional probabilities +L+ P(f I z), P(zI w) and P(zI v) with respect to each +L+ dependence in Figure 3 as follows. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	E-Step: calculate the posterior probability of +L+ each latent variable z given the observation +L+ of image features f, image instances v and +L+ text features w based on the old estimate of +L+ </SectLabel_listItem> <SectLabel_equation> P(f I z), P(zI w) and P(zI v): +L+ PIvi,fj) =	P(fjIzk)P(zkIvi) +L+ (zk +L+ P(zkI wl, fj) =	P(fjIzk)P(zkIwl) +L+ Pk, P(fjIzk,)P(zk,Iwl) +L+ (7) +L+ </SectLabel_equation> <SectLabel_listItem> •	M-Step: re-estimates conditional probabili- +L+ ties P(zkIvi) and P(zkI wl): +L+ </SectLabel_listItem> <SectLabel_equation> PAAP(zkIvi,fj) (8) +L+ PBlBlj, P(zk I wl , fj) (9) +L+ </SectLabel_equation> <SectLabel_bodyText> and conditional probability P(fj I zk ), which +L+ is a mixture portion of posterior probability +L+ of latent variables +L+ </SectLabel_bodyText> <SectLabel_equation> W +L+ V +L+ Z	F +L+ P(fIz) +L+ P +L+ k, P(fj Izk,)P(zk, I vi) +L+ (6) +L+ XP(zkIvi) = +L+ j +L+ XP(zkIwl) = +L+ j +L+ P(fjIzk) a AX +L+ i +L+ L=X +L+ j +L+ P +L+ j, Blj, +L+ X +L+ +(1 — A) +L+ l +L+ +(1—A)X +L+ l +L+ Aij  P( zkI vi, fj) +L+ Pj,Aij, +L+ Blj P(zkIwl,fj) +L+ (10) +L+ � +L+ XAij +L+ A i Pj, Aij, log P(fj I vi) +L+ � +L+ Blj	log P(fj I wl ) +L+ Pj, Blj, +L+ (5) +L+ </SectLabel_equation> <SectLabel_bodyText> where AIVIxIFI E RIVIxIFI is the image instance- +L+ feature co-occurrence matrix, and BIW I xIFI E +L+ RIWIxIFI is the text-to-image feature-level co- +L+ occurrence matrix. Similar to Equation (2), +L+ PAij and PB`j in Equation (5) are the nor- +L+ ij, +L+ malization terms to prevent imbalanced cases. +L+ Furthermore, A acts as a trade-off parameter be- +L+ tween the co-occurrence matrices A and B. In +L+ the extreme case when A = 1, the log-likelihood +L+ objective function ignores all the biases from the +L+ Finally, the clustering function for a certain im- +L+ age v is +L+ </SectLabel_bodyText> <SectLabel_equation> g(v) = argmax P(zIv).	(11) +L+ ZEZ +L+ </SectLabel_equation> <SectLabel_bodyText> From the above equations, we can derive +L+ our annotation-based probabilistic latent semantic +L+ analysis (aPLSA) algorithm. As shown in Algo- +L+ rithm 1, aPLSA iteratively performs the E-Step +L+ and the M-Step in order to seek local optimal +L+ points based on the objective function L in Equa- +L+ tion (5). +L+ </SectLabel_bodyText> <SectLabel_page> 5 +L+ </SectLabel_page> <SectLabel_construct> Algorithm 1 Annotation-based PLSA Algorithm +L+ (aPLSA) +L+ Input: The V-F co-occurrence matrix A and W- +L+ F co-occurrence matrix B. +L+ Output: A clustering (partition) function g : V H +L+ i, which maps an image instance v E V to a latent +L+ variable z E i. +L+ </SectLabel_construct> <SectLabel_listItem> 1: Initial i so that IiI equals the number clus- +L+ ters desired. +L+ 2: Initialize P(zIv), P(zIw), P(f Iz) randomly. +L+ 3: while the change of L in Eq. (5) between two +L+ sequential iterations is greater than a prede- +L+ fined threshold do +L+ 4: E-Step: Update P(zIv, f) and P(zIw, f) +L+ based on Eq. (6) and (7) respectively. +L+ 5: M-Step: Update P(zIv), P(zIw) and +L+ P(f Iz) based on Eq. (8), (9) and (10) re- +L+ spectively. +L+ 6: end while +L+ 7: for all v in V do +L+ 8:	g(v) +— argmaxP(zIv). +L+ 9: end for +L+ 10: Return g. +L+ </SectLabel_listItem> <SectLabel_sectionHeader> 4 Experiments +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we empirically evaluate the aPLSA +L+ algorithm together with some state-of-art base- +L+ line methods on two widely used image corpora, +L+ to demonstrate the effectiveness of our algorithm +L+ aPLSA. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Data Sets +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In order to evaluate the effectiveness of our algo- +L+ rithm aPLSA, we conducted experiments on sev- +L+ eral data sets generated from two image corpora, +L+ Caltech-256 (Griffin et al., 2007) and the fifteen- +L+ scene (Lazebnik et al., 2006). The Caltech-256 +L+ data set has 256 image objective categories, rang- +L+ ing from animals to buildings, from plants to au- +L+ tomobiles, etc. The fifteen-scene data set con- +L+ tains 15 scenes such as store and forest. +L+ From these two corpora, we randomly generated +L+ eleven image clustering tasks, including seven 2- +L+ way clustering tasks, two 4-way clustering task, +L+ one 5-way clustering task and one 8-way cluster- +L+ ing task. The detailed descriptions for these clus- +L+ tering tasks are given in Table 1. In these tasks, +L+ b i 7 and o c t 1 were generated from fifteen-scene +L+ data set, and the rest were from Caltech-256 data +L+ set. +L+ </SectLabel_bodyText> <SectLabel_table> DATA SET INVOLVED CLASSES	DATA SIZE +L+ bi1	skateboard, airplanes	102,800 +L+ bi2	billiards, mars	278, 155 +L+ bi3	cd, greyhound	102,	94 +L+ bi4	electric-guitar, snake	122,112 +L+ bi5	calculator, dolphin	100,106 +L+ bi6	mushroom, teddy-bear	202,	99 +L+ bi7	MIThighway, livingroom	260,289 +L+ quad1	calculator,	diamond-ring,	dolphin,	100, 118, 106,116 +L+ 	microscope +L+ quad2	bonsai, comet, frog, saddle	122, 120, 115, 110 +L+ quint1	frog, kayak, bear, jesus-christ,watch115,102,101,87,	201 +L+ oct1	MIThighway,	MITmountain,	260, 374, 210, 360, +L+ 	kitchen, MITcoast, PARoffice, MIT- tallbuilding, livingroom, bedroom	215, 356, 289, 216 +L+ tune1	coin, horse	123,270 +L+ tune2	socks, spider	111,106 +L+ tune3	galaxy, snowmobile	80,112 +L+ tune4	dice, fern	98,110 +L+ tune5	backpack, lightning, mandolin, swan	151, 136,	93,114 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: The descriptions of all the image clus- +L+ tering tasks used in our experiment. Among +L+ these data sets, b i 7 and o c t 1 were generated +L+ from fifteen-scene data set, and the rest were from +L+ Caltech-256 data set. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> To empirically investigate the parameter A and +L+ the convergence of our algorithm aPLSA, we gen- +L+ erated five more date sets as the development sets. +L+ The detailed description of these five development +L+ sets, namely tune1 to tune5 is listed in Table 1 +L+ as well. +L+ The auxiliary data were crawled from the Flickr +L+ (http://www.flickr.com/) web site dur- +L+ ing August 2007. Flickr is an internet community +L+ where people share photos online and express their +L+ opinions as social tags (annotations) attached to +L+ each image. From Flicker, we collected 19,959 +L+ images and 91,719 related annotations, among +L+ which 2,600 words are distinct. Based on the +L+ method described in Section 3, we estimated the +L+ co-occurrence matrix B between text features and +L+ image features. This co-occurrence matrix B was +L+ used by all the clustering tasks in our experiments. +L+ For data preprocessing, we adopted the bag-of- +L+ features representation of images (Li and Perona, +L+ 2005) in our experiments. Interesting points were +L+ found in the images and described via the SIFT +L+ descriptors (Lowe, 2004). Then, the interesting +L+ points were clustered to generate a codebook to +L+ form an image feature space. The size of code- +L+ book was set to 2, 000 in our experiments. Based +L+ on the codebook, which serves as the image fea- +L+ ture space, each image can be represented as a cor- +L+ responding feature vector to be used in the next +L+ step. +L+ To set our evaluation criterion, we used the +L+ </SectLabel_bodyText> <SectLabel_page> 6 +L+ </SectLabel_page> <SectLabel_table> Data Set	KMeancombined		PLSA		STC	aPLSA +L+ 	separate		separate	combined +L+ bi1	0.645±0.064	0.548±0.031	0.544±0.074	0.537±0.033	0.586±0.139	0.482±0.062 +L+ bi2	0.687±0.003	0.662±0.014	0.464±0.074	0.692±0.001	0.577±0.016	0.455±0.096 +L+ bi3	1.294±0.060	1.300±0.015	1.085±0.073	1.126±0.036	1.103±0.108	1.029±0.074 +L+ bi4	1.227±0.080	1.164±0.053	0.976±0.051	1.038±0.068	1.024±0.089	0.919±0.065 +L+ bi5	1.450±0.058	1.417±0.045	1.426±0.025	1.405±0.040	1.411±0.043	1.377±0.040 +L+ bi6	1.969±0.078	1.852±0.051	1.514±0.039	1.709±0.028	1.589±0.121	1.503±0.030 +L+ bi7	0.686±0.006	0.683±0.004	0.643±0.058	0.632±0.037	0.651±0.012	0.624±0.066 +L+ quad1	0.591±0.094	0.675±0.017	0.488±0.071	0.662±0.013	0.580±0.115	0.432±0.085 +L+ quad2	0.648±0.036	0.646±0.045	0.614±0.062	0.626±0.026	0.591±0.087	0.515±0.098 +L+ quint1	0.557±0.021	0.508±0.104	0.547±0.060	0.539±0.051	0.538±0.100	0.502±0.067 +L+ oct1	0.659±0.031	0.680±0.012	0.340±0.147	0.691±0.002	0.411±0.089	0.306±0.101 +L+ average	0.947±0.029	0.922±0.017	0.786±0.009	0.878±0.006	0.824±0.036	0.741±0.018 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Experimental result in term of entropy for all data sets and evaluation methods. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> entropy to measure the quality of our clustering +L+ results. In information theory, entropy (Shan- +L+ non, 1948) is a measure of the uncertainty as- +L+ sociated with a random variable. In our prob- +L+ lem, entropy serves as a measure of randomness +L+ of clustering result. The entropy of g on a sin- +L+ gle latent variable z is defined to be H(g, z) +L+ - PcEC P(c�z)lo�2 P(cIz), where C is the class +L+ label set of V and P(clz) = ��v�s(v)=zAt(v)=c}� +L+ 11v1s(v)=z}1 , +L+ in which t(v) is the true class label of image v. +L+ Lower entropy H(g, i) indicates less randomness +L+ and thus better clustering result. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Empirical Analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We now empirically analyze the effectiveness of +L+ our aPLSA algorithm. Because, to our best of +L+ knowledge, few existing methods addressed the +L+ problem of image clustering with the help of so- +L+ cial annotation image data, we can only compare +L+ our aPLSA with several state-of-the-art cluster- +L+ ing algorithms that are not directly designed for +L+ our problem. The first baseline is the well-known +L+ KMeans algorithm (MacQueen, 1967). Since our +L+ algorithm is designed based on PLSA (Hofmann, +L+ 1999), we also included PLSA for clustering as a +L+ baseline method in our experiments. +L+ For each of the above two baselines, we have +L+ two strategies: (1) separated: the baseline +L+ method was applied on the target image data only; +L+ (2) combined: the baseline method was applied +L+ to cluster the combined data consisting of both +L+ target image data and the annotated image data. +L+ Clustering results on target image data were used +L+ for evaluation. Note that, in the combined data, all +L+ the annotations were thrown away since baseline +L+ methods evaluated in this paper do not leverage +L+ annotation information. +L+ In addition, we compared our algorithm aPLSA +L+ to a state-of-the-art transfer clustering strategy, +L+ known as self-taught clustering (STC) (Dai et al., +L+ 2008b). STC makes use of auxiliary data to esti- +L+ mate a better feature representation to benefit the +L+ target clustering. In these experiments, the anno- +L+ tated image data were used as auxiliary data in +L+ STC, which does not use the annotation text. +L+ In our experiments, the performance is in the +L+ form of the average entropy and variance of five +L+ repeats by randomly selecting 50 images from +L+ each of the categories. We selected only 50 im- +L+ ages per category, since this paper is focused on +L+ clustering sparse data. Table 2 shows the perfor- +L+ mance with respect to all comparison methods on +L+ each of the image clustering tasks measured by +L+ the entropy criterion. From the tables, we can see +L+ that our algorithm aPLSA outperforms the base- +L+ line methods in all the data sets. We believe that is +L+ because aPLSA can effectively utilize the knowl- +L+ edge from the socially annotated image data. On +L+ average, aPLSA gives rise to 21.8% of entropy re- +L+ duction and as compared to KMeans, 5.7% of en- +L+ tropy reduction as compared to PLSA, and 10.1% +L+ of entropy reduction as compared to S TC. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2.1 Varying Data Size +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We now show how the data size affects aPLSA, +L+ with two baseline methods KMeans and PLSA as +L+ reference. The experiments were conducted on +L+ different amounts of target image data, varying +L+ from 10 to 80. The corresponding experimental +L+ results in average entropy over all the 11 clustering +L+ tasks are shown in Figure 4(a). From this figure, +L+ we observe that aPLSA always yields a significant +L+ reduction in entropy as compared with two base- +L+ line methods KMeans and PLSA, regardless of the +L+ size of target image data that we used. +L+ </SectLabel_bodyText> <SectLabel_page> 7 +L+ </SectLabel_page> <SectLabel_figure> 1 +L+ 0.95 +L+ 0.9 +L+ 0.85 +L+ 0.8 +L+ 0.75 +L+ 0.7 +L+ 10	20	30	40	50	60	70	80 +L+ KMeans +L+ PLSA +L+ aPLSA +L+ 0.75 +L+ 0.65 +L+ 0.6 +L+ 0.55 +L+ 0.45 +L+ 0.7 +L+ 0.5 +L+ 0.4 +L+ 0	0.2	0.4	0.6	0.8	1 +L+ average over 5 development sets +L+ 0.75 +L+ 0.65 +L+ 0.55 +L+ 0.7 +L+ 0.6 +L+ 0.5 +L+ 0	50	100	150	200	250 300 +L+ average over 5 development sets +L+ Data size per category		Number of Iteration +L+ (a)	(b)	(c) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: (a) The entropy curve as a function of different amounts of data per category. (b) The entropy +L+ curve as a function of different number of iterations. (c) The entropy curve as a function of different +L+ trade-off parameter A. +L+ </SectLabel_figureCaption> <SectLabel_subsubsectionHeader> 4.2.2 Parameter Sensitivity +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In aPLSA, there is a trade-off parameter A that af- +L+ fects how the algorithm relies on auxiliary data. +L+ When A = 0, the aPLSA relies only on annotated +L+ image data B. When A = 1, aPLSA relies only +L+ on target image data A, in which case aPLSA de- +L+ generates to PLSA. Smaller A indicates heavier re- +L+ liance on the annotated image data. We have done +L+ some experiments on the development sets to in- +L+ vestigate how different A affect the performance +L+ of aPLSA. We set the number of images per cate- +L+ gory to 50, and tested the performance of aPLSA. +L+ The result in average entropy over all development +L+ sets is shown in Figure 4(b). In the experiments +L+ described in this paper, we set A to 0.2, which is +L+ the best point in Figure 4(b). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2.3 Convergence +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In our experiments, we tested the convergence +L+ property of our algorithm aPLSA as well. Fig- +L+ ure 4(c) shows the average entropy curve given +L+ by aPLSA over all development sets. From this +L+ figure, we see that the entropy decreases very fast +L+ during the first 100 iterations and becomes stable +L+ after 150 iterations. We believe that 200 iterations +L+ is sufficient for aPLSA to converge. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Conclusions +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we proposed a new learning scenario +L+ called heterogeneous transfer learning and illus- +L+ trated its application to image clustering. Image +L+ clustering, a vital component in organizing search +L+ results for query-based image search, was shown +L+ to be improved by transferring knowledge from +L+ unrelated images with annotations in a social Web. +L+ This is done by first learning the high-quality la- +L+ tent variables in the auxiliary data, and then trans- +L+ ferring this knowledge to help improve the cluster- +L+ ing of the target image data. We conducted experi- +L+ ments on two image data sets, using the Flickr data +L+ as the annotated auxiliary image data, and showed +L+ that our aPLSA algorithm can greatly outperform +L+ several state-of-the-art clustering algorithms. +L+ In natural language processing, there are many +L+ future opportunities to apply heterogeneous trans- +L+ fer learning. In (Ling et al., 2008) we have shown +L+ how to classify the Chinese text using English text +L+ as the training data. We may also consider cluster- +L+ ing, topic modeling, question answering, etc., to +L+ be done using data in different feature spaces. We +L+ can consider data in different modalities, such as +L+ video, image and audio, as the training data. Fi- +L+ nally, we will explore the theoretical foundations +L+ and limitations of heterogeneous transfer learning +L+ as well. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgement Qiang Yang thanks Hong +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Kong CERG grant 621307 for supporting the re- +L+ search. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Alina Andreevskaia and Sabine Bergler. 2008. When spe- +L+ cialists and generalists work together: Overcoming do- +L+ main dependence in sentiment tagging. In ACL-08: HLT, +L+ pages 290–298, Columbus, Ohio, June. +L+ Andrew Arnold, Ramesh Nallapati, and William W. Cohen. +L+ 2007. A comparative study of methods for transductive +L+ transfer learning. In ICDM 2007 Workshop on Mining +L+ and Management of Biological Data, pages 77-82. +L+ Andrew Arnold, Ramesh Nallapati, and William W. Cohen. +L+ 2008. Exploiting feature hierarchy for transfer learning in +L+ named entity recognition. In ACL-08: HLT. +L+ Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney. +L+ 2004. A probabilistic framework for semi-supervised +L+ clustering. In ACM SIGKDD 2004, pages 59–68. +L+ John Blitzer, Ryan Mcdonald, and Fernando Pereira. 2006. +L+ Domain adaptation with structural correspondence learn- +L+ ing. In EMNLP 2006, pages 120–128, Sydney, Australia. +L+ </SectLabel_reference> <SectLabel_page> 8 +L+ </SectLabel_page> <SectLabel_reference> John Blitzer, Mark Dredze, and Fernando Pereira. 2007. +L+ Biographies, bollywood, boom-boxes and blenders: Do- +L+ main adaptation for sentiment classification. In ACL 2007, +L+ pages 440–447, Prague, Czech Republic. +L+ Avrim Blum and Tom Mitchell. 1998. Combining labeled +L+ and unlabeled data with co-training. In COLT 1998, pages +L+ 92–100, New York, NY, USA. ACM. +L+ Rich Caruana. 1997. Multitask learning. Machine Learning, +L+ 28(1):41–75. +L+ Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation +L+ with active learning for word sense disambiguation. In +L+ ACL 2007, Prague, Czech Republic. +L+ David A. Cohn and Thomas Hofmann. 2000. The missing +L+ link - a probabilistic model of document content and hy- +L+ pertext connectivity. In NIPS 2000, pages 430–436. +L+ Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang, +L+ and Yong Yu. 2008a. Translated learning: Transfer learn- +L+ ing across different feature spaces. In NIPS 2008, pages +L+ 353–360. +L+ Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu. +L+ 2008b. Self-taught clustering. In ICML 2008, pages 200– +L+ 207. Omnipress. +L+ Hal Daume,III. 2007. Frustratingly easy domain adaptation. +L+ In ACL 2007, pages 256–263, Prague, Czech Republic. +L+ Jesse Davis and Pedro Domingos. 2008. Deep transfer via +L+ second-order markov logic. In AAAI 2008 Workshop on +L+ Transfer Learning, Chicago, USA. +L+ Scott Deerwester, Susan T. Dumais, George W. Furnas, +L+ Thomas K. L, and Richard Harshman. 1990. Indexing by +L+ latent semantic analysis. Journal of the American Society +L+ for Information Science, pages 391–407. +L+ A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max- +L+ imum likelihood from incomplete data via the em algo- +L+ rithm. J. of the Royal Statistical Society, 39:1–38. +L+ Thomas Finley and Thorsten Joachims. 2005. Supervised +L+ clustering with support vector machines. In ICML 2005, +L+ pages 217–224, New York, NY, USA. ACM. +L+ G. Griffin, A. Holub, and P. Perona. 2007. Caltech-256 ob- +L+ ject category dataset. Technical Report 7694, California +L+ Institute of Technology. +L+ Thomas Hofmann. 1999 Probabilistic latent semantic anal- +L+ ysis. In Proc. of Uncertainty in Artificial Intelligence, +L+ UAI99. Pages 289–296 +L+ Thomas Hofmann. 2001. Unsupervised learning by proba- +L+ bilistic latent semantic analysis. Machine Learning. vol- +L+ ume 42, number 1-2, pages 177–196. Kluwer Academic +L+ Publishers. +L+ Jing Jiang and Chengxiang Zhai. 2007. Instance weighting +L+ for domain adaptation in NLP. In ACL 2007, pages 264– +L+ 271, Prague, Czech Republic, June. +L+ Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding +L+ groups in data: an introduction to cluster analysis. John +L+ Wiley and Sons, New York. +L+ Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. +L+ Beyond bags of features: Spatial pyramid matching for +L+ recognizing natural scene categories. In CVPR 2006, +L+ pages 2169–2178, Washington, DC, USA. +L+ Fei-Fei Li and Pietro Perona. 2005. A bayesian hierarchi- +L+ cal model for learning natural scene categories. In CVPR +L+ 2005, pages 524–531, Washington, DC, USA. +L+ Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang, Qiang +L+ Yang, and Yong Yu. 2008. Can chinese web pages be +L+ classified with english data source? In WWW 2008, pages +L+ 969–978, New York, NY, USA. ACM. +L+ Nicolas Loeff, Cecilia Ovesdotter Alm, and David A. +L+ Forsyth. 2006. Discriminating image senses by clustering +L+ with multimodal features. In COLING/ACL 2006 Main +L+ conference poster sessions, pages 547–554. +L+ David G. Lowe. 2004. Distinctive image features from scale- +L+ invariant keypoints. International Journal of Computer +L+ Vision (IJCV) 2004, volume 60, number 2, pages 91–110. +L+ J. B. MacQueen. 1967. Some methods for classification and +L+ analysis of multivariate observations. In Proceedings of +L+ Fifth Berkeley Symposium on Mathematical Statistics and +L+ Probability, pages 1:281–297, Berkeley, CA, USA. +L+ Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec- +L+ tiveness and applicability of co-training. In Proceedings +L+ of the Ninth International Conference on Information and +L+ Knowledge Management, pages 86–93, New York, USA. +L+ Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, +L+ and Andrew Y. Ng. 2007. Self-taught learning: transfer +L+ learning from unlabeled data. In ICML 2007, pages 759– +L+ 766, New York, NY, USA. ACM. +L+ Roi Reichart and Ari Rappoport. 2007. Self-training for +L+ enhancement and domain adaptation of statistical parsers +L+ trained on small datasets. In ACL 2007. +L+ Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap- +L+ poport. 2008. Multi-task active learning for linguistic +L+ annotations. In ACL-08: HLT, pages 861–869. +L+ C. E. Shannon. 1948. A mathematical theory of communi- +L+ cation. Bell system technicaljournal, 27. +L+ J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. +L+ Freeman. 2005. Discovering object categories in image +L+ collections. In ICCV 2005. +L+ Naftali Tishby, Fernando C. Pereira, and William Bialek. The +L+ information bottleneck method. 1999. In Proc. of the 37- +L+ th Annual Allerton Conference on Communication, Con- +L+ trol and Computing, pages 368–377. +L+ Pengcheng Wu and Thomas G. Dietterich. 2004. Improving +L+ svm accuracy by training on auxiliary data sources. In +L+ ICML 2004, pages 110–117, New York, NY, USA. +L+ Yejun Wu and Douglas W. Oard. 2008. Bilingual topic as- +L+ pect classification with a few training examples. In ACM +L+ SIGIR 2008, pages 203–210, New York, NY, USA. +L+ Xiaojin Zhu. 2007. Semi-supervised learning literature sur- +L+ vey. Technical Report 1530, Computer Sciences, Univer- +L+ sity of Wisconsin-Madison. +L+ </SectLabel_reference> <SectLabel_page> 9 +L+ </SectLabel_page>
<SectLabel_title> Investigations on Word Senses and Word Usages +L+ </SectLabel_title> <SectLabel_author> Katrin Erk	Diana McCarthy	Nicholas Gaylord +L+ </SectLabel_author> <SectLabel_affiliation> University of Texas at Austin	University of Sussex	University of Texas at Austin +L+ </SectLabel_affiliation> <SectLabel_email> katrin.erk@mail.utexas.edu	dianam@sussex.ac.uk	nlgaylord@mail.utexas.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The vast majority of work on word senses +L+ has relied on predefined sense invento- +L+ ries and an annotation schema where each +L+ word instance is tagged with the best fit- +L+ ting sense. This paper examines the case +L+ for a graded notion of word meaning in +L+ two experiments, one which uses WordNet +L+ senses in a graded fashion, contrasted with +L+ the “winner takes all” annotation, and one +L+ which asks annotators to judge the similar- +L+ ity of two usages. We find that the graded +L+ responses correlate with annotations from +L+ previous datasets, but sense assignments +L+ are used in a way that weakens the case for +L+ clear cut sense boundaries. The responses +L+ from both experiments correlate with the +L+ overlap of paraphrases from the English +L+ lexical substitution task which bodes well +L+ for the use of substitutes as a proxy for +L+ word sense. This paper also provides two +L+ novel datasets which can be used for eval- +L+ uating computational systems. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The vast majority of work on word sense tag- +L+ ging has assumed that predefined word senses +L+ from a dictionary are an adequate proxy for the +L+ task, although of course there are issues with +L+ this enterprise both in terms of cognitive valid- +L+ ity (Hanks, 2000; Kilgarriff, 1997; Kilgarriff, +L+ 2006) and adequacy for computational linguis- +L+ tics applications (Kilgarriff, 2006). Furthermore, +L+ given a predefined list of senses, annotation efforts +L+ and computational approaches to word sense dis- +L+ ambiguation (WSD) have usually assumed that one +L+ best fitting sense should be selected for each us- +L+ age. While there is usually some allowance made +L+ for multiple senses, this is typically not adopted by +L+ annotators or computational systems. +L+ Research on the psychology of concepts (Mur- +L+ phy, 2002; Hampton, 2007) shows that categories +L+ in the human mind are not simply sets with clear- +L+ cut boundaries: Some items are perceived as +L+ more typical than others (Rosch, 1975; Rosch and +L+ Mervis, 1975), and there are borderline cases on +L+ which people disagree more often, and on whose +L+ categorization they are more likely to change their +L+ minds (Hampton, 1979; McCloskey and Glucks- +L+ berg, 1978). Word meanings are certainly related +L+ to mental concepts (Murphy, 2002). This raises +L+ the question of whether there is any such thing as +L+ the one appropriate sense for a given occurrence. +L+ In this paper we will explore using graded re- +L+ sponses for sense tagging within a novel annota- +L+ tion paradigm. Modeling the annotation frame- +L+ work after psycholinguistic experiments, we do +L+ not train annotators to conform to sense distinc- +L+ tions; rather we assess individual differences by +L+ asking annotators to produce graded ratings in- +L+ stead of making a binary choice. We perform two +L+ annotation studies. In the first one, referred to +L+ as WSsim (Word Sense Similarity), annotators +L+ give graded ratings on the applicability of Word- +L+ Net senses. In the second one, Usim (Usage Sim- +L+ ilarity), annotators rate the similarity of pairs of +L+ occurrences (usages) of a common target word. +L+ Both studies explore whether users make use of +L+ a graded scale or persist in making binary deci- +L+ sions even when there is the option for a graded +L+ response. The first study additionally tests to what +L+ extent the judgments on WordNet senses fall into +L+ clear-cut clusters, while the second study allows +L+ us to explore meaning similarity independently of +L+ any lexicon resource. +L+ </SectLabel_bodyText> <SectLabel_page> 10 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10–18, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_sectionHeader> 2 Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Manual word sense assignment is difficult for +L+ human annotators (Krishnamurthy and Nicholls, +L+ 2000). Reported inter-annotator agreement (ITA) +L+ for fine-grained word sense assignment tasks has +L+ ranged between 69% (Kilgarriff and Rosenzweig, +L+ 2000) for a lexical sample using the HECTOR dic- +L+ tionary and 78.6.% using WordNet (Landes et al., +L+ 1998) in all-words annotation. The use of more +L+ coarse-grained senses alleviates the problem: In +L+ OntoNotes (Hovy et al., 2006), an ITA of 90% is +L+ used as the criterion for the construction of coarse- +L+ grained sense distinctions. However, intriguingly, +L+ for some high-frequency lemmas such as leave +L+ this ITA threshold is not reached even after mul- +L+ tiple re-partitionings of the semantic space (Chen +L+ and Palmer, 2009). Similarly, the performance +L+ of WSD systems clearly indicates that WSD is not +L+ easy unless one adopts a coarse-grained approach, +L+ and then systems tagging all words at best perform +L+ a few percentage points above the most frequent +L+ sense heuristic (Navigli et al., 2007). Good perfor- +L+ mance on coarse-grained sense distinctions may +L+ be more useful in applications than poor perfor- +L+ mance on fine-grained distinctions (Ide and Wilks, +L+ 2006) but we do not know this yet and there is +L+ some evidence to the contrary (Stokoe, 2005). +L+ Rather than focus on the granularity of clus- +L+ ters, the approach we will take in this paper +L+ is to examine the phenomenon of word mean- +L+ ing both with and without recourse to predefined +L+ senses by focusing on the similarity of uses of a +L+ word. Human subjects show excellent agreement +L+ on judging word similarity out of context (Ruben- +L+ stein and Goodenough, 1965; Miller and Charles, +L+ 1991), and human judgments have previously been +L+ used successfully to study synonymy and near- +L+ synonymy (Miller and Charles, 1991; Bybee and +L+ Eddington, 2006). We focus on polysemy rather +L+ than synonymy. Our aim will be to use WSsim +L+ to determine to what extent annotations form co- +L+ hesive clusters. In principle, it should be possi- +L+ ble to use existing sense-annotated data to explore +L+ this question: almost all sense annotation efforts +L+ have allowed annotators to assign multiple senses +L+ to a single occurrence, and the distribution of these +L+ sense labels should indicate whether annotators +L+ viewed the senses as disjoint or not. However, +L+ the percentage of markables that received multi- +L+ ple sense labels in existing corpora is small, and it +L+ varies massively between corpora: In the SemCor +L+ corpus (Landes et al., 1998), only 0.3% of all +L+ markables received multiple sense labels. In the +L+ SENSEVAL-3 English lexical task corpus (Mihal- +L+ cea et al., 2004) (hereafter referred to as SE-3), the +L+ ratio is much higher at 8% of all markables1. This +L+ could mean annotators feel that there is usually a +L+ single applicable sense, or it could point to a bias +L+ towards single-sense assignment in the annotation +L+ guidelines and/or the annotation tool. The WSsim +L+ experiment that we report in this paper is designed +L+ to eliminate such bias as far as possible and we +L+ conduct it on data taken from SemCor and SE-3 so +L+ that we can compare the annotations. Although we +L+ use WordNet for the annotation, our study is not a +L+ study of WordNet per se. We choose WordNet be- +L+ cause it is sufficiently fine-grained to examine sub- +L+ tle differences in usage, and because traditionally +L+ annotated datasets exist to which we can compare +L+ our results. +L+ Predefined dictionaries and lexical resources are +L+ not the only possibilities for annotating lexical +L+ items with meaning. In cross-lingual settings, the +L+ actual translations of a word can be taken as the +L+ sense labels (Resnik and Yarowsky, 2000). Re- +L+ cently, McCarthy and Navigli (2007) proposed +L+ the English Lexical Substitution task (hereafter +L+ referred to as LEXSUB) under the auspices of +L+ SemEval-2007. It uses paraphrases for words in +L+ context as a way of annotating meaning. The task +L+ was proposed following a background of discus- +L+ sions in the WSD community as to the adequacy +L+ of predefined word senses. The LEXSUB dataset +L+ comprises open class words (nouns, verbs, adjec- +L+ tives and adverbs) with token instances of each +L+ word appearing in the context of one sentence +L+ taken from the English Internet Corpus (Sharoff, +L+ 2006). The methodology can only work where +L+ there are paraphrases, so the dataset only contains +L+ words with more than one meaning where at least +L+ two different meanings have near synonyms. For +L+ meanings without obvious substitutes the annota- +L+ tors were allowed to use multiword paraphrases or +L+ words with slightly more general meanings. This +L+ dataset has been used to evaluate automatic sys- +L+ tems which can find substitutes appropriate for the +L+ context. To the best of our knowledge there has +L+ been no study of how the data collected relates to +L+ word sense annotations or judgments of semantic +L+ similarity. In this paper we examine these relation- +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 This is even though both annotation efforts use balanced +L+ corpora, the Brown corpus in the case of SemCor, the British +L+ National Corpus for SE-3. +L+ </SectLabel_footnote> <SectLabel_page> 11 +L+ </SectLabel_page> <SectLabel_bodyText> ships by re-using data from LEXSUB in both new +L+ annotation experiments and testing the results for +L+ correlation. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Annotation +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We conducted two experiments through an on- +L+ line annotation interface. Three annotators partic- +L+ ipated in each experiment; all were native British +L+ English speakers. The first experiment, WSsim, +L+ collected annotator judgments about the applica- +L+ bility of dictionary senses using a 5-point rating +L+ scale. The second, Usim, also utilized a 5-point +L+ scale but collected judgments on the similarity in +L+ meaning between two uses of a word. 2 The scale +L+ was 1 – completely different, 2 – mostly different, +L+ 3 – similar, 4 – very similar and 5 – identical. In +L+ Usim, this scale rated the similarity of the two uses +L+ of the common target word; in WSsim it rated the +L+ similarity between the use of the target word and +L+ the sense description. In both experiments, the an- +L+ notation interface allowed annotators to revisit and +L+ change previously supplied judgments, and a com- +L+ ment box was provided alongside each item. +L+ WSsim. This experiment contained a total of +L+ 430 sentences spanning 11 lemmas (nouns, verbs +L+ and adjectives). For 8 of these lemmas, 50 sen- +L+ tences were included, 25 of them randomly sam- +L+ pled from SemCor 3 and 25 randomly sampled +L+ from SE-3 .4 The remaining 3 lemmas in the ex- +L+ periment each had 10 sentences taken from the +L+ LEXSUB data. +L+ WSsim is a word sense annotation task using +L+ WordNet senses.5 Unlike previous word sense an- +L+ notation projects, we asked annotators to provide +L+ judgments on the applicability of every WordNet +L+ sense of the target lemma with the instruction: 6 +L+ </SectLabel_bodyText> <SectLabel_footnote> 2Throughout this paper, a target word is assumed to be a +L+ word in a given PoS. +L+ 3The SemCor dataset was produced alongside WordNet, +L+ so it can be expected to support the WordNet sense distinc- +L+ tions. The same cannot be said for SE-3. +L+ 4Sentence fragments and sentences with 5 or fewer words +L+ were excluded from the sampling. Annotators were given +L+ the sentences, but not the original annotation from these re- +L+ sources. +L+ 5WordNet 1.7.1 was used in the annotation of both SE-3 +L+ and SemCor; we used the more current WordNet 3.0 after +L+ verifying that the lemmas included in this experiment had the +L+ same senses listed in both versions. Care was taken addition- +L+ ally to ensure that senses were not presented in an order that +L+ reflected their frequency of occurrence. +L+ 6The guidelines for both experiments are avail- +L+ able	at	http://comp.ling.utexas.edu/ +L+ people/katrin erk/graded sense and usage +L+ annotation +L+ </SectLabel_footnote> <SectLabel_construct> Your task is to rate, for each of these descriptions, +L+ how well they reflect the meaning of the boldfaced +L+ word in the sentence. +L+ </SectLabel_construct> <SectLabel_bodyText> Applicability judgments were not binary, but were +L+ instead collected using the five-point scale given +L+ above which allowed annotators to indicate not +L+ only whether a given sense applied, but to what +L+ degree. Each annotator annotated each of the 430 +L+ items. By having multiple annotators per item and +L+ a graded, non-binary annotation scheme we al- +L+ low for and measure differences between annota- +L+ tors, rather than training annotators to conform to +L+ a common sense distinction guideline. By asking +L+ annotators to provide ratings for each individual +L+ sense, we strive to eliminate all bias towards either +L+ single-sense or multiple-sense assignment. In tra- +L+ ditional word sense annotation, such bias could be +L+ introduced directly through annotation guidelines +L+ or indirectly, through tools that make it easier to +L+ assign fewer senses. We focus not on finding the +L+ best fitting sense but collect judgments on the ap- +L+ plicability of all senses. +L+ Usim. This experiment used data from LEXSUB. +L+ For more information on LEXSUB, see McCarthy +L+ and Navigli (2007). 34 lemmas (nouns, verbs, ad- +L+ jectives and adverbs) were manually selected, in- +L+ cluding the 3 lemmas also used in WSsim. We se- +L+ lected lemmas which exhibited a range of mean- +L+ ings and substitutes in the LEXSUB data, with +L+ as few multiword substitutes as possible. Each +L+ lemma is the target in 10 LEXSUB sentences. For +L+ our experiment, we took every possible pairwise +L+ comparison of these 10 sentences for a lemma. We +L+ refer to each such pair of sentences as an SPAIR. +L+ The resulting dataset comprised 45 SPAIRs per +L+ lemma, adding up to 1530 comparisons per anno- +L+ tator overall. +L+ In this annotation experiment, annotators saw +L+ SPAIRs with a common target word and rated the +L+ similarity in meaning between the two uses of the +L+ target word with the instruction: +L+ </SectLabel_bodyText> <SectLabel_construct> Your task is to rate, for each pair of sentences, how +L+ similar in meaning the two boldfaced words are on +L+ a five -point scale. +L+ </SectLabel_construct> <SectLabel_bodyText> In addition annotators had the ability to respond +L+ with “Cannot Decide”, indicating that they were +L+ unable to make an effective comparison between +L+ the two contexts, for example because the mean- +L+ ing of one usage was unclear. This occurred in +L+ 9 paired occurrences during the course of anno- +L+ tation, and these items (paired occurrences) were +L+ </SectLabel_bodyText> <SectLabel_page> 12 +L+ </SectLabel_page> <SectLabel_bodyText> excluded from further analysis. +L+ The purpose of Usim was to collect judgments +L+ about degrees of similarity between a word’s +L+ meaning in different contexts. Unlike WSsim, +L+ Usim does not rely upon any dictionary resource +L+ as a basis for the judgments. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Analyses +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This section reports on analyses on the annotated +L+ data. In all the analyses we use Spearman’s rank +L+ correlation coefficient (p), a nonparametric test, +L+ because the data does not seem to be normally +L+ distributed. We used two-tailed tests in all cases, +L+ rather than assume the direction of the relation- +L+ ship. As noted above, we have three annotators +L+ per task, and each annotator gave judgments for +L+ every sentence (WSsim) or sentence pair (Usim). +L+ Since the annotators may vary as to how they use +L+ the ordinal scale, we do not use the mean of judg- +L+ ments7 but report all individual correlations. All +L+ analyses were done using the R package.8 +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 WSsim analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the WSsim experiment, annotators rated the ap- +L+ plicability of each WordNet 3.0 sense for a given +L+ target word occurrence. Table 1 shows a sample +L+ annotation for the target argument.n. 9 +L+ Pattern of annotation and annotator agree- +L+ ment. Figure 1 shows how often each of the five +L+ judgments on the scale was used, individually and +L+ summed over all annotators. (The y-axis shows +L+ raw counts of each judgment.) We can see from +L+ this figure that the extreme ratings 1 and 5 are used +L+ more often than the intermediate ones, but annota- +L+ tors make use of the full ordinal scale when judg- +L+ ing the applicability of a sense. Also, the figure +L+ shows that annotator 1 used the extreme negative +L+ rating 1 much less than the other two annotators. +L+ Figure 2 shows the percentage of times each judg- +L+ ment was used on senses of three lemmas, differ- +L+ ent.a, interest.n, and win.v. In WordNet, they have +L+ 5, 7, and 4 senses, respectively. The pattern for +L+ win.v resembles the overall distribution of judg- +L+ ments, with peaks at the extreme ratings 1 and 5. +L+ The lemma interest.n has a single peak at rating +L+ 1, partly due to the fact that senses 5 (financial +L+ </SectLabel_bodyText> <SectLabel_footnote> 7We have also performed several of our calculations us- +L+ ing the mean judgment, and they also gave highly significant +L+ results in all the cases we tested. +L+ 8http://www.r-project.org/ +L+ 9We use word.PoS to denote a target word (lemma). +L+ Annotator 1 Annotator 2 Annotator 3 overall +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 1: WSsim experiment: number of times +L+ each judgment was used, by annotator and +L+ summed over all annotators. The y-axis shows raw +L+ counts of each judgment. +L+ different.a	interest.n	win.v +L+ Figure 2: WSsim experiment: percentage of times +L+ each judgment was used for the lemmas differ- +L+ ent.a, interest.n and win.v. Judgment counts were +L+ summed over all three annotators. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> involvement) and 6 (interest group) were rarely +L+ judged to apply. For the lemma different.a, all +L+ judgments have been used with approximately the +L+ same frequency. +L+ We measured the level of agreement between +L+ annotators using Spearman’s p between the judg- +L+ ments of every pair of annotators. The pairwise +L+ correlations were p = 0.506, p = 0.466 and p = +L+ 0.540, all highly significant with p < 2.2e-16. +L+ Agreement with previous annotation in +L+ SemCor and SE-3. 200 of the items in WSsim +L+ had been previously annotated in SemCor, and +L+ 200 in SE-3. This lets us compare the annotation +L+ results across annotation efforts. Table 2 shows +L+ the percentage of items where more than one +L+ sense was assigned in the subset of WSsim from +L+ SemCor (first row), from SE-3 (second row), and +L+ </SectLabel_bodyText> <SectLabel_figure> 1 +L+ 2 +L+ 3 +L+ 5 +L+ </SectLabel_figure> <SectLabel_page> 13 +L+ </SectLabel_page> <SectLabel_table> Sentence	1	2	Senses	5	6	7	Annotator +L+ 			3	4 +L+ This question provoked arguments in America about the	1	4	4	2	1	1	3	Ann. 1 +L+ Norton Anthology of Literature by Women, some of the	4	5	4	2	1	1	4	Ann. 2 +L+ contents of which were said to have had little value as literature.	1	4	5	1	1	1	1	Ann. 3 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: A sample annotation in the WSsim experiment. The senses are: 1:statement, 2:controversy, +L+ 3:debate, 4:literary argument, 5:parameter, 6:variable, 7:line of reasoning +L+ </SectLabel_tableCaption> <SectLabel_table> WSsim judgment +L+ >3 >4 5 +L+ 80.2 57.5 28.3 +L+ 78.0 58.3 27.1 +L+ 78.8 57.4 27.7 +L+ Data	Orig. +L+ W Ssim/SemCor	0.0 +L+ WSsim/SE-3	24.0 +L+ All WSsim +L+ 	p<0.05 pos	neg		p<0.01 pos	neg +L+ Ann. 1	30.8	11.4	23.2	5.9 +L+ Ann. 2	22.2	24.1	19.6	19.6 +L+ Ann. 3	12.7	12.0	10.0	6.0 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Percentage of items with multiple senses +L+ assigned. Orig: in the original SemCor/SE-3 data. +L+ WSsim judgment: items with judgments at or +L+ above the specified threshold. The percentages for +L+ WSsim are averaged over the three annotators. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> all of WSsim (third row). The Orig. column +L+ indicates how many items had multiple labels in +L+ the original annotation (SemCor or SE-3)10. Note +L+ that no item had more than one sense label in +L+ SemCor. The columns under WSsim judgment +L+ show the percentage of items (averaged over +L+ the three annotators) that had judgments at or +L+ above the specified threshold, starting from rating +L+ 3 – similar. Within WSsim, the percentage of +L+ multiple assignments in the three rows is fairly +L+ constant. WSsim avoids the bias to one sense +L+ by deliberately asking for judgments on the +L+ applicability of each sense rather than asking +L+ annotators to find the best one. +L+ To compute the Spearman’s correlation between +L+ the original sense labels and those given in the +L+ WSsim annotation, we converted SemCor and +L+ SE-3 labels to the format used within WSsim: As- +L+ signed senses were converted to a judgment of 5, +L+ and unassigned senses to a judgment of 1. For the +L+ WSsim/SemCor dataset, the correlation between +L+ original and WSsim annotation was p = 0.234, +L+ p = 0.448, and p = 0.390 for the three anno- +L+ tators, each highly significant with p < 2.2e-16. +L+ For the WSsim/SE-3 dataset, the correlations were +L+ p = 0.346, p = 0.449 and p = 0.338, each of them +L+ again highly significant at p < 2.2e-16. +L+ Degree of sense grouping. Next we test to what +L+ extent the sense applicability judgments in the +L+ </SectLabel_bodyText> <SectLabel_footnote> 10Overall, 0.3% of tokens in SemCor have multiple labels, +L+ and 8% of tokens in SE-3, so the multiple label assignment in +L+ our sample is not an underestimate. +L+ </SectLabel_footnote> <SectLabel_tableCaption> Table 3: Percentage of sense pairs that were sig- +L+ nificantly positively (pos) or negatively (neg) cor- +L+ related at p < 0.05 and p < 0.01, shown by anno- +L+ tator. +L+ </SectLabel_tableCaption> <SectLabel_table> 	j>3	j>4	j=5 +L+ Ann. 1	71.9	49.1	8.1 +L+ Ann. 2	55.3	24.7	8.1 +L+ Ann. 3	42.8	24.0	4.9 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4: Percentage of sentences in which at least +L+ two uncorrelated (p > 0.05) or negatively corre- +L+ lated senses have been annotated with judgments +L+ at the specified threshold. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> WSsim task could be explained by more coarse- +L+ grained, categorial sense assignments. We first +L+ test how many pairs of senses for a given lemma +L+ show similar patterns in the ratings that they re- +L+ ceive. Table 3 shows the percentage of sense pairs +L+ that were significantly correlated for each anno- +L+ tator.11 Significantly positively correlated senses +L+ can possibly be reduced to more coarse-grained +L+ senses. Would annotators have been able to des- +L+ ignate a single appropriate sense given these more +L+ coarse-grained senses? Call two senses groupable +L+ if they are significantly positively correlated; in or- +L+ der not to overlook correlations that are relatively +L+ weak but existent, we use a cutoff of p = 0.05 for +L+ significant correlation. We tested how often anno- +L+ tators gave ratings of at least similar, i.e. ratings +L+ > 3, to senses that were not groupable. Table 4 +L+ shows the percentages of items where at least two +L+ non-groupable senses received ratings at or above +L+ the specified threshold. The table shows that re- +L+ gardless of which annotator we look at, over 40% +L+ of all items had two or more non-groupable senses +L+ receive judgments of at least 3 (similar). There +L+ </SectLabel_bodyText> <SectLabel_footnote> 11 We exclude senses that received a uniform rating of 1 on +L+ all items. This concerned 4 senses for annotator 2 and 6 for +L+ annotator 3. +L+ </SectLabel_footnote> <SectLabel_page> 14 +L+ </SectLabel_page> <SectLabel_figure> 1) We study the methods and concepts that each writer uses to +L+ defend the cogency of legal, deliberative, or more generally +L+ political prudence against explicit or implicit charges that +L+ practical thinking is merely a knack or form of cleverness. +L+ 2) Eleven CIRA members have been convicted of criminal +L+ charges and others are awaiting trial. +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: An SPAIR for charge.n. Annotator judg- +L+ ments: 2,3,4 +L+ </SectLabel_figureCaption> <SectLabel_bodyText> were even several items where two or more non- +L+ groupable senses each got a judgment of 5. The +L+ sentence in table 1 is a case where several non- +L+ groupable senses got ratings > 3. This is most +L+ pronounced for Annotator 2, who along with sense +L+ 2 (controversy) assigned senses 1 (statement), 7 +L+ (line of reasoning), and 3 (debate), none of which +L+ are groupable with sense 2. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Usim analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this experiment, ratings between 1 and 5 were +L+ given for every pairwise combination of sentences +L+ for each target lemma. An example of an SPAIR +L+ for charge.n is shown in figure 3. In this case the +L+ verdicts from the annotators were 2, 3 and 4. +L+ Pattern of Annotations and Annotator Agree- +L+ ment Figure 4 gives a bar chart of the judgments +L+ for each annotator and summed over annotators. +L+ We can see from this figure that the annotators +L+ use the full ordinal scale when judging the simi- +L+ larity of a word’s usages, rather than sticking to +L+ the extremes. There is variation across words, de- +L+ pending on the relatedness of each word’s usages. +L+ Figure 5 shows the judgments for the words bar.n, +L+ work.v and raw.a. We see that bar.n has predom- +L+ inantly different usages with a peak for category +L+ 1, work.v has more similar judgments (category 5) +L+ compared to any other category and raw.a has a +L+ peak in the middle category (3). 12 There are other +L+ words, like for example fresh.a, where the spread +L+ is more uniform. +L+ To gauge the level of agreement between anno- +L+ tators, we calculated Spearman’s p between the +L+ judgments of every pair of annotators as in sec- +L+ tion 4.1. The pairwise correlations are all highly +L+ significant (p < 2.2e-16) with Spearman’s p = +L+ 0.502, 0.641 and 0.501 giving an average corre- +L+ lation of 0.548. We also perform leave-one-out re- +L+ sampling following Lapata (2006) which gave us +L+ a Spearman’s correlation of 0.630. +L+ </SectLabel_bodyText> <SectLabel_footnote> 12For figure 5 we sum the judgments over annotators. +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 4: Usim experiment: number of times each +L+ judgment was used, by annotator and summed +L+ over all annotators +L+ </SectLabel_figureCaption> <SectLabel_figure> bar.n	raw.a	work.v +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: Usim experiment: number of times each +L+ judgment was used for bar.n, work.v and raw. a +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Comparison with LEXSUB substitutions Next +L+ we look at whether the Usim judgments on sen- +L+ tence pairs (SPAIRs) correlate with LEXSUB sub- +L+ stitutes. To do this we use the overlap of substi- +L+ tutes provided by the five LEXSUB annotators be- +L+ tween two sentences in an SPAIR. In LEXSUB the +L+ annotators had to replace each item (a target word +L+ within the context of a sentence) with a substitute +L+ that fitted the context. Each annotator was permit- +L+ ted to supply up to three substitutes provided that +L+ they all fitted the context equally. There were 10 +L+ sentences per lemma. For our analyses we take +L+ every SPAIR for a given lemma and calculate the +L+ overlap (inter) of the substitutes provided by the +L+ annotators for the two usages under scrutiny. Let +L+ s1 and s2 be a pair of sentences in an SPAIR and +L+ Annotator 4 Annotator 5 Annotator 6	overall +L+ </SectLabel_bodyText> <SectLabel_figure> 1 +L+ 2 +L+ 3 +L+ 4 +L+ 5 +L+ 1 +L+ 2 +L+ 3 +L+ 4 +L+ 5 +L+ </SectLabel_figure> <SectLabel_page> 15 +L+ </SectLabel_page> <SectLabel_bodyText> x1 and x2 be the multisets of substitutes for the +L+ respective sentences. Let freq(w,x) be the fre- +L+ quency of a substitute w in a multiset x of sub- +L+ stitutes for a given sentence. 13 INTER(s1,s2) = +L+ </SectLabel_bodyText> <SectLabel_equation> �wEx1f1x2 min (freq(w,x1), freq(w,x2)) +L+ max(1x1 1, 1x2 1) +L+ </SectLabel_equation> <SectLabel_bodyText> Using this calculation for each SPAIR we can +L+ now compute the correlation between the Usim +L+ judgments for each annotator and the INTER val- +L+ ues, again using Spearman’s. The figures are +L+ shown in the leftmost block of table 5. The av- +L+ erage correlation for the 3 annotators was 0.488 +L+ and the p-values were all < 2.2e-16. This shows +L+ a highly significant correlation of the Usim judg- +L+ ments and the overlap of substitutes. +L+ We also compare the WSsim judgments against +L+ the LEXSUB substitutes, again using the INTER +L+ measure of substitute overlap. For this analysis, +L+ we only use those WSsim sentences that are origi- +L+ nally from LEXSUB. In WSsim, the judgments for +L+ a sentence comprise judgments for each WordNet +L+ sense of that sentence. In order to compare against +L+ INTER, we need to transform these sentence-wise +L+ ratings in WSsim to a WSsim-based judgment of +L+ sentence similarity. To this end, we compute the +L+ Euclidean Distance14 (ED) between two vectors J1 +L+ and J2 of judgments for two sentences s1, s2 for the +L+ same lemma E. Each of the n indexes of the vector +L+ represent one of the n different WordNet senses +L+ for E. The value at entry i of the vector J1 is the +L+ judgment that the annotator in question (we do not +L+ average over annotators here) provided for sense i +L+ of E for sentence s1. +L+ </SectLabel_bodyText> <SectLabel_equation> (J1[i]—J2[i])2) (1) +L+ </SectLabel_equation> <SectLabel_bodyText> We correlate the Euclidean distances with +L+ INTER. We can only test correlation for the subset +L+ of WSsim that overlaps with the LEXSUB data: the +L+ 30 sentences for investigator.n, function.n and or- +L+ der.v, which together give 135 unique SPAIRs. We +L+ refer to this subset as Wf1U. The results are given +L+ in the third block of table 5. Note that since we are +L+ measuring distance between SPAIRs for WSsim +L+ </SectLabel_bodyText> <SectLabel_footnote> 13The frequency of a substitute in a multiset depends on +L+ the number of LEXSUB annotators that picked the substitute +L+ for this item. +L+ 14We use Euclidean Distance rather than a normalizing +L+ measure like Cosine because a sentence where all ratings are +L+ 5 should be very different from a sentence where all senses +L+ received a rating of 1. +L+ </SectLabel_footnote> <SectLabel_table> Usim All		Usim Wf1U	WSsim Wf1U +L+ ann.	p	p	ann.	p +L+ 4	0.383	0.330	1	-0.520 +L+ 5	0.498	0.635	2	-0.503 +L+ 6	0.584	0.631	3	-0.463 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5: Annotator correlation with LEXSUB sub- +L+ stitute overlap (inter) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> whereas INTER is a measure of similarity, the cor- +L+ relation is negative. The results are highly signif- +L+ icant with individual p-values from < 1.067e-10 +L+ to < 1.551e-08 and a mean correlation of -0.495. +L+ The results in the first and third block of table 5 are +L+ not directly comparable, as the results in the first +L+ block are for all Usim data and not the subset of +L+ LEXSUB with WSsim annotations. We therefore +L+ repeated the analysis for Usim on the subset of +L+ data in WSsim and provide the correlation in the +L+ middle section of table 5. The mean correlation +L+ for Usim on this subset of the data is 0.532, which +L+ is a stronger relationship compared to WSsim, al- +L+ though there is more discrepancy between individ- +L+ ual annotators, with the result for annotator 4 giv- +L+ ing a p-value = 9.139e-05 while the other two an- +L+ notators had p-values < 2.2e-16. +L+ The LEXSUB substitute overlaps between dif- +L+ ferent usages correlate well with both Usim and +L+ WSsim judgments, with a slightly stronger rela- +L+ tionship to Usim, perhaps due to the more compli- +L+ cated representation of word meaning in WSsim +L+ which uses the full set of WordNet senses. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Correlation between WSsim and Usim +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As we showed in section 4.1, WSsim correlates +L+ with previous word sense annotations in SemCor +L+ and SE-3 while allowing the user a more graded +L+ response to sense tagging. As we saw in sec- +L+ tion 4.2, Usim and WSsim judgments both have a +L+ highly significant correlation with similarity of us- +L+ ages as measured using the overlap of substitutes +L+ from LEXSUB. Here, we look at the correlation +L+ of WSsim and Usim, considering again the sub- +L+ set of data that is common to both experiments. +L+ We again transform WSsim sense judgments for +L+ individual sentences to distances between SPAIRs +L+ using Euclidean Distance (ED). The Spearman’s +L+ p range between —0.307 and —0.671, and all re- +L+ sults are highly significant with p-values between +L+ 0.0003 and < 2.2e-16. As above, the correla- +L+ tion is negative because ED is a distance measure +L+ between sentences in an SPAIR, whereas the judg- +L+ </SectLabel_bodyText> <SectLabel_equation> ED(J1,J2) = V1( +L+ n +L+ � +L+ i=1 +L+ </SectLabel_equation> <SectLabel_page> 16 +L+ </SectLabel_page> <SectLabel_bodyText> ments for Usim are similarity judgments. We see +L+ that there is highly significant correlation for every +L+ pairing of annotators from the two experiments. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Discussion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Validity of annotation scheme. Annotator rat- +L+ ings show highly significant correlation on both +L+ tasks. This shows that the tasks are well-defined. +L+ In addition, there is a strong correlation between +L+ WSsim and Usim, which indicates that the poten- +L+ tial bias introduced by the use of dictionary senses +L+ in WSsim is not too prominent. However, we note +L+ that WSsim only contained a small portion of 3 +L+ lemmas (30 sentences and 135 SPAIRs) in com- +L+ mon with Usim, so more annotation is needed to +L+ be certain of this relationship. Given the differ- +L+ ences between annotator 1 and the other annota- +L+ tors in Fig. 1, it would be interesting to collect +L+ judgments for additional annotators. +L+ Graded judgments of use similarity and sense +L+ applicability. The annotators made use of the +L+ full spectrum of ratings, as shown in Figures 1 and +L+ 4. This may be because of a graded perception of +L+ the similarity of uses as well as senses, or because +L+ some uses and senses are very similar. Table 4 +L+ shows that for a large number of WSsim items, +L+ multiple senses that were not significantly posi- +L+ tively correlated got high ratings. This seems to +L+ indicate that the ratings we obtained cannot sim- +L+ ply be explained by more coarse-grained senses. It +L+ may hence be reasonable to pursue computational +L+ models of word meaning that are graded, maybe +L+ even models that do not rely on dictionary senses +L+ at all (Erk and Pado, 2008). +L+ Comparison to previous word sense annotation. +L+ Our graded WSsim annotations do correlate with +L+ traditional “best fitting sense” annotations from +L+ SemCor and SE-3; however, if annotators perceive +L+ similarity between uses and senses as graded, tra- +L+ ditional word sense annotation runs the risk of in- +L+ troducing bias into the annotation. +L+ Comparison to lexical substitutions. There is a +L+ strong correlation between both Usim and WSsim +L+ and the overlap in paraphrases that annotators gen- +L+ erated for LEXSUB. This is very encouraging, and +L+ especially interesting because LEXSUB annotators +L+ freely generated paraphrases rather than selecting +L+ them from a list. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Conclusions +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have introduced a novel annotation paradigm +L+ for word sense annotation that allows for graded +L+ judgments and for some variation between anno- +L+ tators. We have used this annotation paradigm +L+ in two experiments, WSsim and Usim, that shed +L+ some light on the question of whether differences +L+ between word usages are perceived as categorial +L+ or graded. Both datasets will be made publicly +L+ available. There was a high correlation between +L+ annotator judgments within and across tasks, as +L+ well as with previous word sense annotation and +L+ with paraphrases proposed in the English Lex- +L+ ical Substitution task. Annotators made ample +L+ use of graded judgments in a way that cannot +L+ be explained through more coarse-grained senses. +L+ These results suggest that it may make sense to +L+ evaluate WSD systems on a task of graded rather +L+ than categorial meaning characterization, either +L+ through dictionary senses or similarity between +L+ uses. In that case, it would be useful to have more +L+ extensive datasets with graded annotation, even +L+ though this annotation paradigm is more time con- +L+ suming and thus more expensive than traditional +L+ word sense annotation. +L+ As a next step, we will automatically cluster the +L+ judgments we obtained in the WSsim and Usim +L+ experiments to further explore the degree to which +L+ the annotation gives rise to sense grouping. We +L+ will also use the ratings in both experiments to +L+ evaluate automatically induced models of word +L+ meaning. The SemEval-2007 word sense induc- +L+ tion task (Agirre and Soroa, 2007) already allows +L+ for evaluation of automatic sense induction sys- +L+ tems, but compares output to gold-standard senses +L+ from OntoNotes. We hope that the Usim dataset +L+ will be particularly useful for evaluating methods +L+ which relate usages without necessarily producing +L+ hard clusters. Also, we will extend the current +L+ dataset using more annotators and exploring ad- +L+ ditional lexicon resources. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgments. We acknowledge support +L+ from the UK Royal Society for a Dorothy Hodkin +L+ Fellowship to the second author. We thank Sebas- +L+ tian Pado for many helpful discussions, and An- +L+ drew Young for help with the interface. +L+ References +L+ </SectLabel_sectionHeader> <SectLabel_reference> E. Agirre and A. Soroa. 2007. SemEval-2007 +L+ task 2: Evaluating word sense induction and dis- +L+ </SectLabel_reference> <SectLabel_page> 17 +L+ </SectLabel_page> <SectLabel_reference> crimination systems. In Proceedings of the 4th +L+ International Workshop on Semantic Evaluations +L+ (SemEval-2007), pages 7–12, Prague, Czech Repub- +L+ lic. +L+ J. Bybee and D. Eddington. 2006. A usage-based ap- +L+ proach to Spanish verbs of ’becoming’. Language, +L+ 82(2):323–355. +L+ J. Chen and M. Palmer. 2009. Improving English +L+ verb sense disambiguation performance with lin- +L+ guistically motivated features and clear sense dis- +L+ tinction boundaries. Journal of Language Resources +L+ and Evaluation, Special Issue on SemEval-2007. in +L+ press. +L+ K. Erk and S. Pado. 2008. A structured vector space +L+ model for word meaning in context. In Proceedings +L+ of EMNLP-08, Waikiki, Hawaii. +L+ J. A. Hampton. 1979. Polymorphous concepts in se- +L+ mantic memory. Journal of Verbal Learning and +L+ Verbal Behavior, 18:441–461. +L+ J. A. Hampton. 2007. Typicality, graded membership, +L+ and vagueness. Cognitive Science, 31:355–384. +L+ P. Hanks. 2000. Do word meanings exist? Computers +L+ and the Humanities, 34(1-2):205–215(11). +L+ E. H. Hovy, M. Marcus, M. Palmer, S. Pradhan, +L+ L. Ramshaw, and R. Weischedel. 2006. OntoNotes: +L+ The 90% solution. In Proceedings of the Hu- +L+ man Language Technology Conference of the North +L+ American Chapter of the ACL (NAACL-2006), pages +L+ 57–60, New York. +L+ N. Ide and Y. Wilks. 2006. Making sense about +L+ sense. In E. Agirre and P. Edmonds, editors, +L+ Word Sense Disambiguation, Algorithms and Appli- +L+ cations, pages 47–73. Springer. +L+ A. Kilgarriff and J. Rosenzweig. 2000. Framework +L+ and results for English Senseval. Computers and the +L+ Humanities, 34(1-2):15–48. +L+ A. Kilgarriff. 1997. I don’t believe in word senses. +L+ Computers and the Humanities, 31(2):91–113. +L+ A. Kilgarriff. 2006. Word senses. In E. Agirre +L+ and P. Edmonds, editors, Word Sense Disambigua- +L+ tion, Algorithms and Applications, pages 29–46. +L+ Springer. +L+ R. Krishnamurthy and D. Nicholls. 2000. Peeling +L+ an onion: the lexicographers’ experience of man- +L+ ual sense-tagging. Computers and the Humanities, +L+ 34(1-2). +L+ S. Landes, C. Leacock, and R. Tengi. 1998. Build- +L+ ing semantic concordances. In C. Fellbaum, editor, +L+ WordNet: An Electronic Lexical Database. The MIT +L+ Press, Cambridge, MA. +L+ M. Lapata. 2006. Automatic evaluation of information +L+ ordering. Computational Linguistics, 32(4):471– +L+ 484. +L+ D. McCarthy and R. Navigli. 2007. SemEval-2007 +L+ task 10: English lexical substitution task. In Pro- +L+ ceedings of the 4th International Workshop on Se- +L+ mantic Evaluations (SemEval-2007), pages 48–53, +L+ Prague, Czech Republic. +L+ M. McCloskey and S. Glucksberg. 1978. Natural cat- +L+ egories: Well defined or fuzzy sets? Memory & +L+ Cognition, 6:462–472. +L+ R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. +L+ The Senseval-3 English lexical sample task. In +L+ 3rd International Workshop on Semantic Evalua- +L+ tions (SensEval-3) atACL-2004, Barcelona, Spain. +L+ G. Miller and W. Charles. 1991. Contextual correlates +L+ of semantic similarity. Language and cognitive pro- +L+ cesses, 6(1):1–28. +L+ G. L. Murphy. 2002. The Big Book of Concepts. MIT +L+ Press. +L+ R. Navigli, K. C. Litkowski, and O. Hargraves. +L+ 2007. SemEval-2007 task 7: Coarse-grained En- +L+ glish all-words task. In Proceedings of the 4th +L+ International Workshop on Semantic Evaluations +L+ (SemEval-2007), pages 30–35, Prague, Czech Re- +L+ public. +L+ P. Resnik and D. Yarowsky. 2000. Distinguishing +L+ systems and distinguishing senses: New evaluation +L+ methods for word sense disambiguation. Natural +L+ Language Engineering, 5(3):113–133. +L+ E. Rosch and C. B. Mervis. 1975. Family resem- +L+ blance: Studies in the internal structure of cate- +L+ gories. Cognitive Psychology, 7:573–605. +L+ E. Rosch. 1975. Cognitive representations of seman- +L+ tic categories. Journal of Experimental Psychology: +L+ General, 104:192–233. +L+ H. Rubenstein and J. Goodenough. 1965. Contextual +L+ correlates of synonymy. Computational Linguistics, +L+ 8:627–633. +L+ S. Sharoff. 2006. Open-source corpora: Using the net +L+ to fish for linguistic data. International Journal of +L+ Corpus Linguistics, 11(4):435–462. +L+ C. Stokoe. 2005. Differentiating homonymy and pol- +L+ ysemy in information retrieval. In Proceedings of +L+ HLT/EMNLP-05, pages 403–410, Vancouver, B.C., +L+ Canada. +L+ </SectLabel_reference> <SectLabel_page> 18 +L+ </SectLabel_page>
<SectLabel_title> A Comparative Study on Generalization of Semantic Roles in FrameNet +L+ </SectLabel_title> <SectLabel_author> Yuichiroh Matsubayashit	Naoaki Okazakit	Jun’ichi Tsujiit$* +L+ </SectLabel_author> <SectLabel_affiliation> tDepartment of Computer Science, University of Tokyo, Japan +L+ $School of Computer Science, University of Manchester, UK +L+ *National Centre for Text Mining, UK +L+ </SectLabel_affiliation> <SectLabel_email> {y-matsu,okazaki,tsujii}@is.s.u-tokyo.ac.jp +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A number of studies have presented +L+ machine-learning approaches to semantic +L+ role labeling with availability of corpora +L+ such as FrameNet and PropBank. These +L+ corpora define the semantic roles of predi- +L+ cates for each frame independently. Thus, +L+ it is crucial for the machine-learning ap- +L+ proach to generalize semantic roles across +L+ different frames, and to increase the size +L+ of training instances. This paper ex- +L+ plores several criteria for generalizing se- +L+ mantic roles in FrameNet: role hierar- +L+ chy, human-understandable descriptors of +L+ roles, semantic types of filler phrases, and +L+ mappings from FrameNet roles to the- +L+ matic roles of VerbNet. We also pro- +L+ pose feature functions that naturally com- +L+ bine and weight these criteria, based on +L+ the training data. The experimental result +L+ of the role classification shows 19.16% +L+ and 7.42% improvements in error reduc- +L+ tion rate and macro-averaged F 1 score, re- +L+ spectively. We also provide in-depth anal- +L+ yses of the proposed criteria. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Semantic Role Labeling (SRL) is a task of analyz- +L+ ing predicate-argument structures in texts. More +L+ specifically, SRL identifies predicates and their +L+ arguments with appropriate semantic roles. Re- +L+ solving surface divergence of texts (e.g., voice +L+ of verbs and nominalizations) into unified seman- +L+ tic representations, SRL has attracted much at- +L+ tention from researchers into various NLP appli- +L+ cations including question answering (Narayanan +L+ and Harabagiu, 2004; Shen and Lapata, 2007; +L+ </SectLabel_bodyText> <SectLabel_figure> buy.v	PropBank	FrameNet +L+ Frame	buy.01	Commerce buy +L+ Roles	ARG0: buyer	Buyer Goods Seller Money Recipient +L+ 	ARG1: thing bought	... +L+ 	ARG2: seller +L+ 	ARG3: paid +L+ 	ARG4: benefactive +L+ 	... +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1: A comparison of frames for buy.v de- +L+ fined in PropBank and FrameNet +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Moschitti et al., 2007), and information extrac- +L+ tion (Surdeanu et al., 2003). +L+ In recent years, with the wide availability of cor- +L+ pora such as PropBank (Palmer et al., 2005) and +L+ FrameNet (Baker et al., 1998), a number of stud- +L+ ies have presented statistical approaches to SRL +L+ (M`arquez et al., 2008). Figure 1 shows an exam- +L+ ple of the frame definitions for a verb buy in Prop- +L+ Bank and FrameNet. These corpora define a large +L+ number of frames and define the semantic roles for +L+ each frame independently. This fact is problem- +L+ atic in terms of the performance of the machine- +L+ learning approach, because these definitions pro- +L+ duce many roles that have few training instances. +L+ PropBank defines a frame for each sense of +L+ predicates (e.g., buy.01), and semantic roles are +L+ defined in a frame-specific manner (e.g., buyer and +L+ seller for buy.01). In addition, these roles are asso- +L+ ciated with tags such as ARG0-5 and AM-*, which +L+ are commonly used in different frames. Most +L+ SRL studies on PropBank have used these tags +L+ in order to gather a sufficient amount of training +L+ data, and to generalize semantic-role classifiers +L+ across different frames. However, Yi et al. (2007) +L+ reported that tags ARG2 –ARG5 were inconsis- +L+ tent and not that suitable as training instances. +L+ Some recent studies have addressed alternative ap- +L+ proaches to generalizing semantic roles across dif- +L+ ferent frames (Gordon and Swanson, 2007; Zapi- +L+ </SectLabel_bodyText> <SectLabel_page> 19 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19–27, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_figure> Commerce_sell::Buyer Commerce_buy::Buyer +L+ Givi ng:: Reci pi ent +L+ Recipient +L+ Transfer::Recipient +L+ Buyer +L+ Agent +L+ Commerce_sell::Seller Commerce_buy::Seller +L+ Giving::Donor +L+ Transfer::Donor +L+ Donor +L+ Seller +L+ role-to-role relation +L+ hierarchical class +L+ thematic role +L+ role descriptor +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: An example of role groupings using different criteria. +L+ rain et al., 2008). +L+ </SectLabel_figureCaption> <SectLabel_bodyText> FrameNet designs semantic roles as frame spe- +L+ cific, but also defines hierarchical relations of se- +L+ mantic roles among frames. Figure 2 illustrates +L+ an excerpt of the role hierarchy in FrameNet; this +L+ figure indicates that the Buyer role for the Com- +L+ merce buy frame (Commerce buy::Buyer here- +L+ after) and the Commerce sell::Buyer role are in- +L+ herited from the Transfer:: Recipient role. Al- +L+ though the role hierarchy was expected to gener- +L+ alize semantic roles, no positive results for role +L+ classification have been reported (Baldewein et al., +L+ 2004). Therefore, the generalization of semantic +L+ roles across different frames has been brought up +L+ as a critical issue for FrameNet (Gildea and Juraf- +L+ sky, 2002; Shi and Mihalcea, 2005; Giuglea and +L+ Moschitti, 2006) +L+ In this paper, we explore several criteria for gen- +L+ eralizing semantic roles in FrameNet. In addi- +L+ tion to the FrameNet hierarchy, we use various +L+ pieces of information: human-understandable de- +L+ scriptors of roles, semantic types of filler phrases, +L+ and mappings from FrameNet roles to the thematic +L+ roles of VerbNet. We also propose feature func- +L+ tions that naturally combines these criteria in a +L+ machine-learning framework. Using the proposed +L+ method, the experimental result of the role classi- +L+ fication shows 19.16% and 7.42% improvements +L+ in error reduction rate and macro-averaged F1, re- +L+ spectively. We provide in-depth analyses with re- +L+ spect to these criteria, and state our conclusions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Moschitti et al. (2005) first classified roles by us- +L+ ing four coarse-grained classes (Core Roles, Ad- +L+ juncts, Continuation Arguments and Co-referring +L+ Arguments), and built a classifier for each coarse- +L+ grained class to tag PropBank ARG tags. Even +L+ though the initial classifiers could perform rough +L+ estimations of semantic roles, this step was not +L+ able to solve the ambiguity problem in PropBank +L+ ARG2-5. When training a classifier for a seman- +L+ tic role, Baldewein et al. (2004) re-used the train- +L+ ing instances of other roles that were similar to the +L+ target role. As similarity measures, they used the +L+ FrameNet hierarchy, peripheral roles of FrameNet, +L+ and clusters constructed by a EM-based method. +L+ Gordon and Swanson (2007) proposed a general- +L+ ization method for the PropBank roles based on +L+ syntactic similarity in frames. +L+ Many previous studies assumed that thematic +L+ roles bridged semantic roles in different frames. +L+ Gildea and Jurafsky (2002) showed that classifica- +L+ tion accuracy was improved by manually replac- +L+ ing FrameNet roles into 18 thematic roles. Shi +L+ and Mihalcea (2005) and Giuglea and Moschitti +L+ (2006) employed VerbNet thematic roles as the +L+ target of mappings from the roles defined by the +L+ different semantic corpora. Using the thematic +L+ roles as alternatives of ARG tags, Loper et al. +L+ (2007) and Yi et al. (2007) demonstrated that the +L+ classification accuracy of PropBank roles was im- +L+ proved for ARG2 roles, but that it was diminished +L+ for ARG1. Yi et al. (2007) also described that +L+ ARG2–5 were mapped to a variety of thematic +L+ roles. Zapirain et al. (2008) evaluated PropBank +L+ ARG tags and VerbNet thematic roles in a state-of- +L+ the-art SRL system, and concluded that PropBank +L+ ARG tags achieved a more robust generalization of +L+ the roles than did VerbNet thematic roles. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Role Classification +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> SRL is a complex task wherein several problems +L+ are intertwined: frame-evoking word identifica- +L+ tion, frame disambiguation (selecting a correct +L+ frame from candidates for the evoking word), role- +L+ phrase identification (identifying phrases that fill +L+ semantic roles), and role classification (assigning +L+ correct roles to the phrases). In this paper, we fo- +L+ cus on role classification, in which the role gen- +L+ eralization is particularly critical to the machine +L+ learning approach. +L+ In the role classification task, we are given a +L+ sentence, a frame evoking word, a frame, and +L+ </SectLabel_bodyText> <SectLabel_page> 20 +L+ </SectLabel_page> <SectLabel_figure> Hierarchical-relation groups +L+ Role-descriptor groups +L+ Thematic-role groups +L+ Semantic-type groups +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: Examples for each type of role group. +L+ </SectLabel_figureCaption> <SectLabel_figure> INPUT: +L+ frame = Commerce_sell +L+ candidate roles = { Seller, Buyer, Goods, Reason, Time, ... , Place} +L+ sentence = Can't [you] [sell Commerce_sell] [the factory] [to some other +L+ company]? +L+ OUTPUT: +L+ sentence = Can't [you Seller] [sell Commerce_sell] [the factory Goods] +L+ [to some other company Buyer] ? +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: An example of input and output of role +L+ classification. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> phrases that take semantic roles. We are inter- +L+ ested in choosing the correct role from the can- +L+ didate roles for each phrase in the frame. Figure 3 +L+ shows a concrete example of input and output; the +L+ semantic roles for the phrases are chosen from the +L+ candidate roles: Seller, Buyer, Goods, Reason, +L+ ... , and Place. +L+ role at a node in the hierarchy inherits the char- +L+ acteristics of the roles of its ancestor nodes. For +L+ example, Commerce sell::Seller in Figure 2 in- +L+ herits the property of Giving:: Donor. +L+ For Inheritance, Using, Perspective on, and +L+ Subframe relations, we assume that descendant +L+ roles in these relations have the same or special- +L+ ized properties of their ancestors. Hence, for each +L+ role yi, we define the following two role groups, +L+ </SectLabel_bodyText> <SectLabel_equation> Hchild	= {y y = yi V y is a child of yi}, +L+ yi	= {yly = yi V y is a descendant of yij. +L+ Hdesc +L+ yi +L+ </SectLabel_equation> <SectLabel_bodyText> The hierarchical-relation groups in Figure 4 are +L+ the illustrations of Hdesc +L+ </SectLabel_bodyText> <SectLabel_equation> yi	. +L+ </SectLabel_equation> <SectLabel_bodyText> For the relation types Inchoative of and +L+ Causative of, we define role groups in the oppo- +L+ site direction of the hierarchy, +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Design of Role Groups	Hparent	= {yjy = yi V y is a parent of yi}, +L+ </SectLabel_sectionHeader> <SectLabel_equation> 	yi	= {yjy = yi V y is anancestor of yij. +L+ 	Hance +L+ 	yi +L+ </SectLabel_equation> <SectLabel_bodyText> We formalize the generalization of semantic roles +L+ as the act of grouping several roles into a +L+ class. We define a role group as a set of +L+ role labels grouped by a criterion. Figure 4 +L+ shows examples of role groups; a group Giv- +L+ ing::Donor (in the hierarchical-relation groups) +L+ contains the roles Giving::Donor and Com- +L+ merce pay::Buyer. The remainder of this section +L+ describes the grouping criteria in detail. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Hierarchical relations among roles +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> FrameNet defines hierarchical relations among +L+ frames (frame-to-frame relations). Each relation +L+ is assigned one of the seven types of directional +L+ relationships (Inheritance, Using, Perspective on, +L+ Causative of, Inchoative of, Subframe, and Pre- +L+ cedes). Some roles in two related frames are also +L+ connected with role-to-role relations. We assume +L+ that this hierarchy is a promising resource for gen- +L+ eralizing the semantic roles; the idea is that the +L+ This is because lower roles of Inchoative of +L+ and Causative of relations represent more neu- +L+ tral stances or consequential states; for example, +L+ Killing::Victim is a parent of Death:: Protagonist +L+ in the Causative of relation. +L+ Finally, the Precedes relation describes the se- +L+ quence of states and events, but does not spec- +L+ ify the direction of semantic inclusion relations. +L+ Therefore, we simply try Hchild Hdesc Hparent +L+ yi , yi,yi , +L+ and Hynce for this relation type. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Human-understandable role descriptor +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> FrameNet defines each role as frame-specific; in +L+ other words, the same identifier does not appear +L+ in different frames. However, in FrameNet, +L+ human experts assign a human-understandable +L+ name to each role in a rather systematic man- +L+ ner. Some names are shared by the roles in +L+ different frames, whose identifiers are dif- +L+ ferent. Therefore, we examine the semantic +L+ </SectLabel_bodyText> <SectLabel_page> 21 +L+ </SectLabel_page> <SectLabel_bodyText> commonality of these names; we construct an +L+ equivalence class of the roles sharing the same +L+ name. We call these human-understandable +L+ names role descriptors. In Figure 4, the role- +L+ descriptor group Buyer collects the roles Com- +L+ merce pay::Buyer, Commerce buy::Buyer, +L+ and Commerce sell::Buyer. +L+ This criterion may be effective in collecting +L+ similar roles since the descriptors have been anno- +L+ tated by intuition of human experts. As illustrated +L+ in Figure 2, the role descriptors group the seman- +L+ tic roles which are similar to the roles that the +L+ FrameNet hierarchy connects as sister or parent- +L+ child relations. However, role-descriptor groups +L+ cannot express the relations between the roles +L+ as inclusions since they are equivalence classes. +L+ For example, the roles Commerce sell::Buyer +L+ and Commerce buy::Buyer are included in the +L+ role descriptor group Buyer in Figure 2; how- +L+ ever, it is difficult to merge Giving:: Recipient +L+ and Commerce sell::Buyer because the Com- +L+ merce sell:: Buyer has the extra property that one +L+ gives something of value in exchange and a hu- +L+ man assigns different descriptors to them. We ex- +L+ pect that the most effective weighting of these two +L+ criteria will be determined from the training data. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Semantic type of phrases +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We consider that the selectional restriction is help- +L+ ful in detecting the semantic roles. FrameNet pro- +L+ vides information concerning the semantic types +L+ of role phrases (fillers); phrases that play spe- +L+ cific roles in a sentence should fulfill the se- +L+ mantic constraint from this information. For +L+ instance, FrameNet specifies the constraint that +L+ Self motion::Area should be filled by phrases +L+ whose semantic type is Location. Since these +L+ types suggest a coarse-grained categorization of +L+ semantic roles, we construct role groups that con- +L+ tain roles whose semantic types are identical. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.4 Thematic roles of VerbNet +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> VerbNet thematic roles are 23 frame-independent +L+ semantic categories for arguments of verbs, +L+ such as Agent, Patient, Theme and Source. +L+ These categories have been used as consis- +L+ tent labels across verbs. We use a partial +L+ mapping between FrameNet roles and Verb- +L+ Net thematic roles provided by SemLink. 1 +L+ Each group is constructed as a set Tti = +L+ </SectLabel_bodyText> <SectLabel_footnote> 1http://verbs.colorado.edu/semlink/ +L+ </SectLabel_footnote> <SectLabel_bodyText> {yISemLink maps y into the thematic role ti}. +L+ SemLink currently maps 1,726 FrameNet roles +L+ into VerbNet thematic roles, which are 37.61% of +L+ roles appearing at least once in the FrameNet cor- +L+ pus. This may diminish the effect of thematic-role +L+ groups than its potential. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Role classification method +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 5.1 Traditional approach +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We are given a frame-evoking word e, a frame f +L+ and a role phrase x detected by a human or some +L+ automatic process in a sentence s. Let Yf be the +L+ set of semantic roles that FrameNet defines as be- +L+ ing possible role assignments for the frame f, and +L+ let x = {x1, ... , x,,,} be observed features for x +L+ from s, e and f . The task of semantic role classifi- +L+ cation can be formalized as the problem of choos- +L+ ing the most suitable role y� from Yf. Suppose we +L+ have a model P(yIf, x) which yields the condi- +L+ tional probability of the semantic role y for given +L+ f and x. Then we can choose y� as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> y� = argmax P(y� f, x).	(1) +L+ yEYf +L+ </SectLabel_equation> <SectLabel_bodyText> A traditional way to incorporate role groups +L+ into this formalization is to overwrite each role +L+ y in the training and test data with its role +L+ group m(y) according to the memberships of +L+ the group. For example, semantic roles Com- +L+ merce sell::Seller and Giving::Donor can be re- +L+ placed by their thematic-role group Theme::Agent +L+ in this approach. We determine the most suitable +L+ role group c� as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> c� = argmax P-(c�f, x).	(2) +L+ cE �-(y)�yEYf} +L+ </SectLabel_equation> <SectLabel_bodyText> Here, P-(c�f, x) presents the probability of the +L+ role group c for f and x. The role y� is determined +L+ uniquely iff a single role y E Yf is associated +L+ with �c. Some previous studies have employed this +L+ idea to remedy the data sparseness problem in the +L+ training data (Gildea and Jurafsky, 2002). How- +L+ ever, we cannot apply this approach when multi- +L+ ple roles in Yf are contained in the same class. For +L+ example, we can construct a semantic-type group +L+ St::State of affairs in which Giving::Reason and +L+ Giving:: Means are included, as illustrated in Fig- +L+ ure 4. If c� = St:: State of affairs, we cannot dis- +L+ ambiguate which original role is correct. In ad- +L+ dition, it may be more effective to use various +L+ </SectLabel_bodyText> <SectLabel_page> 22 +L+ </SectLabel_page> <SectLabel_bodyText> groupings of roles together in the model. For in- +L+ stance, the model could predict the correct role +L+ Commerce sell::Seller for the phrase “you” in +L+ Figure 3 more confidently, if it could infer its +L+ thematic-role group as Theme::Agent and its par- +L+ ent group Giving::Donor correctly. Although the +L+ ensemble of various groupings seems promising, +L+ we need an additional procedure to prioritize the +L+ groupings for the case where the models for mul- +L+ tiple role groupings disagree; for example, it is un- +L+ satisfactory if two models assign the groups Giv- +L+ ing:: Theme and Theme::Agent to the same phrase. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Role groups as feature functions +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We thus propose another approach that incorpo- +L+ rates group information as feature functions. We +L+ model the conditional probability P(y� f, x) by us- +L+ ing the maximum entropy framework, +L+ </SectLabel_bodyText> <SectLabel_equation> exp(Ei Aigi(x, y)) +L+ �( y� f, x) = Ey,Y, exp(Ei Aigi(x, y)). (3) +L+ </SectLabel_equation> <SectLabel_bodyText> Here, G = Jgi� denotes a set of n feature func- +L+ tions, and A = JAi� denotes a weight vector for +L+ the feature functions. +L+ In general, feature functions for the maximum +L+ entropy model are designed as indicator functions +L+ for possible pairs of xj and y. For example, the +L+ event where the head word of x is “you” (xi = 1) +L+ and x plays the role Commerce sell::Seller in a +L+ sentence is expressed by the indicator function, +L+ </SectLabel_bodyText> <SectLabel_equation> 1 (xi =1A +L+ y = Commerce sell::Seller) +L+ 0 (otherwise) +L+ (4) +L+ </SectLabel_equation> <SectLabel_bodyText> We call this kind of feature function an x-role. +L+ In order to incorporate role groups into the +L+ model, we also include all feature functions for +L+ possible pairs of xj and role groups. Equation 5 +L+ is an example of a feature function for instances +L+ where the head word of x is “you” and y is in the +L+ role group Theme::Agent, +L+ </SectLabel_bodyText> <SectLabel_equation> 1 (xi=1A +L+ y E Theme::Agent) . (5) +L+ 0 (otherwise) +L+ </SectLabel_equation> <SectLabel_bodyText> Thus, this feature function fires for the roles wher- +L+ ever the head word “you” plays Agent (e.g., Com- +L+ merce sell::Seller, Commerce buy::Buyer and +L+ Giving:: Donor). We call this kind of feature func- +L+ tion an x-group function. +L+ In this way, we obtain x-group functions for +L+ all grouping methods, e.g., gtheme +L+ k, ghierarchy +L+ k . +L+ The role-group features will receive more training +L+ instances by collecting instances for fine-grained +L+ roles. Thus, semantic roles with few training in- +L+ stances are expected to receive additional clues +L+ from other training instances via role-group fea- +L+ tures. Another advantage of this approach is that +L+ the usefulness of the different role groups is de- +L+ termined by the training processes in terms of +L+ weights of feature functions. Thus, we do not need +L+ to assume that we have found the best criterion for +L+ grouping roles; we can allow a training process to +L+ choose the criterion. We will discuss the contribu- +L+ tions of different groupings in the experiments. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.3 Comparison with related work +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Baldewein et al. (2004) suggested an approach +L+ that uses role descriptors and hierarchical rela- +L+ tions as criteria for generalizing semantic roles +L+ in FrameNet. They created a classifier for each +L+ frame, additionally using training instances for the +L+ role A to train the classifier for the role B, if the +L+ roles A and B were judged as similar by a crite- +L+ rion. This approach performs similarly to the over- +L+ writing approach, and it may obscure the differ- +L+ ences among roles. Therefore, they only re-used +L+ the descriptors as a similarity measure for the roles +L+ whose coreness was peripheral. 2 +L+ In contrast, we use all kinds of role descriptors +L+ to construct groups. Since we use the feature func- +L+ tions for both the original roles and their groups, +L+ appropriate units for classification are determined +L+ automatically in the training process. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Experiment and Discussion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We used the training set of the Semeval-2007 +L+ Shared task (Baker et al., 2007) in order to ascer- +L+ tain the contributions of role groups. This dataset +L+ consists of the corpus of FrameNet release 1.3 +L+ (containing roughly 150,000 annotations), and an +L+ additional full-text annotation dataset. We ran- +L+ domly extracted 10% of the dataset for testing, and +L+ used the remainder (90%) for training. +L+ Performance was measured by micro- and +L+ macro-averaged F 1(Chang and Zheng, 2008) with +L+ respect to a variety of roles. The micro average bi- +L+ ases each F1 score by the frequencies of the roles, +L+ </SectLabel_bodyText> <SectLabel_footnote> 2In FrameNet, each role is assigned one of four different +L+ types of coreness (core, core-unexpressed, peripheral, extra- +L+ thematic) It represents the conceptual necessity of the roles +L+ in the frame to which it belongs. +L+ </SectLabel_footnote> <SectLabel_equation> giole(x, y) = { +L+ . +L+ gtheme +L+ 2(x, y) = +L+ { +L+ </SectLabel_equation> <SectLabel_page> 23 +L+ </SectLabel_page> <SectLabel_bodyText> and the average is equal to the classification accu- +L+ racy when we calculate it with all of the roles in +L+ the test set. In contrast, the macro average does +L+ not bias the scores, thus the roles having a small +L+ number of instances affect the average more than +L+ the micro average. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.1 Experimental settings +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We constructed a baseline classifier that uses +L+ only the x-role features. The feature de- +L+ sign is similar to that of the previous stud- +L+ ies (M`arquez et al., 2008). The characteristics +L+ of x are: frame, frame evoking word, head +L+ word, content word (Surdeanu et al., 2003), +L+ first/last word, head word of left/right sister, +L+ phrase type, position, voice, syntactic path (di- +L+ rected/undirected/partial), governing category +L+ (Gildea and Jurafsky, 2002), WordNet super- +L+ sense in the phrase, combination features of +L+ frame evoking word & headword, combination +L+ features of frame evoking word & phrase type, +L+ and combination features of voice & phrase type. +L+ We also used PoS tags and stem forms as extra +L+ features of any word-features. +L+ We employed Charniak and Johnson’s rerank- +L+ ing parser (Charniak and Johnson, 2005) to an- +L+ alyze syntactic trees. As an alternative for the +L+ traditional named-entity features, we used Word- +L+ Net supersenses: 41 coarse-grained semantic cate- +L+ gories of words such as person, plant, state, event, +L+ time, location. We used Ciaramita and Altun’s Su- +L+ per Sense Tagger (Ciaramita and Altun, 2006) to +L+ tag the supersenses. The baseline system achieved +L+ 89.00% with respect to the micro-averaged F1. +L+ The x-group features were instantiated similarly +L+ to the x-role features; the x-group features com- +L+ bined the characteristics of x with the role groups +L+ presented in this paper. The total number of fea- +L+ tures generated for all x-roles and x-groups was +L+ 74,873,602. The optimal weights A of the fea- +L+ tures were obtained by the maximum a poste- +L+ rior (MAP) estimation. We maximized an L2- +L+ regularized log-likelihood of the training set us- +L+ ing the Limited-memory BFGS (L-BFGS) method +L+ (Nocedal, 1980). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.2 Effect of role groups +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Table 1 shows the micro and macro averages of F 1 +L+ scores. Each role group type improved the micro +L+ average by 0.5 to 1.7 points. The best result was +L+ obtained by using all types of groups together. The +L+ result indicates that different kinds of group com- +L+ </SectLabel_bodyText> <SectLabel_table> Feature	Micro	Macro	-Err. +L+ Baseline	89.00	68.50	0.00 +L+ role descriptor	90.78	76.58	16.17 +L+ role descriptor (replace)	90.23	76.19	11.23 +L+ hierarchical relation	90.25	72.41	11.40 +L+ semantic type	90.36	74.51	12.38 +L+ VN thematic role	89.50	69.21	4.52 +L+ All	91.10	75.92	19.16 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: The accuracy and error reduction rate of +L+ role classification for each type of role group. +L+ </SectLabel_tableCaption> <SectLabel_table> Feature	#instances	Pre.	Rec.	Micro +L+ baseline	< 10	63.89	38.00	47.66 +L+ 	< 20	69.01	51.26	58.83 +L+ 	<50	75.84	65.85	70.50 +L+ + all groups	< 10	72.57	55.85	63.12 +L+ 	<20	76.30	65.41	70.43 +L+ 	< 50	80.86	74.59	77.60 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: The effect of role groups on the roles with +L+ few instances. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> plement each other with respect to semantic role +L+ generalization. Baldewein et al. (2004) reported +L+ that hierarchical relations did not perform well for +L+ their method and experimental setting; however, +L+ we found that significant improvements could also +L+ be achieved with hierarchical relations. We also +L+ tried a traditional label-replacing approach with +L+ role descriptors (in the third row of Table 1). The +L+ comparison between the second and third rows in- +L+ dicates that mixing the original fine-grained roles +L+ and the role groups does result in a more accurate +L+ classification. +L+ By using all types of groups together, the +L+ model reduced 19.16 % of the classification errors +L+ from the baseline. Moreover, the macro-averaged +L+ F1 scores clearly showed improvements resulting +L+ from using role groups. In order to determine +L+ the reason for the improvements, we measured +L+ the precision, recall, and F1-scores with respect +L+ to roles for which the number of training instances +L+ was at most 10, 20, and 50. In Table 2, we show +L+ that the micro-averaged F1 score for roles hav- +L+ ing 10 instances or less was improved (by 15.46 +L+ points) when all role groups were used. This result +L+ suggests the reason for the effect of role groups; by +L+ bridging similar semantic roles, they supply roles +L+ having a small number of instances with the infor- +L+ mation from other roles. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.3 Analyses of role descriptors +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In Table 1, the largest improvement was obtained +L+ by the use of role descriptors. We analyze the ef- +L+ fect of role descriptors in detail in Tables 3 and 4. +L+ Table 3 shows the micro-averaged F 1 scores of all +L+ </SectLabel_bodyText> <SectLabel_page> 24 +L+ </SectLabel_page> <SectLabel_table> Coreness	#roles	#instances/#role	#groups	#instances/#group	#roles/#group +L+ Core	1902	122.06	655	354.4	2.9 +L+ Peripheral	1924	25.24	250	194.3	7.7 +L+ Extra-thematic	763	13.90	171	62.02	4.5 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4: The analysis of the numbers of roles, instances, and role-descriptor groups, for each type of +L+ coreness. +L+ </SectLabel_tableCaption> <SectLabel_table> Coreness	Micro +L+ Baseline	89.00 +L+ Core	89.51 +L+ Peripheral	90.12 +L+ Extra-thematic	89.09 +L+ All	90.77 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3: The effect of employing role-descriptor +L+ </SectLabel_tableCaption> <SectLabel_bodyText> groups of each type of coreness. +L+ semantic roles when we use role-descriptor groups +L+ constructed from each type of coreness (core3, pe- +L+ ripheral, and extra-thematic) individually. The pe- +L+ ripheral type generated the largest improvements. +L+ Table 4 shows the number of roles associated +L+ with each type of coreness (#roles), the number of +L+ instances for the original roles (#instances/#role), +L+ the number of groups for each type of coreness +L+ (#groups), the number of instances for each group +L+ (#instances/#group), and the number of roles per +L+ each group (#roles/#group). In the peripheral +L+ type, the role descriptors subdivided 1,924 distinct +L+ roles into 250 groups, each of which contained 7.7 +L+ roles on average. The peripheral type included +L+ semantic roles such as place, time, reason, dura- +L+ tion. These semantic roles appear in many frames, +L+ because they have general meanings that can be +L+ shared by different frames. Moreover, the seman- +L+ tic roles of peripheral type originally occurred in +L+ only a small number (25.24) of training instances +L+ on average. Thus, we infer that the peripheral +L+ type generated the largest improvement because +L+ semantic roles in this type acquired the greatest +L+ benefit from the generalization. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.4 Hierarchical relations and relation types +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We analyzed the contributions of the FrameNet hi- +L+ erarchy for each type of role-to-role relations and +L+ for different depths of grouping. Table 5 shows +L+ the micro-averaged F1 scores obtained from var- +L+ ious relation types and depths. The Inheritance +L+ and Using relations resulted in a slightly better ac- +L+ curacy than the other types. We did not observe +L+ any real differences among the remaining five re- +L+ lation types, possibly because there were few se- +L+ </SectLabel_bodyText> <SectLabel_footnote> 3 We include Core-unexpressed in core, because it has a +L+ property of core inside one frame. +L+ </SectLabel_footnote> <SectLabel_table> No.	Relation Type	Micro +L+ -	baseline	89.00 +L+ 1	+ Inheritance (children)	89.52 +L+ 2	+ Inheritance (descendants)	89.70 +L+ 3	+Using (children)	89.35 +L+ 4	+Using (descendants)	89.37 +L+ 5	+ Perspective on (children)	89.01 +L+ 6	+ Perspective on (descendants)	89.01 +L+ 7	+ Subframe (children)	89.04 +L+ 8	+ Subframe (descendants)	89.05 +L+ 9	+ Causative of (parents)	89.03 +L+ 10	+ Causative of (ancestors)	89.03 +L+ 11	+ Inchoative of (parents)	89.02 +L+ 12	+ Inchoative of (ancestors)	89.02 +L+ 13	+Precedes (children)	89.01 +L+ 14	+Precedes (descendants)	89.03 +L+ 15	+Precedes (parents)	89.00 +L+ 16	+Precedes (ancestors)	89.00 +L+ 18	+all relations (2,4,6,8,10,12,14)	90.25 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5: Comparison of the accuracy with differ- +L+ ent types of hierarchical relations. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> mantic roles associated with these types. We ob- +L+ tained better results by using not only groups for +L+ parent roles, but also groups for all ancestors. The +L+ best result was obtained by using all relations in +L+ the hierarchy. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.5 Analyses of different grouping criteria +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Table 6 reports the precision, recall, and micro- +L+ averaged F1 scores of semantic roles with respect +L+ to each coreness type .4 In general, semantic roles +L+ of the core coreness were easily identified by all +L+ of the grouping criteria; even the baseline system +L+ obtained an F 1 score of 91.93. For identifying se- +L+ mantic roles of the peripheral and extra-thematic +L+ types of coreness, the simplest solution, the de- +L+ scriptor criterion, outperformed other criteria. +L+ In Table 7, we categorize feature functions +L+ whose weights are in the top 1000 in terms of +L+ greatest absolute value. The behaviors of the role +L+ groups can be distinguished by the following two +L+ characteristics. Groups of role descriptors and se- +L+ mantic types have large weight values for the first +L+ word and supersense features, which capture the +L+ characteristics of adjunctive phrases. The original +L+ roles and hierarchical-relation groups have strong +L+ </SectLabel_bodyText> <SectLabel_footnote> 4The figures of role descriptors in Tables 4 and 6 differ. +L+ In Table 4, we measured the performance when we used one +L+ or all types of coreness for training. In contrast, in Table 6, +L+ we used all types of coreness for training, but computed the +L+ performance of semantic roles for each coreness separately. +L+ </SectLabel_footnote> <SectLabel_page> 25 +L+ </SectLabel_page> <SectLabel_table> Feature	Type	Pre.	Rec.	Micro +L+ baseline	c	91.07	92.83	91.93 +L+ 	p	81.05	76.03	78.46 +L+ 	e	78.17	66.51	71.87 +L+ + descriptor group	c	92.50	93.41	92.95 +L+ 	p	84.32	82.72	83.51 +L+ 	e	80.91	69.59	74.82 +L+ +hierarchical	c	92.10	93.28	92.68 +L+ relation	p	82.23	79.84	81.01 +L+ class	e	77.94	65.58	71.23 +L+ + semantic	c	92.23	93.31	92.77 +L+ type group	p	83.66	81.76	82.70 +L+ 	e	80.29	67.26	73.20 +L+ + VN thematic	c	91.57	93.06	92.31 +L+ role group	p	80.66	76.95	78.76 +L+ 	e	78.12	66.60	71.90 +L+ + all group	c	92.66	93.61	93.13 +L+ 	p	84.13	82.51	83.31 +L+ 	e	80.77	68.56	74.17 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 6: The precision and recall of each type of +L+ coreness with role groups. Type represents the +L+ type of coreness; c denotes core, p denotes periph- +L+ eral, and e denotes extra-thematic. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> associations with lexical and structural character- +L+ istics such as the syntactic path, content word, and +L+ head word. Table 7 suggests that role-descriptor +L+ groups and semantic-type groups are effective for +L+ peripheral or adjunctive roles, and hierarchical re- +L+ lation groups are effective for core roles. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7 Conclusion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have described different criteria for general- +L+ izing semantic roles in FrameNet. They were: +L+ role hierarchy, human-understandable descriptors +L+ of roles, semantic types of filler phrases, and +L+ mappings from FrameNet roles to thematic roles +L+ of VerbNet. We also proposed a feature design +L+ that combines and weights these criteria using the +L+ training data. The experimental result of the role +L+ classification task showed a 19.16% of the error +L+ reduction and a 7.42% improvement in the macro- +L+ averaged F1 score. In particular, the method we +L+ have presented was able to classify roles having +L+ few instances. We confirmed that modeling the +L+ role generalization at feature level was better than +L+ the conventional approach that replaces semantic +L+ role labels. +L+ Each criterion presented in this paper improved +L+ the accuracy of classification. The most success- +L+ ful criterion was the use of human-understandable +L+ role descriptors. Unfortunately, the FrameNet hi- +L+ erarchy did not outperform the role descriptors, +L+ contrary to our expectations. A future direction +L+ of this study would be to analyze the weakness of +L+ the FrameNet hierarchy in order to discuss possi- +L+ ble improvement of the usage and annotations of +L+ </SectLabel_bodyText> <SectLabel_table> features of x	class type +L+ 	or	hr	rl	st	vn +L+ frame	0	4	0	1	0 +L+ evoking word	3	4	7	3	0 +L+ ew & hw stem	9	34	20	8	0 +L+ ew & phrase type	11	7	11	3	1 +L+ head word	13	19	8	3	1 +L+ hw stem	11	17	8	8	1 +L+ content word	7	19	12	3	0 +L+ cw stem	11	26	13	5	0 +L+ cw PoS	4	5	14	15	2 +L+ directed path	19	27	24	6	7 +L+ undirected path	21	35	17	2	6 +L+ partial path	15	18	16	13	5 +L+ last word	15	18	12	3	2 +L+ first word	11	23	53	26	10 +L+ supersense	7	7	35	25	4 +L+ position	4	6	30	9	5 +L+ others	27	29	33	19	6 +L+ total	188	298	313	152	50 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 7: The analysis of the top 1000 feature func- +L+ tions. Each number denotes the number of feature +L+ functions categorized in the corresponding cell. +L+ Notations for the columns are as follows. ‘or’: +L+ original role, ‘hr’: hierarchical relation, ‘rd’: role +L+ descriptor, ‘st’: semantic type, and ‘vn’: VerbNet +L+ thematic role. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> the hierarchy. +L+ Since we used the latest release of FrameNet +L+ in order to use a greater number of hierarchical +L+ role-to-role relations, we could not make a direct +L+ comparison of performance with that of existing +L+ systems; however we may say that the 89.00% F 1 +L+ micro-average of our baseline system is roughly +L+ comparable to the 88.93% value of Bejan and +L+ Hathaway (2007) for SemEval-2007 (Baker et al., +L+ 2007). 5 In addition, the methodology presented in +L+ this paper applies generally to any SRL resources; +L+ we are planning to determine several grouping cri- +L+ teria from existing linguistic resources and to ap- +L+ ply the methodology to the PropBank corpus. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgments +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The authors thank Sebastian Riedel for his useful +L+ comments on our work. This work was partially +L+ supported by Grant-in-Aid for Specially Promoted +L+ Research (MEXT, Japan). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Collin F. Baker, Charles J. Fillmore, and John B. Lowe. +L+ 1998. The berkeley framenet project. In Proceed- +L+ ings of Coling -ACL 1998, pages 86–90. +L+ Collin Baker, Michael Ellsworth, and Katrin Erk. +L+ 2007. Semeval-2007 task 19: Frame semantic struc- +L+ </SectLabel_reference> <SectLabel_footnote> 5There were two participants that performed whole SRL +L+ in SemEval-2007. Bejan and Hathaway (2007) evaluated role +L+ classification accuracy separately for the training data. +L+ </SectLabel_footnote> <SectLabel_page> 26 +L+ </SectLabel_page> <SectLabel_reference> ture extraction. In Proceedings of SemEval-2007, +L+ pages 99–104. +L+ Ulrike Baldewein, Katrin Erk, Sebastian Pad´o, and +L+ Detlef Prescher. 2004. Semantic role labeling +L+ with similarity based generalization using EM-based +L+ clustering. In Proceedings of Senseval-3, pages 64– +L+ 68. +L+ Cosmin Adrian Bejan and Chris Hathaway. 2007. +L+ UTD-SRL: A Pipeline Architecture for Extract- +L+ ing Frame Semantic Structures. In Proceedings +L+ of SemEval-2007, pages 460–463. Association for +L+ Computational Linguistics. +L+ X. Chang and Q. Zheng. 2008. Knowledge Ele- +L+ ment Extraction for Knowledge-Based Learning Re- +L+ sources Organization. Lecture Notes in Computer +L+ Science, 4823:102–113. +L+ Eugene Charniak and Mark Johnson. 2005. Coarse- +L+ to-fine n-best parsing and MaxEnt discriminative +L+ reranking. In Proceedings of the 43rd Annual Meet- +L+ ing on Association for Computational Linguistics, +L+ pages 173–180. +L+ Massimiliano Ciaramita and Yasemin Altun. 2006. +L+ Broad-coverage sense disambiguation and informa- +L+ tion extraction with a supersense sequence tagger. In +L+ Proceedings ofEMNLP-2006, pages 594–602. +L+ Daniel Gildea and Daniel Jurafsky. 2002. Automatic +L+ labeling of semantic roles. Computational Linguis- +L+ tics, 28(3):245–288. +L+ Ana-Maria Giuglea and Alessandro Moschitti. 2006. +L+ Semantic role labeling via FrameNet, VerbNet and +L+ PropBank. In Proceedings of the 21st International +L+ Conference on Computational Linguistics and the +L+ 44th Annual Meeting of the ACL, pages 929–936. +L+ Andrew Gordon and Reid Swanson. 2007. General- +L+ izing semantic role annotations across syntactically +L+ similar verbs. In Proceedings of ACL-2007, pages +L+ 192–199. +L+ Edward Loper, Szu-ting Yi, and Martha Palmer. 2007. +L+ Combining lexical resources: Mapping between +L+ propbank and verbnet. In Proceedings of the 7th In- +L+ ternational Workshop on Computational Semantics, +L+ pages 118–128. +L+ Lluis M`arquez, Xavier Carreras, Kenneth C. +L+ Litkowski, and Suzanne Stevenson. 2008. Se- +L+ mantic role labeling: an introduction to the special +L+ issue. Computational linguistics, 34(2):145–159. +L+ Alessandro Moschitti, Ana-Maria Giuglea, Bonaven- +L+ tura Coppola, and Roberto Basili. 2005. Hierar- +L+ chical semantic role labeling. In Proceedings of +L+ CoNLL-2005, pages 201–204. +L+ Alessandro Moschitti, Silvia Quarteroni, Roberto +L+ Basili, and Suresh Manandhar. 2007. Exploiting +L+ syntactic and shallow semantic kernels for question +L+ answer classification. In Proceedings of ACL-07, +L+ pages 776–783. +L+ Srini Narayanan and Sanda Harabagiu. 2004. Ques- +L+ tion answering based on semantic structures. In Pro- +L+ ceedings of Coling-2004, pages 693–701. +L+ Jorge Nocedal. 1980. Updating quasi-newton matrices +L+ with limited storage. Mathematics of Computation, +L+ 35(151):773–782. +L+ Martha Palmer, Daniel Gildea, and Paul Kingsbury. +L+ 2005. The proposition bank: An annotated cor- +L+ pus of semantic roles. Computational Linguistics, +L+ 31(1):71–106. +L+ Dan Shen and Mirella Lapata. 2007. Using semantic +L+ roles to improve question answering. In Proceed- +L+ ings ofEMNLP-CoNLL 2007, pages 12–21. +L+ Lei Shi and Rada Mihalcea. 2005. Putting Pieces To- +L+ gether: Combining FrameNet, VerbNet and Word- +L+ Net for Robust Semantic Parsing. In Proceedings of +L+ CICLing-2005, pages 100–111. +L+ Mihai Surdeanu, Sanda Harabagiu, John Williams, and +L+ Paul Aarseth. 2003. Using predicate-argument +L+ structures for information extraction. In Proceed- +L+ ings ofACL-2003, pages 8–15. +L+ Szu-ting Yi, Edward Loper, and Martha Palmer. 2007. +L+ Can semantic roles generalize across genres? In +L+ Proceedings ofHLT-NAACL 2007, pages 548–555. +L+ Be˜nat Zapirain, Eneko Agirre, and Lluis M`arquez. +L+ 2008. Robustness and generalization of role sets: +L+ PropBank vs. VerbNet. In Proceedings of ACL-08: +L+ HLT, pages 550–558. +L+ </SectLabel_reference> <SectLabel_page> 27 +L+ </SectLabel_page>
<SectLabel_title> Unsupervised Argument Identification for Semantic Role Labeling +L+ </SectLabel_title> <SectLabel_author> Omri Abend' Roi Reichart2 Ari Rappoport' +L+ </SectLabel_author> <SectLabel_affiliation> 'Institute of Computer Science, 2ICNC +L+ Hebrew University of Jerusalem +L+ </SectLabel_affiliation> <SectLabel_email> {omria01|roiri|arir}@cs.huji.ac.il +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The task of Semantic Role Labeling +L+ (SRL) is often divided into two sub-tasks: +L+ verb argument identification, and argu- +L+ ment classification. Current SRL algo- +L+ rithms show lower results on the identifi- +L+ cation sub-task. Moreover, most SRL al- +L+ gorithms are supervised, relying on large +L+ amounts of manually created data. In +L+ this paper we present an unsupervised al- +L+ gorithm for identifying verb arguments, +L+ where the only type of annotation required +L+ is POS tagging. The algorithm makes use +L+ of a fully unsupervised syntactic parser, +L+ using its output in order to detect clauses +L+ and gather candidate argument colloca- +L+ tion statistics. We evaluate our algorithm +L+ on PropBank10, achieving a precision of +L+ 56%, as opposed to 47% of a strong base- +L+ line. We also obtain an 8% increase in +L+ precision for a Spanish corpus. This is +L+ the first paper that tackles unsupervised +L+ verb argument identification without using +L+ manually encoded rules or extensive lexi- +L+ cal or syntactic resources. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Semantic Role Labeling (SRL) is a major NLP +L+ task, providing a shallow sentence-level semantic +L+ analysis. SRL aims at identifying the relations be- +L+ tween the predicates (usually, verbs) in the sen- +L+ tence and their associated arguments. +L+ The SRL task is often viewed as consisting of +L+ two parts: argument identification (ARGID) and ar- +L+ gument classification. The former aims at identi- +L+ fying the arguments of a given predicate present +L+ in the sentence, while the latter determines the +L+ type of relation that holds between the identi- +L+ fied arguments and their corresponding predicates. +L+ The division into two sub-tasks is justified by +L+ the fact that they are best addressed using differ- +L+ ent feature sets (Pradhan et al., 2005). Perfor- +L+ mance in the ARGID stage is a serious bottleneck +L+ for general SRL performance, since only about +L+ 81% of the arguments are identified, while about +L+ 95% of the identified arguments are labeled cor- +L+ rectly (M`arquez et al., 2008). +L+ SRL is a complex task, which is reflected by the +L+ algorithms used to address it. A standard SRL al- +L+ gorithm requires thousands to dozens of thousands +L+ sentences annotated with POS tags, syntactic an- +L+ notation and SRL annotation. Current algorithms +L+ show impressive results but only for languages and +L+ domains where plenty of annotated data is avail- +L+ able, e.g., English newspaper texts (see Section 2). +L+ Results are markedly lower when testing is on a +L+ domain wider than the training one, even in En- +L+ glish (see the WSJ-Brown results in (Pradhan et +L+ al., 2008)). +L+ Only a small number of works that do not re- +L+ quire manually labeled SRL training data have +L+ been done (Swier and Stevenson, 2004; Swier and +L+ Stevenson, 2005; Grenager and Manning, 2006). +L+ These papers have replaced this data with the +L+ VerbNet (Kipper et al., 2000) lexical resource or +L+ a set of manually written rules and supervised +L+ parsers. +L+ A potential answer to the SRL training data bot- +L+ tleneck are unsupervised SRL models that require +L+ little to no manual effort for their training. Their +L+ output can be used either by itself, or as training +L+ material for modern supervised SRL algorithms. +L+ In this paper we present an algorithm for unsu- +L+ pervised argument identification. The only type of +L+ annotation required by our algorithm is POS tag- +L+ </SectLabel_bodyText> <SectLabel_page> 28 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 28–36, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> ging, which needs relatively little manual effort. +L+ The algorithm consists of two stages. As pre- +L+ processing, we use a fully unsupervised parser to +L+ parse each sentence. Initially, the set of possi- +L+ ble arguments for a given verb consists of all the +L+ constituents in the parse tree that do not contain +L+ that predicate. The first stage of the algorithm +L+ attempts to detect the minimal clause in the sen- +L+ tence that contains the predicate in question. Us- +L+ ing this information, it further reduces the possible +L+ arguments only to those contained in the minimal +L+ clause, and further prunes them according to their +L+ position in the parse tree. In the second stage we +L+ use pointwise mutual information to estimate the +L+ collocation strength between the arguments and +L+ the predicate, and use it to filter out instances of +L+ weakly collocating predicate argument pairs. +L+ We use two measures to evaluate the perfor- +L+ mance of our algorithm, precision and F-score. +L+ Precision reflects the algorithm’s applicability for +L+ creating training data to be used by supervised +L+ SRL models, while the standard SRL F-score mea- +L+ sures the model’s performance when used by it- +L+ self. The first stage of our algorithm is shown to +L+ outperform a strong baseline both in terms of F- +L+ score and of precision. The second stage is shown +L+ to increase precision while maintaining a reason- +L+ able recall. +L+ We evaluated our model on sections 2-21 of +L+ Propbank. As is customary in unsupervised pars- +L+ ing work (e.g. (Seginer, 2007)), we bounded sen- +L+ tence length by 10 (excluding punctuation). Our +L+ first stage obtained a precision of 52.8%, which is +L+ more than 6% improvement over the baseline. Our +L+ second stage improved precision to nearly 56%, a +L+ 9.3% improvement over the baseline. In addition, +L+ we carried out experiments on Spanish (on sen- +L+ tences of length bounded by 15, excluding punctu- +L+ ation), achieving an increase of over 7.5% in pre- +L+ cision over the baseline. Our algorithm increases +L+ F–score as well, showing an 1.8% improvement +L+ over the baseline in English and a 2.2% improve- +L+ ment in Spanish. +L+ Section 2 reviews related work. In Section 3 we +L+ detail our algorithm. Sections 4 and 5 describe the +L+ experimental setup and results. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The advance of machine learning based ap- +L+ proaches in this field owes to the usage of large +L+ scale annotated corpora. English is the most stud- +L+ ied language, using the FrameNet (FN) (Baker et +L+ al., 1998) and PropBank (PB) (Palmer et al., 2005) +L+ resources. PB is a corpus well suited for evalu- +L+ ation, since it annotates every non-auxiliary verb +L+ in a real corpus (the WSJ sections of the Penn +L+ Treebank). PB is a standard corpus for SRL eval- +L+ uation and was used in the CoNLL SRL shared +L+ tasks of 2004 (Carreras and M`arquez, 2004) and +L+ 2005 (Carreras and M`arquez, 2005). +L+ Most work on SRL has been supervised, requir- +L+ ing dozens of thousands of SRL annotated train- +L+ ing sentences. In addition, most models assume +L+ that a syntactic representation of the sentence is +L+ given, commonly in the form of a parse tree, a de- +L+ pendency structure or a shallow parse. Obtaining +L+ these is quite costly in terms of required human +L+ annotation. +L+ The first work to tackle SRL as an indepen- +L+ dent task is (Gildea and Jurafsky, 2002), which +L+ presented a supervised model trained and evalu- +L+ ated on FrameNet. The CoNLL shared tasks of +L+ 2004 and 2005 were devoted to SRL, and stud- +L+ ied the influence of different syntactic annotations +L+ and domain changes on SRL results. Computa- +L+ tional Linguistics has recently published a special +L+ issue on the task (M`arquez et al., 2008), which +L+ presents state-of-the-art results and surveys the lat- +L+ est achievements and challenges in the field. +L+ Most approaches to the task use a multi-level +L+ approach, separating the task to an ARGID and an +L+ argument classification sub-tasks. They then use +L+ the unlabeled argument structure (without the se- +L+ mantic roles) as training data for the ARGID stage +L+ and the entire data (perhaps with other features) +L+ for the classification stage. Better performance +L+ is achieved on the classification, where state- +L+ of-the-art supervised approaches achieve about +L+ 81% F-score on the in-domain identification task, +L+ of which about 95% are later labeled correctly +L+ (M`arquez et al., 2008). +L+ There have been several exceptions to the stan- +L+ dard architecture described in the last paragraph. +L+ One suggestion poses the problem of SRL as a se- +L+ quential tagging of words, training an SVM clas- +L+ sifier to determine for each word whether it is in- +L+ side, outside or in the beginning of an argument +L+ (Hacioglu and Ward, 2003). Other works have in- +L+ tegrated argument classification and identification +L+ into one step (Collobert and Weston, 2007), while +L+ others went further and combined the former two +L+ along with parsing into a single model (Musillo +L+ </SectLabel_bodyText> <SectLabel_page> 29 +L+ </SectLabel_page> <SectLabel_bodyText> and Merlo, 2006). +L+ Work on less supervised methods has been +L+ scarce. Swier and Stevenson (2004) and Swier +L+ and Stevenson (2005) presented the first model +L+ that does not use an SRL annotated corpus. How- +L+ ever, they utilize the extensive verb lexicon Verb- +L+ Net, which lists the possible argument structures +L+ allowable for each verb, and supervised syntac- +L+ tic tools. Using VerbNet along with the output of +L+ a rule-based chunker (in 2004) and a supervised +L+ syntactic parser (in 2005), they spot instances in +L+ the corpus that are very similar to the syntactic +L+ patterns listed in VerbNet. They then use these as +L+ seed for a bootstrapping algorithm, which conse- +L+ quently identifies the verb arguments in the corpus +L+ and assigns their semantic roles. +L+ Another less supervised work is that +L+ of (Grenager and Manning, 2006), which presents +L+ a Bayesian network model for the argument +L+ structure of a sentence. They use EM to learn +L+ the model’s parameters from unannotated data, +L+ and use this model to tag a test corpus. However, +L+ ARGID was not the task of that work, which dealt +L+ solely with argument classification. ARGID was +L+ performed by manually-created rules, requiring a +L+ supervised or manual syntactic annotation of the +L+ corpus to be annotated. +L+ The three works above are relevant but incom- +L+ parable to our work, due to the extensive amount +L+ of supervision (namely, VerbNet and a rule-based +L+ or supervised syntactic system) they used, both in +L+ detecting the syntactic structure and in detecting +L+ the arguments. +L+ Work has been carried out in a few other lan- +L+ guages besides English. Chinese has been studied +L+ in (Xue, 2008). Experiments on Catalan and Span- +L+ ish were done in SemEval 2007 (M`arquez et al., +L+ 2007) with two participating systems. Attempts +L+ to compile corpora for German (Burdchardt et al., +L+ 2006) and Arabic (Diab et al., 2008) are also un- +L+ derway. The small number of languages for which +L+ extensive SRL annotated data exists reflects the +L+ considerable human effort required for such en- +L+ deavors. +L+ Some SRL works have tried to use unannotated +L+ data to improve the performance of a base su- +L+ pervised model. Methods used include bootstrap- +L+ ping approaches (Gildea and Jurafsky, 2002; Kate +L+ and Mooney, 2007), where large unannotated cor- +L+ pora were tagged with SRL annotation, later to +L+ be used to retrain the SRL model. Another ap- +L+  proach used similarity measures either between +L+ verbs (Gordon and Swanson, 2007) or between +L+ nouns (Gildea and Jurafsky, 2002) to overcome +L+ lexical sparsity. These measures were estimated +L+ using statistics gathered from corpora augmenting +L+ the model’s training data, and were then utilized +L+ to generalize across similar verbs or similar argu- +L+ ments. +L+ Attempts to substitute full constituency pars- +L+ ing by other sources of syntactic information have +L+ been carried out in the SRL community. Sugges- +L+ tions include posing SRL as a sequence labeling +L+ problem (M`arquez et al., 2005) or as an edge tag- +L+ ging problem in a dependency representation (Ha- +L+ cioglu, 2004). Punyakanok et al. (2008) provide +L+ a detailed comparison between the impact of us- +L+ ing shallow vs. full constituency syntactic infor- +L+ mation in an English SRL system. Their results +L+ clearly demonstrate the advantage of using full an- +L+ notation. +L+ The identification of arguments has also been +L+ carried out in the context of automatic subcatego- +L+ rization frame acquisition. Notable examples in- +L+ clude (Manning, 1993; Briscoe and Carroll, 1997; +L+ Korhonen, 2002) who all used statistical hypothe- +L+ sis testing to filter a parser’s output for arguments, +L+ with the goal of compiling verb subcategorization +L+ lexicons. However, these works differ from ours +L+ as they attempt to characterize the behavior of a +L+ verb type, by collecting statistics from various in- +L+ stances of that verb, and not to determine which +L+ are the arguments of specific verb instances. +L+ The algorithm presented in this paper performs +L+ unsupervised clause detection as an intermedi- +L+ ate step towards argument identification. Super- +L+ vised clause detection was also tackled as a sepa- +L+ rate task, notably in the CoNLL 2001 shared task +L+ (Tjong Kim Sang and D`ejean, 2001). Clause in- +L+ formation has been applied to accelerating a syn- +L+ tactic parser (Glaysher and Moldovan, 2006). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Algorithm +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we describe our algorithm. It con- +L+ sists of two stages, each of which reduces the set +L+ of argument candidates, which a-priori contains all +L+ consecutive sequences of words that do not con- +L+ tain the predicate in question. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Algorithm overview +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As pre-processing, we use an unsupervised parser +L+ that generates an unlabeled parse tree for each sen- +L+ </SectLabel_bodyText> <SectLabel_page> 30 +L+ </SectLabel_page> <SectLabel_bodyText> tence (Seginer, 2007). This parser is unique in that +L+ it is able to induce a bracketing (unlabeled pars- +L+ ing) from raw text (without even using POS tags) +L+ achieving state-of-the-art results. Since our algo- +L+ rithm uses millions to tens of millions sentences, +L+ we must use very fast tools. The parser’s high +L+ speed (thousands of words per second) enables us +L+ to process these large amounts of data. +L+ The only type of supervised annotation we +L+ use is POS tagging. We use the taggers MX- +L+ POST (Ratnaparkhi, 1996) for English and Tree- +L+ Tagger (Schmid, 1994) for Spanish, to obtain POS +L+ tags for our model. +L+ The first stage of our algorithm uses linguisti- +L+ cally motivated considerations to reduce the set of +L+ possible arguments. It does so by confining the set +L+ of argument candidates only to those constituents +L+ which obey the following two restrictions. First, +L+ they should be contained in the minimal clause +L+ containing the predicate. Second, they should be +L+ k-th degree cousins of the predicate in the parse +L+ tree. We propose a novel algorithm for clause de- +L+ tection and use its output to determine which of +L+ the constituents obey these two restrictions. +L+ The second stage of the algorithm uses point- +L+ wise mutual information to rule out constituents +L+ that appear to be weakly collocating with the pred- +L+ icate in question. Since a predicate greatly re- +L+ stricts the type of arguments with which it may +L+ appear (this is often referred to as “selectional re- +L+ strictions”), we expect it to have certain character- +L+ istic arguments with which it is likely to collocate. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Clause detection stage +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The main idea behind this stage is the observation +L+ that most of the arguments of a predicate are con- +L+ tained within the minimal clause that contains the +L+ predicate. We tested this on our development data +L+ – section 24 of the WSJ PTB, where we saw that +L+ 86% of the arguments that are also constituents +L+ (in the gold standard parse) were indeed contained +L+ in that minimal clause (as defined by the tree la- +L+ bel types in the gold standard parse that denote +L+ a clause, e.g., S, SBAR). Since we are not pro- +L+ vided with clause annotation (or any label), we at- +L+ tempted to detect them in an unsupervised manner. +L+ Our algorithm attempts to find sub-trees within the +L+ parse tree, whose structure resembles the structure +L+ of a full sentence. This approximates the notion of +L+ a clause. +L+ </SectLabel_bodyText> <SectLabel_figure> VBP L +L+ L +L+ VBP L +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1: An example of an unlabeled POS tagged +L+ parse tree. The middle tree is the ST of ‘reach’ +L+ with the root as the encoded ancestor. The bot- +L+ tom one is the ST with its parent as the encoded +L+ ancestor. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Statistics gathering. In order to detect which +L+ of the verb’s ancestors is the minimal clause, we +L+ score each of the ancestors and select the one that +L+ maximizes the score. We represent each ancestor +L+ using its Spinal Tree (ST). The ST of a given +L+ verb’s ancestor is obtained by replacing all the +L+ constituents that do not contain the verb by a leaf +L+ having a label. This effectively encodes all the k- +L+ th degree cousins of the verb (for every k). The +L+ leaf labels are either the word’s POS in case the +L+ constituent is a leaf, or the generic label “L” de- +L+ noting a non-leaf. See Figure 1 for an example. +L+ In this stage we collect statistics of the occur- +L+ rences of STs in a large corpus. For every ST in +L+ the corpus, we count the number of times it oc- +L+ curs in a form we consider to be a clause (positive +L+ examples), and the number of times it appears in +L+ other forms (negative examples). +L+ Positive examples are divided into two main +L+ types. First, when the ST encodes the root an- +L+ cestor (as in the middle tree of Figure 1); second, +L+ when the ancestor complies to a clause lexico- +L+ syntactic pattern. In many languages there is a +L+ small set of lexico-syntactic patterns that mark a +L+ clause, e.g. the English ‘that’, the German ‘dass’ +L+ and the Spanish ‘que’. The patterns which were +L+ used in our experiments are shown in Figure 2. +L+ For each verb instance, we traverse over its an- +L+ </SectLabel_bodyText> <SectLabel_figure> L +L+ L +L+ L +L+ L +L+ IN +L+ DT +L+ NNS +L+ The +L+ materials +L+ L +L+ L +L+ VBP +L+ L +L+ in	DT NN +L+ NNS +L+ IN +L+ students +L+ CD +L+ about	90 +L+ each	set +L+ reach +L+ L +L+ L +L+ L	L +L+ L	L +L+ </SectLabel_figure> <SectLabel_page> 31 +L+ </SectLabel_page> <SectLabel_figure> English +L+ TO + VB. The constituent starts with “to” followed by a verb in infinitive form. +L+ WP. The constituent is preceded by a Wh-pronoun. +L+ That. The constituent is preceded by a “that” marked by an “IN” POS tag indicating that it is a subordinating conjunction. +L+ Spanish +L+ CQUE. The constituent is preceded by a word with the POS “CQUE” which denotes the word “que” as a con-junction. +L+ INT. The constituent is preceded by a word with the POS “INT” which denotes an interrogative pronoun. +L+ CSUB. The constituent is preceded by a word with one of the POSs “CSUBF”, “CSUBI” or “CSUBX”, which denote a subordinating conjunction. +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: The set of lexico-syntactic patterns that +L+ mark clauses which were used by our model. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> cestors from top to bottom. For each of them we +L+ update the following counters: sentence(5T) for +L+ the root ancestor’s 5T, patternz (5T) for the ones +L+ complying to the i-th lexico-syntactic pattern and +L+ negative(5T) for the other ancestors1. +L+ Clause detection. At test time, when detecting +L+ the minimal clause of a verb instance, we use +L+ the statistics collected in the previous stage. De- +L+ note the ancestors of the verb with A1 ... Am. +L+ For each of them, we calculate clause(5TA, ) +L+ and total (5TA, ). clause(5TA,) is the sum +L+ of sentence(5TA,) and patternz (5TA,) if this +L+ ancestor complies to the i-th pattern (if there +L+ is no such pattern, clause(5TA,) is equal to +L+ sentence (5TA, )). total (5TA,) is the sum of +L+ clause(5TA,) and negative(5TA, ). +L+ The selected ancestor is given by: +L+ clause(STA, ) +L+ </SectLabel_bodyText> <SectLabel_equation> (1) Amax = argmaxA,  total(STA,) +L+ </SectLabel_equation> <SectLabel_bodyText> An 5T whose total(5T) is less than a small +L+ threshold2 is not considered a candidate to be the +L+ minimal clause, since its statistics may be un- +L+ reliable. In case of a tie, we choose the low- +L+ est constituent that obtained the maximal score. +L+ </SectLabel_bodyText> <SectLabel_footnote> 1If while traversing the tree, we encounter an ancestor +L+ whose first word is preceded by a coordinating conjunction +L+ (marked by the POS tag “CC”), we refrain from performing +L+ any additional counter updates. Structures containing coor- +L+ dinating conjunctions tend not to obey our lexico-syntactic +L+ rules. +L+ 2We used 4 per million sentences, derived from develop- +L+ ment data. +L+ </SectLabel_footnote> <SectLabel_bodyText> If there is only one verb in the sentence3 or if +L+ clause(5TA,) = 0 for every 1 G j G m, we +L+ choose the top level constituent by default to be +L+ the minimal clause containing the verb. Other- +L+ wise, the minimal clause is defined to be the yield +L+ of the selected ancestor. +L+ Argument identification. For each predicate in +L+ the corpus, its argument candidates are now de- +L+ fined to be the constituents contained in the min- +L+ imal clause containing the predicate. However, +L+ these constituents may be (and are) nested within +L+ each other, violating a major restriction on SRL +L+ arguments. Hence we now prune our set, by keep- +L+ ing only the siblings of all of the verb’s ancestors, +L+ as is common in supervised SRL (Xue and Palmer, +L+ 2004). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Using collocations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We use the following observation to filter out some +L+ superfluous argument candidates: since the argu- +L+ ments of a predicate many times bear a semantic +L+ connection with that predicate, they consequently +L+ tend to collocate with it. +L+ We collect collocation statistics from a large +L+ corpus, which we annotate with parse trees and +L+ POS tags. We mark arguments using the argu- +L+ ment detection algorithm described in the previous +L+ two sections, and extract all (predicate, argument) +L+ pairs appearing in the corpus. Recall that for each +L+ sentence, the arguments are a subset of the con- +L+ stituents in the parse tree. +L+ We use two representations of an argument: one +L+ is the POS tag sequence of the terminals contained +L+ in the argument, the other is its head word4. The +L+ predicate is represented as the conjunction of its +L+ lemma with its POS tag. +L+ Denote the number of times a predicate x +L+ appeared with an argument y by nxy. Denote +L+ the total number of (predicate, argument) pairs +L+ by N. Using these notations, we define the +L+ following quantities: nx = Eynxy, ny = Exnxy, +L+ p(x) = n�N , p(y) = n�N and p(x, y) = nx N . The +L+ pointwise mutual information of x and y is then +L+ given by: +L+ </SectLabel_bodyText> <SectLabel_footnote> 3In this case, every argument in the sentence must be re- +L+ lated to that verb. +L+ 4Since we do not have syntactic labels, we use an approx- +L+ imate notion. For English we use the Bikel parser default +L+ head word rules (Bikel, 2004). For Spanish, we use the left- +L+ most word. +L+ </SectLabel_footnote> <SectLabel_page> 32 +L+ </SectLabel_page> <SectLabel_equation> (2) PMI(x, y) = log  p( x) P(y) =log n�y +L+ (n� �ny)/N +L+ </SectLabel_equation> <SectLabel_bodyText> PMI effectively measures the ratio between +L+ the number of times x and y appeared together and +L+ the number of times they were expected to appear, +L+ had they been independent. +L+ At test time, when an (x, y) pair is observed, we +L+ check if PMI (x, y), computed on the large cor- +L+ pus, is lower than a threshold a for either of x’s +L+ representations. If this holds, for at least one rep- +L+ resentation, we prune all instances of that (x, y) +L+ pair. The parameter a may be selected differently +L+ for each of the argument representations. +L+ In order to avoid using unreliable statistics, +L+ we apply this for a given pair only if n .ny N> +L+ r, for some parameter r. That is, we consider +L+ PMI (x, y) to be reliable, only if the denomina- +L+ tor in equation (2) is sufficiently large. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Experimental Setup +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Corpora. We used the PropBank corpus for de- +L+ velopment and for evaluation on English. Section +L+ 24 was used for the development of our model, +L+ and sections 2 to 21 were used as our test data. +L+ The free parameters of the collocation extraction +L+ phase were tuned on the development data. Fol- +L+ lowing the unsupervised parsing literature, multi- +L+ ple brackets and brackets covering a single word +L+ are omitted. We exclude punctuation according +L+ to the scheme of (Klein, 2005). As is customary +L+ in unsupervised parsing (e.g. (Seginer, 2007)), we +L+ bounded the lengths of the sentences in the cor- +L+ pus to be at most 10 (excluding punctuation). This +L+ results in 207 sentences in the development data, +L+ containing a total of 132 different verbs and 173 +L+ verb instances (of the non-auxiliary verbs in the +L+ SRL task, see ‘evaluation’ below) having 403 ar- +L+ guments. The test data has 6007 sentences con- +L+ taining 1008 different verbs and 5130 verb in- +L+ stances (as above) having 12436 arguments. +L+ Our algorithm requires large amounts of data +L+ to gather argument structure and collocation pat- +L+ terns. For the statistics gathering phase of the +L+ clause detection algorithm, we used 4.5M sen- +L+ tences of the NANC (Graff, 1995) corpus, bound- +L+ ing their length in the same manner. In order +L+ to extract collocations, we used 2M sentences +L+ from the British National Corpus (Burnard, 2000) +L+ and about 29M sentences from the Dmoz cor- +L+ pus (Gabrilovich and Markovitch, 2005). Dmoz +L+ is a web corpus obtained by crawling and clean- +L+ ing the URLs in the Open Directory Project +L+ (dmoz.org). All of the above corpora were parsed +L+ using Seginer’s parser and POS-tagged by MX- +L+ POST (Ratnaparkhi, 1996). +L+ For our experiments on Spanish, we used 3.3M +L+ sentences of length at most 15 (excluding punctua- +L+ tion) extracted from the Spanish Wikipedia. Here +L+ we chose to bound the length by 15 due to the +L+ smaller size of the available test corpus. The +L+ same data was used both for the first and the sec- +L+ ond stages. Our development and test data were +L+ taken from the training data released for the Se- +L+ mEval 2007 task on semantic annotation of Span- +L+ ish (M`arquez et al., 2007). This data consisted +L+ of 1048 sentences of length up to 15, from which +L+ 200 were randomly selected as our development +L+ data and 848 as our test data. The development +L+ data included 313 verb instances while the test +L+ data included 1279. All corpora were parsed us- +L+ ing the Seginer parser and tagged by the “Tree- +L+ Tagger” (Schmid, 1994). +L+ Baselines. Since this is the first paper, to our +L+ knowledge, which addresses the problem of unsu- +L+ pervised argument identification, we do not have +L+ any previous results to compare to. We instead +L+ compare to a baseline which marks all k-th degree +L+ cousins of the predicate (for every k) as arguments +L+ (this is the second pruning we use in the clause +L+ detection stage). We name this baseline the ALL +L+ COUSINS baseline. We note that a random base- +L+ line would score very poorly since any sequence of +L+ terminals which does not contain the predicate is +L+ a possible candidate. Therefore, beating this ran- +L+ dom baseline is trivial. +L+ Evaluation. Evaluation is carried out using +L+ standard SRL evaluation software5. The algorithm +L+ is provided with a list of predicates, whose argu- +L+ ments it needs to annotate. For the task addressed +L+ in this paper, non-consecutive parts of arguments +L+ are treated as full arguments. A match is consid- +L+ ered each time an argument in the gold standard +L+ data matches a marked argument in our model’s +L+ output. An unmatched argument is an argument +L+ which appears in the gold standard data, and fails +L+ to appear in our model’s output, and an exces- +L+ sive argument is an argument which appears in +L+ our model’s output but does not appear in the gold +L+ standard. Precision and recall are defined accord- +L+ ingly. We report an F-score as well (the harmonic +L+ mean of precision and recall). We do not attempt +L+ </SectLabel_bodyText> <SectLabel_footnote> 5http://www.lsi.upc.edu/—srlconll/soft.html#software. +L+ </SectLabel_footnote> <SectLabel_page> 33 +L+ </SectLabel_page> <SectLabel_bodyText> to identify multi-word verbs, and therefore do not +L+ report the model’s performance in identifying verb +L+ boundaries. +L+ Since our model detects clauses as an interme- +L+ diate product, we provide a separate evaluation +L+ of this task for the English corpus. We show re- +L+ sults on our development data. We use the stan- +L+ dard parsing F-score evaluation measure. As a +L+ gold standard in this evaluation, we mark for each +L+ of the verbs in our development data the minimal +L+ clause containing it. A minimal clause is the low- +L+ est ancestor of the verb in the parse tree that has +L+ a syntactic label of a clause according to the gold +L+ standard parse of the PTB. A verb is any terminal +L+ marked by one of the POS tags of type verb ac- +L+ cording to the gold standard POS tags of the PTB. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Results +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our results are shown in Table 1. The left section +L+ presents results on English and the right section +L+ presents results on Spanish. The top line lists re- +L+ sults of the clause detection stage alone. The next +L+ two lines list results of the full algorithm (clause +L+ detection + collocations) in two different settings +L+ of the collocation stage. The bottom line presents +L+ the performance of the ALL COUSINS baseline. +L+ In the “Collocation Maximum Precision” set- +L+ ting the parameters of the collocation stage (a and +L+ r) were generally tuned such that maximal preci- +L+ sion is achieved while preserving a minimal recall +L+ level (40% for English, 20% for Spanish on the de- +L+ velopment data). In the “Collocation Maximum F- +L+ score” the collocation parameters were generally +L+ tuned such that the maximum possible F-score for +L+ the collocation algorithm is achieved. +L+ The best or close to best F-score is achieved +L+ when using the clause detection algorithm alone +L+ (59.14% for English, 23.34% for Spanish). Note +L+ that for both English and Spanish F-score im- +L+ provements are achieved via a precision improve- +L+ ment that is more significant than the recall degra- +L+ dation. F-score maximization would be the aim of +L+ a system that uses the output of our unsupervised +L+ ARGID by itself. +L+ The “Collocation Maximum Precision” +L+ achieves the best precision level (55.97% for +L+ English, 21.8% for Spanish) but at the expense +L+ of the largest recall loss. Still, it maintains a +L+ reasonable level of recall. The “Collocation +L+ Maximum F-score” is an example of a model that +L+ provides a precision improvement (over both the +L+ baseline and the clause detection stage) with a +L+ relatively small recall degradation. In the Spanish +L+ experiments its F-score (23.87%) is even a bit +L+ higher than that of the clause detection stage +L+ (23.34%). +L+ The full two–stage algorithm (clause detection +L+ + collocations) should thus be used when we in- +L+ tend to use the model’s output as training data for +L+ supervised SRL engines or supervised ARGID al- +L+ gorithms. +L+ In our algorithm, the initial set of potential ar- +L+ guments consists of constituents in the Seginer +L+ parser’s parse tree. Consequently the fraction +L+ of arguments that are also constituents (81.87% +L+ for English and 51.83% for Spanish) poses an +L+ upper bound on our algorithm’s recall. Note +L+ that the recall of the ALL COUSINS baseline is +L+ 74.27% (45.75%) for English (Spanish). This +L+ score emphasizes the baseline’s strength, and jus- +L+ tifies the restriction that the arguments should be +L+ k-th cousins of the predicate. The difference be- +L+ tween these bounds for the two languages provides +L+ a partial explanation for the corresponding gap in +L+ the algorithm’s performance. +L+ Figure 3 shows the precision of the collocation +L+ model (on development data) as a function of the +L+ amount of data it was given. We can see that +L+ the algorithm reaches saturation at about 5M sen- +L+ tences. It achieves this precision while maintain- +L+ ing a reasonable recall (an average recall of 43.1% +L+ after saturation). The parameters of the colloca- +L+ tion model were separately tuned for each corpus +L+ size, and the graph displays the maximum which +L+ was obtained for each of the corpus sizes. +L+ To better understand our model’s performance, +L+ we performed experiments on the English cor- +L+ pus to test how well its first stage detects clauses. +L+ Clause detection is used by our algorithm as a step +L+ towards argument identification, but it can be of +L+ potential benefit for other purposes as well (see +L+ Section 2). The results are 23.88% recall and 40% +L+ precision. As in the ARGID task, a random se- +L+ lection of arguments would have yielded an ex- +L+ tremely poor result. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Conclusion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this work we presented the first algorithm for ar- +L+ gument identification that uses neither supervised +L+ syntactic annotation nor SRL tagged data. We +L+ have experimented on two languages: English and +L+ Spanish. The straightforward adaptability of un- +L+ </SectLabel_bodyText> <SectLabel_page> 34 +L+ </SectLabel_page> <SectLabel_table> 	English (Test Data)			Spanish (Test Data) +L+ 	Precision	Recall	F1	Precision	Recall	F1 +L+ Clause Detection	52.84	67.14	59.14	18.00	33.19	23.34 +L+ Collocation Maximum F–score	54.11	63.53	58.44	20.22	29.13	23.87 +L+ Collocation Maximum Precision	55.97	40.02	46.67	21.80	18.47	20.00 +L+ ALL COUSINS baseline	46.71	74.27	57.35	14.16	45.75	21.62 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Precision, Recall and F 1 score for the different stages of our algorithm. Results are given for English (PTB, sentences +L+ length bounded by 10, left part of the table) and Spanish (SemEval 2007 Spanish SRL task, right part of the table). The results +L+ of the collocation (second) stage are given in two configurations, Collocation Maximum F-score and Collocation Maximum +L+ Precision (see text). The upper bounds on Recall, obtained by taking all arguments output by our unsupervised parser, are +L+ 81.87% for English and 51.83% for Spanish. +L+ </SectLabel_tableCaption> <SectLabel_figure> Number of Sentences (Millions) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: The performance of the second stage on English +L+ (squares) vs. corpus size. The precision of the baseline (trian- +L+ gles) and of the first stage (circles) is displayed for reference. +L+ The graph indicates the maximum precision obtained for each +L+ corpus size. The graph reaches saturation at about 5M sen- +L+ tences. The average recall of the sampled points from there +L+ on is 43.1%. Experiments were performed on the English +L+ development data. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> supervised models to different languages is one +L+ of their most appealing characteristics. The re- +L+ cent availability of unsupervised syntactic parsers +L+ has offered an opportunity to conduct research on +L+ SRL, without reliance on supervised syntactic an- +L+ notation. This work is the first to address the ap- +L+ plication of unsupervised parses to an SRL related +L+ task. +L+ Our model displayed an increase in precision of +L+ 9% in English and 8% in Spanish over a strong +L+ baseline. Precision is of particular interest in this +L+ context, as instances tagged by high quality an- +L+ notation could be later used as training data for +L+ supervised SRL algorithms. In terms of F–score, +L+ our model showed an increase of 1.8% in English +L+ and of 2.2% in Spanish over the baseline. +L+ Although the quality of unsupervised parses is +L+ currently low (compared to that of supervised ap- +L+ proaches), using great amounts of data in identi- +L+ fying recurring structures may reduce noise and +L+ in addition address sparsity. The techniques pre- +L+ sented in this paper are based on this observation, +L+ using around 35M sentences in total for English +L+ and 3.3M sentences for Spanish. +L+ As this is the first work which addressed un- +L+ supervised ARGID, many questions remain to be +L+ explored. Interesting issues to address include as- +L+ sessing the utility of the proposed methods when +L+ supervised parses are given, comparing our model +L+ to systems with no access to unsupervised parses +L+ and conducting evaluation using more relaxed +L+ measures. +L+ Unsupervised methods for syntactic tasks have +L+ matured substantially in the last few years. No- +L+ table examples are (Clark, 2003) for unsupervised +L+ POS tagging and (Smith and Eisner, 2006) for un- +L+ supervised dependency parsing. Adapting our al- +L+ gorithm to use the output of these models, either to +L+ reduce the little supervision our algorithm requires +L+ (POS tagging) or to provide complementary syn- +L+ tactic information, is an interesting challenge for +L+ future work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Collin F. Baker, Charles J. Fillmore and John B. Lowe, +L+ 1998. The Berkeley FrameNet Project. ACL- +L+ COLING ’98. +L+ Daniel M. Bikel, 2004. Intricacies of Collins’ Parsing +L+ Model. Computational Linguistics, 30(4):479–511. +L+ Ted Briscoe, John Carroll, 1997. Automatic Extraction +L+ of Subcategorization from Corpora. Applied NLP +L+ 1997. +L+ Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea +L+ Kowalski, Sebastian Pad and Manfred Pinkal, 2006 +L+ The SALSA Corpus: a German Corpus Resource for +L+ Lexical Semantics. LREC ’06. +L+ Lou Burnard, 2000. User Reference Guide for the +L+ British National Corpus. Technical report, Oxford +L+ University. +L+ Xavier Carreras and Lluis M`arquez, 2004. Intro- +L+ duction to the CoNLL–2004 Shared Task: Semantic +L+ Role Labeling. CoNLL ’04. +L+ </SectLabel_reference> <SectLabel_figure> 0	2	4	6	8	10 +L+ 48 +L+ 46 +L+ 44 +L+ 42 +L+ 52 +L+ 50 +L+ Second Stage +L+ First Stage +L+ Baseline +L+ </SectLabel_figure> <SectLabel_page> 35 +L+ </SectLabel_page> <SectLabel_reference> Xavier Carreras and Lluis M`arquez, 2005. Intro- +L+ duction to the CoNLL –2005 Shared Task: Semantic +L+ Role Labeling. CoNLL ’05. +L+ Alexander Clark, 2003. Combining Distributional and +L+ Morphological Information for Part of Speech In- +L+ duction. EACL ’03. +L+ Ronan Collobert and Jason Weston, 2007. Fast Se- +L+ mantic Extraction Using a Novel Neural Network +L+ Architecture. ACL ’07. +L+ Mona Diab, Aous Mansouri, Martha Palmer, Olga +L+ Babko-Malaya, Wajdi Zaghouani, Ann Bies and +L+ Mohammed Maamouri, 2008. A pilot Arabic Prop- +L+ Bank. LREC ’08. +L+ Evgeniy Gabrilovich and Shaul Markovitch, 2005. +L+ Feature Generation for Text Categorization using +L+ World Knowledge. IJCAI ’05. +L+ Daniel Gildea and Daniel Jurafsky, 2002. Automatic +L+ Labeling of Semantic Roles. Computational Lin- +L+ guistics, 28(3):245–288. +L+ Elliot Glaysher and Dan Moldovan, 2006. Speed- +L+ ing Up Full Syntactic Parsing by Leveraging Partial +L+ Parsing Decisions. COLING/ACL ’06 poster ses- +L+ sion. +L+ Andrew Gordon and Reid Swanson, 2007. Generaliz- +L+ ing Semantic Role Annotations across Syntactically +L+ Similar Verbs. ACL ’07. +L+ David Graff, 1995. North American News Text Cor- +L+ pus. Linguistic Data Consortium. LDC95T21. +L+ Trond Grenager and Christopher D. Manning, 2006. +L+ Unsupervised Discovery of a Statistical Verb Lexi- +L+ con. EMNLP ’06. +L+ Kadri Hacioglu, 2004. Semantic Role Labeling using +L+ Dependency Trees. COLING’04. +L+ Kadri Hacioglu and Wayne Ward, 2003. Target Word +L+ Detection and Semantic Role Chunking using Sup- +L+ port Vector Machines. HLT-NAACL ’03. +L+ Rohit J. Kate and Raymond J. Mooney, 2007. Semi- +L+ Supervised Learning for Semantic Parsing using +L+ Support Vector Machines. HLT–NAACL ’07. +L+ Karin Kipper, Hoa Trang Dang and Martha Palmer, +L+ 2000. Class-Based Construction of a Verb Lexicon. +L+ AAAI ’00. +L+ Dan Klein, 2005. The Unsupervised Learning ofNatu- +L+ ral Language Structure. Ph.D. thesis, Stanford Uni- +L+ versity. +L+ Anna Korhonen, 2002. Subcategorization Acquisition. +L+ Ph.D. thesis, University of Cambridge. +L+ Christopher D. Manning, 1993. Automatic Acquisition +L+ of a Large Subcategorization Dictionary. ACL ’93. +L+ Lluis M`arquez, Xavier Carreras, Kenneth C. Lit- +L+ tkowski and Suzanne Stevenson, 2008. Semantic +L+ Role Labeling: An introdution to the Special Issue. +L+ Computational Linguistics, 34(2):145–159 +L+ Lluis M`arquez, Jesus Gim`enez Pere Comas and Neus +L+ Catal`a, 2005. Semantic Role Labeling as Sequential +L+ Tagging. CoNLL’05. +L+ Lluis M`arquez, Lluis Villarejo, M. A. Marti and Mar- +L+ iona Taul`e, 2007. SemEval–2007 Task 09: Multi- +L+ level Semantic Annotation of Catalan and Spanish. +L+ The 4th international workshop on Semantic Evalu- +L+ ations (SemEval ’07). +L+ Gabriele Musillo and Paula Merlo, 2006. Accurate +L+ Parsing of the proposition bank. HLT-NAACL ’06. +L+ Martha Palmer, Daniel Gildea and Paul Kingsbury, +L+ 2005. The Proposition Bank: A Corpus Annotated +L+ with Semantic Roles. Computational Linguistics, +L+ 31(1):71–106. +L+ Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, +L+ Wayne Ward, James H. Martin and Daniel Jurafsky, +L+ 2005. Support Vector Learning for Semantic Argu- +L+ ment Classification. Machine Learning, 60(1):11– +L+ 39. +L+ Sameer Pradhan, Wayne Ward, James H. Martin, 2008. +L+ Towards Robust Semantic Role Labeling. Computa- +L+ tional Linguistics, 34(2):289–310. +L+ Adwait Ratnaparkhi, 1996. Maximum Entropy Part- +L+ Of-Speech Tagger. EMNLP ’96. +L+ Helmut Schmid, 1994. Probabilistic Part-of-Speech +L+ Tagging Using Decision Trees International Confer- +L+ ence on New Methods in Language Processing. +L+ Yoav Seginer, 2007. Fast Unsupervised Incremental +L+ Parsing. ACL ’07. +L+ Noah A. Smith and Jason Eisner, 2006. Annealing +L+ Structural Bias in Multilingual Weighted Grammar +L+ Induction. ACL ’06. +L+ Robert S. Swier and Suzanne Stevenson, 2004. Unsu- +L+ pervised Semantic Role Labeling. EMNLP ’04. +L+ Robert S. Swier and Suzanne Stevenson, 2005. Ex- +L+ ploiting a Verb Lexicon in Automatic Semantic Role +L+ Labelling. EMNLP ’05. +L+ Erik F. Tjong Kim Sang and Herv´e D´ejean, 2001. In- +L+ troduction to the CoNLL-2001 Shared Task: Clause +L+ Identification. CoNLL ’01. +L+ Nianwen Xue and Martha Palmer, 2004. Calibrating +L+ Features for Semantic Role Labeling. EMNLP ’04. +L+ Nianwen Xue, 2008. Labeling Chinese Predicates +L+ with Semantic Roles. Computational Linguistics, +L+ 34(2):225–255. +L+ </SectLabel_reference> <SectLabel_page> 36 +L+ </SectLabel_page>
<SectLabel_title> Brutus: A Semantic Role Labeling System Incorporating CCG, CFG, and +L+ Dependency Features +L+ </SectLabel_title> <SectLabel_author> Stephen A. Boxwell, Dennis Mehay, and Chris Brew +L+ </SectLabel_author> <SectLabel_affiliation> Department of Linguistics +L+ The Ohio State University +L+ </SectLabel_affiliation> <SectLabel_email> {boxwe11,mehay,cbrew}@1ing.ohio-state.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We describe a semantic role labeling system +L+ that makes primary use of CCG-based fea- +L+ tures. Most previously developed systems +L+ are CFG-based and make extensive use of a +L+ treepath feature, which suffers from data spar- +L+ sity due to its use of explicit tree configura- +L+ tions. CCG affords ways to augment treepath- +L+ based features to overcome these data sparsity +L+ issues. By adding features over CCG word- +L+ word dependencies and lexicalized verbal sub- +L+ categorization frames (“supertags”), we can +L+ obtain an F-score that is substantially better +L+ than a previous CCG-based SRL system and +L+ competitive with the current state of the art. A +L+ manual error analysis reveals that parser errors +L+ account for many of the errors of our system. +L+ This analysis also suggests that simultaneous +L+ incremental parsing and semantic role labeling +L+ may lead to performance gains in both tasks. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Semantic Role Labeling (SRL) is the process of assign- +L+ ing semantic roles to strings of words in a sentence ac- +L+ cording to their relationship to the semantic predicates +L+ expressed in the sentence. The task is difficult because +L+ the relationship between syntactic relations like “sub- +L+ ject” and “object” do not always correspond to seman- +L+ tic relations like “agent” and “patient”. An effective +L+ semantic role labeling system must recognize the dif- +L+ ferences between different configurations: +L+ </SectLabel_bodyText> <SectLabel_listItem> (a) [The man]Arg0 opened [the door]A�g1 [for +L+ him]Arg3 [today]ArgM-TMP. +L+ (b) [The door]A�g1 opened. +L+ (c) [The door]A�g1 was opened by [a man]A�g0. +L+ </SectLabel_listItem> <SectLabel_bodyText> We use Propbank (Palmer et al., 2005), a corpus of +L+ newswire text annotated with verb predicate semantic +L+ role information that is widely used in the SRL litera- +L+ ture (M`arquez et al., 2008). Rather than describe se- +L+ mantic roles in terms of “agent” or “patient”, Propbank +L+ defines semantic roles on a verb-by-verb basis. For ex- +L+ ample, the verb open encodes the OPENER as Arg0, the +L+ OPENEE as Arg1, and the beneficiary of the OPENING +L+ action as Arg3. Propbank also defines a set of adjunct +L+ roles, denoted by the letter M instead of a number. For +L+ example, ArgM-TMP denotes a temporal role, like “to- +L+ day”. By using verb-specific roles, Propbank avoids +L+ specific claims about parallels between the roles of dif- +L+ ferent verbs. +L+ We follow the approach in (Punyakanok et al., 2008) +L+ in framing the SRL problem as a two-stage pipeline: +L+ identification followed by labeling. During identifica- +L+ tion, every word in the sentence is labeled either as +L+ bearing some (as yet undetermined) semantic role or +L+ not . This is done for each verb. Next, during label- +L+ ing, the precise verb-specific roles for each word are +L+ determined. In contrast to the approach in (Punyakanok +L+ et al., 2008), which tags constituents directly, we tag +L+ headwords and then associate them with a constituent, +L+ as in a previous CCG-based approach (Gildea and +L+ Hockenmaier, 2003). Another difference is our choice +L+ of parsers. Brutus uses the CCG parser of (Clark and +L+ Curran, 2007, henceforth the C&C parser), Charniak’s +L+ parser (Charniak, 2001) for additional CFG-based fea- +L+ tures, and MALT parser (Nivre et al., 2007) for de- +L+ pendency features, while (Punyakanok et al., 2008) +L+ use results from an ensemble of parses from Char- +L+ niak’s Parser and a Collins parser (Collins, 2003; Bikel, +L+ 2004). Finally, the system described in (Punyakanok et +L+ al., 2008) uses a joint inference model to resolve dis- +L+ crepancies between multiple automatic parses. We do +L+ not employ a similar strategy due to the differing no- +L+ tions of constituency represented in our parsers (CCG +L+ having a much more fluid notion of constituency and +L+ the MALT parser using a different approach entirely). +L+ For the identification and labeling steps, we train +L+ a maximum entropy classifier (Berger et al., 1996) +L+ over sections 02-21 of a version of the CCGbank cor- +L+ pus (Hockenmaier and Steedman, 2007) that has been +L+ augmented by projecting the Propbank semantic anno- +L+ tations (Boxwell and White, 2008). We evaluate our +L+ SRL system’s argument predictions at the word string +L+ level, making our results directly comparable for each +L+ argument labeling.1 +L+ In the following, we briefly introduce the CCG +L+ grammatical formalism and motivate its use in SRL +L+ (Sections 2–3). Our main contribution is to demon- +L+ strate that CCG — arguably a more expressive and lin- +L+ </SectLabel_bodyText> <SectLabel_footnote> 1This is guaranteed by our string-to-string mapping from +L+ the original Propbank to the CCGbank. +L+ </SectLabel_footnote> <SectLabel_page> 37 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 37–45, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> guistically appealing syntactic framework than vanilla +L+ CFGs — is a viable basis for the SRL task. This is sup- +L+ ported by our experimental results, the setup and details +L+ of which we give in Sections 4–10. In particular, using +L+ CCG enables us to map semantic roles directly onto +L+ verbal categories, an innovation of our approach that +L+ leads to performance gains (Section 7). We conclude +L+ with an error analysis (Section 11), which motivates +L+ our discussion of future research for computational se- +L+ mantics with CCG (Section 12). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Combinatory Categorial Grammar +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Combinatory Categorial Grammar (Steedman, 2000) +L+ is a grammatical framework that describes syntactic +L+ structure in terms of the combinatory potential of the +L+ lexical (word-level) items. Rather than using standard +L+ part-of-speech tags and grammatical rules, CCG en- +L+ codes much of the combinatory potential of each word +L+ by assigning a syntactically informative category. For +L+ example, the verb loves has the category (s\np)/np, +L+ which could be read “the kind of word that would be +L+ a sentence if it could combine with a noun phrase on +L+ the right and a noun phrase on the left”. Further, CCG +L+ has the advantage of a transparent interface between the +L+ way the words combine and their dependencies with +L+ other words. Word-word dependencies in the CCG- +L+ bank are encoded using predicate-argument (PARG) +L+ relations. PARG relations are defined by the functor +L+ word, the argument word, the category of the functor +L+ word and which argument slot of the functor category +L+ is being filled. For example, in the sentence John loves +L+ Mary (figure 1), there are two slots on the verbal cat- +L+ egory to be filled by NP arguments. The first argu- +L+ ment (the subject) fills slot 1. This can be encoded +L+ as <loves,john,(s\np)/np,1>, indicating the head of +L+ the functor, the head of the argument, the functor cat- +L+ egory and the argument slot. The second argument +L+ (the direct object) fills slot 2. This can be encoded as +L+ <loves,mary,(s\np)/np,2>. One of the potential ad- +L+ vantages to using CCGbank-style PARG relations is +L+ that they uniformly encode both local and long-range +L+ dependencies — e.g., the noun phrase the Mary that +L+ John loves expresses the same set of two dependencies. +L+ We will show this to be a valuable tool for semantic +L+ role prediction. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Potential Advantages to using CCG +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> There are many potential advantages to using the CCG +L+ formalism in SRL. One is the uniformity with which +L+ CCG can express equivalence classes of local and long- +L+ range (including unbounded) dependencies. CFG- +L+ based approaches often rely on examining potentially +L+ long sequences of categories (or treepaths) between the +L+ verb and the target word. Because there are a number of +L+ different treepaths that correspond to a single relation +L+ (figure 2), this approach can suffer from data sparsity. +L+ CCG, however, can encode all treepath-distinct expres- +L+ sions of a single grammatical relation into a single +L+ predicate-argument relationship (figure 3). This fea- +L+ ture has been shown (Gildea and Hockenmaier, 2003) +L+ to be an effective substitute for treepath-based features. +L+ But while predicate-argument-based features are very +L+ effective, they are still vulnerable both to parser er- +L+ rors and to cases where the semantics of a sentence +L+ do not correspond directly to syntactic dependencies. +L+ To counteract this, we use both kinds of features with +L+ the expectation that the treepath feature will provide +L+ low-level detail to compensate for missed, incorrect or +L+ syntactically impossible dependencies. +L+ Another advantage of a CCG-based approach (and +L+ lexicalist approaches in general) is the ability to en- +L+ code verb-specific argument mappings. An argument +L+ mapping is a link between the CCG category and the +L+ semantic roles that are likely to go with each of its ar- +L+ guments. The projection of argument mappings onto +L+ CCG verbal categories is explored in (Boxwell and +L+ White, 2008). We describe this feature in more detail +L+ in section 7. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Identification and Labeling Models +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> As in previous approaches to SRL, Brutus uses a two- +L+ stage pipeline of maximum entropy classifiers. In ad- +L+ dition, we train an argument mapping classifier (de- +L+ scribed in more detail below) whose predictions are +L+ used as features for the labeling model. The same +L+ features are extracted for both treebank and automatic +L+ parses. Automatic parses were generated using the +L+ C&C CCG parser (Clark and Curran, 2007) with its +L+ derivation output format converted to resemble that of +L+ the CCGbank. This involved following the derivational +L+ bracketings of the C&C parser’s output and recon- +L+ structing the backpointers to the lexical heads using an +L+ in-house implementation of the basic CCG combina- +L+ tory operations. All classifiers were trained to 500 iter- +L+ ations of L-BFGS training — a quasi-Newton method +L+ from the numerical optimization literature (Liu and No- +L+ cedal, 1989) — using Zhang Le’s maxent toolkit. 2 To +L+ prevent overfitting we used Gaussian priors with global +L+ variances of 1 and 5 for the identifier and labeler, re- +L+ spectively.3 The Gaussian priors were determined em- +L+ pirically by testing on the development set. +L+ Both the identifier and the labeler use the following +L+ features: +L+ </SectLabel_bodyText> <SectLabel_listItem> (1) Words. Words drawn from a 3 word window +L+ around the target word ,4 with each word asso- +L+ ciated with a binary indicator feature. +L+ (2) Part of Speech. Part of Speech tags drawn +L+ from a 3 word window around the target word, +L+ </SectLabel_listItem> <SectLabel_footnote> 2Available for download at http://homepages. +L+ inf.ed.ac.uk/s0450736/maxent_toolkit. +L+ html. +L+ 3Gaussian priors achieve a smoothing effect (to prevent +L+ overfitting) by penalizing very large feature weights. +L+ 4The size of the window was determined experimentally +L+ on the development set – we use the same window sizes +L+ throughout. +L+ </SectLabel_footnote> <SectLabel_page> 38 +L+ </SectLabel_page> <SectLabel_figure> Robin	fixed	the car +L+ np	(s\np)/np np/n	n +L+ np +L+ s\np +L+ s +L+ John	loves	Mary +L+ > +L+ > +L+ � +L+ np	(s[dcl]\np)/np	np +L+ 		> +L+ 		� +L+ 	s[dcl]\np +L+ 	s[dcl] +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1:	This sentence has two depen- +L+ dencies:	<loves,mary,(s\np)/np,2>	and +L+ <loves,john,(s\np)/np, 1 > +L+ </SectLabel_figureCaption> <SectLabel_figure> fixed +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: The semantic relation (Arg1) between ‘car’ +L+ and ‘fixed’ in both phrases is the same, but the +L+ treepaths — traced with arrows above — are differ- +L+ ent: (V>VP<NP<N and V>VP>S>RC>N<N, re- +L+ spectively). +L+ </SectLabel_figureCaption> <SectLabel_figure> the car	that	Robin	fixed +L+ np/n	n	(np\np)/(s/np)	np	(s\np)/np +L+ >T +L+ s/(s\np) +L+ 	> 	>s +L+ np +L+ s/np +L+ > +L+ � +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: CCG word-word dependencies are passed +L+ up through subordinate clauses, encoding the rela- +L+ tion between car and fixed the same in both cases: +L+ (s\np)/np.2.—> (Gildea and Hockenmaier, 2003) +L+ </SectLabel_figureCaption> <SectLabel_bodyText> with each associated with a binary indicator +L+ feature. +L+ </SectLabel_bodyText> <SectLabel_listItem> (3) CCG Categories. CCG categories drawn from +L+ </SectLabel_listItem> <SectLabel_bodyText> a 3 word window around the target word, with +L+ each associated with a binary indicator feature. +L+ </SectLabel_bodyText> <SectLabel_listItem> (4) Predicate. The lemma of the predicate we are +L+ tagging. E.g. fix is the lemma offixed. +L+ (5) Result Category Detail. The grammatical fea- +L+ </SectLabel_listItem> <SectLabel_bodyText> ture on the category of the predicate (indicat- +L+ ing declarative, passive, progressive, etc). This +L+ can be read off the verb category: declarative +L+ for eats: (s[dcl]\np)/np or progressive for run- +L+ ning:s[ng]\np. +L+ </SectLabel_bodyText> <SectLabel_listItem> (6) Before/After. A binary indicator variable indi- +L+ cating whether the target word is before or after +L+ the verb. +L+ (7) Treepath. The sequence of CCG categories +L+ </SectLabel_listItem> <SectLabel_bodyText> representing the path through the derivation +L+ from the predicate to the target word. For +L+ the relationship between fixed and car in the +L+ first sentence of figure 3, the treepath is +L+ (s[dcl]\np)/np>s[dcl]\np<np<n, with > and +L+ < indicating movement up and down the tree, +L+ respectively. +L+ </SectLabel_bodyText> <SectLabel_listItem> (8) Short Treepath. Similar to the above treepath +L+ </SectLabel_listItem> <SectLabel_bodyText> feature, except the path stops at the highest +L+ node under the least common subsumer that +L+ is headed by the target word (this is the con- +L+ stituent that the role would be marked on if we +L+ identified this terminal as a role-bearing word). +L+ Again, for the relationship between fixed and +L+ car in the first sentence of figure 3, the short +L+ treepath is (s[dcl]\np)/np>s[dcl]\np<np. +L+ </SectLabel_bodyText> <SectLabel_listItem> (9) NP Modified. A binary indicator feature indi- +L+ cating whether the target word is modified by +L+ an NP modifier.5 +L+ </SectLabel_listItem> <SectLabel_footnote> 5This is easily read off of the CCG PARG relationships. +L+ </SectLabel_footnote> <SectLabel_figure> S +L+ �� � ��� +L+ VP +L+ � �� +L+ � +L+ V	NP +L+ � � +L+ fixed Det N +L+ NP +L+ Robin +L+ the +L+ car +L+ N +L+ the +L+ RC +L+ car +L+ Rel +L+ ^S +L+ that +L+ NP +L+ VP +L+ Robin +L+ V +L+ NP +L+ �� � ��� +L+ Det +L+ N +L+ �� � ��� +L+ np\np +L+ np +L+ </SectLabel_figure> <SectLabel_page> 39 +L+ </SectLabel_page> <SectLabel_listItem> (10) Subcategorization. A sequence of the cate- +L+ </SectLabel_listItem> <SectLabel_bodyText> gories that the verb combines with in the CCG +L+ derivation tree. For the first sentence in fig- +L+ ure 3, the correct subcategorization would be +L+ np,np. Notice that this is not necessarily a re- +L+ statement of the verbal category – in the second +L+ sentence of figure 3, the correct subcategoriza- +L+ tion is s/(s\np),(np\np)/(s[dcl]/np),np. +L+ </SectLabel_bodyText> <SectLabel_listItem> (11) PARG feature. We follow a previous CCG- +L+ </SectLabel_listItem> <SectLabel_bodyText> based approach (Gildea and Hockenmaier, +L+ 2003) in using a feature to describe the PARG +L+ relationship between the two words, if one ex- +L+ ists. If there is a dependency in the PARG +L+ structure between the two words, then this fea- +L+ ture is defined as the conjunction of (1) the cat- +L+ egory of the functor, (2) the argument slot that +L+ is being filled in the functor category, and (3) +L+ an indication as to whether the functor (—>) or +L+ the argument (+—) is the lexical head. For ex- +L+ ample, to indicate the relationship between car +L+ and fixed in both sentences of figure 3, the fea- +L+ ture is (s\np)/np.2.—>. +L+ The labeler uses all of the previous features, plus the +L+ following: +L+ </SectLabel_bodyText> <SectLabel_listItem> (12) Headship. A binary indicator feature as to +L+ </SectLabel_listItem> <SectLabel_bodyText> whether the functor or the argument is the lex- +L+ ical head of the dependency between the two +L+ words, if one exists. +L+ </SectLabel_bodyText> <SectLabel_listItem> (13) Predicate and Before/After. The conjunction +L+ </SectLabel_listItem> <SectLabel_bodyText> of two earlier features: the predicate lemma +L+ and the Before/After feature. +L+ </SectLabel_bodyText> <SectLabel_listItem> (14) Rel Clause. Whether the path from predicate +L+ </SectLabel_listItem> <SectLabel_bodyText> to target word passes through a relative clause +L+ (e.g., marked by the word ‘that’ or any other +L+ word with a relativizer category). +L+ </SectLabel_bodyText> <SectLabel_listItem> (15) PP features. When the target word is a prepo- +L+ </SectLabel_listItem> <SectLabel_bodyText> sition, we define binary indicator features for +L+ the word, POS, and CCG category of the head +L+ of the topmost NP in the prepositional phrase +L+ headed by a preposition (a.k.a. the ‘lexical +L+ head’ of the PP). So, if on heads the phrase ‘on +L+ the third Friday’, then we extract features re- +L+ lating to Friday for the preposition on. This is +L+ null when the target word is not a preposition. +L+ </SectLabel_bodyText> <SectLabel_listItem> (16) Argument Mappings. If there is a PARG rela- +L+ </SectLabel_listItem> <SectLabel_bodyText> tion between the predicate and the target word, +L+ the argument mapping is the most likely pre- +L+ dicted role to go with that argument. These +L+ mappings are predicted using a separate classi- +L+ fier that is trained primarily on lexical informa- +L+ tion of the verb, its immediate string-level con- +L+ text, and its observed arguments in the train- +L+ ing data. This feature is null when there is +L+ no PARG relation between the predicate and +L+ the target word. The Argument Mapping fea- +L+ ture can be viewed as a simple prediction about +L+ some of the non-modifier semantic roles that a +L+ verb is likely to express. We use this informa- +L+ tion as a feature and not a hard constraint to +L+ allow other features to overrule the recommen- +L+ dation made by the argument mapping classi- +L+ fier. The features used in the argument map- +L+ ping classifier are described in detail in section +L+ 7. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 CFG based Features +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In addition to CCG-based features, features can be +L+ drawn from a traditional CFG-style approach when +L+ they are available. Our motivation for this is twofold. +L+ First, others (Punyakanok et al., 2008, e.g.), have found +L+ that different parsers have different error patterns, and +L+ so using multiple parsers can yield complementary +L+ sources of correct information. Second, we noticed +L+ that, although the CCG-based system performed well +L+ on head word labeling, performance dropped when +L+ projecting these labels to the constituent level (see sec- +L+ tions 8 and 9 for more). This may have to do with the +L+ fact that CCG is not centered around a constituency- +L+ based analysis, as well as with inconsistencies between +L+ CCG and Penn Treebank-style bracketings (the latter +L+ being what was annotated in the original Propbank). +L+ Penn Treebank-derived features are used in the iden- +L+ tifier, labeler, and argument mapping classifiers. For +L+ automatic parses, we use Charniak’s parser (Charniak, +L+ 2001). For gold-standard parses, we remove func- +L+ tional tag and trace information from the Penn Tree- +L+ bank parses before we extract features over them, so as +L+ to simulate the conditions of an automatic parse. The +L+ Penn Treebank features are as follows: +L+ </SectLabel_bodyText> <SectLabel_listItem> (17) CFG Treepath. A sequence of traditional +L+ CFG-style categories representing the path +L+ from the verb to the target word. +L+ (18) CFG Short Treepath. Analogous to the CCG- +L+ based short treepath feature. +L+ (19) CFG Subcategorization. Analogous to the +L+ CCG-based subcategorization feature. +L+ (20) CFG Least Common Subsumer. The cate- +L+ gory of the root of the smallest tree that domi- +L+ nates both the verb and the target word. +L+ </SectLabel_listItem> <SectLabel_sectionHeader> 6 Dependency Parser Features +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Finally, several features can be extracted from a de- +L+ pendency representation of the same sentence. Au- +L+ tomatic dependency relations were produced by the +L+ MALT parser. We incorporate MALT into our col- +L+ lection of parses because it provides detailed informa- +L+ tion on the exact syntactic relations between word pairs +L+ (subject, object, adverb, etc) that is not found in other +L+ automatic parsers. The features used from the depen- +L+ dency parses are listed below: +L+ </SectLabel_bodyText> <SectLabel_page> 40 +L+ </SectLabel_page> <SectLabel_listItem> (21) DEP-Exists A binary indicator feature show- +L+ ing whether or not there is a dependency be- +L+ tween the target word and the predicate. +L+ (22) DEP-Type If there is a dependency between +L+ the target word and the predicate, what type of +L+ dependency it is (SUBJ, OBJ, etc). +L+ </SectLabel_listItem> <SectLabel_sectionHeader> 7 Argument Mapping Model +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> An innovation in our approach is to use a separate clas- +L+ sifier to predict an argument mapping feature. An ar- +L+ gument mapping is a mapping from the syntactic argu- +L+ ments of a verbal category to the semantic arguments +L+ that should correspond to them (Boxwell and White, +L+ 2008). In order to generate examples of the argument +L+ mapping for training purposes, it is necessary to em- +L+ ploy the PARG relations for a given sentence to identify +L+ the headwords of each of the verbal arguments. That is, +L+ we use the PARG relations to identify the headwords of +L+ each of the constituents that are arguments of the verb. +L+ Next, the appropriate semantic role that corresponds to +L+ that headword (given by Propbank) is identified. This +L+ is done by climbing the CCG derivation tree towards +L+ the root until we find a semantic role corresponding to +L+ the verb in question — i.e., by finding the point where +L+ the constituent headed by the verbal category combines +L+ with the constituent headed by the argument in ques- +L+ tion. These semantic roles are then marked on the cor- +L+ responding syntactic argument of the verb. +L+ As an example, consider the sentence The boy loves +L+ a girl. (figure 4). By examining the arguments that the +L+ verbal category combines with in the treebank, we can +L+ identify the corresponding semantic role for each argu- +L+ ment that is marked on the verbal category. We then use +L+ these tags to train the Argument Mapping model, which +L+ will predict likely argument mappings for verbal cate- +L+ gories based on their local surroundings and the head- +L+ words of their arguments, similar to the supertagging +L+ approaches used to label the informative syntactic cat- +L+ egories of the verbs (Bangalore and Joshi, 1999; Clark, +L+ 2002), except tagging “one level above” the syntax. +L+ The Argument Mapping Predictor uses the following +L+ features: +L+ </SectLabel_bodyText> <SectLabel_listItem> (23) Predicate. The lemma of the predicate, as be- +L+ fore. +L+ (24) Words. Words drawn from a 5 word window +L+ around the target word, with each word associ- +L+ ated with a binary indicator feature, as before. +L+ (25) Parts of Speech. Part of Speech tags drawn +L+ from a 5 word window around the target word, +L+ with each tag associated with a binary indicator +L+ feature, as before. +L+ (26) CCG Categories. CCG categories drawn from +L+ </SectLabel_listItem> <SectLabel_bodyText> a 5 word window around the target word, with +L+ each category associated with a binary indica- +L+ tor feature, as before. +L+ </SectLabel_bodyText> <SectLabel_figure> the boy	loves	a	girl +L+ np/n	n	(s[dcl]\npArg0)/npArg1 np/n	n +L+ np — Arga	np — Argl +L+ � +L+ s[dcl] +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: By looking at the constituents that the verb +L+ combines with, we can identify the semantic roles cor- +L+ responding to the arguments marked on the verbal cat- +L+ egory. +L+ </SectLabel_figureCaption> <SectLabel_listItem> (27) Argument Data. The word, POS, and CCG +L+ category, and treepath of the headwords of each +L+ of the verbal arguments (i.e., PARG depen- +L+ dents), each encoded as a separate binary in- +L+ dicator feature. +L+ (28) Number of arguments. The number of argu- +L+ ments marked on the verb. +L+ (29) Words of Arguments. The head words of each +L+ of the verb’s arguments. +L+ (30) Subcategorization. The CCG categories that +L+ combine with this verb. This includes syntactic +L+ adjuncts as well as arguments. +L+ (31) CFG-Sisters. The POS categories of the sis- +L+ ters of this predicate in the CFG representation. +L+ (32) DEP-dependencies. The individual depen- +L+ </SectLabel_listItem> <SectLabel_bodyText> dency types of each of the dependencies re- +L+ lating to the verb (SBJ, OBJ, ADV, etc) taken +L+ from the dependency parse. We also incorpo- +L+ rate a single feature representing the entire set +L+ of dependency types associated with this verb +L+ into a single feature, representing the set of de- +L+ pendencies as a whole. +L+ Given these features with gold standard parses, our +L+ argument mapping model can predict entire argument +L+ mappings with an accuracy rate of 87.96% on the test +L+ set, and 87.70% on the development set. We found the +L+ features generated by this model to be very useful for +L+ semantic role prediction, as they enable us to make de- +L+ cisions about entire sets of semantic roles associated +L+ with individual lemmas, rather than choosing them in- +L+ dependently of each other. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8 Enabling Cross-System Comparison +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The Brutus system is designed to label headwords of +L+ semantic roles, rather than entire constituents. How- +L+ ever, because most SRL systems are designed to label +L+ constituents rather than headwords, it is necessary to +L+ project the roles up the derivation to the correct con- +L+ stituent in order to make a meaningful comparison of +L+ the system’s performance. This introduces the poten- +L+ tial for further error, so we report results on the ac- +L+ curacy of headwords as well as the correct string of +L+ words. We deterministically move the role to the high- +L+ est constituent in the derivation that is headed by the +L+ s[dcl]\np +L+ </SectLabel_bodyText> <SectLabel_page> 41 +L+ </SectLabel_page> <SectLabel_table> 	P	R	F +L+ P. et al (treebank)	86.22%	87.40%	86.81% +L+ Brutus (treebank)	88.29%	86.39%	87.33% +L+ P. et al (automatic)	77.09%	75.51%	76.29% +L+ Brutus (automatic)	76.73%	70.45%	73.45% +L+ </SectLabel_table> <SectLabel_figure> a man	with	glasses spoke +L+ np/n	n	(np\np)/np	np	s\np +L+ �np +L+ np\np +L+ np - speak.Arg0 +L+ s +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: The role is moved towards the root until the +L+ original node is no longer the head of the marked con- +L+ stituent. +L+ </SectLabel_figureCaption> <SectLabel_table> 	P	R	F +L+ G&H (treebank)	67.5%	60.0%	63.5% +L+ Brutus (treebank)	88.18%	85.00%	86.56% +L+ G&H (automatic)	55.7%	49.5%	52.4% +L+ Brutus (automatic)	76.06%	70.15%	72.99% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Accuracy of semantic role prediction using +L+ only CCG based features. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> originally tagged terminal. In most cases, this corre- +L+ sponds to the node immediately dominated by the low- +L+ est common subsuming node of the the target word and +L+ the verb (figure 5). In some cases, the highest con- +L+ stituent that is headed by the target word is not imme- +L+ diately dominated by the lowest common subsuming +L+ node (figure 6). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9 Results +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Using a version of Brutus incorporating only the CCG- +L+ based features described above, we achieve better re- +L+ sults than a previous CCG based system (Gildea and +L+ Hockenmaier, 2003, henceforth G&H). This could be +L+ due to a number of factors, including the fact that our +L+ system employs a different CCG parser, uses a more +L+ complete mapping of the Propbank onto the CCGbank, +L+ uses a different machine learning approach,6 and has a +L+ richer feature set. The results for constituent tagging +L+ accuracy are shown in table 1. +L+ As expected, by incorporating Penn Treebank-based +L+ features and dependency features, we obtain better re- +L+ sults than with the CCG-only system. The results for +L+ gold standard parses are comparable to the winning +L+ system of the CoNLL 2005 shared task on semantic +L+ role labeling (Punyakanok et al., 2008). Other systems +L+ (Toutanova et al., 2008; Surdeanu et al., 2007; Johans- +L+ son and Nugues, 2008) have also achieved comparable +L+ results – we compare our system to (Punyakanok et +L+ al., 2008) due to the similarities in our approaches. The +L+ performance of the full system is shown in table 2. +L+ Table 3 shows the ability of the system to predict +L+ the correct headwords of semantic roles. This is a nec- +L+ essary condition for correctness of the full constituent, +L+ but not a sufficient one. In parser evaluation, Carroll, +L+ Minnen, and Briscoe (Carroll et al., 2003) have argued +L+ 6G&H use a generative model with a back-off lattice, +L+ whereas we use a maximum entropy classifier. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 2: Accuracy of semantic role prediction using +L+ CCG, CFG, and MALT based features. +L+ </SectLabel_tableCaption> <SectLabel_table> 	P	R	F +L+ Headword (treebank)	88.94%	86.98%	87.95% +L+ Boundary (treebank)	88.29%	86.39%	87.33% +L+ Headword (automatic)	82.36%	75.97%	79.04% +L+ Boundary (automatic)	76.33%	70.59%	73.35% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3: Accuracy of the system for labeling semantic +L+ roles on both constituent boundaries and headwords. +L+ Headwords are easier to predict than boundaries, re- +L+ flecting CCG’s focus on word-word relations rather +L+ than constituency. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> for dependencies as a more appropriate means of eval- +L+ uation, reflecting the focus on headwords from con- +L+ stituent boundaries. We argue that, especially in the +L+ heavily lexicalized CCG framework, headword evalu- +L+ ation is more appropriate, reflecting the emphasis on +L+ headword combinatorics in the CCG formalism. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 10 The Contribution of the New Features +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Two features which are less frequently used in SRL +L+ research play a major role in the Brutus system: The +L+ PARG feature (Gildea and Hockenmaier, 2003) and +L+ the argument mapping feature. Removing them has +L+ a strong effect on accuracy when labeling treebank +L+ parses, as shown in our feature ablation results in ta- +L+ ble 4. We do not report results including the Argu- +L+ ment Mapping feature but not the PARG feature, be- +L+ cause some predicate-argument relation information is +L+ assumed in generating the Argument Mapping feature. +L+ </SectLabel_bodyText> <SectLabel_table> 	P	R	F +L+ +PARG +AM	88.77%	86.15%	87.44% +L+ +PARG -AM	88.42%	85.78%	87.08% +L+ -PARG -AM	87.92%	84.65%	86.26% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4: The effects of removing key features from the +L+ system on gold standard parses. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> The same is true for automatic parses, as shown in ta- +L+ ble 5. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 11 Error Analysis +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Many of the errors made by the Brutus system can be +L+ traced directly to erroneous parses, either in the auto- +L+ matic or treebank parse. In some cases, PP attachment +L+ </SectLabel_bodyText> <SectLabel_page> 42 +L+ </SectLabel_page> <SectLabel_figure> with	even brief exposures	causing	symptoms +L+ (((vp\vp)/vp[ng])/np	n/n	n/n	n	(s[ng]\np)/np	np +L+ 	� 	� +L+ n +L+ s[ng]\np +L+ n +L+ np — cause.Arg0 +L+ 		� +L+ (vp\vp)/vp[ng]			� +L+ 	vp\vp +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6: In this case, with is the head of with even brief exposures, so the role is correctly marked on even brief +L+ exposures (based on wsj 0003.2). +L+ </SectLabel_figureCaption> <SectLabel_table> � +L+ 	P	R	F +L+ +PARG +AM	74.14%	62.09%	67.58% +L+ +PARG -AM	70.02%	64.68%	67.25% +L+ -PARG -AM	73.90%	61.15%	66.93% +L+ a form	of	asbestos used to make filters +L+ np	(np\np)/np	np	np\np +L+ 		� +L+ 		� +L+ 	np\np +L+ 	np — Arg1 +L+ 	� +L+ np +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5: The effects of removing key features from the +L+ system on automatic parses. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> ambiguities cause a role to be marked too high in the +L+ derivation. In the sentence the company stopped using +L+ asbestos in 1956 (figure 7), the correct Arg 1 of stopped +L+ is using asbestos. However, because in 1956 is erro- +L+ neously modifying the verb using rather than the verb +L+ stopped in the treebank parse, the system trusts the syn- +L+ tactic analysis and places Arg1 of stopped on using as- +L+ bestos in 1956. This particular problem is caused by an +L+ annotation error in the original Penn Treebank that was +L+ carried through in the conversion to CCGbank. +L+ Another common error deals with genitive construc- +L+ tions. Consider the phrase a form of asbestos used +L+ to make filters. By CCG combinatorics, the relative +L+ clause could either attach to asbestos or to a form of +L+ asbestos. The gold standard CCG parse attaches the +L+ relative clause to a form of asbestos (figure 8). Prop- +L+ bank agrees with this analysis, assigning Arg1 of use +L+ to the constituent a form of asbestos. The automatic +L+ parser, however, attaches the relative clause low – to +L+ asbestos (figure 9). When the system is given the au- +L+ tomatically generated parse, it incorrectly assigns the +L+ semantic role to asbestos. In cases where the parser at- +L+ taches the relative clause correctly, the system is much +L+ more likely to assign the role correctly. +L+ Problems with relative clause attachment to genitives +L+ are not limited to automatic parses – errors in gold- +L+ standard treebank parses cause similar problems when +L+ Treebank parses disagree with Propbank annotator in- +L+ tuitions. In the phrase a group of workers exposed to +L+ asbestos (figure 10), the gold standard CCG parse at- +L+ taches the relative clause to workers. Propbank, how- +L+ ever, annotates a group of workers as Arg 1 of exposed, +L+ rather than following the parse and assigning the role +L+ only to workers. The system again follows the parse +L+ and incorrectly assigns the role to workers instead of a +L+ group of workers. Interestingly, the C&C parser opts +L+ for high attachment in this instance, resulting in the +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 8: CCGbank gold-standard parse of a relative +L+ clause attachment. The system correctly identifies a +L+ form of asbestos as Arg1 of used. (wsj 0003.1) +L+ </SectLabel_figureCaption> <SectLabel_figure> a form	of	asbestos used to make filters +L+ np	(np\np)/np np — Arg1	np\np +L+ � +L+ 	� +L+ np\np +L+ � +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 9: Automatic parse of the noun phrase in fig- +L+ ure 8. Incorrect relative clause attachment causes the +L+ misidentification of asbestos as a semantic role bearing +L+ unit. (wsj 0003.1) +L+ </SectLabel_figureCaption> <SectLabel_bodyText> correct prediction of a group of workers as Arg1 of ex- +L+ posed in the automatic parse. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 12 Future Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> As described in the error analysis section, a large num- +L+ ber of errors in the system are attributable to errors in +L+ the CCG derivation, either in the gold standard or in +L+ automatically generated parses. Potential future work +L+ may focus on developing an improved CCG parser us- +L+ ing the revised (syntactic) adjunct-argument distinc- +L+ tions (guided by the Propbank annotation) described in +L+ (Boxwell and White, 2008). This resource, together +L+ with the reasonable accuracy (,: 90%) with which ar- +L+ gument mappings can be predicted, suggests the possi- +L+ bility of an integrated, simultaneous syntactic-semantic +L+ parsing process, similar to that of (Musillo and Merlo, +L+ 2006; Merlo and Musillo, 2008). We expect this would +L+ improve the reliability and accuracy of both the syntac- +L+ tic and semantic analysis components. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 13 Acknowledgments +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This research was funded by NSF grant IIS-0347799. +L+ We are deeply indebted to Julia Hockenmaier for the +L+ </SectLabel_bodyText> <SectLabel_figure> np +L+ np +L+ </SectLabel_figure> <SectLabel_page> 43 +L+ </SectLabel_page> <SectLabel_figure> the company	stopped	using	asbestos	in 1956 +L+ np	((s[dcl]\np)/(s[ng]\np)) (s[ng]\np)/np	np	(s\np)\(s\np) +L+ 	� +L+ s[ng]\np +L+ s[ng]\np - stop.Arg1 +L+ s[dcl]\np +L+ s[dcl] +L+ � +L+ � +L+ � +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 7: An example of how incorrect PP attachment can cause an incorrect labeling. Stop.Arg1 should cover us- +L+ ing asbestos rather than using asbestos in 1956. This sentence is based on wsj 0003.3, with the structure simplified +L+ for clarity. +L+ </SectLabel_figureCaption> <SectLabel_figure> a group	of	workers	exposed to asbestos +L+ np	(np\np)/np np - exposed.Arg1	np\np +L+ 					� +L+ 					� +L+ 					� +L+ 				np +L+ 			np\np +L+ 		np +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 10: Propbank annotates a group of workers as Arg1 of exposed, while CCGbank attaches the relative clause +L+ low. The system incorrectly labels workers as a role bearing unit. (Gold standard – wsj 0003.1) +L+ </SectLabel_figureCaption> <SectLabel_bodyText> use of her PARG generation tool. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Srinivas Bangalore and Aravind Joshi. 1999. Su- +L+ pertagging: An approach to almost parsing. Com- +L+ putational Linguistics, 25(2):237–265. +L+ Adam L. Berger, S. Della Pietra, and V. Della Pietra. +L+ 1996. A maximum entropy approach to natural +L+ language processing. Computational Linguistics, +L+ 22(1):39–71. +L+ D.M. Bikel. 2004. Intricacies of Collins’ parsing +L+ model. Computational Linguistics, 30(4):479–511. +L+ Stephen A. Boxwell and Michael White. 2008. +L+ Projecting propbank roles onto the ccgbank. In +L+ Proceedings of the Sixth International Language +L+ Resources and Evaluation Conference (LREC-08), +L+ Marrakech, Morocco. +L+ J. Carroll, G. Minnen, and T. Briscoe. 2003. Parser +L+ evaluation. Treebanks: Building and Using Parsed +L+ Corpora, pages 299–316. +L+ E. Charniak. 2001. Immediate-head parsing for lan- +L+ guage models. In Proc. ACL-01, volume 39, pages +L+ 116–123. +L+ Stephen Clark and James R. Curran. 2007. Wide- +L+ coverage Efficient Statistical Parsing with CCG and +L+ Log-linear Models. Computational Linguistics, +L+ 33(4):493–552. +L+ Stephen Clark. 2002. Supertagging for combinatory +L+ categorial grammar. In Proceedings of the 6th In- +L+ ternational Workshop on Tree Adjoining Grammars +L+ and Related Frameworks (TAG+6), pages 19–24, +L+ Venice, Italy. +L+ M. Collins. 2003. Head-driven statistical models for +L+ natural language parsing. Computational Linguis- +L+ tics, 29(4):589–637. +L+ Daniel Gildea and Julia Hockenmaier. 2003. Identi- +L+ fying semantic roles using Combinatory Categorial +L+ Grammar. In Proc. EMNLP-03. +L+ Julia Hockenmaier and Mark Steedman. 2007. CCG- +L+ bank: A Corpus of CCG Derivations and Depen- +L+ dency Structures Extracted from the Penn Treebank. +L+ Computational Linguistics, 33(3):355–396. +L+ R. Johansson and P. Nugues. 2008. Dependency- +L+ based syntactic–semantic analysis with PropBank +L+ and NomBank. Proceedings of CoNLL –2008. +L+ D C Liu and Jorge Nocedal. 1989. On the limited +L+ memory method for large scale optimization. Math- +L+ ematical Programming B, 45(3). +L+ Lluis M`arquez, Xavier Carreras, Kenneth C. Litowski, +L+ and Suzanne Stevenson. 2008. Semantic Role La- +L+ beling: An Introduction to the Special Issue. Com- +L+ putational Linguistics, 34(2):145–159. +L+ Paola Merlo and Gabrile Musillo. 2008. Semantic +L+ parsing for high-precision semantic role labelling. In +L+ Proceedings of CONLL-08, Manchester, UK. +L+ Gabriele Musillo and Paola Merlo. 2006. Robust pars- +L+ ing of the proposition bank. In Proceedings of the +L+ EACL 2006 Workshop ROMAND, Trento. +L+ J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, +L+ S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt- +L+ Parser: A language-independent system for data- +L+ driven dependency parsing. Natural Language En- +L+ gineering, 13(02):95–135. +L+ Martha Palmer, Daniel Gildea, and Paul Kingsbury. +L+ 2005. The Proposition Bank: An Annotated Cor- +L+ pus of Semantic Roles. Computational Linguistics, +L+ 31(1):71–106. +L+ </SectLabel_reference> <SectLabel_page> 44 +L+ </SectLabel_page> <SectLabel_reference> Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008. +L+ The Importance of Syntactic Parsing and Inference +L+ in Semantic Role Labeling. Computational Linguis- +L+ tics, 34(2):257–287. +L+ Mark Steedman. 2000. The Syntactic Process. MIT +L+ Press. +L+ M. Surdeanu, L. M`arquez, X. Carreras, and P. Comas. +L+ 2007. Combination strategies for semantic role la- +L+ beling. Journal of Artificial Intelligence Research, +L+ 29:105–151. +L+ K. Toutanova, A. Haghighi, and C.D. Manning. 2008. +L+ A global joint model for semantic role labeling. +L+ Computational Linguistics, 34(2):161–191. +L+ </SectLabel_reference> <SectLabel_page> 45 +L+ </SectLabel_page>
<SectLabel_title> Exploiting Heterogeneous Treebanks for Parsing +L+ </SectLabel_title> <SectLabel_author> Zheng-Yu Niu, Haifeng Wang, Hua Wu +L+ </SectLabel_author> <SectLabel_affiliation> Toshiba (China) Research and Development Center +L+ </SectLabel_affiliation> <SectLabel_address> 5/F., Tower W2, Oriental Plaza, Beijing, 100738, China +L+ </SectLabel_address> <SectLabel_email> {niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cn +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We address the issue of using heteroge- +L+ neous treebanks for parsing by breaking +L+ it down into two sub-problems, convert- +L+ ing grammar formalisms of the treebanks +L+ to the same one, and parsing on these +L+ homogeneous treebanks. First we pro- +L+ pose to employ an iteratively trained tar- +L+ get grammar parser to perform grammar +L+ formalism conversion, eliminating prede- +L+ fined heuristic rules as required in previ- +L+ ous methods. Then we provide two strate- +L+ gies to refine conversion results, and adopt +L+ a corpus weighting technique for parsing +L+ on homogeneous treebanks. Results on the +L+ Penn Treebank show that our conversion +L+ method achieves 42% error reduction over +L+ the previous best result. Evaluation on +L+ the Penn Chinese Treebank indicates that a +L+ converted dependency treebank helps con- +L+ stituency parsing and the use of unlabeled +L+ data by self-training further increases pars- +L+ ing f-score to 85.2%, resulting in 6% error +L+ reduction over the previous best result. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The last few decades have seen the emergence of +L+ multiple treebanks annotated with different gram- +L+ mar formalisms, motivated by the diversity of lan- +L+ guages and linguistic theories, which is crucial to +L+ the success of statistical parsing (Abeille et al., +L+ 2000; Brants et al., 1999; Bohmova et al., 2003; +L+ Han et al., 2002; Kurohashi and Nagao, 1998; +L+ Marcus et al., 1993; Moreno et al., 2003; Xue et +L+ al., 2005). Availability of multiple treebanks cre- +L+ ates a scenario where we have a treebank anno- +L+ tated with one grammar formalism, and another +L+ treebank annotated with another grammar formal- +L+ ism that we are interested in. We call the first +L+ a source treebank, and the second a target tree- +L+ bank. We thus encounter a problem of how to +L+ use these heterogeneous treebanks for target gram- +L+ mar parsing. Here heterogeneous treebanks refer +L+ to two or more treebanks with different grammar +L+ formalisms, e.g., one treebank annotated with de- +L+ pendency structure (DS) and the other annotated +L+ with phrase structure (PS). +L+ It is important to acquire additional labeled data +L+ for the target grammar parsing through exploita- +L+ tion of existing source treebanks since there is of- +L+ ten a shortage of labeled data. However, to our +L+ knowledge, there is no previous study on this is- +L+ sue. +L+ Recently there have been some works on us- +L+ ing multiple treebanks for domain adaptation of +L+ parsers, where these treebanks have the same +L+ grammar formalism (McClosky et al., 2006b; +L+ Roark and Bacchiani, 2003). Other related works +L+ focus on converting one grammar formalism of a +L+ treebank to another and then conducting studies on +L+ the converted treebank (Collins et al., 1999; Forst, +L+ 2003; Wang et al., 1994; Watkinson and Manand- +L+ har, 2001). These works were done either on mul- +L+ tiple treebanks with the same grammar formalism +L+ or on only one converted treebank. We see that +L+ their scenarios are different from ours as we work +L+ with multiple heterogeneous treebanks. +L+ For the use of heterogeneous treebanks1, we +L+ propose a two-step solution: (1) converting the +L+ grammar formalism of the source treebank to the +L+ target one, (2) refining converted trees and using +L+ them as additional training data to build a target +L+ grammar parser. +L+ For grammar formalism conversion, we choose +L+ the DS to PS direction for the convenience of the +L+ comparison with existing works (Xia and Palmer, +L+ 2001; Xia et al., 2008). Specifically, we assume +L+ that the source grammar formalism is dependency +L+ </SectLabel_bodyText> <SectLabel_footnote> 1Here we assume the existence of two treebanks. +L+ </SectLabel_footnote> <SectLabel_page> 46 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 46–54, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> grammar, and the target grammar formalism is +L+ phrase structure grammar. +L+ Previous methods for DS to PS conversion +L+ (Collins et al., 1999; Covington, 1994; Xia and +L+ Palmer, 2001; Xia et al., 2008) often rely on pre- +L+ defined heuristic rules to eliminate converison am- +L+ biguity, e.g., minimal projection for dependents, +L+ lowest attachment position for dependents, and the +L+ selection of conversion rules that add fewer num- +L+ ber of nodes to the converted tree. In addition, the +L+ validity of these heuristic rules often depends on +L+ their target grammars. To eliminate the heuristic +L+ rules as required in previous methods, we propose +L+ to use an existing target grammar parser (trained +L+ on the target treebank) to generate N-best parses +L+ for each sentence in the source treebank as conver- +L+ sion candidates, and then select the parse consis- +L+ tent with the structure of the source tree as the con- +L+ verted tree. Furthermore, we attempt to use con- +L+ verted trees as additional training data to retrain +L+ the parser for better conversion candidates. The +L+ procedure of tree conversion and parser retraining +L+ will be run iteratively until a stopping condition is +L+ satisfied. +L+ Since some converted trees might be imper- +L+ fect from the perspective of the target grammar, +L+ we provide two strategies to refine conversion re- +L+ sults: (1) pruning low-quality trees from the con- +L+ verted treebank, (2) interpolating the scores from +L+ the source grammar and the target grammar to se- +L+ lect better converted trees. Finally we adopt a cor- +L+ pus weighting technique to get an optimal combi- +L+ nation of the converted treebank and the existing +L+ target treebank for parser training. +L+ We have evaluated our conversion algorithm on +L+ a dependency structure treebank (produced from +L+ the Penn Treebank) for comparison with previous +L+ work (Xia et al., 2008). We also have investi- +L+ gated our two-step solution on two existing tree- +L+ banks, the Penn Chinese Treebank (CTB) (Xue et +L+ al., 2005) and the Chinese Dependency Treebank +L+ (CDT)2 (Liu et al., 2006). Evaluation on WSJ data +L+ demonstrates that it is feasible to use a parser for +L+ grammar formalism conversion and the conversion +L+ benefits from converted trees used for parser re- +L+ training. Our conversion method achieves 93.8% +L+ f-score on dependency trees produced from WSJ +L+ section 22, resulting in 42% error reduction over +L+ the previous best result for DS to PS conversion. +L+ Results on CTB show that score interpolation is +L+ </SectLabel_bodyText> <SectLabel_footnote> 2Available at http://ir.hit.edu.cn/. +L+ </SectLabel_footnote> <SectLabel_bodyText> more effective than instance pruning for the use +L+ of converted treebanks for parsing and converted +L+ CDT helps parsing on CTB. When coupled with +L+ self-training technique, a reranking parser with +L+ CTB and converted CDT as labeled data achieves +L+ 85.2% f-score on CTB test set, an absolute 1.0% +L+ improvement (6% error reduction) over the previ- +L+ ous best result for Chinese parsing. +L+ The rest of this paper is organized as follows. In +L+ Section 2, we first describe a parser based method +L+ for DS to PS conversion, and then we discuss pos- +L+ sible strategies to refine conversion results, and +L+ finally we adopt the corpus weighting technique +L+ for parsing on homogeneous treebanks. Section +L+ 3 provides experimental results of grammar for- +L+ malism conversion on a dependency treebank pro- +L+ duced from the Penn Treebank. In Section 4, we +L+ evaluate our two-step solution on two existing het- +L+ erogeneous Chinese treebanks. Section 5 reviews +L+ related work and Section 6 concludes this work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Our Two-Step Solution +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 2.1 Grammar Formalism Conversion +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Previous DS to PS conversion methods built a +L+ converted tree by iteratively attaching nodes and +L+ edges to the tree with the help of conversion +L+ rules and heuristic rules, based on current head- +L+ dependent pair from a source dependency tree and +L+ the structure of the built tree (Collins et al., 1999; +L+ Covington, 1994; Xia and Palmer, 2001; Xia et +L+ al., 2008). Some observations can be made on +L+ these methods: (1) for each head-dependent pair, +L+ only one locally optimal conversion was kept dur- +L+ ing tree-building process, at the risk of pruning +L+ globally optimal conversions, (2) heuristic rules +L+ are required to deal with the problem that one +L+ head-dependent pair might have multiple conver- +L+ sion candidates, and these heuristic rules are usu- +L+ ally hand-crafted to reflect the structural prefer- +L+ ence in their target grammars. To overcome these +L+ limitations, we propose to employ a parser to gen- +L+ erate N-best parses as conversion candidates and +L+ then use the structural information of source trees +L+ to select the best parse as a converted tree. +L+ We formulate our conversion method as fol- +L+ lows. +L+ Let CDS be a source treebank annotated with +L+ DS and CPS be a target treebank annotated with +L+ PS. Our goal is to convert the grammar formalism +L+ of CDS to that of CPS. +L+ We first train a constituency parser on CPS +L+ </SectLabel_bodyText> <SectLabel_page> 47 +L+ </SectLabel_page> <SectLabel_bodyText> Input: CPS, CDS, Q, and a constituency parser	Output: Converted trees Cps +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Initialize: +L+ — Set Cps,0 as null, DevScore=0, q=0; +L+ — Split CPS into training set CPS,train and development set CPS,dev; +L+ — Train the parser on CPS,train and denote it by Pq–l; +L+ 2. Repeat: +L+ — Use Pq_i to generate N-best PS parses for each sentence in CDS, and convert PS to DS for each parse; +L+ — For each sentence in CDS Do +L+ o �t=argmaxt Score (xi,t), and select the �t-th parse as a converted tree for this sentence; +L+ — Let CD S,q S represent these converted trees, and let Ctrain=CPS,train U CDSS,q ; +L+ — Train the parser on Ctrain, and denote the updated parser by Pq; +L+ — Let DevScoreq be the f-score of Pq on CPS,dev; +L+ — If DevScoreq > DevScore Then DevScore=DevScoreq, and Cps=Cps,q; +L+ — Else break; +L+ — q++; +L+ Until q > Q +L+ </SectLabel_listItem> <SectLabel_tableCaption> Table 1: Our algorithm for DS to PS conversion. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> (90% trees in CPS as training set CPS,train, and +L+ other trees as development set CPS,dev) and then +L+ let the parser generate N-best parses for each sen- +L+ tence in CDS. +L+ Let n be the number of sentences (or trees) in +L+ CDS and ni be the number of N-best parses gen- +L+ erated by the parser for the i-th (1 < i < n) sen- +L+ tence in CDS. Let xi,t be the t-th (1 < t < ni) +L+ parse for the i-th sentence. Let yi be the tree of the +L+ i-th (1 < i < n) sentence in CDS. +L+ To evaluate the quality of xi,t as a conversion +L+ candidate for yi, we convert xi,t to a dependency +L+ tree (denoted as xDS) and then use unlabeled de- +L+ pendency f-score to measure the similarity be- +L+ tween xDS and yi. Let Score(xi,t) denote the +L+ unlabeled dependency f-score of xDS against yi. +L+ Then we determine the converted tree for yi by +L+ maximizing Score(xi,t) over the N-best parses. +L+ The conversion from PS to DS works as fol- +L+ lows: +L+ Step 1. Use a head percolation table to find the +L+ head of each constituent in xi,t. +L+ Step 2. Make the head of each non-head child +L+ depend on the head of the head child for each con- +L+ stituent. +L+ Unlabeled dependency f-score is a harmonic +L+ mean of unlabeled dependency precision and unla- +L+ beled dependency recall. Precision measures how +L+ many head-dependent word pairs found in xDS +L+ i,t +L+ are correct and recall is the percentage of head- +L+ dependent word pairs defined in the gold-standard +L+ tree that are found in xDS. Here we do not take +L+ dependency tags into consideration for evaluation +L+ since they cannot be obtained without more so- +L+ phisticated rules. +L+ To improve the quality of N-best parses, we at- +L+ tempt to use the converted trees as additional train- +L+ ing data to retrain the parser. The procedure of +L+ tree conversion and parser retraining can be run it- +L+ eratively until a termination condition is satisfied. +L+ Here we use the parser’s f-score on CPS,dev as a +L+ termination criterion. If the update of training data +L+ hurts the performance on CPS,dev, then we stop +L+ the iteration. +L+ Table 1 shows this DS to PS conversion algo- +L+ rithm. Q is an upper limit of the number of loops, +L+ and Q>0. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Target Grammar Parsing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Through grammar formalism conversion, we have +L+ successfully turned the problem of using hetero- +L+ geneous treebanks for parsing into the problem of +L+ parsing on homogeneous treebanks. Before using +L+ converted source treebank for parsing, we present +L+ two strategies to refine conversion results. +L+ Instance Pruning For some sentences in +L+ CDS, the parser might fail to generate high qual- +L+ ity N-best parses, resulting in inferior converted +L+ trees. To clean the converted treebank, we can re- +L+ move the converted trees with low unlabeled de- +L+ pendency f-scores (defined in Section 2.1) before +L+ using the converted treebank for parser training +L+ </SectLabel_bodyText> <SectLabel_page> 48 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: A parse tree in CTB for a sentence of +L+ " t1t A<world> -X <every> � <country> A +L+ V�<people> N<all> 4E<with> H A<eyes> +L+ R hJ <cast> # A<Hong Kong> " with +L+ "People from all over the world are cast- +L+ ing their eyes on Hong Kong" as its English +L+ translation. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> because these trees are "misleading" training in- +L+ stances. The number of removed trees will be de- +L+ termined by cross validation on development set. +L+ Score Interpolation Unlabeled dependency +L+ f-scores used in Section 2.1 measure the quality of +L+ converted trees from the perspective of the source +L+ grammar only. In extreme cases, the top best +L+ parses in the N-best list are good conversion can- +L+ didates but we might select a parse ranked quite +L+ low in the N-best list since there might be con- +L+ flicts of syntactic structure definition between the +L+ source grammar and the target grammar. +L+ Figure 1 shows an example for illustration of +L+ a conflict between the grammar of CDT and +L+ that of CTB. According to Chinese head percola- +L+ tion tables used in the PS to DS conversion tool +L+ "Penn2Malt" 3 and Charniak’s parser4, the head +L+ of VP-2 is the word " 4E " (a preposition, with +L+ "BA" as its POS tag in CTB), and the head of +L+ IP-OBJ is R hJ " . Therefore the word " R +L+ hJ" depends on the word "4E" . But according +L+ to the annotation scheme in CDT (Liu et al., 2006), +L+ the word "4E" is a dependent of the word "R +L+ hJ " . The conflicts between the two grammars +L+ may lead to the problem that the selected parses +L+ based on the information of the source grammar +L+ might not be preferred from the perspective of the +L+ </SectLabel_bodyText> <SectLabel_footnote> 3Available at http://w3.msi.vxu.se/—nivre/. +L+ 4Available at http://www.cs.brown.edu/—ec/. +L+ </SectLabel_footnote> <SectLabel_bodyText> target grammar. +L+ Therefore we modified the selection metric in +L+ Section 2.1 by interpolating two scores, the prob- +L+ ability of a conversion candidate from the parser +L+ and its unlabeled dependency f-score, shown as +L+ follows: +L+ </SectLabel_bodyText> <SectLabel_equation> Score(xi,t) = AxProb(xi,t)+(1—A)xScore(xi,t). (1) +L+ </SectLabel_equation> <SectLabel_bodyText> The intuition behind this equation is that converted +L+ trees should be preferred from the perspective of +L+ both the source grammar and the target grammar. +L+ Here 0 < A < 1. Prob(xi,t) is a probability pro- +L+ duced by the parser for xi,t (0 < Prob(xi,t) < 1). +L+ The value of A will be tuned by cross validation on +L+ development set. +L+ After grammar formalism conversion, the prob- +L+ lem now we face has been limited to how to build +L+ parsing models on multiple homogeneous tree- +L+ bank. A possible solution is to simply concate- +L+ nate the two treebanks as training data. However +L+ this method may lead to a problem that if the size +L+ of CPS is significantly less than that of converted +L+ CDS, converted CDS may weaken the effect CPS +L+ might have. One possible solution is to reduce the +L+ weight of examples from converted CDS in parser +L+ training. Corpus weighting is exactly such an ap- +L+ proach, with the weight tuned on development set, +L+ that will be used for parsing on homogeneous tree- +L+ banks in this paper. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Experiments of Grammar Formalism +L+ Conversion +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 3.1 Evaluation on WSJ section 22 +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Xia et al. (2008) used WSJ section 19 from the +L+ Penn Treebank to extract DS to PS conversion +L+ rules and then produced dependency trees from +L+ WSJ section 22 for evaluation of their DS to PS +L+ conversion algorithm. They showed that their +L+ conversion algorithm outperformed existing meth- +L+ ods on the WSJ data. For comparison with their +L+ work, we conducted experiments in the same set- +L+ ting as theirs: using WSJ section 19 (1844 sen- +L+ tences) as CPS, producing dependency trees from +L+ WSJ section 22 (1700 sentences) as CDS5, and +L+ using labeled bracketing f-scores from the tool +L+ "EVALB" on WSJ section 22 for performance +L+ evaluation. +L+ </SectLabel_bodyText> <SectLabel_footnote> 5We used the tool "Penn2Malt" to produce dependency +L+ structures from the Penn Treebank, which was also used for +L+ PS to DS conversion in our conversion algorithm. +L+ </SectLabel_footnote> <SectLabel_page> 49 +L+ </SectLabel_page> <SectLabel_table> 	DevScore	All the sentences +L+ 		LR	LP	F +L+ Models	(%)	(%)	(%)	(%) +L+ The best result of +L+ Xia et al. (2008)	-	90.7	88.1	89.4 +L+ Q-0-method	86.8	92.2	92.8	92.5 +L+ Q-10-method	88.0	93.4	94.1	93.8 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Comparison with the work of Xia et al. +L+ </SectLabel_tableCaption> <SectLabel_table> (2008) on WSJ section 22. +L+ 		All the sentences +L+ 	DevScore	LR	LP	F +L+ Models	(%)	(%)	(%)	(%) +L+ Q-0-method	91.0	91.6	92.5	92.1 +L+ Q-10-method	91.6	93.1	94.1	93.6 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3: Results of our algorithm on WSJ section +L+ 2-18 and 20-22. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> We employed Charniak’s maximum entropy in- +L+ spired parser (Charniak, 2000) to generate N-best +L+ (N=200) parses. Xia et al. (2008) used POS +L+ tag information, dependency structures and depen- +L+ dency tags in test set for conversion. Similarly, we +L+ used POS tag information in the test set to restrict +L+ search space of the parser for generation of better +L+ N-best parses. +L+ We evaluated two variants of our DS to PS con- +L+ version algorithm: +L+ Q-0-method: We set the value of Q as 0 for a +L+ baseline method. +L+ Q-10-method: We set the value of Q as 10 to +L+ see whether it is helpful for conversion to retrain +L+ the parser on converted trees. +L+ Table 2 shows the results of our conversion al- +L+ gorithm on WSJ section 22. In the experiment +L+ of Q-10-method, DevScore reached the highest +L+ value of 88.0% when q was 1. Then we used +L+ Cps,1 as the conversion result. Finally Q-10- +L+ method achieved an f-score of 93.8% on WSJ sec- +L+ tion 22, an absolute 4.4% improvement (42% er- +L+ ror reduction) over the best result of Xia et al. +L+ (2008). Moreover, Q-10-method outperformed Q- +L+ 0-method on the same test set. These results indi- +L+ cate that it is feasible to use a parser for DS to PS +L+ conversion and the conversion benefits from the +L+ use of converted trees for parser retraining. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Evaluation on WSJ section 2-18 and +L+ 20-22 +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this experiment we evaluated our conversion al- +L+ gorithm on a larger test set, WSJ section 2-18 and +L+ 20-22 (totally 39688 sentences). Here we also +L+ used WSJ section 19 as CPS. Other settings for +L+ </SectLabel_bodyText> <SectLabel_table> Training data	All the sentences +L+ 	LR	LP	F +L+ 	(%)	(%)	(%) +L+ 1 x CTB + CDTPS	84.7	85.1	84.9 +L+ 2 x CTB + CDTPS	85.1	85.6	85.3 +L+ 5 x CTB + CDTPS	85.0	85.5	85.3 +L+ 10 x CTB +CDTPS	85.3	85.8	85.6 +L+ 20 x CTB +CDTPS	85.1	85.3	85.2 +L+ 50 x CTB +CDTPS	84.9	85.3	85.1 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4: Results of the generative parser on the de- +L+ velopment set, when trained with various weight- +L+ ing of CTB training set and CDTPS. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> this experiment are as same as that in Section 3. 1, +L+ except that here we used a larger test set. +L+ Table 3 provides the f-scores of our method with +L+ Q equal to 0 or 10 on WSJ section 2-18 and +L+ 20-22. +L+ With Q-10-method, DevScore reached the high- +L+ est value of 91.6% when q was 1. Finally Q- +L+ 10-method achieved an f-score of 93.6% on WSJ +L+ section 2-18 and 20-22, better than that of Q-0- +L+ method and comparable with that of Q-10-method +L+ in Section 3.1. It confirms our previous finding +L+ that the conversion benefits from the use of con- +L+ verted trees for parser retraining. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Experiments of Parsing +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We investigated our two-step solution on two ex- +L+ isting treebanks, CDT and CTB, and we used CDT +L+ as the source treebank and CTB as the target tree- +L+ bank. +L+ CDT consists of 60k Chinese sentences, anno- +L+ tated with POS tag information and dependency +L+ structure information (including 28 POS tags, and +L+ 24 dependency tags) (Liu et al., 2006). We did not +L+ use POS tag information as inputs to the parser in +L+ our conversion method due to the difficulty of con- +L+ version from CDT POS tags to CTB POS tags. +L+ We used a standard split of CTB for perfor- +L+ mance evaluation, articles 1-270 and 400-1151 as +L+ training set, articles 301-325 as development set, +L+ and articles 271-300 as test set. +L+ We used Charniak’s maximum entropy inspired +L+ parser and their reranker (Charniak and Johnson, +L+ 2005) for target grammar parsing, called a gener- +L+ ative parser (GP) and a reranking parser (RP) re- +L+ spectively. We reported ParseVal measures from +L+ the EVALB tool. +L+ </SectLabel_bodyText> <SectLabel_page> 50 +L+ </SectLabel_page> <SectLabel_table> 		All the sentences +L+ 		LR	LP	F +L+ Models	Training data	(%)	(%)	(%) +L+ GP	CTB	79.9	82.2	81.0 +L+ RP	CTB	82.0	84.6	83.3 +L+ GP	10 x CTB + CDTPS	80.4	82.7	81.5 +L+ RP	10 x CTB + CDTPS	82.8	84.7	83.8 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5: Results of the generative parser (GP) and +L+ the reranking parser (RP) on the test set, when +L+ trained on only CTB training set or an optimal +L+ combination of CTB training set and CDTPS. +L+ </SectLabel_tableCaption> <SectLabel_subsectionHeader> 4.1 Results of a Baseline Method to Use CDT +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We used our conversion algorithm6 to convert the +L+ grammar formalism of CDT to that of CTB. Let +L+ CDTPS denote the converted CDT by our method. +L+ The average unlabeled dependency f-score of trees +L+ in CDTPS was 74.4%, and their average index in +L+ 200-best list was 48. +L+ We tried the corpus weighting method when +L+ combining CDTPS with CTB training set (abbre- +L+ viated as CTB for simplicity) as training data, by +L+ gradually increasing the weight (including 1, 2, 5, +L+ 10, 20, 50) of CTB to optimize parsing perfor- +L+ mance on the development set. Table 4 presents +L+ the results of the generative parser with various +L+ weights of CTB on the development set. Consid- +L+ ering the performance on the development set, we +L+ decided to give CTB a relative weight of 10. +L+ Finally we evaluated two parsing models, the +L+ generative parser and the reranking parser, on the +L+ test set, with results shown in Table 5. When +L+ trained on CTB only, the generative parser and the +L+ reranking parser achieved f-scores of 81.0% and +L+ 83.3%. The use of CDTPS as additional training +L+ data increased f-scores of the two models to 81.5% +L+ and 83.8%. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Results of Two Strategies for a Better Use +L+ of CDT +L+ </SectLabel_subsectionHeader> <SectLabel_subsubsectionHeader> 4.2.1 Instance Pruning +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> We used unlabeled dependency f-score of each +L+ converted tree as the criterion to rank trees in +L+ CDTPS and then kept only the top M trees +L+ with high f-scores as training data for pars- +L+ ing, resulting in a corpus CDTPMS. M var- +L+ ied from 100% xICDTPSI to 10% xICDTPSI +L+ with 10%xICDTPSI as the interval. ICDTPSI +L+ </SectLabel_bodyText> <SectLabel_footnote> 6The setting for our conversion algorithm in this experi- +L+ ment was as same as that in Section 3.1. In addition, we used +L+ CTB training set as CPS,tr�i�, and CTB development set as +L+ CPS,dev. +L+ </SectLabel_footnote> <SectLabel_table> 		All the sentences +L+ 		LR	LP	F +L+ Models	Training data	(%)	(%)	(%) +L+ GP	CTB + CDTa S	81.4	82.8	82.1 +L+ RP	CTB + CDTa S	83.0	85.4	84.2 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 6: Results of the generative parser and the +L+ reranking parser on the test set, when trained on +L+ an optimal combination of CTB training set and +L+ converted CDT. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> is the number of trees in CDTPS. Then +L+ we tuned the value of M by optimizing the +L+ parser’s performance on the development set with +L+ 10 x CTB+CDTPMS as training data. Finally the op- +L+ timal value of M was 100%x I CDT I. It indicates +L+ that even removing very few converted trees hurts +L+ the parsing performance. A possible reason is that +L+ most of non-perfect parses can provide useful syn- +L+ tactic structure information for building parsing +L+ models. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.2 Score Interpolation +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> We used �Score(xj,t)7 to replace Score(xj,t) in +L+ our conversion algorithm and then ran the updated +L+ algorithm on CDT. Let CDTP S denote the con- +L+ verted CDT by this updated conversion algorithm. +L+ The values of A (varying from 0.0 to 1.0 with 0.1 +L+ as the interval) and the CTB weight (including 1, +L+ 2, 5, 10, 20, 50) were simultaneously tuned on the +L+ development set8. Finally we decided that the op- +L+ timal value of A was 0.4 and the optimal weight of +L+ CTB was 1, which brought the best performance +L+ on the development set (an f-score of 86.1%). In +L+ comparison with the results in Section 4.1, the +L+ average index of converted trees in 200-best list +L+ increased to 2, and their average unlabeled depen- +L+ dency f-score dropped to 65.4%. It indicates that +L+ structures of converted trees become more consis- +L+ tent with the target grammar, as indicated by the +L+ increase of average index of converted trees, fur- +L+ ther away from the source grammar. +L+ Table 6 provides f-scores of the generative +L+ parser and the reranker on the test set, when +L+ trained on CTB and CDTP S. We see that the +L+ performance of the reranking parser increased to +L+ </SectLabel_bodyText> <SectLabel_footnote> 7Before calculating	�Score(xi,t),	we normal- +L+ ized the values of Prob(xi,t) for each N-best list +L+ by	(1)	Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,*)), +L+ (2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,*)),	resulting +L+ in that their maximum value was 1 and their minimum value +L+ was 0. +L+ 8Due to space constraint, we do not show f-scores of the +L+ parser with different values of A and the CTB weight. +L+ </SectLabel_footnote> <SectLabel_page> 51 +L+ </SectLabel_page> <SectLabel_table> 		All the sentences +L+ 		LR	LP	F +L+ Models	Training data	(%)	(%)	(%) +L+ Self-trained GP	10 � T+10 � D+P	83.0	84.5	83.7 +L+ Updated RP	CTB+CDT. S	84.3	86.1	85.2 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 7: Results of the self-trained gen- +L+ erative parser and updated reranking parser +L+ on the test set. 10 x T+10 x D+P stands for +L+ 10 x CTB+10 x CDTP s+PDC. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> 84.2% f-score, better than the result of the rerank- +L+ ing parser with CTB and CDTPS as training data +L+ (shown in Table 5). It indicates that the use of +L+ probability information from the parser for tree +L+ conversion helps target grammar parsing. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Using Unlabeled Data for Parsing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Recent studies on parsing indicate that the use of +L+ unlabeled data by self-training can help parsing +L+ on the WSJ data, even when labeled data is rel- +L+ atively large (McClosky et al., 2006a; Reichart +L+ and Rappoport, 2007). It motivates us to em- +L+ ploy self-training technique for Chinese parsing. +L+ We used the POS tagged People Daily corpus9 +L+ (Jan. 1998—Jun. 1998, and Jan. 2000—Dec. +L+ 2000) (PDC) as unlabeled data for parsing. First +L+ we removed the sentences with less than 3 words +L+ or more than 40 words from PDC to ease pars- +L+ ing, resulting in 820k sentences. Then we ran the +L+ reranking parser in Section 4.2.2 on PDC and used +L+ the parses on PDC as additional training data for +L+ the generative parser. Here we tried the corpus +L+ weighting technique for an optimal combination +L+ of CTB, CDTP s and parsed PDC, and chose the +L+ relative weight of both CTB and CDTP s as 10 +L+ by cross validation on the development set. Fi- +L+ nally we retrained the generative parser on CTB, +L+ CDTP s and parsed PDC. Furthermore, we used +L+ this self-trained generative parser as a base parser +L+ to retrain the reranker on CTB and CDTP s. +L+ Table 7 shows the performance of self-trained +L+ generative parser and updated reranker on the test +L+ set, with CTB and CDTP s as labeled data. We see +L+ that the use of unlabeled data by self-training fur- +L+ ther increased the reranking parser’s performance +L+ from 84.2% to 85.2%. Our results on Chinese data +L+ confirm previous findings on English data shown +L+ in (McClosky et al., 2006a; Reichart and Rap- +L+ poport, 2007). +L+ </SectLabel_bodyText> <SectLabel_footnote> 9Available at http://icl.pku.edu.cn/. +L+ </SectLabel_footnote> <SectLabel_subsectionHeader> 4.4 Comparison with Previous Studies for +L+ Chinese Parsing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Table 8 and 9 present the results of previous stud- +L+ ies on CTB. All the works in Table 8 used CTB +L+ articles 1-270 as labeled data. In Table 9, Petrov +L+ and Klein (2007) trained their model on CTB ar- +L+ ticles 1-270 and 400-1151, and Burkett and Klein +L+ (2008) used the same CTB articles and parse trees +L+ of their English translation (from the English Chi- +L+ nese Translation Treebank) as training data. Com- +L+ paring our result in Table 6 with that of Petrov +L+ and Klein (2007), we see that CDTP s helps pars- +L+ ing on CTB, which brought 0.9% f-score improve- +L+ ment. Moreover, the use of unlabeled data further +L+ boosted the parsing performance to 85.2%, an ab- +L+ solute 1.0% improvement over the previous best +L+ result presented in Burkett and Klein (2008). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Recently there have been some studies address- +L+ ing how to use treebanks with same grammar for- +L+ malism for domain adaptation of parsers. Roark +L+ and Bachiani (2003) presented count merging and +L+ model interpolation techniques for domain adap- +L+ tation of parsers. They showed that their sys- +L+ tem with count merging achieved a higher perfor- +L+ mance when in-domain data was weighted more +L+ heavily than out-of-domain data. McClosky et al. +L+ (2006b) used self-training and corpus weighting to +L+ adapt their parser trained on WSJ corpus to Brown +L+ corpus. Their results indicated that both unla- +L+ beled in-domain data and labeled out-of-domain +L+ data can help domain adaptation. In comparison +L+ with these works, we conduct our study in a dif- +L+ ferent setting where we work with multiple het- +L+ erogeneous treebanks. +L+ Grammar formalism conversion makes it possi- +L+ ble to reuse existing source treebanks for the study +L+ of target grammar parsing. Wang et al. (1994) +L+ employed a parser to help conversion of a tree- +L+ bank from a simple phrase structure to a more in- +L+ formative phrase structure and then used this con- +L+ verted treebank to train their parser. Collins et al. +L+ (1999) performed statistical constituency parsing +L+ of Czech on a treebank that was converted from +L+ the Prague Dependency Treebank under the guid- +L+ ance of conversion rules and heuristic rules, e.g., +L+ one level of projection for any category, minimal +L+ projection for any dependents, and fixed position +L+ of attachment. Xia and Palmer (2001) adopted bet- +L+ ter heuristic rules to build converted trees, which +L+ </SectLabel_bodyText> <SectLabel_page> 52 +L+ </SectLabel_page> <SectLabel_table> Models	< 40 words			All the sentences +L+ 	LR	LP	F	LR	LP	F +L+ 	(%)	(%)	(%)	(%)	(%)	(%) +L+ Bikel & Chiang (2000)	76.8	77.8	77.3	-	-	- +L+ Chiang & Bikel (2002)	78.8	81.1	79.9	-	-	- +L+ Levy & Manning (2003)	79.2	78.4	78.8	-	-	- +L+ Bikel’s thesis (2004)	78.0	81.2	79.6	-	-	- +L+ Xiong et. al. (2005)	78.7	80.1	79.4	-	-	- +L+ Chen et. al. (2005)	81.0	81.7	81.2	76.3	79.2	77.7 +L+ Wang et. al. (2006)	79.2	81.1	80.1	76.2	78.0	77.1 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data. +L+ </SectLabel_tableCaption> <SectLabel_table> 	< 40 words			All the sentences +L+ 	LR	LP	F	LR	LP	F +L+ Models	(%)	(%)	(%)	(%)	(%)	(%) +L+ Petrov & Klein (2007)	85.7	86.9	86.3	81.9	84.8	83.3 +L+ Burkett & Klein (2008)	-	-	-	-	-	84.2 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 9: Results of previous studies on CTB with more labeled data. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> reflected the structural preference in their target +L+ grammar. For acquisition of better conversion +L+ rules, Xia et al. (2008) proposed to automati- +L+ cally extract conversion rules from a target tree- +L+ bank. Moreover, they presented two strategies to +L+ solve the problem that there might be multiple +L+ conversion rules matching the same input depen- +L+ dency tree pattern: (1) choosing the most frequent +L+ rules, (2) preferring rules that add fewer number +L+ of nodes and attach the subtree lower. +L+ In comparison with the works of Wang et al. +L+ (1994) and Collins et al. (1999), we went fur- +L+ ther by combining the converted treebank with the +L+ existing target treebank for parsing. In compar- +L+ ison with previous conversion methods (Collins +L+ et al., 1999; Covington, 1994; Xia and Palmer, +L+ 2001; Xia et al., 2008) in which for each head- +L+ dependent pair, only one locally optimal conver- +L+ sion was kept during tree-building process, we +L+ employed a parser to generate globally optimal +L+ syntactic structures, eliminating heuristic rules for +L+ conversion. In addition, we used converted trees to +L+ retrain the parser for better conversion candidates, +L+ while Wang et al. (1994) did not exploit the use of +L+ converted trees for parser retraining. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Conclusion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have proposed a two-step solution to deal with +L+ the issue of using heterogeneous treebanks for +L+ parsing. First we present a parser based method +L+ to convert grammar formalisms of the treebanks to +L+ the same one, without applying predefined heuris- +L+ tic rules, thus turning the original problem into the +L+ problem of parsing on homogeneous treebanks. +L+ Then we present two strategies, instance pruning +L+ and score interpolation, to refine conversion re- +L+ sults. Finally we adopt the corpus weighting tech- +L+ nique to combine the converted source treebank +L+ with the existing target treebank for parser train- +L+ ing. +L+ The study on the WSJ data shows the benefits of +L+ our parser based approach for grammar formalism +L+ conversion. Moreover, experimental results on the +L+ Penn Chinese Treebank indicate that a converted +L+ dependency treebank helps constituency parsing, +L+ and it is better to exploit probability information +L+ produced by the parser through score interpolation +L+ than to prune low quality trees for the use of the +L+ converted treebank. +L+ Future work includes further investigation of +L+ our conversion method for other pairs of grammar +L+ formalisms, e.g., from the grammar formalism of +L+ the Penn Treebank to more deep linguistic formal- +L+ ism like CCG, HPSG, or LFG. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Anne Abeille, Lionel Clement and Francois Toussenel. 2000. +L+ Building a Treebank for French. In Proceedings of LREC +L+ 2000, pages 87-94. +L+ Daniel Bikel and David Chiang. 2000. Two Statistical Pars- +L+ ing Models Applied to the Chinese Treebank. In Proceed- +L+ ings of the Second SIGHAN workshop, pages 1-6. +L+ Daniel Bikel. 2004. On the Parameter Space of Generative +L+ Lexicalized Statistical Parsing Models. Ph.D. thesis, Uni- +L+ versity of Pennsylvania. +L+ Alena Bohmova, Jan Hajic, Eva Hajicova and Barbora +L+ Vidova-Hladka. 2003. The Prague Dependency Tree- +L+ bank: A Three-Level Annotation Scenario. Treebanks: +L+ </SectLabel_reference> <SectLabel_page> 53 +L+ </SectLabel_page> <SectLabel_reference> Building and Using Annotated Corpora. Kluwer Aca- +L+ demic Publishers, pages 103-127. +L+ Thorsten Brants, Wojciech Skut and Hans Uszkoreit. 1999. +L+ Syntactic Annotation of a German Newspaper Corpus. In +L+ Proceedings of the ATALA Treebank Workshop, pages 69- +L+ 76. +L+ David Burkett and Dan Klein. 2008. Two Languages are +L+ Better than One (for Syntactic Parsing). In Proceedings of +L+ EMNLP 2008, pages 877-886. +L+ Eugene Charniak. 2000. A Maximum Entropy Inspired +L+ Parser. In Proceedings of NAACL 2000, pages 132-139. +L+ Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine +L+ N-Best Parsing and MaxEnt Discriminative Reranking. In +L+ Proceedings ofACL 2005, pages 173-180. +L+ Ying Chen, Hongling Sun and Dan Jurafsky. 2005. A Cor- +L+ rigendum to Sun and Jurafsky (2004) Shallow Semantic +L+ Parsing of Chinese. University of Colorado at Boulder +L+ CSLR Tech Report TR-CSLR-2005-01. +L+ David Chiang and Daniel M. Bikel. 2002. Recovering La- +L+ tent Information in Treebanks. In Proceedings of COL- +L+ ING 2002, pages 1-7. +L+ Micheal Collins, Lance Ramshaw, Jan Hajic and Christoph +L+ Tillmann. 1999. A Statistical Parser for Czech. In Pro- +L+ ceedings ofACL 1999, pages 505-512. +L+ Micheal Covington. 1994. GB Theory as Dependency +L+ Grammar. Research Report AI-1992-03. +L+ Martin Forst. 2003. Treebank Conversion - Establishing +L+ a Testsuite for a Broad-Coverage LFG from the TIGER +L+ Treebank. In Proceedings of LINC at EACL 2003, pages +L+ 25-32. +L+ Chunghye Han, Narae Han, Eonsuk Ko and Martha Palmer. +L+ 2002. Development and Evaluation of a Korean Treebank +L+ and its Application to NLP. In Proceedings ofLREC 2002, +L+ pages 1635-1642. +L+ Sadao Kurohashi and Makato Nagao. 1998. Building a +L+ Japanese Parsed Corpus While Improving the Parsing Sys- +L+ tem. In Proceedings of LREC 1998, pages 719-724. +L+ Roger Levy and Christopher Manning. 2003. Is It Harder to +L+ Parse Chinese, or the Chinese Treebank? In Proceedings +L+ ofACL 2003, pages 439-446. +L+ Ting Liu, Jinshan Ma and Sheng Li. 2006. Building a Depen- +L+ dency Treebank for Improving Chinese Parser. Journal of +L+ Chinese Language and Computing, 16(4):207-224. +L+ Mitchell P. Marcus, Beatrice Santorini and Mary Ann +L+ Marcinkiewicz. 1993. Building a Large Annotated Cor- +L+ pus of English: The Penn Treebank. Computational Lin- +L+ guistics, 19(2):313-330. +L+ David McClosky, Eugene Charniak and Mark Johnson. +L+ 2006a. Effective Self-Training for Parsing. In Proceed- +L+ ings of NAACL 2006, pages 152-159. +L+ David McClosky, Eugene Charniak and Mark Johnson. +L+ 2006b. Reranking and Self-Training for Parser Adapta- +L+ tion. In Proceedings of COLING/ACL 2006, pages 337- +L+ 344. +L+ Antonio Moreno, Susana Lopez, Fernando Sanchez and +L+ Ralph Grishman. 2003. Developing a Syntactic Anno- +L+ tation Scheme and Tools for a Spanish Treebank. Tree- +L+ banks: Building and Using Annotated Corpora. Kluwer +L+ Academic Publishers, pages 149-163. +L+ Slav Petrov and Dan Klein. 2007. Improved Inference for +L+ Unlexicalized Parsing. In Proceedings of HLT/NAACL +L+ 2007, pages 404-411. +L+ Roi Reichart and Ari Rappoport. 2007. Self-Training for En- +L+ hancement and Domain Adaptation of Statistical Parsers +L+ Trained on Small Datasets. In Proceedings of ACL 2007, +L+ pages 616-623. +L+ Brian Roark and Michiel Bacchiani. 2003. Supervised and +L+ Unsupervised PCFG Adaptation to Novel Domains. In +L+ Proceedings of HLT/NAACL 2003, pages 126-133. +L+ Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su. 1994. +L+ An Automatic Treebank Conversion Algorithm for Corpus +L+ Sharing. In Proceedings ofACL 1994, pages 248-254. +L+ Mengqiu Wang, Kenji Sagae and Teruko Mitamura. 2006. A +L+ Fast, Accurate Deterministic Parser for Chinese. In Pro- +L+ ceedings of COLING/ACL 2006, pages 425-432. +L+ Stephen Watkinson and Suresh Manandhar. 2001. Translat- +L+ ing Treebank Annotation for Evaluation. In Proceedings +L+ of ACL Workshop on Evaluation Methodologies for Lan- +L+ guage and Dialogue Systems, pages 1-8. +L+ Fei Xia and Martha Palmer. 2001. Converting Dependency +L+ Structures to Phrase Structures. In Proceedings of HLT +L+ 2001, pages 1-5. +L+ Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer +L+ and Dipti Misra. Sharma. 2008. Towards a Multi- +L+ Representational Treebank. In Proceedings of the 7th In- +L+ ternational Workshop on Treebanks and Linguistic Theo- +L+ ries, pages 159-170. +L+ Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin and +L+ Yueliang Qian. 2005. Parsing the Penn Chinese Tree- +L+ bank with Semantic Knowledge. In Proceedings of IJC- +L+ NLP 2005, pages 70-81. +L+ Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer. +L+ 2005. The Penn Chinese TreeBank: Phrase Structure An- +L+ notation of a Large Corpus. Natural Language Engineer- +L+ ing, 11(2):207-238. +L+ </SectLabel_reference> <SectLabel_page> 54 +L+ </SectLabel_page>
<SectLabel_title> Cross Language Dependency Parsing using a Bilingual Lexicon* +L+ </SectLabel_title> <SectLabel_author> Hai Zhao(O— )tt, Yan Song(*,,O)t, Chunyu Kitt, Guodong Zhout +L+ </SectLabel_author> <SectLabel_affiliation> tDepartment of Chinese, Translation and Linguistics +L+ City University of Hong Kong +L+ </SectLabel_affiliation> <SectLabel_address> 83 Tat Chee Avenue, Kowloon, Hong Kong, China +L+ </SectLabel_address> <SectLabel_affiliation> $School of Computer Science and Technology +L+ </SectLabel_affiliation> <SectLabel_address> Soochow University, Suzhou, China 2'5006 +L+ </SectLabel_address> <SectLabel_email> {haizhao,yansong,ctckit}@cityu.edu.hk, gdzhou@suda.edu.cn +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper proposes an approach to en- +L+ hance dependency parsing in a language +L+ by using a translated treebank from an- +L+ other language. A simple statistical ma- +L+ chine translation method, word-by-word +L+ decoding, where not a parallel corpus but +L+ a bilingual lexicon is necessary, is adopted +L+ for the treebank translation. Using an en- +L+ semble method, the key information ex- +L+ tracted from word pairs with dependency +L+ relations in the translated text is effectively +L+ integrated into the parser for the target lan- +L+ guage. The proposed method is evaluated +L+ in English and Chinese treebanks. It is +L+ shown that a translated English treebank +L+ helps a Chinese parser obtain a state-of- +L+ the-art result. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Although supervised learning methods bring state- +L+ of-the-art outcome for dependency parser infer- +L+ ring (McDonald et al., 2005; Hall et al., 2007), a +L+ large enough data set is often required for specific +L+ parsing accuracy according to this type of meth- +L+ ods. However, to annotate syntactic structure, ei- +L+ ther phrase- or dependency-based, is a costly job. +L+ Until now, the largest treebanks' in various lan- +L+ guages for syntax learning are with around one +L+ million words (or some other similar units). Lim- +L+ ited data stand in the way of further performance +L+ enhancement. This is the case for each individual +L+ language at least. But, this is not the case as we +L+ observe all treebanks in different languages as a +L+ whole. For example, of ten treebanks for CoNLL- +L+ 2007 shared task, none includes more than 500K +L+ </SectLabel_bodyText> <SectLabel_footnote> �The study is partially supported by City University of +L+ Hong Kong through the Strategic Research Grant 7002037 +L+ and 7002388. The first author is sponsored by a research fel- +L+ lowship from CTL, City University of Hong Kong. +L+ 'It is a tradition to call an annotated syntactic corpus as +L+ treebank in parsing community. +L+ </SectLabel_footnote> <SectLabel_bodyText> tokens, while the sum of tokens from all treebanks +L+ is about two million (Nivre et al., 2007). +L+ As different human languages or treebanks +L+ should share something common, this makes it +L+ possible to let dependency parsing in multiple lan- +L+ guages be beneficial with each other. In this pa- +L+ per, we study how to improve dependency parsing +L+ by using (automatically) translated texts attached +L+ with transformed dependency information. As a +L+ case study, we consider how to enhance a Chinese +L+ dependency parser by using a translated English +L+ treebank. What our method relies on is not the +L+ close relation of the chosen language pair but the +L+ similarity of two treebanks, this is the most differ- +L+ ent from the previous work. +L+ Two main obstacles are supposed to confront in +L+ a cross-language dependency parsing task. The +L+ first is the cost of translation. Machine translation +L+ has been shown one of the most expensive lan- +L+ guage processing tasks, as a great deal of time and +L+ space is required to perform this task. In addition, +L+ a standard statistical machine translation method +L+ based on a parallel corpus will not work effec- +L+ tively if it is not able to find a parallel corpus that +L+ right covers source and target treebanks. How- +L+ ever, dependency parsing focuses on the relations +L+ of word pairs, this allows us to use a dictionary- +L+ based translation without assuming a parallel cor- +L+ pus available, and the training stage of translation +L+ may be ignored and the decoding will be quite fast +L+ in this case. The second difficulty is that the out- +L+ puts of translation are hardly qualified for the pars- +L+ ing purpose. The most challenge in this aspect is +L+ morphological preprocessing. We regard that the +L+ morphological issue should be handled aiming at +L+ the specific language, our solution here is to use +L+ character-level features for a target language like +L+ Chinese. +L+ The rest of the paper is organized as follows. +L+ The next section presents some related existing +L+ work. Section 3 describes the procedure on tree- +L+ </SectLabel_bodyText> <SectLabel_page> 55 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> bank translation and dependency transformation. +L+ Section 4 describes a dependency parser for Chi- +L+ nese as a baseline. Section 5 describes how a +L+ parser can be strengthened from the translated +L+ treebank. The experimental results are reported in +L+ Section 6. Section 7 looks into a few issues con- +L+ cerning the conditions that the proposed approach +L+ is suitable for. Section 8 concludes the paper. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 The Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> As this work is about exploiting extra resources to +L+ enhance an existing parser, it is related to domain +L+ adaption for parsing that has been draw some in- +L+ terests in recent years. Typical domain adaptation +L+ tasks often assume annotated data in new domain +L+ absent or insufficient and a large scale unlabeled +L+ data available. As unlabeled data are concerned, +L+ semi-supervised or unsupervised methods will be +L+ naturally adopted. In previous works, two basic +L+ types of methods can be identified to enhance an +L+ existing parser from additional resources. The first +L+ is usually focus on exploiting automatic generated +L+ labeled data from the unlabeled data (Steedman +L+ et al., 2003; McClosky et al., 2006; Reichart and +L+ Rappoport, 2007; Sagae and Tsujii, 2007; Chen +L+ et al., 2008), the second is on combining super- +L+ vised and unsupervised methods, and only unla- +L+ beled data are considered (Smith and Eisner, 2006; +L+ Wang and Schuurmans, 2008; Koo et al., 2008). +L+ Our purpose in this study is to obtain a further +L+ performance enhancement by exploiting treebanks +L+ in other languages. This is similar to the above +L+ first type of methods, some assistant data should +L+ be automatically generated for the subsequent pro- +L+ cessing. The differences are what type of data are +L+ concerned with and how they are produced. In our +L+ method, a machine translation method is applied +L+ to tackle golden-standard treebank, while all the +L+ previous works focus on the unlabeled data. +L+ Although cross-language technique has been +L+ used in other natural language processing tasks, +L+ it is basically new for syntactic parsing as few +L+ works were concerned with this issue. The rea- +L+ son is straightforward, syntactic structure is too +L+ complicated to be properly translated and the cost +L+ of translation cannot be afforded in many cases. +L+ However, we empirically find this difficulty may +L+ be dramatically alleviated as dependencies rather +L+ than phrases are used for syntactic structure repre- +L+ sentation. Even the translation outputs are not so +L+ good as the expected, a dependency parser for the +L+ target language can effectively make use of them +L+ by only considering the most related information +L+ extracted from the translated text. +L+ The basic idea to support this work is to make +L+ use of the semantic connection between different +L+ languages. In this sense, it is related to the work of +L+ (Merlo et al., 2002) and (Burkett and Klein, 2008). +L+ The former showed that complementary informa- +L+ tion about English verbs can be extracted from +L+ their translations in a second language (Chinese) +L+ and the use of multilingual features improves clas- +L+ sification performance of the English verbs. The +L+ latter iteratively trained a model to maximize the +L+ marginal likelihood of tree pairs, with alignments +L+ treated as latent variables, and then jointly parsing +L+ bilingual sentences in a translation pair. The pro- +L+ posed parser using features from monolingual and +L+ mutual constraints helped its log-linear model to +L+ achieve better performance for both monolingual +L+ parsers and machine translation system. In this +L+ work, cross-language features will be also adopted +L+ as the latter work. However, although it is not es- +L+ sentially different, we only focus on dependency +L+ parsing itself, while the parsing scheme in (Bur- +L+ kett and Klein, 2008) based on a constituent rep- +L+ resentation. +L+ Among of existing works that we are aware of, +L+ we regard that the most similar one to ours is (Ze- +L+ man and Resnik, 2008), who adapted a parser to a +L+ new language that is much poorer in linguistic re- +L+ sources than the source language. However, there +L+ are two main differences between their work and +L+ ours. The first is that they considered a pair of suf- +L+ ficiently related languages, Danish and Swedish, +L+ and made full use of the similar characteristics of +L+ two languages. Here we consider two quite dif- +L+ ferent languages, English and Chinese. As fewer +L+ language properties are concerned, our approach +L+ holds the more possibility to be extended to other +L+ language pairs than theirs. The second is that a +L+ parallel corpus is required for their work and a +L+ strict statistical machine translation procedure was +L+ performed, while our approach holds a merit of +L+ simplicity as only a bilingual lexicon is required. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Treebank Translation and Dependency +L+ Transformation +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 3.1 Data +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As a case study, this work will be conducted be- +L+ tween the source language, English, and the tar- +L+ get language, Chinese, namely, we will investigate +L+ </SectLabel_bodyText> <SectLabel_page> 56 +L+ </SectLabel_page> <SectLabel_bodyText> how a translated English treebank enhances a Chi- +L+ nese dependency parser. +L+ For English data, the Penn Treebank (PTB) 3 +L+ is used. The constituency structures is converted +L+ to dependency trees by using the same rules as +L+ (Yamada and Matsumoto, 2003) and the standard +L+ training/development/test split is used. However, +L+ only training corpus (sections 2-21) is used for +L+ this study. For Chinese data, the Chinese Treebank +L+ (CTB) version 4.0 is used in our experiments. The +L+ same rules for conversion and the same data split +L+ is adopted as (Wang et al., 2007): files 1-270 and +L+ 400-931 as training, 271-300 as testing and files +L+ 301-325 as development. We use the gold stan- +L+ dard segmentation and part-of-speech (POS) tags +L+ in both treebanks. +L+ As a bilingual lexicon is required for our task +L+ and none of existing lexicons are suitable for trans- +L+ lating PTB, two lexicons, LDC Chinese-English +L+ Translation Lexicon Version 2.0 (LDC2002L27), +L+ and an English to Chinese lexicon in StarDict2, +L+ are conflated, with some necessary manual exten- +L+ sions, to cover 99% words appearing in the PTB +L+ (the most part of the untranslated words are named +L+ entities.). This lexicon includes 123K entries. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Translation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A word-by-word statistical machine translation +L+ strategy is adopted to translate words attached +L+ with the respective dependency information from +L+ the source language to the target one. In detail, a +L+ word-based decoding is used, which adopts a log- +L+ linear framework as in (Och and Ney, 2002) with +L+ only two features, translation model and language +L+ model, +L+ </SectLabel_bodyText> <SectLabel_equation> exp[E2i� 1 Aihi(c, e)] +L+ E, exp[E2i� 1 Aihi(c, e)] +L+ Where +L+ h1 (c, e) = log(p .y(c�e)) +L+ </SectLabel_equation> <SectLabel_bodyText> is the translation model, which is converted from +L+ the bilingual lexicon, and +L+ </SectLabel_bodyText> <SectLabel_equation> h2 (c, e) = log (pO (c)) +L+ </SectLabel_equation> <SectLabel_bodyText> is the language model, a word trigram model +L+ trained from the CTB. In our experiment, we set +L+ two weights A1 = A2 = 1. +L+ </SectLabel_bodyText> <SectLabel_footnote> 2StarDict is an open source dictionary software, available +L+ at http://stardict.sourceforge.net/. +L+ </SectLabel_footnote> <SectLabel_bodyText> The conversion process of the source treebank +L+ is completed by three steps as the following: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Bind POS tag and dependency relation of a +L+ word with itself; +L+ 2. Translate the PTB text into Chinese word by +L+ </SectLabel_listItem> <SectLabel_bodyText> word. Since we use a lexicon rather than a parallel +L+ corpus to estimate the translation probabilities, we +L+ simply assign uniform probabilities to all transla- +L+ tion options. Thus the decoding process is actu- +L+ ally only determined by the language model. Sim- +L+ ilar to the “bag translation” experiment in (Brown +L+ et al., 1990), the candidate target sentences made +L+ up by a sequence of the optional target words are +L+ ranked by the trigram language model. The output +L+ sentence will be generated only if it is with maxi- +L+ mum probability as follows, +L+ </SectLabel_bodyText> <SectLabel_equation> c = argmax{pO(c)p.y(c�e)} +L+ = argmax pO (c) +L+ = argmaxn pO (w,) +L+ </SectLabel_equation> <SectLabel_bodyText> A beam search algorithm is used for this process +L+ to find the best path from all the translation op- +L+ tions; As the training stage, especially, the most +L+ time-consuming alignment sub-stage, is skipped, +L+ the translation only includes a decoding procedure +L+ that takes about 4.5 hours for about one million +L+ words of the PTB in a 2.8GHz PC. +L+ </SectLabel_bodyText> <SectLabel_listItem> 3. After the target sentence is generated, the at- +L+ </SectLabel_listItem> <SectLabel_bodyText> tached POS tags and dependency information of +L+ each English word will also be transferred to each +L+ corresponding Chinese word. As word order is of- +L+ ten changed after translation, the pointer of each +L+ dependency relationship, represented by a serial +L+ number, should be re-calculated. +L+ Although we try to perform an exact word-by- +L+ word translation, this aim cannot be fully reached +L+ in fact, as the following case is frequently encoun- +L+ tered, multiple English words have to be translated +L+ into one Chinese word. To solve this problem, +L+ we use a policy that lets the output Chinese word +L+ only inherits the attached information of the high- +L+ est syntactic head in the original multiple English +L+ words. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Dependency Parsing: Baseline +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 4.1 Learning Model and Features +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> According to (McDonald and Nivre, 2007), all +L+ data-driven models for dependency parsing that +L+ have been proposed in recent years can be de- +L+ scribed as either graph-based or transition-based. +L+ </SectLabel_bodyText> <SectLabel_equation> P(cle) = +L+ </SectLabel_equation> <SectLabel_page> 57 +L+ </SectLabel_page> <SectLabel_tableCaption> Table 1: Feature Notations +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Although the former will be also used as compari- +L+ son, the latter is chosen as the main parsing frame- +L+ work by this study for the sake of efficiency. In de- +L+ tail, a shift-reduce method is adopted as in (Nivre, +L+ 2003), where a classifier is used to make a parsing +L+ decision step by step. In each step, the classifier +L+ checks a word pair, namely, s, the top of a stack +L+ that consists of the processed words, and, i, the +L+ first word in the (input) unprocessed sequence, to +L+ determine if a dependent relation should be estab- +L+ lished between them. Besides two dependency arc +L+ building actions, a shift action and a reduce ac- +L+ tion are also defined to maintain the stack and the +L+ unprocessed sequence. In this work, we adopt a +L+ left-to-right arc-eager parsing model, that means +L+ that the parser scans the input sequence from left +L+ to right and right dependents are attached to their +L+ heads as soon as possible (Hall et al., 2007). +L+ While memory-based and margin-based learn- +L+ ing approaches such as support vector machines +L+ are popularly applied to shift-reduce parsing, we +L+ apply maximum entropy model as the learning +L+ model for efficient training and adopting over- +L+ lapped features as our work in (Zhao and Kit, +L+ 2008), especially, those character-level ones for +L+ Chinese parsing. Our implementation of maxi- +L+ mum entropy adopts L-BFGS algorithm for pa- +L+ rameter optimization as usual. +L+ With notations defined in Table 1, a feature set +L+ as shown in Table 2 is adopted. Here, we explain +L+ some terms in Tables 1 and 2. We used a large +L+ scale feature selection approach as in (Zhao et al., +L+ 2009) to obtain the feature set in Table 2. Some +L+ feature notations in this paper are also borrowed +L+ from that work. +L+ The feature curroot returns the root of a par- +L+ tial parsing tree that includes a specified node. +L+ The feature charseq returns a character sequence +L+ whose members are collected from all identified +L+ children for a specified word. +L+ In Table 2, as for concatenating multiple sub- +L+ strings into a feature string, there are two ways, +L+ seq and bag. The former is to concatenate all sub- +L+ strings without do something special. The latter +L+ will remove all duplicated substrings, sort the rest +L+ and concatenate all at last. +L+ Note that we systemically use a group of +L+ character-level features. Surprisingly, as to our +L+ best knowledge, this is the first report on using this +L+ type of features in Chinese dependency parsing. +L+ Although (McDonald et al., 2005) used the pre- +L+ fix of each word form instead of word form itself +L+ as features, character-level features here for Chi- +L+ nese is essentially different from that. As Chinese +L+ is basically a character-based written language. +L+ Character plays an important role in many means, +L+ most characters can be formed as single-character +L+ words, and Chinese itself is character-order free +L+ rather than word-order free to some extent. In ad- +L+ dition, there is often a close connection between +L+ the meaning of a Chinese word and its first or last +L+ character. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Parsing using a Beam Search Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In Table 2, the feature preact� returns the previous +L+ parsing action type, and the subscript n stands for +L+ the action order before the current action. These +L+ are a group of Markovian features. Without this +L+ type of features, a shift-reduce parser may directly +L+ scan through an input sequence in linear time. +L+ Otherwise, following the work of (Duan et al., +L+ 2007) and (Zhao, 2009), the parsing algorithm is +L+ to search a parsing action sequence with the max- +L+ imal probability. +L+ </SectLabel_bodyText> <SectLabel_equation> Y5di = argmax p(di �di-1di-2...)� +L+ i +L+ </SectLabel_equation> <SectLabel_bodyText> where 5di is the object parsing action sequence, +L+ p(di � di-1...) is the conditional probability, and di +L+ </SectLabel_bodyText> <SectLabel_table> Meaning +L+ The word in the top of stack +L+ The first word below the top of stack. +L+ The first word before(after) the word +L+ in the top of stack. +L+ The first (second) word in the +L+ unprocessed sequence, etc. +L+ Dependent direction +L+ Head +L+ Leftmost child +L+ Rightmost child +L+ Right nearest child +L+ word form +L+ POS tag of word +L+ coarse POS: the first letter of POS tag of word +L+ coarse POS: the first two POS tags of word +L+ the left nearest verb +L+ The first character of a word +L+ The first two characters of a word +L+ The last character of a word +L+ The last two characters of a word +L+ ’s, i.e., ‘s.dprel’ means dependent label +L+ of character in the top of stack +L+ Feature combination, i.e., ‘s.char+i.char’ +L+ means both s.char and i.char work as a +L+ feature function. +L+ Notation +L+ s +L+ s' +L+ s-1,s1... +L+ i, i+1,... +L+ dir +L+ h +L+ lm +L+ rm +L+ rn +L+ form +L+ pos +L+ cpos1 +L+ cpos2 +L+ lnverb +L+ char1 +L+ char2 +L+ char-1 +L+ char-2 +L+ . +L+ + +L+ </SectLabel_table> <SectLabel_page> 58 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: A comparison before and after translation +L+ is i-th parsing action. We use a beam search algo- +L+ rithm to find the object parsing action sequence. +L+ </SectLabel_figureCaption> <SectLabel_table> Table 2: Features for Parsing +L+ 	in . f orm, n = 0, 1 i.f orm + i1.form +L+ 	in.char2 + in+1.char2, n = —1, 0 +L+ 	i.char_1 + i1.char_1 +L+ 	in.char_2 n = 0, 3 +L+ 	i1.char_2 +i2.char_2 +i3.char_2 i.lnverb.char_2 +L+ 	i3.pos +L+ 	in.pos + in+1.pos, n = 0, 1 +L+ 	i_2.cpos1 + i_1.cpos1 +L+ 	i1 .cpos1 + i2.cpos1 + i3.cpos1 +L+ 	s'2.char1 +L+ 	s'.char_2 + s'1.char_2 s'_2.cpos2 +L+ 	s'_1.cpos2 + s'1.cpos2 s'.cpos2 + s'1.cpos2 s’. children.cpos2.seq s’. children. dprel.seq s’.subtree.depth +L+ 	s'.h. f orm + s'.rm.cpos1 s'.lm.char2 + s'.char2 s.h. children.dprel.seq s.lm.dprel +L+ 	s.char_2 + i1.char_2 +L+ 	s.charn + i.charn, n = —1,1 +L+ 	s _ 1.pos + i1 .pos +L+ 	s.pos + in.pos, n = —1, 0, 1 +L+ 	s : illinePath. f orm.bag s'.form + i.form +L+ 	s'.char2 + in.char2, n = —1, 0, 1 +L+ 	s.curroot.pos + i.pos +L+ 	s.curroot.char2 + i.char2 s.children.cpos2.seq + i.children.cpos2.seq s.children.cpos2.seq + i.children.cpos2.seq + s.cpos2 + i.cpos2 +L+ 	s'.children.dprel.seq + i.children.dprel.seq +L+ 	preact_ 1 preact_2 preact_2+preact_ 1 +L+ </SectLabel_table> <SectLabel_sectionHeader> 5 Exploiting the Translated Treebank +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> As we cannot expect too much for a word-by-word +L+ translation, only word pairs with dependency rela- +L+ tion in translated text are extracted as useful and +L+ reliable information. Then some features based +L+ on a query in these word pairs according to the +L+ current parsing state (namely, words in the cur- +L+ rent stack and input) will be derived to enhance +L+ the Chinese parser. +L+ A translation sample can be seen in Figure 1. +L+ Although most words are satisfactorily translated, +L+ to generate effective features, what we still have to +L+ consider at first is the inconsistence between the +L+ translated text and the target text. +L+ In Chinese, word lemma is always its word form +L+ itself, this is a convenient characteristic in com- +L+ putational linguistics and makes lemma features +L+ unnecessary for Chinese parsing at all. However, +L+ Chinese has a special primary processing task, i.e., +L+ word segmentation. Unfortunately, word defini- +L+ tions for Chinese are not consistent in various lin- +L+ guistical views, for example, seven segmentation +L+ conventions for computational purpose are for- +L+ mally proposed since the first Bakeoff3. +L+ Note that CTB or any other Chinese treebank +L+ has its own word segmentation guideline. Chi- +L+ nese word should be strictly segmented according +L+ to the guideline before POS tags and dependency +L+ relations are annotated. However, as we say the +L+ </SectLabel_bodyText> <SectLabel_footnote> 3Bakeoff is a Chinese processing share task held by +L+ SIGHAN. +L+ </SectLabel_footnote> <SectLabel_page> 59 +L+ </SectLabel_page> <SectLabel_bodyText> English treebank is translated into Chinese word +L+ by word, Chinese words in the translated text are +L+ exactly some entries from the bilingual lexicon, +L+ they are actually irregular phrases, short sentences +L+ or something else rather than words that follows +L+ any existing word segmentation convention. If the +L+ bilingual lexicon is not carefully selected or re- +L+ fined according to the treebank where the Chinese +L+ parser is trained from, then there will be a serious +L+ inconsistence on word segmentation conventions +L+ between the translated and the target treebanks. +L+ As all concerned feature values here are calcu- +L+ lated from the searching result in the translated +L+ word pair list according to the current parsing +L+ state, and a complete and exact match cannot be +L+ always expected, our solution to the above seg- +L+ mentation issue is using a partial matching strat- +L+ egy based on characters that the words include. +L+ Above all, a translated word pair list, L, is ex- +L+ tracted from the translated treebank. Each item in +L+ the list consists of three elements, dependant word +L+ (dp), head word (hd) and the frequency of this pair +L+ in the translated treebank, f . +L+ There are two basic strategies to organize the +L+ features derived from the translated word pair list. +L+ The first is to find the most matching word pair +L+ in the list and extract some properties from it, +L+ such as the matched length, part-of-speech tags +L+ and so on, to generate features. Note that a +L+ matching priority serial should be defined afore- +L+ hand in this case. The second is to check every +L+ matching models between the current parsing state +L+ and the partially matched word pair. In an early +L+ version of our approach, the former was imple- +L+ mented. However, It is proven to be quite inef- +L+ ficient in computation. Thus we adopt the sec- +L+ ond strategy at last. Two matching model fea- +L+ ture functions, 0(•) and 0(•), are correspondingly +L+ defined as follows. The return value of 0(•) or +L+ 0(•) is the logarithmic frequency of the matched +L+ item. There are four input parameters required +L+ by the function 0(•). Two parameters of them +L+ are about which part of the stack(input) words is +L+ chosen, and other two are about which part of +L+ each item in the translated word pair is chosen. +L+ These parameters could be set to full or charn as +L+ shown in Table 1, where n = ..., —2, —1, 1, 2, .... +L+ For example, a possible feature could be +L+ �(s. f ull, i.chari, dp. f ull, hd.char1 ), it tries to +L+ find a match in L by comparing stack word and +L+ dp word, and the first character of input word +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 3: Features based on the translated treebank +L+ </SectLabel_tableCaption> <SectLabel_bodyText> and the first character of hd word. If such +L+ a match item in L is found, then 0(•) returns +L+ log(f ). There are three input parameters required +L+ by the function 0(•). One parameter is about +L+ which part of the stack(input) words is chosen, +L+ and the other is about which part of each item +L+ in the translated word pair is chosen. The third +L+ is about the matching type that may be set to +L+ dependant, head, or root. For example, the +L+ function 0(i.chari, hd. f ull, root) tries to find a +L+ match in L by comparing the first character of in- +L+ put word and the whole dp word. If such a match +L+ item in L is found, then �(•) returns log(f) as hd +L+ occurs as ROOT f times. +L+ As having observed that CTB and PTB share a +L+ similar POS guideline. A POS pair list from PTB +L+ is also extract. Two types of features, rootscore +L+ and pairscore are used to make use of such infor- +L+ mation. Both of them returns the logarithmic value +L+ of the frequency for a given dependent event. The +L+ difference is, rootscore counts for the given POS +L+ tag occurring as ROOT, and pairscore counts for +L+ two POS tag combination occurring for a depen- +L+ dent relationship. +L+ A full adapted feature list that is derived from +L+ the translated word pairs is in Table 3. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Evaluation Results +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The quality of the parser is measured by the pars- +L+ ing accuracy or the unlabeled attachment score +L+ (UAS), i.e., the percentage of tokens with correct +L+ head. Two types of scores are reported for compar- +L+ ison: “UAS without p” is the UAS score without +L+ all punctuation tokens and “UAS with p” is the one +L+ with all punctuation tokens. +L+ The results with different feature sets are in Ta- +L+ ble 4. As the features preactn are involved, a +L+ </SectLabel_bodyText> <SectLabel_table> 0(i.char3, s'. f ull, dp.char3, hd. f ull)+i.char3 +L+ +s'. f orm +L+ 0(i.char3, s.char2, dp.char3, hd.char2)+s.char2 +L+ 0(i.char3, s. f ull, dp.char3, hd.char2)+s. form +L+ ,O(s'.char-2, hd.char-2, head)+i.pos+s'.pos +L+ 0(i.char3, s. f ull, dp.char3, hd.char2)+s. f ull +L+ 0(s'. f ull, i.char4, dp. f ull, hd.char4)+s'.pos+i.pos +L+ ,O(i. f ull, hd.char2, root)+i.pos+s.pos +L+ ,O(i. f ull, hd.char2, root)+i.pos+s'.pos +L+ ,O(s. f ull, dp. f ull, dependant)+i.pos +L+ pairscore(s'.pos, i.pos)+s'. f orm+i. f orm +L+ rootscore(s'.pos)+s'. f orm+i. f orm +L+ rootscore (s'.pos)+i.pos +L+ </SectLabel_table> <SectLabel_page> 60 +L+ </SectLabel_page> <SectLabel_bodyText> beam search algorithm with width 5 is used for +L+ parsing, otherwise, a simple shift-reduce decoding +L+ is used. It is observed that the features derived +L+ from the translated text bring a significant perfor- +L+ mance improvement as high as 1.3%. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 4: The results with different feature sets +L+ features with p without p +L+ </SectLabel_tableCaption> <SectLabel_table> baseline	-d	0.846	0.858 +L+ +d°	0.848	0.860 +L+ +Tb	-d	0.859	0.869 +L+ +d	0.861	0.870 +L+ °+d: using three Markovian features preact and +L+ beam search decoding. +L+ b+T: using features derived from the translated text +L+ as in Table 3. +L+ </SectLabel_table> <SectLabel_figureCaption> Figure 2: Performance vs. dependency length +L+ </SectLabel_figureCaption> <SectLabel_bodyText> To compare our parser to the state-of-the-art +L+ counterparts, we use the same testing data as +L+ (Wang et al., 2005) did, selecting the sentences +L+ length up to 40. Table 5 shows the results achieved +L+ by other researchers and ours (UAS with p), which +L+ indicates that our parser outperforms any other +L+ ones 4. However, our results is only slightly better +L+ than that of (Chen et al., 2008) as only sentences +L+ whose lengths are less than 40 are considered. As +L+ our full result is much better than the latter, this +L+ comparison indicates that our approach improves +L+ the performance for those longer sentences. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 5: Comparison against the state-of-the-art +L+ 	full	up to 40 +L+ </SectLabel_tableCaption> <SectLabel_table> (McDonald and Pereira, 2006)°	-	0.825 +L+ (Wang et al., 2007)	-	0.866 +L+ (Chen et al., 2008)	0.852	0.884 +L+ Ours	0.861	0.889 +L+ °This results was reported in (Wang et al., 2007). +L+ </SectLabel_table> <SectLabel_bodyText> The experimental results in (McDonald and +L+ Nivre, 2007) show a negative impact on the pars- +L+ ing accuracy from too long dependency relation. +L+ For the proposed method, the improvement rela- +L+ tive to dependency length is shown in Figure 2. +L+ From the figure, it is seen that our method gives +L+ observable better performance when dependency +L+ lengths are larger than 4. Although word order is +L+ changed, the results here show that the useful in- +L+ formation from the translated treebank still help +L+ those long distance dependencies. +L+ </SectLabel_bodyText> <SectLabel_footnote> 4There is a slight exception: using the same data splitting, +L+ (Yu et al., 2008) reported UAS without p as 0.873 versus ours, +L+ 0.870. +L+ </SectLabel_footnote> <SectLabel_sectionHeader> 7 Discussion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> If a treebank in the source language can help im- +L+ prove parsing in the target language, then there +L+ must be something common between these two +L+ languages, or more precisely, these two corre- +L+ sponding treebanks. (Zeman and Resnik, 2008) +L+ assumed that the morphology and syntax in the +L+ language pair should be very similar, and that is +L+ so for the language pair that they considered, Dan- +L+ ish and Swedish, two very close north European +L+ languages. Thus it is somewhat surprising that +L+ we show a translated English treebank may help +L+ Chinese parsing, as English and Chinese even be- +L+ long to two different language systems. However, +L+ it will not be so strange if we recognize that PTB +L+ and CTB share very similar guidelines on POS and +L+ syntactics annotation. Since it will be too abstract +L+ in discussing the details of the annotation guide- +L+ lines, we look into the similarities of two treebanks +L+ from the matching degree of two word pair lists. +L+ The reason is that the effectiveness of the proposed +L+ method actually relies on how many word pairs at +L+ every parsing states can find their full or partial +L+ matched partners in the translated word pair list. +L+ Table 6 shows such a statistics on the matching +L+ degree distribution from all training samples for +L+ Chinese parsing. The statistics in the table suggest +L+ that most to-be-check word pairs during parsing +L+ have a full or partial hitting in the translated word +L+ pair list. The latter then obtains an opportunity to +L+ provide a great deal of useful guideline informa- +L+ tion to help determine how the former should be +L+ tackled. Therefore we have cause for attributing +L+ the effectiveness of the proposed method to the +L+ similarity of these two treebanks. From Table 6, +L+ </SectLabel_bodyText> <SectLabel_page> 61 +L+ </SectLabel_page> <SectLabel_bodyText> we also find that the partial matching strategy de- +L+ fined in Section 5 plays a very important role in +L+ improving the whole matching degree. Note that +L+ our approach is not too related to the characteris- +L+ tics of two languages. Our discussion here brings +L+ an interesting issue, which difference is more im- +L+ portant in cross language processing, between two +L+ languages themselves or the corresponding anno- +L+ tated corpora? This may be extensively discussed +L+ in the future work. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 6: Matching degree distribution +L+ </SectLabel_tableCaption> <SectLabel_table> dependant-match head-match Percent (%) +L+ None	None	9.6 +L+ None	Partial	16.2 +L+ None	Full	9.9 +L+ Partial	None	12.4 +L+ Partial	Partial	42.6 +L+ Partial	Full	7.3 +L+ Full	None	3.7 +L+ Full	Partial	7.0 +L+ Full	Full	0.2 +L+ </SectLabel_table> <SectLabel_bodyText> Note that only a bilingual lexicon is adopted in +L+ our approach. We regard it one of the most mer- +L+ its for our approach. A lexicon is much easier to +L+ be obtained than an annotated corpus. One of the +L+ remained question about this work is if the bilin- +L+ gual lexicon should be very specific for this kind +L+ of tasks. According to our experiences, actually, it +L+ is not so sensitive to choose a highly refined lexi- +L+ con or not. We once found many words, mostly +L+ named entities, were outside the lexicon. Thus +L+ we managed to collect a named entity translation +L+ dictionary to enhance the original one. However, +L+ this extra effort did not receive an observable per- +L+ formance improvement in return. Finally we re- +L+ alize that a lexicon that can guarantee two word +L+ pair lists highly matched is sufficient for this work, +L+ and this requirement may be conveniently satis- +L+ fied only if the lexicon consists of adequate high- +L+ frequent words from the source treebank. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8 Conclusion and Future Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We propose a method to enhance dependency +L+ parsing in one language by using a translated tree- +L+ bank from another language. A simple statisti- +L+ cal machine translation technique, word-by-word +L+ decoding, where only a bilingual lexicon is nec- +L+ essary, is used to translate the source treebank. +L+ As dependency parsing is concerned with the re- +L+ lations of word pairs, only those word pairs with +L+ dependency relations in the translated treebank are +L+ chosen to generate some additional features to en- +L+ hance the parser for the target language. The ex- +L+ perimental results in English and Chinese tree- +L+ banks show the proposed method is effective and +L+ helps the Chinese parser in this work achieve a +L+ state-of-the-art result. +L+ Note that our method is evaluated in two tree- +L+ banks with a similar annotation style and it avoids +L+ using too many linguistic properties. Thus the +L+ method is in the hope of being used in other simi- +L+ larly annotated treebanks 5. For an immediate ex- +L+ ample, we may adopt a translated Chinese tree- +L+ bank to improve English parsing. Although there +L+ are still something to do, the remained key work +L+ has been as simple as considering how to deter- +L+ mine the matching strategy for searching the trans- +L+ lated word pair list in English according to the +L+ framework of our method. . +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgements +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We’d like to give our thanks to three anonymous +L+ reviewers for their insightful comments, Dr. Chen +L+ Wenliang for for helpful discussions and Mr. Liu +L+ Jun for helping us fix a bug in our scoring pro- +L+ gram. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Peter F. Brown, John Cocke, Stephen A. Della Pietra, +L+ Vincent J. Della Pietra, Fredrick Jelinek, John D. +L+ Lafferty, Robert L. Mercer, and Paul S. Roossin. +L+ 1990. A statistical approach to machine translation. +L+ Computational Linguistics, 16(2):79–85. +L+ David Burkett and Dan Klein. 2008. Two lan- +L+ guages are better than one (for syntactic parsing). In +L+ EMNLP-2008, pages 877–886, Honolulu, Hawaii, +L+ USA. +L+ Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi- +L+ moto, Yujie Zhang, and Hitoshi Isahara. 2008. De- +L+ pendency parsing with short dependency relations +L+ in unlabeled data. In Proceedings of IJCNLP-2008, +L+ Hyderabad, India, January 8-10. +L+ Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba- +L+ bilistic parsing action models for multi-lingual de- +L+ pendency parsing. In Proceedings of the CoNLL +L+ Shared Task Session of EMNLP-CoNLL 2007, pages +L+ 940–946, Prague, Czech, June 28-30. +L+ Johan Hall, Jens Nilsson, Joakim Nivre, +L+ G¨ulsen Eryiˇgit, Be´ata Megyesi, Mattias Nils- +L+ son, and Markus Saers. 2007. Single malt or +L+ </SectLabel_reference> <SectLabel_footnote> 5For example, Catalan and Spanish treebanks from the +L+ AnCora(-Es/Ca) Multilevel Annotated Corpus that are an- +L+ notated by the Universitat de Barcelona (CLiC-UB) and the +L+ Universitat Politecnica de Catalunya (UPC). +L+ </SectLabel_footnote> <SectLabel_page> 62 +L+ </SectLabel_page> <SectLabel_reference> blended? a study in multilingual parser optimiza- +L+ tion. In Proceedings of the CoNLL Shared Task +L+ Session of EMNLP-CoNLL 2007, pages 933–939, +L+ Prague, Czech, June. +L+ Terry Koo, Xavier Carreras, and Michael Collins. +L+ 2008. Simple semi-supervised dependency parsing. +L+ In Proceedings of ACL-08: HLT, pages 595–603, +L+ Columbus, Ohio, USA, June. +L+ David McClosky, Eugene Charniak, and Mark John- +L+ son. 2006. Reranking and self-training for parser +L+ adaptation. In Proceedings of ACL-COLING 2006, +L+ pages 337–344, Sydney, Australia, July. +L+ Ryan McDonald and Joakim Nivre. 2007. Charac- +L+ terizing the errors of data-driven dependency pars- +L+ ing models. In Proceedings of the 2007 Joint Con- +L+ ference on Empirical Methods in Natural Language +L+ Processing and Computational Natural Language +L+ Learning (EMNLP-CoNLL 2007), pages 122–131, +L+ Prague, Czech, June 28-30. +L+ Ryan McDonald and Fernando Pereira. 2006. Online +L+ learning of approximate dependency parsing algo- +L+ rithms. In Proceedings of EACL-2006, pages 81–88, +L+ Trento, Italy, April. +L+ Ryan McDonald, Koby Crammer, and Fernando +L+ Pereira. 2005. Online large-margin training of de- +L+ pendency parsers. In Proceedings of ACL-2005, +L+ pages 91–98, Ann Arbor, Michigan, USA, June 25- +L+ 30. +L+ Paola Merlo, Suzanne Stevenson, Vivian Tsang, and +L+ Gianluca Allaria. 2002. A multilingual paradigm +L+ for automatic verb classification. In ACL-2002, +L+ pages 207–214, Philadelphia, Pennsylvania, USA. +L+ Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc- +L+ Donald, Jens Nilsson, Sebastian Riedel, and Deniz +L+ Yuret. 2007. The conll 2007 shared task on de- +L+ pendency parsing. In Proceedings of the CoNLL +L+ Shared Task Session of EMNLP-CoNLL 2007, page +L+ 915 - 932, Prague, Czech, June. +L+ Joakim Nivre. 2003. An efficient algorithm for projec- +L+ tive dependency parsing. In Proceedings of IWPT- +L+ 2003), pages 149–160, Nancy, France, April 23-25. +L+ Franz Josef Och and Hermann Ney. 2002. Discrimina- +L+ tive training and maximum entropy models for sta- +L+ tistical machine translation. In Proceedings ofACL- +L+ 2002, pages 295–302, Philadelphia, USA, July. +L+ Roi Reichart and Ari Rappoport. 2007. Self-training +L+ for enhancement and domain adaptation of statistical +L+ parsers trained on small datasets. In Proceedings of +L+ ACL-2007, pages 616–623, Prague, Czech Republic, +L+ June. +L+ Kenji Sagae and Jun' ichi Tsujii. 2007. Dependency +L+ parsing and domain adaptation with lr models and +L+ parser ensembles. In Proceedings of the CoNLL +L+ Shared Task Session of EMNLP-CoNLL 2007, page +L+ 1044 - 1050, Prague, Czech, June 28-30. +L+ Noah A. Smith and Jason Eisner. 2006. Annealing +L+ structural bias in multilingual weighted grammar in- +L+ duction. In Proceedings of ACL-COLING 2006, +L+ page 569 - 576, Sydney, Australia, July. +L+ Mark Steedman, Miles Osborne, Anoop Sarkar, +L+ Stephen Clark, Rebecca Hwa, Julia Hockenmaier, +L+ Paul Ruhlen, Steven Baker, and Jeremiah Crim. +L+ 2003. Bootstrapping statistical parsers from small +L+ datasets. In Proceedings of EACL-2003, page +L+ 331 - 338, Budapest, Hungary, April. +L+ Qin Iris Wang and Dale Schuurmans. 2008. Semi- +L+ supervised convex training for dependency parsing. +L+ In Proceedings of ACL-08: HLT, pages 532–540, +L+ Columbus, Ohio, USA, June. +L+ Qin Iris Wang, Dale Schuurmans, and Dekang Lin. +L+ 2005. Strictly lexical dependency parsing. In Pro- +L+ ceedings of IWPT-2005, pages 152–159, Vancouver, +L+ BC, Canada, October. +L+ Qin Iris Wang, Dekang Lin, and Dale Schuurmans. +L+ 2007. Simple training of dependency parsers via +L+ structured boosting. In Proceedings of IJCAI 2007, +L+ pages 1756–1762, Hyderabad, India, January. +L+ Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta- +L+ tistical dependency analysis with support vector +L+ machines. In Proceedings of IWPT-2003), page +L+ 195 - 206, Nancy, France, April. +L+ Kun Yu, Daisuke Kawahara, and Sadao Kurohashi. +L+ 2008. Chinese dependency parsing with large +L+ scale automatically constructed case structures. In +L+ Proceedings of COLING-2008, pages 1049–1056, +L+ Manchester, UK, August. +L+ Daniel Zeman and Philip Resnik. 2008. Cross- +L+ language parser adaptation between related lan- +L+ guages. In Proceedings of IJCNLP 2008 Workshop +L+ on NLP for Less Privileged Languages, pages 35– +L+ 42, Hyderabad, India, January. +L+ Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and +L+ semantic dependencies with two single-stage max- +L+ imum entropy models. In Proceeding of CoNLL- +L+ 2008, pages 203–207, Manchester, UK. +L+ Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong +L+ Zhou. 2009. Multilingual dependency learning: +L+ A huge feature engineering method to semantic de- +L+ pendency parsing. In Proceedings of CoNLL-2009, +L+ Boulder, Colorado, USA. +L+ Hai Zhao. 2009. Character-level dependencies in +L+ chinese: Usefulness and learning. In EACL-2009, +L+ pages 879–887, Athens, Greece. +L+ </SectLabel_reference> <SectLabel_page> 63 +L+ </SectLabel_page>
<SectLabel_title> Topological Field Parsing of German +L+ </SectLabel_title> <SectLabel_author> Jackie Chi Kit Cheung +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ University of Toronto +L+ </SectLabel_affiliation> <SectLabel_address> Toronto, ON, M5S 3G4, Canada +L+ </SectLabel_address> <SectLabel_email> jcheung@cs.toronto.edu +L+ </SectLabel_email> <SectLabel_author> Gerald Penn +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ University of Toronto +L+ </SectLabel_affiliation> <SectLabel_address> Toronto, ON, M5S 3G4, Canada +L+ </SectLabel_address> <SectLabel_email> gpenn@cs.toronto.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Freer-word-order languages such as Ger- +L+ man exhibit linguistic phenomena that +L+ present unique challenges to traditional +L+ CFG parsing. Such phenomena produce +L+ discontinuous constituents, which are not +L+ naturally modelled by projective phrase +L+ structure trees. In this paper, we exam- +L+ ine topological field parsing, a shallow +L+ form of parsing which identifies the ma- +L+ jor sections of a sentence in relation to +L+ the clausal main verb and the subordinat- +L+ ing heads. We report the results of topo- +L+ logical field parsing of German using the +L+ unlexicalized, latent variable-based Berke- +L+ ley parser (Petrov et al., 2006) Without +L+ any language- or model-dependent adapta- +L+ tion, we achieve state-of-the-art results on +L+ the T¨uBa-D/Z corpus, and a modified NE- +L+ GRA corpus that has been automatically +L+ annotated with topological fields (Becker +L+ and Frank, 2002). We also perform a qual- +L+ itative error analysis of the parser output, +L+ and discuss strategies to further improve +L+ the parsing results. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Freer-word-order languages such as German ex- +L+ hibit linguistic phenomena that present unique +L+ challenges to traditional CFG parsing. Topic focus +L+ ordering and word order constraints that are sen- +L+ sitive to phenomena other than grammatical func- +L+ tion produce discontinuous constituents, which are +L+ not naturally modelled by projective (i.e., with- +L+ out crossing branches) phrase structure trees. In +L+ this paper, we examine topological field parsing, a +L+ shallow form of parsing which identifies the ma- +L+ jor sections of a sentence in relation to the clausal +L+ main verb and subordinating heads, when present. +L+ We report the results of parsing German using +L+ the unlexicalized, latent variable-based Berkeley +L+ parser (Petrov et al., 2006). Without any language- +L+ or model-dependent adaptation, we achieve state- +L+ of-the-art results on the T¨uBa-D/Z corpus (Telljo- +L+ hann et al., 2004), with a Fl-measure of 95.15% +L+ using gold POS tags. A further reranking of +L+ the parser output based on a constraint involv- +L+ ing paired punctuation produces a slight additional +L+ performance gain. To facilitate comparison with +L+ previous work, we also conducted experiments on +L+ a modified NEGRA corpus that has been automat- +L+ ically annotated with topological fields (Becker +L+ and Frank, 2002), and found that the Berkeley +L+ parser outperforms the method described in that +L+ work. Finally, we perform a qualitative error anal- +L+ ysis of the parser output on the T¨uBa-D/Z corpus, +L+ and discuss strategies to further improve the pars- +L+ ing results. +L+ German syntax and parsing have been studied +L+ using a variety of grammar formalisms. Hocken- +L+ maier (2006) has translated the German TIGER +L+ corpus (Brants et al., 2002) into a CCG-based +L+ treebank to model word order variations in Ger- +L+ man. Foth et al. (2004) consider a version of de- +L+ pendency grammars known as weighted constraint +L+ dependency grammars for parsing German sen- +L+ tences. On the NEGRA corpus (Skut et al., 1998), +L+ they achieve an accuracy of 89.0% on parsing de- +L+ pendency edges. In Callmeier (2000), a platform +L+ for efficient HPSG parsing is developed. This +L+ parser is later extended by Frank et al. (2003) +L+ with a topological field parser for more efficient +L+ parsing of German. The system by Rohrer and +L+ Forst (2006) produces LFG parses using a manu- +L+ ally designed grammar and a stochastic parse dis- +L+ ambiguation process. They test on the TIGER cor- +L+ pus and achieve an Fl-measure of 84.20%. In +L+ Dubey and Keller (2003), PCFG parsing of NE- +L+ GRA is improved by using sister-head dependen- +L+ cies, which outperforms standard head lexicaliza- +L+ tion as well as an unlexicalized model. The best +L+ </SectLabel_bodyText> <SectLabel_page> 64 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 64–72, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> performing model with gold tags achieve an F1 +L+ of 75.60%. Sister-head dependencies are useful in +L+ this case because of the flat structure of NEGRA’s +L+ trees. +L+ In contrast to the deeper approaches to parsing +L+ described above, topological field parsing identi- +L+ fies the major sections of a sentence in relation +L+ to the clausal main verb and subordinating heads, +L+ when present. Like other forms of shallow pars- +L+ ing, topological field parsing is useful as the first +L+ stage to further processing and eventual seman- +L+ tic analysis. As mentioned above, the output of +L+ a topological field parser is used as a guide to +L+ the search space of a HPSG parsing algorithm in +L+ Frank et al. (2003). In Neumann et al. (2000), +L+ topological field parsing is part of a divide-and- +L+ conquer strategy for shallow analysis of German +L+ text with the goal of improving an information ex- +L+ traction system. +L+ Existing work in identifying topological fields +L+ can be divided into chunkers, which identify the +L+ lowest-level non-recursive topological fields, and +L+ parsers, which also identify sentence and clausal +L+ structure. +L+ Veenstra et al. (2002) compare three approaches +L+ to topological field chunking based on finite state +L+ transducers, memory-based learning, and PCFGs +L+ respectively. It is found that the three techniques +L+ perform about equally well, with F1 of 94.1% us- +L+ ing POS tags from the TnT tagger, and 98.4% with +L+ gold tags. In Liepert (2003), a topological field +L+ chunker is implemented using a multi-class ex- +L+ tension to the canonically two-class support vec- +L+ tor machine (SVM) machine learning framework. +L+ Parameters to the machine learning algorithm are +L+ fine-tuned by a genetic search algorithm, with a +L+ resulting F1-measure of 92.25%. Training the pa- +L+ rameters to SVM does not have a large effect on +L+ performance, increasing the F1-measure in the test +L+ set by only 0.11%. +L+ The corpus-based, stochastic topological field +L+ parser of Becker and Frank (2002) is based on +L+ a standard treebank PCFG model, in which rule +L+ probabilities are estimated by frequency counts. +L+ This model includes several enhancements, which +L+ are also found in the Berkeley parser. First, +L+ they use parameterized categories, splitting non- +L+ terminals according to linguistically based intu- +L+ itions, such as splitting different clause types (they +L+ do not distinguish different clause types as basic +L+ categories, unlike T¨uBa-D/Z). Second, they take +L+ into account punctuation, which may help iden- +L+ tify clause boundaries. They also binarize the very +L+ flat topological tree structures, and prune rules +L+ that only occur once. They test their parser on a +L+ version of the NEGRA corpus, which has been +L+ annotated with topological fields using a semi- +L+ automatic method. +L+ Ule (2003) proposes a process termed Directed +L+ Treebank Refinement (DTR). The goal of DTR is +L+ to refine a corpus to improve parsing performance. +L+ DTR is comparable to the idea of latent variable +L+ grammars on which the Berkeley parser is based, +L+ in that both consider the observed treebank to be +L+ less than ideal and both attempt to refine it by split- +L+ ting and merging nonterminals. In this work, split- +L+ ting and merging nonterminals are done by consid- +L+ ering the nonterminals’ contexts (i.e., their parent +L+ nodes) and the distribution of their productions. +L+ Unlike in the Berkeley parser, splitting and merg- +L+ ing are distinct stages, rather than parts of a sin- +L+ gle iteration. Multiple splits are found first, then +L+ multiple rounds of merging are performed. No +L+ smoothing is done. As an evaluation, DTR is ap- +L+ plied to topological field parsing of the T¨uBa-D/Z +L+ corpus. We discuss the performance of these topo- +L+ logical field parsers in more detail below. +L+ All of the topological parsing proposals pre- +L+ date the advent of the Berkeley parser. The exper- +L+ iments of this paper demonstrate that the Berke- +L+ ley parser outperforms previous methods, many of +L+ which are specialized for the task of topological +L+ field chunking or parsing. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Topological Field Model of German +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Topological fields are high-level linear fields in +L+ an enclosing syntactic region, such as a clause +L+ (H¨ohle, 1983). These fields may have constraints +L+ on the number of words or phrases they contain, +L+ and do not necessarily form a semantically co- +L+ herent constituent. Although it has been argued +L+ that a few languages have no word-order con- +L+ straints whatsoever, most “free word-order” lan- +L+ guages (even Warlpiri) have at the very least some +L+ sort of sentence- or clause-initial topic field fol- +L+ lowed by a second position that is occupied by +L+ clitics, a finite verb or certain complementizers +L+ and subordinating conjunctions. In a few Ger- +L+ manic languages, including German, the topology +L+ is far richer than that, serving to identify all of +L+ the components of the verbal head of a clause, +L+ except for some cases of long-distance dependen- +L+ </SectLabel_bodyText> <SectLabel_page> 65 +L+ </SectLabel_page> <SectLabel_bodyText> cies. Topological fields are useful, because while +L+ Germanic word order is relatively free with respect +L+ to grammatical functions, the order of the topolog- +L+ ical fields is strict and unvarying. +L+ </SectLabel_bodyText> <SectLabel_table> Type	Fields +L+ VL	(KOORD) (C) (MF) VC (NF) +L+ V1	(KOORD) (LV) LK (MF) (VC) (NF) +L+ V2	(KOORD) (LV) VF LK (MF) (VC) (NF) +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Topological field model of German. +L+ Simplified from T¨uBa-D/Z corpus’s annotation +L+ schema (Telljohann et al., 2006). +L+ </SectLabel_tableCaption> <SectLabel_bodyText> In the German topological field model, clauses +L+ belong to one of three types: verb-last (VL), verb- +L+ second (V2), and verb-first (V 1), each with a spe- +L+ cific sequence of topological fields (Table 1). VL +L+ clauses include finite and non-finite subordinate +L+ clauses, V2 sentences are typically declarative +L+ sentences and WH-questions in matrix clauses, +L+ and V1 sentences include yes-no questions, and +L+ certain conditional subordinate clauses. Below, +L+ we give brief descriptions of the most common +L+ topological fields. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	VF (Vorfeld or ‘pre-field’) is the first con- +L+ </SectLabel_listItem> <SectLabel_bodyText> stituent in sentences of the V2 type. This is +L+ often the topic of the sentence, though as an +L+ anonymous reviewer pointed out, this posi- +L+ tion does not correspond to a single function +L+ with respect to information structure. (e.g., +L+ the reviewer suggested this case, where VF +L+ contains the focus: –Wer kommt zur Party? +L+ –Peter kommt zur Party. –Who is coming to +L+ the Party? –Peter is coming to the party.) +L+ </SectLabel_bodyText> <SectLabel_listItem> •	LK (Linke Klammer or ‘left bracket’) is the +L+ </SectLabel_listItem> <SectLabel_bodyText> position for finite verbs in V1 and V2 sen- +L+ tences. It is replaced by a complementizer +L+ with the field label C in VL sentences. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	MF (Mittelfeld or ‘middle field’) is an op- +L+ </SectLabel_listItem> <SectLabel_bodyText> tional field bounded on the left by LK and +L+ on the right by the verbal complex VC or +L+ by NF. Most verb arguments, adverbs, and +L+ prepositional phrases are found here, unless +L+ they have been fronted and put in the VF, or +L+ are prosodically heavy and postposed to the +L+ NF field. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	VC is the verbal complex field. It includes +L+ infinite verbs, as well as finite verbs in VL +L+ sentences. +L+ •	NF (Nachfeld or ‘post-field’) contains +L+ prosodically heavy elements such as post- +L+ posed prepositional phrases or relative +L+ clauses. +L+ •	KOORD1 (Koordinationsfeld or ‘coordina- +L+ tion field’) is a field for clause-level conjunc- +L+ tions. +L+ • +L+ </SectLabel_listItem> <SectLabel_bodyText> LV (Linksversetzung or ‘left dislocation’) is +L+ used for resumptive constructions involving +L+ left dislocation. For a detailed linguistic +L+ treatment, see (Frey, 2004). +L+ Exceptions to the topological field model as de- +L+ scribed above do exist. For instance, parenthetical +L+ constructions exist as a mostly syntactically inde- +L+ pendent clause inside another sentence. In our cor- +L+ pus, they are attached directly underneath a clausal +L+ node without any intervening topological field, as +L+ in the following example. In this example, the par- +L+ enthetical construction is highlighted in bold print. +L+ Some clause and topological field labels under the +L+ NF field are omitted for clarity. +L+ </SectLabel_bodyText> <SectLabel_listItem> (1) (a) (SIMPX “(VF Man) (LK muft) (VC verstehen) ” +L+ , (SIMPX sagte er), “ (NF daft diese +L+ Minderheiten seit langer Zeit massiv von den +L+ Nazis bedroht werden)). ” +L+ (b) Translation: “One must understand,” he said, +L+ “that these minorities have been massively +L+ threatened by the Nazis for a long time.” +L+ </SectLabel_listItem> <SectLabel_sectionHeader> 3 A Latent Variable Parser +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> For our experiments, we used the latent variable- +L+ based Berkeley parser (Petrov et al., 2006). La- +L+ tent variable parsing assumes that an observed +L+ treebank represents a coarse approximation of +L+ an underlying, optimally refined grammar which +L+ makes more fine-grained distinctions in the syn- +L+ tactic categories. For example, the noun phrase +L+ category NP in a treebank could be viewed as a +L+ coarse approximation of two noun phrase cate- +L+ gories corresponding to subjects and object, NPˆS, +L+ and NPˆVP. +L+ The Berkeley parser automates the process of +L+ finding such distinctions. It starts with a simple bi- +L+ narized X-bar grammar style backbone, and goes +L+ through iterations of splitting and merging non- +L+ terminals, in order to maximize the likelihood of +L+ the training set treebank. In the splitting stage, +L+ </SectLabel_bodyText> <SectLabel_footnote> 1The T¨uBa-D/Z corpus distinguishes coordinating and +L+ non-coordinating particles, as well as clausal and field co- +L+ ordination. These distinctions need not concern us for this +L+ explanation. +L+ </SectLabel_footnote> <SectLabel_page> 66 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: “I could never have done that just for aesthetic reasons.” Sample T¨uBa-D/Z tree, with topolog- +L+ ical field annotations and edge labels. Topological field layer in bold. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> an Expectation-Maximization algorithm is used to +L+ find a good split for each nonterminal. In the +L+ merging stage, categories that have been over- +L+ split are merged together to keep the grammar size +L+ tractable and reduce sparsity. Finally, a smoothing +L+ stage occurs, where the probabilities of rules for +L+ each nonterminal are smoothed toward the prob- +L+ abilities of the other nonterminals split from the +L+ same syntactic category. +L+ The Berkeley parser has been applied to the +L+ T¨uBaD/Z corpus in the constituent parsing shared +L+ task of the ACL-2008 Workshop on Parsing Ger- +L+ man (Petrov and Klein, 2008), achieving an Fl- +L+ measure of 85.10% and 83.18% with and without +L+ gold standard POS tags respectively2. We chose +L+ the Berkeley parser for topological field parsing +L+ because it is known to be robust across languages, +L+ and because it is an unlexicalized parser. Lexi- +L+ calization has been shown to be useful in more +L+ general parsing applications due to lexical depen- +L+ dencies in constituent parsing (e.g. (K¨ubler et al., +L+ 2006; Dubey and Keller, 2003) in the case of Ger- +L+ man). However, topological fields explain a higher +L+ level of structure pertaining to clause-level word +L+ order, and we hypothesize that lexicalization is un- +L+ likely to be helpful. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Experiments +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 4.1 Data +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> For our experiments, we primarily used the T¨uBa- +L+ D/Z (T¨ubinger Baumbank des Deutschen / Schrift- +L+ sprache) corpus, consisting of 26116 sentences +L+ (20894 training, 2611 development, 2089 test, +L+ with a further 522 sentences held out for future ex- +L+ </SectLabel_bodyText> <SectLabel_footnote> 2 This evaluation considered grammatical functions as +L+ well as the syntactic category. +L+ </SectLabel_footnote> <SectLabel_bodyText> periments)3 taken from the German newspaper die +L+ tageszeitung. The corpus consists of four levels +L+ of annotation: clausal, topological, phrasal (other +L+ than clausal), and lexical. We define the task of +L+ topological field parsing to be recovering the first +L+ two levels of annotation, following Ule (2003). +L+ We also tested the parser on a version of the NE- +L+ GRA corpus derived by Becker and Frank (2002), +L+ in which syntax trees have been made projec- +L+ tive and topological fields have been automatically +L+ added through a series of linguistically informed +L+ tree modifications. All internal phrasal structure +L+ nodes have also been removed. The corpus con- +L+ sists of 20596 sentences, which we split into sub- +L+ sets of the same size as described by Becker and +L+ Frank (2002)4. The set of topological fields in +L+ this corpus differs slightly from the one used in +L+ T¨uBa-D/Z, making no distinction between clause +L+ types, nor consistently marking field or clause +L+ conjunctions. Because of the automatic anno- +L+ tation of topological fields, this corpus contains +L+ numerous annotation errors. Becker and Frank +L+ (2002) manually corrected their test set and eval- +L+ uated the automatic annotation process, reporting +L+ labelled precision and recall of 93.0% and 93.6% +L+ compared to their manual annotations. There are +L+ also punctuation-related errors, including miss- +L+ ing punctuation, sentences ending in commas, and +L+ sentences composed of single punctuation marks. +L+ We test on this data in order to provide a bet- +L+ ter comparison with previous work. Although we +L+ could have trained the model in Becker and Frank +L+ (2002) on the T¨uBa-D/Z corpus, it would not have +L+ </SectLabel_bodyText> <SectLabel_footnote> 3These are the same splits into training, development, and +L+ test sets as in the ACL-08 Parsing German workshop. This +L+ corpus does not include sentences of length greater than 40. +L+ 416476 training sentences, 1000 development, 1058 test- +L+ ing, and 2062 as held-out data. We were unable to obtain +L+ the exact subsets used by Becker and Frank (2002). We will +L+ discuss the ramifications of this on our evaluation procedure. +L+ </SectLabel_footnote> <SectLabel_page> 67 +L+ </SectLabel_page> <SectLabel_table> Gold tags	Edge labels	LP%	LR%	F1%	CB	CB0%	CB < 2%	EXACT% +L+ -	-	93.53	93.17	93.35	0.08	94.59	99.43	79.50 +L+ +	-	95.26	95.04	95.15	0.07	95.35	99.52	83.86 +L+ -	+	92.38	92.67	92.52	0.11	92.82	99.19	77.79 +L+ +	+	92.36	92.60	92.48	0.11	92.82	99.19	77.64 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Parsing results for topological fields and clausal constituents on the T¨uBa-D/Z corpus. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> been a fair comparison, as the parser depends quite +L+ heavily on NEGRA’s annotation scheme. For ex- +L+ ample, T¨uBa-D/Z does not contain an equiva- +L+ lent of the modified NEGRA’s parameterized cat- +L+ egories; there exist edge labels in T¨uBaD/Z, but +L+ they are used to mark head-dependency relation- +L+ ships, not subtypes of syntactic categories. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4.2 Results +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We first report the results of our experiments on +L+ the T¨uBa-D/Z corpus. For the T¨uBa-D/Z corpus, +L+ we trained the Berkeley parser using the default +L+ parameter settings. The grammar trainer attempts +L+ six iterations of splitting, merging, and smoothing +L+ before returning the final grammar. Intermediate +L+ grammars after each step are also saved. There +L+ were training and test sentences without clausal +L+ constituents or topological fields, which were ig- +L+ nored by the parser and by the evaluation. As +L+ part of our experiment design, we investigated the +L+ effect of providing gold POS tags to the parser, +L+ and the effect of incorporating edge labels into the +L+ nonterminal labels for training and parsing. In all +L+ cases, gold annotations which include gold POS +L+ tags were used when training the parser. +L+ We report the standard PARSEVAL measures +L+ of parser performance in Table 2, obtained by the +L+ evalb program by Satoshi Sekine and Michael +L+ Collins. This table shows the results after five it- +L+ erations of grammar modification, parameterized +L+ over whether we provide gold POS tags for pars- +L+ ing, and edge labels for training and parsing. The +L+ number of iterations was determined by experi- +L+ ments on the development set. In the evaluation, +L+ we do not consider edge labels in determining +L+ correctness, but do consider punctuation, as Ule +L+ (2003) did. If we ignore punctuation in our evalu- +L+ ation, we obtain an F1-measure of 95.42% on the +L+ best model (+ Gold tags, - Edge labels). +L+ Whether supplying gold POS tags improves +L+ performance depends on whether edge labels are +L+ considered in the grammar. Without edge labels, +L+ gold POS tags improve performance by almost +L+ two points, corresponding to a relative error reduc- +L+ tion of 33%. In contrast, performance is negatively +L+ affected when edge labels are used and gold POS +L+ tags are supplied (i.e., + Gold tags, + Edge la- +L+ bels), making the performance worse than not sup- +L+ plying gold tags. Incorporating edge label infor- +L+ mation does not appear to improve performance, +L+ possibly because it oversplits the initial treebank +L+ and interferes with the parser’s ability to determine +L+ optimal splits for refining the grammar. +L+ </SectLabel_bodyText> <SectLabel_table> Parser	LP%	LR%	Fl% +L+ T¨uBa-D/Z +L+ This work	95.26	95.04	95.15 +L+ Ule	unknown	unknown	91.98 +L+ NEGRA - from Becker and Frank (2002) +L+ BF02 (len. < 40)	92.1	91.6	91.8 +L+ NEGRA - our experiments +L+ This work (len. < 40)	90.74	90.87	90.81 +L+ BF02 (len. < 40)	89.54	88.14	88.83 +L+ This work (all)	90.29	90.51	90.40 +L+ BF02 (all)	89.07	87.80	88.43 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3: BF02 = (Becker and Frank, 2002). Pars- +L+ ing results for topological fields and clausal con- +L+ stituents. Results from Ule (2003) and our results +L+ were obtained using different training and test sets. +L+ The first row of results of Becker and Frank (2002) +L+ are from that paper; the rest were obtained by our +L+ own experiments using that parser. All results con- +L+ sider punctuation in evaluation. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> To facilitate a more direct comparison with pre- +L+ vious work, we also performed experiments on the +L+ modified NEGRA corpus. In this corpus, topo- +L+ logical fields are parameterized, meaning that they +L+ are labelled with further syntactic and semantic in- +L+ formation. For example, VF is split into VF-REL +L+ for relative clauses, and VF-TOPIC for those con- +L+ taining topics in a verb-second sentence, among +L+ others. All productions in the corpus have also +L+ been binarized. Tuning the parameter settings on +L+ the development set, we found that parameterized +L+ categories, binarization, and including punctua- +L+ tion gave the best F1 performance. First-order +L+ horizontal and zeroth order vertical markoviza- +L+ </SectLabel_bodyText> <SectLabel_page> 68 +L+ </SectLabel_page> <SectLabel_bodyText> tion after six iterations of splitting, merging, and +L+ smoothing gave the best F1 result of 91.78%. We +L+ parsed the corpus with both the Berkeley parser +L+ and the best performing model of Becker and +L+ Frank (2002). +L+ The results of these experiments on the test set +L+ for sentences of length 40 or less and for all sen- +L+ tences are shown in Table 3. We also show other +L+ results from previous work for reference. We +L+ find that we achieve results that are better than +L+ the model in Becker and Frank (2002) on the test +L+ set. The difference is statistically significant (p = +L+ 0.0029, Wilcoxon signed-rank). +L+ The results we obtain using the parser of Becker +L+ and Frank (2002) are worse than the results de- +L+ scribed in that paper. We suggest the following +L+ reasons for this discrepancy. While the test set +L+ used in the paper was manually corrected for eval- +L+ uation, we did not correct our test set, because it +L+ would be difficult to ensure that we adhered to the +L+ same correction guidelines. No details of the cor- +L+ rection process were provided in the paper, and de- +L+ scriptive grammars of German provide insufficient +L+ guidance on many of the examples in NEGRA on +L+ issues such as ellipses, short infinitival clauses, +L+ and expanded participial constructions modifying +L+ nouns. Also, because we could not obtain the ex- +L+ act sets used for training, development, and test- +L+ ing, we had to recreate the sets by randomly split- +L+ ting the corpus. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Category Specific Results +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We now return to the T¨uBa-D/Z corpus for a +L+ more detailed analysis, and examine the category- +L+ specific results for our best performing model (+ +L+ Gold tags, - Edge labels). Overall, Table 4 shows +L+ that the best performing topological field cate- +L+ gories are those that have constraints on the type +L+ of word that is allowed to fill it (finite verbs in +L+ LK, verbs in VC, complementizers and subordi- +L+ nating conjunctions in C). VF, in which only one +L+ constituent may appear, also performs relatively +L+ well. Topological fields that can contain a vari- +L+ able number of heterogeneous constituents, on the +L+ other hand, have poorer F1-measure results. MF, +L+ which is basically defined relative to the positions +L+ of fields on either side of it, is parsed several points +L+ below LK, C, and VC in accuracy. NF, which +L+ contains different kinds of extraposed elements, is +L+ parsed at a substantially worse level. +L+ Poorly parsed categories tend to occur infre- +L+  quently, including LV, which marks a rare re- +L+ sumptive construction; FKOORD, which marks +L+ topological field coordination; and the discourse +L+ marker DM. The other clause-level constituents +L+ (PSIMPX for clauses in paratactic constructions, +L+ RSIMPX for relative clauses, and SIMPX for +L+ other clauses) also perform below average. +L+ </SectLabel_bodyText> <SectLabel_table> Topological Fields +L+ Category	#	LP%	LR%	Fl% +L+ PARORD	20	100.00	100.00	100.00 +L+ VCE	3	100.00	100.00	100.00 +L+ LK	2186	99.68	99.82	99.75 +L+ C	642	99.53	98.44	98.98 +L+ VC	1777	98.98	98.14	98.56 +L+ VF	2044	96.84	97.55	97.20 +L+ KOORD	99	96.91	94.95	95.92 +L+ MF	2931	94.80	95.19	94.99 +L+ NF	643	83.52	81.96	82.73 +L+ FKOORD	156	75.16	73.72	74.43 +L+ LV	17	10.00	5.88	7.41 +L+ Clausal Constituents +L+ Category	#	LP%	LR%	Fl% +L+ SIMPX	2839	92.46	91.97	92.21 +L+ RSIMPX	225	91.23	92.44	91.83 +L+ PSIMPX	6	100.00	66.67	80.00 +L+ DM	28	59.26	57.14	58.18 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4: Category-specific results using grammar +L+ with no edge labels and passing in gold POS tags. +L+ </SectLabel_tableCaption> <SectLabel_subsectionHeader> 4.4 Reranking for Paired Punctuation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> While experimenting with the development set +L+ of T¨uBa-D/Z, we noticed that the parser some- +L+ times returns parses, in which paired punctuation +L+ (e.g. quotation marks, parentheses, brackets) is +L+ not placed in the same clause–a linguistically im- +L+ plausible situation. In these cases, the high-level +L+ information provided by the paired punctuation is +L+ overridden by the overall likelihood of the parse +L+ tree. To rectify this problem, we performed a sim- +L+ ple post-hoc reranking of the 50-best parses pro- +L+ duced by the best parameter settings (+ Gold tags, +L+ - Edge labels), selecting the first parse that places +L+ paired punctuation in the same clause, or return- +L+ ing the best parse if none of the 50 parses satisfy +L+ the constraint. This procedure improved the F1- +L+ measure to 95.24% (LP = 95.39%, LR = 95.09%). +L+ Overall, 38 sentences were parsed with paired +L+ punctuation in different clauses, of which 16 were +L+ reranked. Of the 38 sentences, reranking improved +L+ performance in 12 sentences, did not affect perfor- +L+ mance in 23 sentences (of which 10 already had a +L+ perfect parse), and hurt performance in three sen- +L+ tences. A two-tailed sign test suggests that rerank- +L+ </SectLabel_bodyText> <SectLabel_page> 69 +L+ </SectLabel_page> <SectLabel_bodyText> ing improves performance (p = 0.0352). We dis- +L+ cuss below why sentences with paired punctuation +L+ in different clauses can have perfect parse results. +L+ To investigate the upper-bound in performance +L+ that this form of reranking is able to achieve, we +L+ calculated some statistics on our (+ Gold tags, - +L+ Edge labels) 50-best list. We found that the aver- +L+ age rank of the best scoring parse by F1-measure +L+ is 2.61, and the perfect parse is present for 1649 +L+ of the 2088 sentences at an average rank of 1.90. +L+ The oracle F1-measure is 98.12%, indicating that +L+ a more comprehensive reranking procedure might +L+ allow further performance gains. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.5 Qualitative Error Analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As a further analysis, we extracted the worst scor- +L+ ing fifty sentences by F1-measure from the parsed +L+ test set (+ Gold tags, - Edge labels), and compared +L+ them against the gold standard trees, noting the +L+ cause of the error. We analyze the parses before +L+ reranking, to see how frequently the paired punc- +L+ tuation problem described above severely affects a +L+ parse. The major mistakes made by the parser are +L+ summarized in Table 5. +L+ </SectLabel_bodyText> <SectLabel_table> Problem	Freq. +L+ Misidentification of Parentheticals	19 +L+ Coordination problems	13 +L+ Too few SIMPX	10 +L+ Paired punctuation problem	9 +L+ Other clause boundary errors	7 +L+ Other	6 +L+ Too many SIMPX	3 +L+ Clause type misidentification	2 +L+ MF/NF boundary	2 +L+ LV	2 +L+ VF/MF boundary	2 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5: Types and frequency of parser errors in +L+ the fifty worst scoring parses by F1-measure, us- +L+ ing parameters (+ Gold tags, - Edge labels). +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Misidentification of Parentheticals Parentheti- +L+ cal constructions do not have any dependencies on +L+ the rest of the sentence, and exist as a mostly syn- +L+ tactically independent clause inside another sen- +L+ tence. They can occur at the beginning, end, or +L+ in the middle of sentences, and are often set off +L+ orthographically by punctuation. The parser has +L+ problems identifying parenthetical constructions, +L+ often positing a parenthetical construction when +L+ that constituent is actually attached to a topolog- +L+ ical field in a neighbouring clause. The follow- +L+ ing example shows one such misidentification in +L+ bracket notation. Clause internal topological fields +L+ are omitted for clarity. +L+ </SectLabel_bodyText> <SectLabel_listItem> (2) (a) T¨uBa-D/Z: (SIMPX Weder das Ausmafi der +L+ </SectLabel_listItem> <SectLabel_bodyText> Sch¨onheit noch der fr¨uhere oder sp¨atere +L+ Zeitpunkt der Geburt macht einen der Zwillinge +L+ f¨ur eine Mutter mehr oder weniger echt / +L+ authentisch / ¨uberlegen). +L+ </SectLabel_bodyText> <SectLabel_listItem> (b) Parser: (SIMPX Weder das Ausmafi der +L+ </SectLabel_listItem> <SectLabel_bodyText> Sch¨onheit noch der fr¨uhere oder sp¨atere +L+ Zeitpunkt der Geburt macht einen der Zwillinge +L+ f¨ur eine Mutter mehr oder weniger echt) +L+ (PARENTHETICAL / authentisch / +L+ ¨uberlegen.) +L+ </SectLabel_bodyText> <SectLabel_listItem> (c) Translation: “Neither the degree of beauty nor +L+ </SectLabel_listItem> <SectLabel_bodyText> the earlier or later time of birth makes one of the +L+ twins any more or less real/authentic/superior to +L+ a mother.” +L+ We hypothesized earlier that lexicalization is +L+ unlikely to give us much improvement in perfor- +L+ mance, because topological fields work on a do- +L+ main that is higher than that of lexical dependen- +L+ cies such as subcategorization frames. However, +L+ given the locally independent nature of legitimate +L+ parentheticals, a limited form of lexicalization or +L+ some other form of stronger contextual informa- +L+ tion might be needed to improve identification per- +L+ formance. +L+ Coordination Problems The second most com- +L+ mon type of error involves field and clause coordi- +L+ nations. This category includes missing or incor- +L+ rect FKOORD fields, and conjunctions of clauses +L+ that are misidentified. In the following example, +L+ the conjoined MFs and following NF in the cor- +L+ rect parse tree are identified as a single long MF. +L+ </SectLabel_bodyText> <SectLabel_listItem> (3)	(a) T¨uBa-D/Z: Auf dem europ¨aischen Kontinent +L+ </SectLabel_listItem> <SectLabel_bodyText> aber hat (FKOORD (MF kein Land und keine +L+ Macht ein derartiges Interesse an guten +L+ Beziehungen zu Ruf land) und (MF auch kein +L+ Land solche Erfahrungen im Umgang mit +L+ Ruf land)) (NF wie Deutschland). +L+ </SectLabel_bodyText> <SectLabel_listItem> (b) Parser: Auf dem europ¨aischen Kontinent aber +L+ </SectLabel_listItem> <SectLabel_bodyText> hat (MF kein Land und keine Macht ein +L+ derartiges Interesse an guten Beziehungen zu +L+ Ruf land und auch kein Land solche +L+ Erfahrungen im Umgang mit Ruf land wie +L+ Deutschland). +L+ </SectLabel_bodyText> <SectLabel_listItem> (c) Translation: “On the European continent, +L+ </SectLabel_listItem> <SectLabel_bodyText> however, no land and no power has such an +L+ interest in good relations with Russia (as +L+ Germany), and also no land (has) such +L+ experience in dealing with Russia as Germany.” +L+ Other Clause Errors Other clause-level errors +L+ include the parser predicting too few or too many +L+ clauses, or misidentifying the clause type. Clauses +L+ are sometimes confused with NFs, and there is one +L+ case of a relative clause being misidentified as a +L+ </SectLabel_bodyText> <SectLabel_page> 70 +L+ </SectLabel_page> <SectLabel_bodyText> main clause with an intransitive verb, as the finite +L+ verb appears at the end of the clause in both cases. +L+ Some clause errors are tied to incorrect treatment +L+ of elliptical constructions, in which an element +L+ that is inferable from context is missing. +L+ Paired Punctuation Problems with paired +L+ punctuation are the fourth most common type of +L+ error. Punctuation is often a marker of clause +L+ or phrase boundaries. Thus, predicting paired +L+ punctuation incorrectly can lead to incorrect +L+ parses, as in the following example. +L+ </SectLabel_bodyText> <SectLabel_listItem> (4) (a) “ Auch (SIMPX wenn der Krieg heute ein +L+ </SectLabel_listItem> <SectLabel_bodyText> Mobilisierungsfaktor ist) ” , so Pau, “ (SIMPX +L+ die Leute sehen , dafi man f¨ur die Arbeit wieder +L+ auf die Strafie gehen mufi) . ” +L+ </SectLabel_bodyText> <SectLabel_listItem> (b) Parser: (SIMPX “ (LV Auch (SIMPX wenn der +L+ </SectLabel_listItem> <SectLabel_bodyText> Krieg heute ein Mobilisierungsfaktor ist)) ” , so +L+ Pau, “ (SIMPX die Leute sehen , dafi man f¨ur +L+ die Arbeit wieder auf die Strafie gehen mufi)) . ” +L+ </SectLabel_bodyText> <SectLabel_listItem> (c) Translation: “Even if the war is a factor for +L+ </SectLabel_listItem> <SectLabel_bodyText> mobilization,” said Pau, “the people see, that +L+ one must go to the street for employment again.” +L+ Here, the parser predicts a spurious SIMPX +L+ clause spanning the text of the entire sentence, but +L+ this causes the second pair of quotation marks to +L+ be parsed as belonging to two different clauses. +L+ The parser also predicts an incorrect LV field. Us- +L+ ing the paired punctuation constraint, our rerank- +L+ ing procedure was able to correct these errors. +L+ Surprisingly, there are cases in which paired +L+ punctuation does not belong inside the same +L+ clause in the gold parses. These cases are ei- +L+ ther extended quotations, in which each of the +L+ quotation mark pair occurs in a different sen- +L+ tence altogether, or cases where the second of the +L+ quotation mark pair must be positioned outside +L+ of other sentence-final punctuation due to ortho- +L+ graphic conventions. Sentence-final punctuation +L+ is typically placed outside a clause in this version +L+ of T¨uBa-D/Z. +L+ Other Issues Other incorrect parses generated +L+ by the parser include problems with the infre- +L+ quently occurring topological fields like LV and +L+ DM, inability to determine the boundary between +L+ MF and NF in clauses without a VC field sepa- +L+ rating the two, and misidentifying appositive con- +L+ structions. Another issue is that although the +L+ parser output may disagree with the gold stan- +L+ dard tree in T¨uBa-D/Z, the parser output may be +L+ a well-formed topological field parse for the same +L+ sentence with a different interpretation, for ex- +L+ ample because of attachment ambiguity. Each of +L+ the authors independently checked the fifty worst- +L+ scoring parses, and determined whether each parse +L+ produced by the Berkeley parser could be a well- +L+ formed topological parse. Where there was dis- +L+ agreement, we discussed our judgments until we +L+ came to a consensus. Of the fifty parses, we de- +L+ termined that nine, or 18%, could be legitimate +L+ parses. Another five, or 10%, differ from the gold +L+ standard parse only in the placement of punctua- +L+ tion. Thus, the Fl-measures we presented above +L+ may be underestimating the parser’s performance. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Conclusion and Future Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we examined applying the latent- +L+ variable Berkeley parser to the task of topological +L+ field parsing of German, which aims to identify the +L+ high-level surface structure of sentences. Without +L+ any language or model-dependent adaptation, we +L+ obtained results which compare favourably to pre- +L+ vious work in topological field parsing. We further +L+ examined the results of doing a simple reranking +L+ process, constraining the output parse to put paired +L+ punctuation in the same clause. This reranking +L+ was found to result in a minor performance gain. +L+ Overall, the parser performs extremely well in +L+ identifying the traditional left and right brackets +L+ of the topological field model; that is, the fields +L+ C, LK, and VC. The parser achieves basically per- +L+ fect results on these fields in the T¨uBa-D/Z corpus, +L+ with Fl-measure scores for each at over 98.5%. +L+ These scores are higher than previous work in the +L+ simpler task of topological field chunking. The fo- +L+ cus of future research should thus be on correctly +L+ identifying the infrequently occuring fields and +L+ constructions, with parenthetical constructions be- +L+ ing a particular concern. Possible avenues of fu- +L+ ture research include doing a more comprehensive +L+ discriminative reranking of the parser output. In- +L+ corporating more contextual information might be +L+ helpful to identify discourse-related constructions +L+ such as parentheses, and the DM and LV topolog- +L+ ical fields. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgements +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We are grateful to Markus Becker, Anette Frank, +L+ Sandra Kuebler, and Slav Petrov for their invalu- +L+ able help in gathering the resources necessary for +L+ our experiments. This work is supported in part +L+ by the Natural Sciences and Engineering Research +L+ Council of Canada. +L+ </SectLabel_bodyText> <SectLabel_page> 71 +L+ </SectLabel_page> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> M. Becker and A. Frank. 2002. A stochastic topo- +L+ logical parser for German. In Proceedings of the +L+ 19th International Conference on Computational +L+ Linguistics, pages 71–77. +L+ S. Brants, S. Dipper, S. Hansen, W. Lezius, and +L+ G. Smith. 2002. The TIGER Treebank. In Proceed- +L+ ings of the Workshop on Treebanks and Linguistic +L+ Theories, pages 24–41. +L+ U. Callmeier. 2000. PET–a platform for experimen- +L+ tation with efficient HPSG processing techniques. +L+ Natural Language Engineering, 6(01):99–107. +L+ A. Dubey and F. Keller. 2003. Probabilistic parsing +L+ for German using sister-head dependencies. In Pro- +L+ ceedings of the 41st Annual Meeting of the Associa- +L+ tion for Computational Linguistics, pages 96–103. +L+ K.A. Foth, M. Daum, and W. Menzel. 2004. A +L+ broad-coverage parser for German based on defea- +L+ sible constraints. Constraint Solving and Language +L+ Processing. +L+ A. Frank, M. Becker, B. Crysmann, B. Kiefer, and +L+ U. Schaefer. 2003. Integrated shallow and deep +L+ parsing: TopP meets HPSG. In Proceedings of the +L+ 41st Annual Meeting of the Association for Compu- +L+ tational Linguistics, pages 104–111. +L+ W. Frey. 2004. Notes on the syntax and the pragmatics +L+ of German Left Dislocation. In H. Lohnstein and +L+ S. Trissler, editors, The Syntax and Semantics of the +L+ Left Periphery, pages 203–233. Mouton de Gruyter, +L+ Berlin. +L+ J. Hockenmaier. 2006. Creating a CCGbank and a +L+ Wide-Coverage CCG Lexicon for German. In Pro- +L+ ceedings of the 21st International Conference on +L+ Computational Linguistics and 44th Annual Meet- +L+ ing of the Association for Computational Linguis- +L+ tics, pages 505–512. +L+ T.N. H¨ohle. 1983. Topologische Felder. Ph.D. thesis, +L+ K¨oln. +L+ S. K¨ubler, E.W. Hinrichs, and W. Maier. 2006. Is it re- +L+ ally that difficult to parse German? In Proceedings +L+ of EMNLP. +L+ M. Liepert. 2003. Topological Fields Chunking for +L+ German with SVM’s: Optimizing SVM-parameters +L+ with GA’s. In Proceedings of the International Con- +L+ ference on Recent Advances in Natural Language +L+ Processing (RANLP), Bulgaria. +L+ G. Neumann, C. Braun, and J. Piskorski. 2000. A +L+ Divide-and-Conquer Strategy for Shallow Parsing +L+ of German Free Texts. In Proceedings of the sixth +L+ conference on Applied natural language processing, +L+ pages 239–246. Morgan Kaufmann Publishers Inc. +L+ San Francisco, CA, USA. +L+ S. Petrov and D. Klein. 2008. Parsing German with +L+ Latent Variable Grammars. In Proceedings of the +L+ ACL-08: HLT Workshop on Parsing German (PaGe- +L+ 08), pages 33–39. +L+ S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. +L+ Learning accurate, compact, and interpretable tree +L+ annotation. In Proceedings of the 21st Interna- +L+ tional Conference on Computational Linguistics and +L+ 44th Annual Meeting of the Association for Compu- +L+ tational Linguistics, pages 433–440, Sydney, Aus- +L+ tralia, July. Association for Computational Linguis- +L+ tics. +L+ C. Rohrer and M. Forst. 2006. Improving coverage +L+ and parsing quality of a large-scale LFG for Ger- +L+ man. In Proceedings of the Language Resources +L+ and Evaluation Conference (LREC-2006), Genoa, +L+ Italy. +L+ W. Skut, T. Brants, B. Krenn, and H. Uszkoreit. +L+ 1998. A Linguistically Interpreted Corpus of Ger- +L+ man Newspaper Text. Proceedings of the ESSLLI +L+ Workshop on Recent Advances in Corpus Annota- +L+ tion. +L+ H. Telljohann, E. Hinrichs, and S. Kubler. 2004. +L+ The T¨uBa-D/Z treebank: Annotating German with a +L+ context-free backbone. In Proceedings of the Fourth +L+ International Conference on Language Resources +L+ and Evaluation (LREC 2004), pages 2229–2235. +L+ H. Telljohann, E.W. Hinrichs, S. Kubler, and H. Zins- +L+ meister. 2006. Stylebook for the Tubingen Tree- +L+ bank of Written German (T¨uBa-D/Z). Seminar fur +L+ Sprachwissenschaft, Universitat Tubingen, Tubin- +L+ gen, Germany. +L+ T. Ule. 2003. Directed Treebank Refinement for PCFG +L+ Parsing. In Proceedings of Workshop on Treebanks +L+ and Linguistic Theories (TLT) 2003, pages 177–188. +L+ J. Veenstra, F.H. M¨uller, and T. Ule. 2002. Topolog- +L+ ical field chunking for German. In Proceedings of +L+ the Sixth Conference on Natural Language Learn- +L+ ing, pages 56–62. +L+ </SectLabel_reference> <SectLabel_page> 72 +L+ </SectLabel_page>
<SectLabel_title> Unsupervised Multilingual Grammar Induction +L+ </SectLabel_title> <SectLabel_author> Benjamin Snyder, Tahira Naseem, and Regina Barzilay +L+ </SectLabel_author> <SectLabel_affiliation> Computer Science and Artificial Intelligence Laboratory +L+ Massachusetts Institute of Technology +L+ </SectLabel_affiliation> <SectLabel_email> {bsnyder, tahira, regina}@csail.mit.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We investigate the task of unsupervised +L+ constituency parsing from bilingual par- +L+ allel corpora. Our goal is to use bilin- +L+ gual cues to learn improved parsing mod- +L+ els for each language and to evaluate these +L+ models on held-out monolingual test data. +L+ We formulate a generative Bayesian model +L+ which seeks to explain the observed par- +L+ allel data through a combination of bilin- +L+ gual and monolingual parameters. To this +L+ end, we adapt a formalism known as un- +L+ ordered tree alignment to our probabilistic +L+ setting. Using this formalism, our model +L+ loosely binds parallel trees while allow- +L+ ing language-specific syntactic structure. +L+ We perform inference under this model us- +L+ ing Markov Chain Monte Carlo and dy- +L+ namic programming. Applying this model +L+ to three parallel corpora (Korean-English, +L+ Urdu-English, and Chinese-English) we +L+ find substantial performance gains over +L+ the CCM model, a strong monolingual +L+ baseline. On average, across a variety of +L+ testing scenarios, our model achieves an +L+ 8.8 absolute gain in F-measure. 1 +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper we investigate the task of unsuper- +L+ vised constituency parsing when bilingual paral- +L+ lel text is available. Our goal is to improve pars- +L+ ing performance on monolingual test data for each +L+ language by using unsupervised bilingual cues at +L+ training time. Multilingual learning has been suc- +L+ cessful for other linguistic induction tasks such as +L+ lexicon acquisition, morphological segmentation, +L+ and part-of-speech tagging (Genzel, 2005; Snyder +L+ and Barzilay, 2008; Snyder et al., 2008; Snyder +L+ </SectLabel_bodyText> <SectLabel_footnote> 1Code and the outputs of our experiments are available at +L+ http://groups.csail.mit.edu/rbg/code/multiling induction. +L+ </SectLabel_footnote> <SectLabel_bodyText> et al., 2009). We focus here on the unsupervised +L+ induction of unlabeled constituency brackets. This +L+ task has been extensively studied in a monolingual +L+ setting and has proven to be difficult (Charniak +L+ and Carroll, 1992; Klein and Manning, 2002). +L+ The key premise of our approach is that am- +L+ biguous syntactic structures in one language may +L+ correspond to less uncertain structures in the other +L+ language. For instance, the English sentence I +L+ saw [the student [from MIT]] exhibits the classic +L+ problem of PP-attachment ambiguity. However, +L+ its Urdu translation, literally glossed as I [[MIT of] +L+ student] saw, uses a genitive phrase that may only +L+ be attached to the adjacent noun phrase. Know- +L+ ing the correspondence between these sentences +L+ should help us resolve the English ambiguity. +L+ One of the main challenges of unsupervised +L+ multilingual learning is to exploit cross-lingual +L+ patterns discovered in data, while still allowing +L+ a wide range of language-specific idiosyncrasies. +L+ To this end, we adapt a formalism known as un- +L+ ordered tree alignment (Jiang et al., 1995) to +L+ a probabilistic setting. Under this formalism, +L+ any two trees can be embedded in an alignment +L+ tree. This alignment tree allows arbitrary parts +L+ of the two trees to diverge in structure, permitting +L+ language-specific grammatical structure to be pre- +L+ served. Additionally, a computational advantage +L+ of this formalism is that the marginalized probabil- +L+ ity over all possible alignments for any two trees +L+ can be efficiently computed with a dynamic pro- +L+ gram in linear time. +L+ We formulate a generative Bayesian model +L+ which seeks to explain the observed parallel data +L+ through a combination of bilingual and mono- +L+ lingual parameters. Our model views each pair +L+ of sentences as having been generated as fol- +L+ lows: First an alignment tree is drawn. Each +L+ node in this alignment tree contains either a soli- +L+ tary monolingual constituent or a pair of coupled +L+ bilingual constituents. For each solitary mono- +L+ </SectLabel_bodyText> <SectLabel_page> 73 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 73–81, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> lingual constituent, a sequence of part-of-speech +L+ tags is drawn from a language-specific distribu- +L+ tion. For each pair of coupled bilingual con- +L+ stituents, a pair of part-of-speech sequences are +L+ drawn jointly from a cross-lingual distribution. +L+ Word-level alignments are then drawn based on +L+ the tree alignment. Finally, parallel sentences are +L+ assembled from these generated part-of-speech se- +L+ quences and word-level alignments. +L+ To perform inference under this model, we use +L+ a Metropolis-Hastings within-Gibbs sampler. We +L+ sample pairs of trees and then compute marginal- +L+ ized probabilities over all possible alignments us- +L+ ing dynamic programming. +L+ We test the effectiveness of our bilingual gram- +L+ mar induction model on three corpora of parallel +L+ text: English-Korean, English-Urdu and English- +L+ Chinese. The model is trained using bilingual +L+ data with automatically induced word-level align- +L+ ments, but is tested on purely monolingual data +L+ for each language. In all cases, our model out- +L+ performs a state-of-the-art baseline: the Con- +L+ stituent Context Model (CCM) (Klein and Man- +L+ ning, 2002), sometimes by substantial margins. +L+ On average, over all the testing scenarios that we +L+ studied, our model achieves an absolute increase +L+ in F-measure of 8.8 points, and a 19% reduction +L+ in error relative to a theoretical upper bound. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The unsupervised grammar induction task has +L+ been studied extensively, mostly in a monolin- +L+ gual setting (Charniak and Carroll, 1992; Stolcke +L+ and Omohundro, 1994; Klein and Manning, 2002; +L+ Seginer, 2007). While PCFGs perform poorly on +L+ this task, the CCM model (Klein and Manning, +L+ 2002) has achieved large gains in performance and +L+ is among the state-of-the-art probabilistic models +L+ for unsupervised constituency parsing. We there- +L+ fore use CCM as our basic model of monolingual +L+ syntax. +L+ While there has been some previous work on +L+ bilingual CFG parsing, it has mainly focused on +L+ improving MT systems rather than monolingual +L+ parsing accuracy. Research in this direction was +L+ pioneered by (Wu, 1997), who developed Inver- +L+ sion Transduction Grammars to capture cross- +L+ lingual grammar variations such as phrase re- +L+ orderings. More general formalisms for the same +L+ purpose were later developed (Wu and Wong, +L+ 1998; Chiang, 2005; Melamed, 2003; Eisner, +L+ 2003; Zhang and Gildea, 2005; Blunsom et al., +L+ 2008). We know of only one study which eval- +L+ uates these bilingual grammar formalisms on the +L+ task of grammar induction itself (Smith and Smith, +L+ 2004). Both our model and even the monolingual +L+ CCM baseline yield far higher performance on the +L+ same Korean-English corpus. +L+ Our approach is closer to the unsupervised +L+ bilingual parsing model developed by Kuhn +L+ (2004), which aims to improve monolingual per- +L+ formance. Assuming that trees induced over paral- +L+ lel sentences have to exhibit certain structural reg- +L+ ularities, Kuhn manually specifies a set of rules +L+ for determining when parsing decisions in the two +L+ languages are inconsistent with GIZA++ word- +L+ level alignments. By incorporating these con- +L+ straints into the EM algorithm he was able to im- +L+ prove performance over a monolingual unsuper- +L+ vised PCFG. Still, the performance falls short of +L+ state-of-the-art monolingual models such as the +L+ CCM. +L+ More recently, there has been a body of work +L+ attempting to improve parsing performance by ex- +L+ ploiting syntactically annotated parallel data. In +L+ one strand of this work, annotations are assumed +L+ only in a resource-rich language and are projected +L+ onto a resource-poor language using the parallel +L+ data (Hwa et al., 2005; Xi and Hwa, 2005). In +L+ another strand of work, syntactic annotations are +L+ assumed on both sides of the parallel data, and a +L+ model is trained to exploit the parallel data at test +L+ time as well (Smith and Smith, 2004; Burkett and +L+ Klein, 2008). In contrast to this work, our goal +L+ is to explore the benefits of multilingual grammar +L+ induction in a fully unsupervised setting. +L+ We finally note a recent paper which uses pa- +L+ rameter tying to improve unsupervised depen- +L+ dency parse induction (Cohen and Smith, 2009). +L+ While the primary performance gains occur when +L+ tying related parameters within a language, some +L+ additional benefit is observed through bilingual ty- +L+ ing, even in the absence of a parallel corpus. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Model +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We propose an unsupervised Bayesian model for +L+ learning bilingual syntactic structure using paral- +L+ lel corpora. Our key premise is that difficult-to- +L+ learn syntactic structures of one language may cor- +L+ respond to simpler or less uncertain structures in +L+ the other language. We treat the part-of-speech +L+ tag sequences of parallel sentences, as well as their +L+ </SectLabel_bodyText> <SectLabel_page> 74 +L+ </SectLabel_page> <SectLabel_figure> (ii) +L+ (iii) +L+ (i) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1: A pair of trees (i) and two possible alignment trees. In (ii), no empty spaces are inserted, but +L+ the order of one of the original tree’s siblings has been reversed. In (iii), only two pairs of nodes have +L+ been aligned (indicated by arrows) and many empty spaces inserted. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> word-level alignments, as observed data. We ob- +L+ tain these word-level alignments using GIZA++ +L+ (Och and Ney, 2003). +L+ Our model seeks to explain this observed data +L+ through a generative process whereby two aligned +L+ parse trees are produced jointly. Though they +L+ are aligned, arbitrary parts of the two trees are +L+ permitted to diverge, accommodating language- +L+ specific grammatical structure. In effect, our +L+ model loosely binds the two trees: node-to-node +L+ alignments need only be used where repeated +L+ bilingual patterns can be discovered in the data. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Tree Alignments +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We achieve this loose binding of trees by adapting +L+ unordered tree alignment (Jiang et al., 1995) to a +L+ probabilistic setting. Under this formalism, any +L+ two trees can be aligned using an alignment tree. +L+ The alignment tree embeds the original two trees +L+ within it: each node is labeled by a pair (x, y), +L+ (A, y), or (x, A) where x is a node from the first +L+ tree, y is a node from the second tree, and A is an +L+ empty space. The individual structure of each tree +L+ must be preserved under the embedding with the +L+ exception of sibling order (to allow variations in +L+ phrase and word order). +L+ The flexibility of this formalism can be demon- +L+ strated by two extreme cases: (1) an alignment be- +L+ tween two trees may actually align none of their +L+ individual nodes, instead inserting an empty space +L+ A for each of the original two trees’ nodes. (2) +L+ if the original trees are isomorphic to one an- +L+ other, the alignment may match their nodes ex- +L+ actly, without inserting any empty spaces. See +L+ Figure 1 for an example. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Model overview +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As our basic model of syntactic structure, we +L+ adopt the Constituent-Context Model (CCM) of +L+ Klein and Manning (2002). Under this model, +L+ the part-of-speech sequence of each span in a sen- +L+ tence is generated either as a constituent yield +L+ — if it is dominated by a node in the tree — +L+ or otherwise as a distituent yield. For example, +L+ in the bracketed sentence [John/NNP [climbed/VB +L+ [the/DT tree/NN]]], the sequence VB DT NN is gen- +L+ erated as a constituent yield, since it constitutes a +L+ complete bracket in the tree. On the other hand, +L+ the sequence VB DT is generated as a distituent, +L+ since it does not. Besides these yields, the con- +L+ texts (two surrounding POS tags) of constituents +L+ and distituents are generated as well. In this exam- +L+ ple, the context of the constituent VB DT NN would +L+ be (NNP, #), while the context of the distituent VB +L+ DT would be (NNP, NN). The CCM model em- +L+ ploys separate multinomial distributions over con- +L+ stituents, distituents, constituent contexts, and dis- +L+ tituent contexts. While this model is deficient — +L+ each observed subsequence of part-of-speech tags +L+ is generated many times over — its performance +L+ is far higher than that of unsupervised PCFGs. +L+ Under our bilingual model, each pair of sen- +L+ tences is assumed to have been generated jointly in +L+ the following way: First, an unlabeled alignment +L+ tree is drawn uniformly from the set of all such +L+ trees. This alignment tree specifies the structure +L+ of each of the two individual trees, as well as the +L+ pairs of nodes which are aligned and those which +L+ are not aligned (i.e. paired with a A). +L+ For each pair of aligned nodes, a correspond- +L+ ing pair of constituents and contexts are jointly +L+ drawn from a bilingual distribution. For unaligned +L+ nodes (i.e. nodes paired with a A in the alignment +L+ </SectLabel_bodyText> <SectLabel_page> 75 +L+ </SectLabel_page> <SectLabel_bodyText> tree), a single constituent and context are drawn, +L+ from language-specific distributions. Distituents +L+ and their contexts are also drawn from language- +L+ specific distributions. Finally, word-level align- +L+ ments are drawn based on the structure of the +L+ alignment tree. +L+ In the next two sections, we describe our model +L+ in more formal detail by specifying the parame- +L+ ters and generative process by which sentences are +L+ formed. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Parameters +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our model employs a number of multinomial dis- +L+ tributions: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	7r�D : over constituent yields of language i, +L+ •	7r�D : over distituent yields of language i, +L+ •	��D : over constituent contexts of language i, +L+ •	��D : over distituent contexts of language i, +L+ •	w : over pairs of constituent yields, one from +L+ the first language and the other from the sec- +L+ ond language, +L+ •	Gzpair : over a finite set of integer val- +L+ ues {—m, ... , —2, —1, 0,1, 2, ... , m}, mea- +L+ suring the Giza-score of aligned tree node +L+ pairs (see below), +L+ •	Gznode : over a finite set of integer values +L+ {—m, . . ., —2, —1, 01, measuring the Giza- +L+ score of unaligned tree nodes (see below). +L+ </SectLabel_listItem> <SectLabel_bodyText> The first four distributions correspond exactly to +L+ the parameters of the CCM model. Parameter w is +L+ a “coupling parameter” which measures the com- +L+ patibility of tree-aligned constituent yield pairs. +L+ The final two parameters measure the compatibil- +L+ ity of syntactic alignments with the observed lexi- +L+ cal GIZA++ alignments. Intuitively, aligned nodes +L+ should have a high density of word-level align- +L+ ments between them, and unaligned nodes should +L+ have few lexical alignments. +L+ More formally, consider a tree-aligned node +L+ pair (n1, n2) with corresponding yields (y1, y2). +L+ We call a word-level alignment good if it aligns +L+ a word in y1 with a word in y2. We call a word- +L+ level alignment bad if it aligns a word in y1 with +L+ a word outside y2, or vice versa. The Giza- +L+ score for (n1, n2) is the number of good word +L+ alignments minus the number of bad word align- +L+ ments. For example, suppose the constituent my +L+ long name is node-aligned to its Urdu translation +L+ mera lamba naam. If only the word-pairs my/mera +L+ and name/naam are aligned, then the Giza-score +L+ for this node-alignment would be 2. If however, +L+ the English word long were (incorrectly) aligned +L+ under GIZA++ to some Urdu word outside the cor- +L+ responding constituent, then the score would drop +L+ to 1. This score could even be negative if the num- +L+ ber of bad alignments exceeds those that are good. +L+ Distribution Gzpair provides a probability for these +L+ scores (up to some fixed absolute value). +L+ For an unaligned node n with corresponding +L+ yield y, only bad GIZA++ alignments are possible, +L+ thus the Giza-score for these nodes will always be +L+ zero or negative. Distribution Gznode provides a +L+ probability for these scores (down to some fixed +L+ value). We want our model to find tree alignments +L+ such that both aligned node pairs and unaligned +L+ nodes have high Giza-score. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3.4 Generative Process +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Now we describe the stochastic process whereby +L+ the observed parallel sentences and their word- +L+ level alignments are generated, according to our +L+ model. +L+ As the first step in the Bayesian generative pro- +L+ cess, all the multinomial parameters listed in the +L+ previous section are drawn from their conjugate +L+ priors — Dirichlet distributions of appropriate di- +L+ mension. Then, each pair of word-aligned parallel +L+ sentences is generated through the following pro- +L+ cess: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. A pair of binary trees T1 and T2 along with +L+ </SectLabel_listItem> <SectLabel_bodyText> an alignment tree A are drawn according to +L+ P(T1, T2, A). A is an alignment tree for T1 +L+ and T2 if it can be obtained by the follow- +L+ ing steps: First insert blank nodes (labeled by +L+ A) into T1 and T2. Then permute the order +L+ of sibling nodes such that the two resulting +L+ trees T10 and T20 are identical in structure. Fi- +L+ nally, overlay T10 and T20 to obtain A. We ad- +L+ ditionally require that A contain no extrane- +L+ ous nodes – that is no nodes with two blank +L+ labels (A, A). See Figure 1 for an example. +L+ We define the distribution P(T1, T2, A) to be +L+ uniform over all pairs of binary trees and their +L+ alignments. +L+ </SectLabel_bodyText> <SectLabel_listItem> 2. For each node in A of the form (n1, A) (i.e. +L+ nodes in T1 left unaligned by A), draw +L+ (i) a constituent yield according to 7r�1 , +L+ 76 +L+ (ii) a constituent context according to 0C1, +L+ (iii) a Giza-score according to Gznode. +L+ 3. For each node in A of the form (A, n2) (i.e. +L+ nodes in T2 left unaligned by A), draw +L+ (i) a constituent yield according to 7rC2 , +L+ (ii) a constituent context according to 0C2, +L+ (iii) a Giza-score according to Gznode. +L+ 4. For each node in A of the form (n1, n2) (i.e. +L+ tree-aligned node pairs), draw +L+ (i) a pair of constituent yields (y1, y2) ac- +L+ cording to: +L+ </SectLabel_listItem> <SectLabel_equation> 0C1 (y1) - 0C2(y2) - W (y1, y2)	(1) +L+ Z +L+ </SectLabel_equation> <SectLabel_bodyText> which is a product of experts combining +L+ the language specific context-yield dis- +L+ tributions as well as the coupling distri- +L+ bution W with normalization constant Z, +L+ </SectLabel_bodyText> <SectLabel_listItem> (ii) a pair of contexts according to the ap- +L+ propriate language-specific parameters, +L+ (iii) a Giza-score according to Gzpair. +L+ 5. For each span in TZ not dominated by a node +L+ </SectLabel_listItem> <SectLabel_bodyText> (for each language i E 11, 2}), draw a dis- +L+ tituent yield according to 7r�Z and a distituent +L+ context according to 0�Z. +L+ </SectLabel_bodyText> <SectLabel_listItem> 6. Draw actual word-level alignments consis- +L+ </SectLabel_listItem> <SectLabel_bodyText> tent with the Giza-scores, according to a uni- +L+ form distribution. +L+ In the next section we turn to the problem of +L+ inference under this model when only the part- +L+ of-speech tag sequences of parallel sentences and +L+ their word-level alignments are observed. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3.5 Inference +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Given a corpus of paired part-of-speech tag se- +L+ quences (s1, s2) and their GIZA++ alignments +L+ g, we would ideally like to predict the set of +L+ tree pairs (T1, T2) which have highest proba- +L+ bility when conditioned on the observed data: +L+ P(T1, T2Is1, s2, g). We could rewrite this by +L+ explicitly integrating over the yield, context, cou- +L+ pling, Giza-score parameters as well as the align- +L+ ment trees. However, since maximizing this in- +L+ tegral directly would be intractable, we resort to +L+ standard Markov chain sampling techniques. We +L+ use Gibbs sampling (Hastings, 1970) to draw trees +L+ for each sentence conditioned on those drawn for +L+ all other sentences. The samples form a Markov +L+ chain which is guaranteed to converge to the true +L+ joint distribution over all sentences. +L+ In the monolingual setting, there is a well- +L+ known tree sampling algorithm (Johnson et al., +L+ 2007). This algorithm proceeds in top-down fash- +L+ ion by sampling individual split points using the +L+ marginal probabilities of all possible subtrees. +L+ These marginals can be efficiently pre-computed +L+ and form the “inside” table of the famous Inside- +L+ Outside algorithm. However, in our setting, trees +L+ come in pairs, and their joint probability crucially +L+ depends on their alignment. +L+ For the i1h parallel sentence, we wish to jointly +L+ sample the pair of trees (T1,T2)Z together with +L+ their alignment AZ. To do so directly would in- +L+ volve simultaneously marginalizing over all pos- +L+ sible subtrees as well as all possible alignments +L+ between such subtrees when sampling upper-level +L+ split points. We know of no obvious algorithm +L+ for computing this marginal. We instead first sam- +L+ ple the pair of trees (T1, T2)Z from a simpler pro- +L+ posal distribution Q. Our proposal distribution as- +L+ sumes that no nodes of the two trees are aligned +L+ and therefore allows us to use the recursive top- +L+ down sampling algorithm mentioned above. After +L+ a new tree pair T* _ (T1*, T2* )Z is drawn from Q, +L+ we accept the pair with the following probability: +L+ </SectLabel_bodyText> <SectLabel_equation> � � +L+ min 1, +L+ P(T* T—Z, A—Z) Q(TIT—Z, A—Z) +L+ P(T IT—Z, A—Z) Q(T* IT—Z, A—Z) +L+ </SectLabel_equation> <SectLabel_bodyText> where T is the previously sampled tree-pair for +L+ sentence i, P is the true model probability, and +L+ Q is the probability under the proposal distribu- +L+ tion. This use of a tractable proposal distribution +L+ and acceptance ratio is known as the Metropolis- +L+ Hastings algorithm and it preserves the conver- +L+ gence guarantee of the Gibbs sampler (Hastings, +L+ 1970). To compute the terms P(T*IT—Z,A—Z) +L+ and P(T IT—Z, A—Z) in the acceptance ratio above, +L+ we need to marginalize over all possible align- +L+ ments between tree pairs. +L+ Fortunately, for any given pair of trees T1 and +L+ T2 this marginalization can be computed using +L+ a dynamic program in time O (I T1 I I T2 I) . Here +L+ we provide a very brief sketch. For every pair +L+ of nodes n1 E T1, n2 E T2, a table stores the +L+ marginal probability of the subtrees rooted at n1 +L+ and n2, respectively. A dynamic program builds +L+ this table from the bottom up: For each node pair +L+ n1, n2, we sum the probabilities of all local align- +L+ ment configurations, each multiplied by the appro- +L+ </SectLabel_bodyText> <SectLabel_page> 77 +L+ </SectLabel_page> <SectLabel_bodyText> priate marginals already computed in the table for +L+ lower-level node pairs. This algorithm is an adap- +L+ tation of the dynamic program presented in (Jiang +L+ et al., 1995) for finding minimum cost alignment +L+ trees (Fig. 5 of that publication). +L+ Once a pair of trees (T1, T2) has been sam- +L+ pled, we can proceed to sample an alignment tree +L+ AIT1, T2.2 We sample individual alignment deci- +L+ sions from the top down, at each step using the +L+ alignment marginals for the remaining subtrees +L+ (already computed using the afore-mentioned dy- +L+ namic program). Once the triple (T1, T2, A) has +L+ been sampled, we move on to the next parallel sen- +L+ tence. +L+ We avoid directly sampling parameter val- +L+ ues, instead using the marginalized closed forms +L+ for multinomials with Dirichlet conjugate-priors +L+ using counts and hyperparameter pseudo-counts +L+ (Gelman et al., 2004). Note that in the case of +L+ yield pairs produced according to Distribution 1 +L+ (in step 4 of the generative process) conjugacy is +L+ technically broken, since the yield pairs are no +L+ longer produced by a single multinomial distribu- +L+ tion. Nevertheless, we count the produced yields +L+ as if they had been generated separately by each +L+ of the distributions involved in the numerator of +L+ Distribution 1. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Experimental setup +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We test our model on three corpora of bilin- +L+ gual parallel sentences: English-Korean, English- +L+ Urdu, and English-Chinese. Though the model is +L+ trained using parallel data, during testing it has ac- +L+ cess only to monolingual data. This set-up ensures +L+ that we are testing our model’s ability to learn bet- +L+ ter parameters at training time, rather than its abil- +L+ ity to exploit parallel data at test time. Following +L+ (Klein and Manning, 2002), we restrict our model +L+ to binary trees, though we note that the alignment +L+ trees do not follow this restriction. +L+ Data The Penn Korean Treebank (Han et al., +L+ 2002) consists of 5,083 Korean sentences trans- +L+ lated into English for the purposes of language +L+ training in a military setting. Both the Korean +L+ and English sentences are annotated with syntactic +L+ trees. We use the first 4,000 sentences for training +L+ and the last 1,083 sentences for testing. We note +L+ that in the Korean data, a separate tag is given for +L+ </SectLabel_bodyText> <SectLabel_footnote> 2Sampling the alignment tree is important, as it provides +L+ us with counts of aligned constituents for the coupling pa- +L+ rameter. +L+ </SectLabel_footnote> <SectLabel_bodyText> each morpheme. We simply concatenate all the +L+ morpheme tags given for each word and treat the +L+ concatenation as a single tag. This procedure re- +L+ sults in 199 different tags. The English-Urdu par- +L+ allel corpus3 consists of 4,325 sentences from the +L+ first three sections of the Penn Treebank and their +L+ Urdu translations annotated at the part-of-speech +L+ level. The Urdu side of this corpus does not pro- +L+ vide tree annotations so here we can test parse ac- +L+ curacy only on English. We use the remaining +L+ sections of the Penn Treebank for English test- +L+ ing. The English-Chinese treebank (Bies et al., +L+ 2007) consists of 3,850 Chinese newswire sen- +L+ tences translated into English. Both the English +L+ and Chinese sentences are annotated with parse +L+ trees. We use the first 4/5 for training and the final +L+ 1/5 for testing. +L+ During preprocessing of the corpora we remove +L+ all punctuation marks and special symbols, fol- +L+ lowing the setup in previous grammar induction +L+ work (Klein and Manning, 2002). To obtain lex- +L+ ical alignments between the parallel sentences we +L+ employ GIZA++ (Och and Ney, 2003). We use in- +L+ tersection alignments, which are one-to-one align- +L+ ments produced by taking the intersection of one- +L+ to-many alignments in each direction. These one- +L+ to-one intersection alignments tend to have higher +L+ precision. +L+ We initialize the trees by making uniform split +L+ decisions recursively from the top down for sen- +L+ tences in both languages. Then for each pair of +L+ parallel sentences we randomly sample an initial +L+ alignment tree for the two sampled trees. +L+ Baseline We implement a Bayesian version of +L+ the CCM as a baseline. This model uses the same +L+ inference procedure as our bilingual model (Gibbs +L+ sampling). In fact, our model reduces to this +L+ Bayesian CCM when it is assumed that no nodes +L+ between the two parallel trees are ever aligned +L+ and when word-level alignments are ignored. We +L+ also reimplemented the original EM version of +L+ CCM and found virtually no difference in perfor- +L+ mance when using EM or Gibbs sampling. In both +L+ cases our implementation achieves F-measure in +L+ the range of 69-70% on WSJ10, broadly in line +L+ with the performance reported by Klein and Man- +L+ ning (2002). +L+ Hyperparameters Klein (2005) reports using +L+ smoothing pseudo-counts of 2 for constituent +L+ </SectLabel_bodyText> <SectLabel_footnote> 3 http://www.crulp.org +L+ </SectLabel_footnote> <SectLabel_page> 78 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 2: The F-measure of the CCM baseline (dotted line) and bilingual model (solid line) plotted on +L+ the y-axis, as the maximum sentence length in the test set is increased (x-axis). Results are averaged over +L+ all training scenarios given in Table 1. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> yields and contexts and 8 for distituent yields and +L+ contexts. In our Bayesian model, these similar +L+ smoothing counts occur as the parameters of the +L+ Dirichlet priors. For Korean we found that the +L+ baseline performed well using these values. How- +L+ ever, on our English and Chinese data, we found +L+ that somewhat higher smoothing values worked +L+ best, so we utilized values of 20 and 80 for con- +L+ stituent and distituent smoothing counts, respec- +L+ tively. +L+ Our model additionally requires hyperparam- +L+ eter values for w (the coupling distribution for +L+ aligned yields), Gzpair and Gznode (the distribu- +L+ tions over Giza-scores for aligned nodes and un- +L+ aligned nodes, respectively). For w we used a +L+ symmetric Dirichlet prior with parameter 1. For +L+ Gzpair and Gznode, in order to create a strong bias +L+ towards high Giza-scores, we used non-symmetric +L+ Dirichlet priors. In both cases, we capped the ab- +L+ solute value of the scores at 3, to prevent count +L+ sparsity. In the case of Gzpair we gave pseudo- +L+ counts of 1,000 for negative values and zero, and +L+ pseudo-counts of 1,000,000 for positive scores. +L+ For Gznode we gave a pseudo-count of 1,000,000 +L+ for a score of zero, and 1,000 for all nega- +L+ tive scores. This very strong prior bias encodes +L+ our intuition that syntactic alignments which re- +L+ spect lexical alignments should be preferred. Our +L+ method is not sensitive to these exact values and +L+ any reasonably strong bias gave similar results. +L+ In all our experiments, we consider the hyper- +L+ parameters fixed and observed values. +L+ Testing and evaluation As mentioned above, +L+ we test our model only on monolingual data, +L+ where the parallel sentences are not provided to +L+ the model. To predict the bracketings of these +L+ monolingual test sentences, we take the smoothed +L+ counts accumulated in the final round of sampling +L+ over the training data and perform a maximum +L+ likelihood estimate of the monolingual CCM pa- +L+ rameters. These parameters are then used to pro- +L+ duce the highest probability bracketing of the test +L+ set. +L+ To evaluate both our model as well as the base- +L+ line, we use (unlabeled) bracket precision, re- +L+ call, and F-measure (Klein and Manning, 2002). +L+ Following previous work, we include the whole- +L+ sentence brackets but ignore single-word brack- +L+ ets. We perform experiments on different subsets +L+ of training and testing data based on the sentence- +L+ length. In particular we experimented with sen- +L+ tence length limits of 10, 20, and 30 for both the +L+ training and testing sets. We also report the upper +L+ bound on F-measure for binary trees. We average +L+ the results over 10 separate sampling runs. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 Results +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Table 1 reports the full results of our experiments. +L+ In all testing scenarios the bilingual model out- +L+ performs its monolingual counterpart in terms of +L+ both precision and recall. On average, the bilin- +L+ gual model gains 10.2 percentage points in preci- +L+ sion, 7.7 in recall, and 8.8 in F-measure. The gap +L+ between monolingual performance and the binary +L+ tree upper bound is reduced by over 19%. +L+ The extent of the gain varies across pairings. +L+ For instance, the smallest improvement is ob- +L+ served for English when trained with Urdu. The +L+ Korean-English pairing results in substantial im- +L+ provements for Korean and quite large improve- +L+ ments for English, for which the absolute gain +L+ reaches 28 points in F-measure. In the case of Chi- +L+ nese and English, the gains for English are fairly +L+ minimal whereas those for Chinese are quite sub- +L+ </SectLabel_bodyText> <SectLabel_page> 79 +L+ </SectLabel_page> <SectLabel_table> 	Max Sent. Length		Monolingual			Bilingual			Upper Bound +L+ 	Test	Train	Precision	Recall	F1	Precision	Recall	F1	F1 +L+ 		10	52.74	39.53	45.19	57.76	43.30	49.50	85.6 +L+ 	10	20	41.87	31.38	35.87	61.66	46.22	52.83	85.6 +L+ 		30	33.43	25.06	28.65	64.41	48.28	55.19	85.6 +L+ 	20	20	35.12	25.12	29.29	56.96	40.74	47.50	83.3 +L+ 		30	26.26	18.78	21.90	60.07	42.96	50.09	83.3 +L+ 	30	30	23.95	16.81	19.76	58.01	40.73	47.86	82.4 +L+ 		10	71.07	62.55	66.54	75.63	66.56	70.81	93.6 +L+ 	10	20	71.35	62.79	66.80	77.61	68.30	72.66	93.6 +L+ 		30	71.37	62.81	66.82	77.87	68.53	72.91	93.6 +L+ 	20	20	64.28	54.73	59.12	70.44	59.98	64.79	91.9 +L+ 		30	64.29	54.75	59.14	70.81	60.30	65.13	91.9 +L+ 	30	30	63.63	54.17	58.52	70.11	59.70	64.49	91.9 +L+ 		10	50.09	34.18	40.63	37.46	25.56	30.39	81.0 +L+ 	10	20	58.86	40.17	47.75	50.24	34.29	40.76	81.0 +L+ 		30	64.81	44.22	52.57	68.24	46.57	55.36	81.0 +L+ 	20	20	41.90	30.52	35.31	38.64	28.15	32.57	84.3 +L+ 		30	52.83	38.49	44.53	58.50	42.62	49.31	84.3 +L+ 	30	30	46.35	33.67	39.00	51.40	37.33	43.25	84.1 +L+ 		10	39.87	27.71	32.69	40.62	28.23	33.31	81.9 +L+ 	10	20	43.44	30.19	35.62	47.54	33.03	38.98	81.9 +L+ 		30	43.63	30.32	35.77	54.09	37.59	44.36	81.9 +L+ 	20	20	29.80	23.46	26.25	36.93	29.07	32.53	88.0 +L+ 		30	30.05	23.65	26.47	43.99	34.63	38.75	88.0 +L+ 	30	30	24.46	19.41	21.64	39.61	31.43	35.05	88.4 +L+ 		10	57.98	45.68	51.10	73.43	57.85	64.71	88.1 +L+ 	10	20	70.57	55.60	62.20	80.24	63.22	70.72	88.1 +L+ 		30	75.39	59.40	66.45	79.04	62.28	69.67	88.1 +L+ 	20	20	57.78	43.86	49.87	67.26	51.06	58.05	86.3 +L+ 		30	63.12	47.91	54.47	64.45	48.92	55.62	86.3 +L+ 	30	30	57.36	43.02	49.17	57.97	43.48	49.69	85.7 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Unlabeled precision, recall and F-measure for the monolingual baseline and the bilingual model +L+ on several test sets. We report results for different combinations of maximum sentence length in both the +L+ training and test sets. The right most column, in all cases, contains the maximum F-measure achievable +L+ using binary trees. The best performance for each test-length is highlighted in bold. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> stantial. This asymmetry should not be surprising, +L+ as Chinese on its own seems to be quite a bit more +L+ difficult to parse than English. +L+ We also investigated the impact of sentence +L+ length for both the training and testing sets. For +L+ our model, adding sentences of greater length to +L+ the training set leads to increases in parse accu- +L+ racy for short sentences. For the baseline, how- +L+ ever, adding this additional training data degrades +L+ performance in the case of English paired with Ko- +L+ rean. Figure 2 summarizes the performance of +L+ our model for different sentence lengths on sev- +L+ eral of the test-sets. As shown in the figure, the +L+ largest improvements tend to occur at longer sen- +L+ tence lengths. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Conclusion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have presented a probabilistic model for bilin- +L+ gual grammar induction which uses raw parallel +L+ text to learn tree pairs and their alignments. Our +L+ formalism loosely binds the two trees, using bilin- +L+ gual patterns when possible, but allowing substan- +L+ tial language-specific variation. We tested our +L+ model on three test sets and showed substantial +L+ improvement over a state-of-the-art monolingual +L+ baseline.4 +L+ </SectLabel_bodyText> <SectLabel_footnote> 4The authors acknowledge the support of the NSF (CA- +L+ REER grant IIS-0448168, grant IIS-0835445, and grant IIS- +L+ 0835652). Thanks to Amir Globerson and members of the +L+ MIT NLP group for their helpful suggestions. Any opinions, +L+ findings, or conclusions are those of the authors, and do not +L+ necessarily reflect the views of the funding organizations +L+ </SectLabel_footnote> <SectLabel_page> 80 +L+ </SectLabel_page> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Ann Bies, Martha Palmer, Justin Mott, and Colin +L+ Warner. 2007. English Chinese translation treebank +L+ v 1.0. LDC2007T02. +L+ Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. +L+ Bayesian synchronous grammar induction. In Pro- +L+ ceedings of NIPS. +L+ David Burkett and Dan Klein. 2008. Two languages +L+ are better than one (for syntactic parsing). In Pro- +L+ ceedings of EMNLP, pages 877–886. +L+ Eugene Charniak and Glen Carroll. 1992. Two exper- +L+ iments on learning probabilistic dependency gram- +L+ mars from corpora. In Proceedings of the AAAI +L+ Workshop on Statistically-Based NLP Techniques, +L+ pages 1–13. +L+ David Chiang. 2005. A hierarchical phrase-based +L+ model for statistical machine translation. In Pro- +L+ ceedings of the ACL, pages 263–270. +L+ Shay B. Cohen and Noah A. Smith. 2009. Shared lo- +L+ gistic normal distributions for soft parameter tying +L+ in unsupervised grammar induction. In Proceedings +L+ of the NAACL/HLT. +L+ Jason Eisner. 2003. Learning non-isomorphic tree +L+ mappings for machine translation. In The Compan- +L+ ion Volume to the Proceedings of the ACL, pages +L+ 205–208. +L+ Andrew Gelman, John B. Carlin, Hal S. Stern, and +L+ Donald B. Rubin. 2004. Bayesian data analysis. +L+ Chapman and Hall/CRC. +L+ Dmitriy Genzel. 2005. Inducing a multilingual dictio- +L+ nary from a parallel multitext in related languages. +L+ In Proceedings of EMNLP/HLT, pages 875–882. +L+ C. Han, N.R. Han, E.S. Ko, H. Yi, and M. Palmer. +L+ 2002. Penn Korean Treebank: Development and +L+ evaluation. In Proc. Pacific Asian Conf. Language +L+ and Comp. +L+ W. K. Hastings. 1970. Monte carlo sampling meth- +L+ ods using Markov chains and their applications. +L+ Biometrika, 57:97–109. +L+ R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and +L+ O. Kolak. 2005. Bootstrapping parsers via syntactic +L+ projection across parallel texts. Journal of Natural +L+ Language Engineering, 11(3):311–325. +L+ T. Jiang, L. Wang, and K. Zhang. 1995. Alignment of +L+ trees – an alternative to tree edit. Theoretical Com- +L+ puter Science, 143(1):137–148. +L+ M. Johnson, T. Griffiths, and S. Goldwater. 2007. +L+ Bayesian inference for PCFGs via Markov chain +L+ Monte Carlo. In Proceedings of the NAACL/HLT, +L+ pages 139–146. +L+ Dan Klein and Christopher D. Manning. 2002. A +L+ generative constituent-context model for improved +L+ grammar induction. In Proceedings of the ACL, +L+ pages 128–135. +L+ D. Klein. 2005. The Unsupervised Learning of Natu- +L+ ral Language Structure. Ph.D. thesis, Stanford Uni- +L+ versity. +L+ Jonas Kuhn. 2004. Experiments in parallel-text based +L+ grammar induction. In Proceedings of the ACL, +L+ pages 470–477. +L+ I. Dan Melamed. 2003. Multitext grammars +L+ and synchronous parsers. In Proceedings of the +L+ NAACL/HLT, pages 79–86. +L+ Franz Josef Och and Hermann Ney. 2003. A sys- +L+ tematic comparison of various statistical alignment +L+ models. Computational Linguistics, 29(1):19–51. +L+ Yoav Seginer. 2007. Fast unsupervised incremental +L+ parsing. In Proceedings of the ACL, pages 384–391. +L+ David A. Smith and Noah A. Smith. 2004. Bilingual +L+ parsing with factored estimation: Using English to +L+ parse Korean. In Proceeding of EMNLP, pages 49– +L+ 56. +L+ Benjamin Snyder and Regina Barzilay. 2008. Un- +L+ supervised multilingual learning for morphological +L+ segmentation. In Proceedings of the ACL/HLT, +L+ pages 737–745. +L+ Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, +L+ and Regina Barzilay. 2008. Unsupervised multi- +L+ lingual learning for POS tagging. In Proceedings of +L+ EMNLP, pages 1041–1050. +L+ Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, +L+ and Regina Barzilay. 2009. Adding more languages +L+ improves unsupervised multilingual part-of-speech +L+ tagging: A Bayesian non-parametric approach. In +L+ Proceedings of the NAACL/HLT. +L+ Andreas Stolcke and Stephen M. Omohundro. 1994. +L+ Inducing probabilistic grammars by Bayesian model +L+ merging. In Proceedings of ICGI, pages 106–118. +L+ Dekai Wu and Hongsing Wong. 1998. Machine +L+ translation with a stochastic grammatical channel. +L+ In Proceedings of the ACL/COLING, pages 1408– +L+ 1415. +L+ Dekai Wu. 1997. Stochastic inversion transduction +L+ grammars and bilingual parsing of parallel corpora. +L+ Computational Linguistics, 23(3):377–403. +L+ Chenhai Xi and Rebecca Hwa. 2005. A backoff +L+ model for bootstrapping resources for non-english +L+ languages. In Proceedings of EMNLP, pages 851 – +L+ 858. +L+ Hao Zhang and Daniel Gildea. 2005. Stochastic lex- +L+ icalized inversion transduction grammar for align- +L+ ment. In Proceedings of the ACL, pages 475–482. +L+ </SectLabel_reference> <SectLabel_page> 81 +L+ </SectLabel_page>
<SectLabel_title> Reinforcement Learning for Mapping Instructions to Actions +L+ </SectLabel_title> <SectLabel_author> S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay +L+ </SectLabel_author> <SectLabel_affiliation> Computer Science and Artificial Intelligence Laboratory +L+ Massachusetts Institute of Technology +L+ </SectLabel_affiliation> <SectLabel_email> {branavan, harr, lsz, regina}@csail.mit.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we present a reinforce- +L+ ment learning approach for mapping nat- +L+ ural language instructions to sequences of +L+ executable actions. We assume access to +L+ a reward function that defines the qual- +L+ ity of the executed actions. During train- +L+ ing, the learner repeatedly constructs ac- +L+ tion sequences for a set of documents, ex- +L+ ecutes those actions, and observes the re- +L+ sulting reward. We use a policy gradient +L+ algorithm to estimate the parameters of a +L+ log-linear model for action selection. We +L+ apply our method to interpret instructions +L+ in two domains — Windows troubleshoot- +L+ ing guides and game tutorials. Our results +L+ demonstrate that this method can rival su- +L+ pervised learning techniques while requir- +L+ ing few or no annotated training exam- +L+ ples.1 +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The problem of interpreting instructions written +L+ in natural language has been widely studied since +L+ the early days of artificial intelligence (Winograd, +L+ 1972; Di Eugenio, 1992). Mapping instructions to +L+ a sequence of executable actions would enable the +L+ automation of tasks that currently require human +L+ participation. Examples include configuring soft- +L+ ware based on how-to guides and operating simu- +L+ lators using instruction manuals. In this paper, we +L+ present a reinforcement learning framework for in- +L+ ducing mappings from text to actions without the +L+ need for annotated training examples. +L+ For concreteness, consider instructions from a +L+ Windows troubleshooting guide on deleting tem- +L+ porary folders, shown in Figure 1. We aim to map +L+ </SectLabel_bodyText> <SectLabel_footnote> 1Code, data, and annotations used in this work are avail- +L+ able at http://groups.csail.mit.edu/rbg/code/rl/ +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 1: A Windows troubleshooting article de- +L+ scribing how to remove the “msdownld.tmp” tem- +L+ porary folder. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> this text to the corresponding low-level commands +L+ and parameters. For example, properly interpret- +L+ ing the third instruction requires clicking on a tab, +L+ finding the appropriate option in a tree control, and +L+ clearing its associated checkbox. +L+ In this and many other applications, the valid- +L+ ity of a mapping can be verified by executing the +L+ induced actions in the corresponding environment +L+ and observing their effects. For instance, in the +L+ example above we can assess whether the goal +L+ described in the instructions is achieved, i.e., the +L+ folder is deleted. The key idea of our approach +L+ is to leverage the validation process as the main +L+ source of supervision to guide learning. This form +L+ of supervision allows us to learn interpretations +L+ of natural language instructions when standard su- +L+ pervised techniques are not applicable, due to the +L+ lack of human-created annotations. +L+ Reinforcement learning is a natural framework +L+ for building models using validation from an envi- +L+ ronment (Sutton and Barto, 1998). We assume that +L+ supervision is provided in the form of a reward +L+ function that defines the quality of executed ac- +L+ tions. During training, the learner repeatedly con- +L+ structs action sequences for a set of given docu- +L+ ments, executes those actions, and observes the re- +L+ sulting reward. The learner’s goal is to estimate a +L+ </SectLabel_bodyText> <SectLabel_page> 82 +L+ </SectLabel_page> <SectLabel_note> Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82–90, +L+ Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP +L+ </SectLabel_note> <SectLabel_bodyText> policy — a distribution over actions given instruc- +L+ tion text and environment state — that maximizes +L+ future expected reward. Our policy is modeled in a +L+ log-linear fashion, allowing us to incorporate fea- +L+ tures of both the instruction text and the environ- +L+ ment. We employ a policy gradient algorithm to +L+ estimate the parameters of this model. +L+ We evaluate our method on two distinct applica- +L+ tions: Windows troubleshooting guides and puz- +L+ zle game tutorials. The key findings of our ex- +L+ periments are twofold. First, models trained only +L+ with simple reward signals achieve surprisingly +L+ high results, coming within 11% of a fully su- +L+ pervised method in the Windows domain. Sec- +L+ ond, augmenting unlabeled documents with even +L+ a small fraction of annotated examples greatly re- +L+ duces this performance gap, to within 4% in that +L+ domain. These results indicate the power of learn- +L+ ing from this new form of automated supervision. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Grounded Language Acquisition Our work +L+ fits into a broader class of approaches that aim to +L+ learn language from a situated context (Mooney, +L+ 2008a; Mooney, 2008b; Fleischman and Roy, +L+ 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, +L+ 2001). Instances of such approaches include +L+ work on inferring the meaning of words from +L+ video data (Roy and Pentland, 2002; Barnard and +L+ Forsyth, 2001), and interpreting the commentary +L+ of a simulated soccer game (Chen and Mooney, +L+ 2008). Most of these approaches assume some +L+ form of parallel data, and learn perceptual co- +L+ occurrence patterns. In contrast, our emphasis +L+ is on learning language by proactively interacting +L+ with an external environment. +L+ Reinforcement Learning for Language Pro- +L+ cessing Reinforcement learning has been previ- +L+ ously applied to the problem of dialogue manage- +L+ ment (Scheffler and Young, 2002; Roy et al., 2000; +L+ Litman et al., 2000; Singh et al., 1999). These +L+ systems converse with a human user by taking ac- +L+ tions that emit natural language utterances. The +L+ reinforcement learning state space encodes infor- +L+ mation about the goals of the user and what they +L+ say at each time step. The learning problem is to +L+ find an optimal policy that maps states to actions, +L+ through a trial-and-error process of repeated inter- +L+ action with the user. +L+ Reinforcement learning is applied very differ- +L+ ently in dialogue systems compared to our setup. +L+ In some respects, our task is more easily amenable +L+ to reinforcement learning. For instance, we are not +L+ interacting with a human user, so the cost of inter- +L+ action is lower. However, while the state space can +L+ be designed to be relatively small in the dialogue +L+ management task, our state space is determined by +L+ the underlying environment and is typically quite +L+ large. We address this complexity by developing +L+ a policy gradient algorithm that learns efficiently +L+ while exploring a small subset of the states. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Problem Formulation +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our task is to learn a mapping between documents +L+ and the sequence of actions they express. Figure 2 +L+ shows how one example sentence is mapped to +L+ three actions. +L+ Mapping Text to Actions As input, we are +L+ given a document d, comprising a sequence of sen- +L+ tences (u1, ... , ut), where each ui is a sequence +L+ of words. Our goal is to map d to a sequence of +L+ actions a� = (a0, ... , a,1). Actions are predicted +L+ and executed sequentially.2 +L+ An action a = (c, R, W') encompasses a com- +L+ mand c, the command’s parameters R, and the +L+ words W' specifying c and R. Elements of R re +L+ fer to objects available in the environment state, as +L+ described below. Some parameters can also refer +L+ to words in document d. Additionally, to account +L+ for words that do not describe any actions, c can +L+ be a null command. +L+ The Environment The environment state £ +L+ specifies the set of objects available for interac- +L+ tion, and their properties. In Figure 2, £ is shown +L+ on the right. The environment state £ changes +L+ in response to the execution of command c with +L+ parameters R according to a transition distribu- +L+ tion p(£'J£, c, R). This distribution is a priori un- +L+ known to the learner. As we will see in Section 5, +L+ our approach avoids having to directly estimate +L+ this distribution. +L+ State To predict actions sequentially, we need to +L+ track the state of the document-to-actions map- +L+ ping over time. A mapping state s is a tuple +L+ (£, d, j, W), where £ refers to the current environ- +L+ ment state; j is the index of the sentence currently +L+ being interpreted in document d; and W contains +L+ words that were mapped by previous actions for +L+ </SectLabel_bodyText> <SectLabel_footnote> 2That is, action ai is executed before ai+1 is predicted. +L+ </SectLabel_footnote> <SectLabel_page> 83 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 2: A three-step mapping from an instruction sentence to a sequence of actions in Windows 2000. +L+ For each step, the figure shows the words selected by the action, along with the corresponding system +L+ command and its parameters. The words of W' are underlined, and the words of W are highlighted in +L+ grey. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the same sentence. The mapping state s is ob- +L+ served after each action. +L+ The initial mapping state s0 for document d is +L+ (£d, d, 0, 0); £d is the unique starting environment +L+ state for d. Performing action a in state s = +L+ (£, d, j, W) leads to a new state s' according to +L+ distribution p(s'Is, a), defined as follows: £ tran- +L+ sitions according to p(£'I£, c, R), W is updated +L+ with a’s selected words, and j is incremented if +L+ all words of the sentence have been mapped. For +L+ the applications we consider in this work, environ- +L+ ment state transitions, and consequently mapping +L+ state transitions, are deterministic. +L+ Training During training, we are provided with +L+ a set D of documents, the ability to sample from +L+ the transition distribution, and a reward function +L+ r(h). Here, h = (s0, a0, ... , sn—1, an—1, sn) is +L+ a history of states and actions visited while in- +L+ terpreting one document. r(h) outputs a real- +L+ valued score that correlates with correct action +L+ selection.3 We consider both immediate reward, +L+ which is available after each action, and delayed +L+ reward, which does not provide feedback until the +L+ last action. For example, task completion is a de- +L+ layed reward that produces a positive value after +L+ the final action only if the task was completed suc- +L+ cessfully. We will also demonstrate how manu- +L+ ally annotated action sequences can be incorpo- +L+ rated into the reward. +L+ </SectLabel_bodyText> <SectLabel_footnote> 3In most reinforcement learning problems, the reward +L+ function is defined over state-action pairs, as r(s, a) — in this +L+ case, r(h) _ Et r(st, at), and our formulation becomes a +L+ standard finite-horizon Markov decision process. Policy gra- +L+ dient approaches allow us to learn using the more general +L+ case of history-based reward. +L+ </SectLabel_footnote> <SectLabel_bodyText> The goal of training is to estimate parameters 0 +L+ of the action selection distribution p(aI s, 0), called +L+ the policy. Since the reward correlates with ac- +L+ tion sequence correctness, the 0 that maximizes +L+ expected reward will yield the best actions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 A Log-Linear Model for Actions +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our goal is to predict a sequence of actions. We +L+ construct this sequence by repeatedly choosing an +L+ action given the current mapping state, and apply- +L+ ing that action to advance to a new state. +L+ Given a state s = (£, d, j, W), the space of pos- +L+ sible next actions is defined by enumerating sub- +L+ spans of unused words in the current sentence (i.e., +L+ subspans of the jth sentence of d not in W), and +L+ the possible commands and parameters in envi- +L+ ronment state £ .4 We model the policy distribu- +L+ tion p(aIs; 0) over this action space in a log-linear +L+ fashion (Della Pietra et al., 1997; Lafferty et al., +L+ 2001), giving us the flexibility to incorporate a di- +L+ verse range of features. Under this representation, +L+ the policy distribution is: +L+ </SectLabel_bodyText> <SectLabel_equation> ee-�(s,a) +L+ p(aI s; 0) = � ee-0(s,a') ,	(1) +L+ a/ +L+ </SectLabel_equation> <SectLabel_bodyText> where 0(s, a) E Rn is an n-dimensional feature +L+ representation. During test, actions are selected +L+ according to the mode of this distribution. +L+ </SectLabel_bodyText> <SectLabel_footnote> 4For parameters that refer to words, the space of possible +L+ values is defined by the unused words in the current sentence. +L+ </SectLabel_footnote> <SectLabel_page> 84 +L+ </SectLabel_page> <SectLabel_sectionHeader> 5 Reinforcement Learning +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> During training, our goal is to find the optimal pol- +L+ icy p(aIs; 0). Since reward correlates with correct +L+ action selection, a natural objective is to maximize +L+ expected future reward — that is, the reward we +L+ expect while acting according to that policy from +L+ state s. Formally, we maximize the value function: +L+ </SectLabel_bodyText> <SectLabel_equation> Vo(s) = Ep(hJo) [r(h)] ,	(2) +L+ </SectLabel_equation> <SectLabel_bodyText> where the history h is the sequence of states and +L+ actions encountered while interpreting a single +L+ document d E D. This expectation is averaged +L+ over all documents in D. The distribution p(hI0) +L+ returns the probability of seeing history h when +L+ starting from state s and acting according to a pol- +L+ icy with parameters 0. This distribution can be de- +L+ composed into a product over time steps: +L+ Input: A document set D, +L+ Feature representation �, +L+ Reward function r(h), +L+ Number of iterations T +L+ Initialization: Set 0 to small random values. +L+ </SectLabel_bodyText> <SectLabel_listItem> 1 fori=1 ... Tdo +L+ 2	foreach d E D do +L+ 3	Sample history h — p(hl0) where +L+ h = (s0, a0, ... , an-1, sn) as follows: +L+ 3a	fort =0 ... n-1do +L+ 3b	Sample action at — p(alst; 0) +L+ 3c	Execute at on state st: st+1 — p(slst, at) +L+ end +L+ 4	A +- Et (O(st, at) — Ea, 0(st, a')p(a' l st; 0)) +L+ 5	0+-0+r(h)A +L+ end +L+ end +L+ Output: Estimate of parameters 0 +L+ Algorithm 1: A policy gradient algorithm. +L+ </SectLabel_listItem> <SectLabel_equation> p(hI0) = n�1 H p(atIst; 0)p(st+1 Ist, at).	(3) +L+ t=0 +L+ </SectLabel_equation> <SectLabel_subsectionHeader> 5.1 A Policy Gradient Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our reinforcement learning problem is to find the +L+ parameters 0 that maximize Vo from equation 2. +L+ Although there is no closed form solution, policy +L+ gradient algorithms (Sutton et al., 2000) estimate +L+ the parameters 0 by performing stochastic gradi- +L+ ent ascent. The gradient of Vo is approximated by +L+ interacting with the environment, and the resulting +L+ reward is used to update the estimate of 0. Policy +L+ gradient algorithms optimize a non-convex objec- +L+ tive and are only guaranteed to find a local opti- +L+ mum. However, as we will see, they scale to large +L+ state spaces and can perform well in practice. +L+ To find the parameters 0 that maximize the ob- +L+ jective, we first compute the derivative of Vo. Ex- +L+ panding according to the product rule, we have: +L+ </SectLabel_bodyText> <SectLabel_equation> a0Vo(s) = Ep(hJo) Lr(h)	�0 log p(atI st; 0) , +L+ t +L+ </SectLabel_equation> <SectLabel_bodyText> (4)  where the inner sum is over all time steps t in +L+ the current history h. Expanding the inner partial +L+ derivative we observe that: +L+ </SectLabel_bodyText> <SectLabel_equation> �a0 log p(aIs; 0) = 0(s, a)—	0(s, a')p(a'Is; 0), +L+ a/ +L+ </SectLabel_equation> <SectLabel_bodyText> (5)  which is the derivative of a log-linear distribution. +L+ Equation 5 is easy to compute directly. How- +L+ ever, the complete derivative of Vo in equation 4 +L+ is intractable, because computing the expectation +L+ would require summing over all possible histo- +L+ ries. Instead, policy gradient algorithms employ +L+ stochastic gradient ascent by computing a noisy +L+ estimate of the expectation using just a subset of +L+ the histories. Specifically, we draw samples from +L+ p(hI0) by acting in the target environment, and +L+ use these samples to approximate the expectation +L+ in equation 4. In practice, it is often sufficient to +L+ sample a single history h for this approximation. +L+ Algorithm 1 details the complete policy gradi- +L+ ent algorithm. It performs T iterations over the +L+ set of documents D. Step 3 samples a history that +L+ maps each document to actions. This is done by +L+ repeatedly selecting actions according to the cur- +L+ rent policy, and updating the state by executing the +L+ selected actions. Steps 4 and 5 compute the empir- +L+ ical gradient and update the parameters 0. +L+ In many domains, interacting with the environ- +L+ ment is expensive. Therefore, we use two tech- +L+ niques that allow us to take maximum advantage +L+ of each environment interaction. First, a his- +L+ tory h = (s0, a0, ... , sn) contains subsequences +L+ (si, ai,... sn) for i = 1 to n — 1, each with its +L+ own reward value given by the environment as a +L+ side effect of executing h. We apply the update +L+ from equation 5 for each subsequence. Second, +L+ for a sampled history h, we can propose alterna- +L+ tive histories h' that result in the same commands +L+ and parameters with different word spans. We can +L+ again apply equation 5 for each h', weighted by its +L+ probability under the current policy, P�h'Jl1 . +L+ </SectLabel_bodyText> <SectLabel_page> 85 +L+ </SectLabel_page> <SectLabel_bodyText> The algorithm we have presented belongs to +L+ a family of policy gradient algorithms that have +L+ been successfully used for complex tasks such as +L+ robot control (Ng et al., 2003). Our formulation is +L+ unique in how it represents natural language in the +L+ reinforcement learning framework. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Reward Functions and ML Estimation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We can design a range of reward functions to guide +L+ learning, depending on the availability of anno- +L+ tated data and environment feedback. Consider the +L+ case when every training document d E D is an- +L+ notated with its correct sequence of actions, and +L+ state transitions are deterministic. Given these ex- +L+ amples, it is straightforward to construct a reward +L+ function that connects policy gradient to maxi- +L+ mum likelihood. Specifically, define a reward +L+ function r(h) that returns one when h matches the +L+ annotation for the document being analyzed, and +L+ zero otherwise. Policy gradient performs stochas- +L+ tic gradient ascent on the objective from equa- +L+ tion 2, performing one update per document. For +L+ document d, this objective becomes: +L+ </SectLabel_bodyText> <SectLabel_equation> Ep(h�e)[r(h)] = X r(h)p(h� e) = p(hd� e), +L+ h +L+ </SectLabel_equation> <SectLabel_bodyText> where hd is the history corresponding to the an- +L+ notated action sequence. Thus, with this reward +L+ policy gradient is equivalent to stochastic gradient +L+ ascent with a maximum likelihood objective. +L+ At the other extreme, when annotations are +L+ completely unavailable, learning is still possi- +L+ ble given informative feedback from the environ- +L+ ment. Crucially, this feedback only needs to cor- +L+ relate with action sequence quality. We detail +L+ environment-based reward functions in the next +L+ section. As our results will show, reward func- +L+ tions built using this kind of feedback can provide +L+ strong guidance for learning. We will also con- +L+ sider reward functions that combine annotated su- +L+ pervision with environment feedback. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 Applying the Model +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We study two applications of our model: follow- +L+ ing instructions to perform software tasks, and +L+ solving a puzzle game using tutorial guides. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.1 Microsoft Windows Help and Support +L+ On its Help and Support website,5 Microsoft pub- +L+ lishes a number of articles describing how to per- +L+ </SectLabel_subsectionHeader> <SectLabel_footnote> 5support.microsoft.com +L+ </SectLabel_footnote> <SectLabel_table> Notation +L+ o Parameter referring to an environment object +L+ L Set of object class names (e.g. “button”) +L+ V Vocabulary +L+ Features on W and object o +L+ Test if o is visible in s +L+ Test if o has input focus +L+ Test if o is in the foreground +L+ Test if o was previously interacted with +L+ Test if o came into existence since last action +L+ Min. edit distance between w E W and object labels in s +L+ Features on words in W, command c, and object o +L+ `dc' EC,wEV:test ifc'=cand wEW +L+ `dc' E C, l E L: test if c' = c and l is the class of o +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Example features in the Windows do- +L+ main. All features are binary, except for the nor- +L+ malized edit distance which is real-valued. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> form tasks and troubleshoot problems in the Win- +L+ dows operating systems. Examples of such tasks +L+ include installing patches and changing security +L+ settings. Figure 1 shows one such article. +L+ Our goal is to automatically execute these sup- +L+ port articles in the Windows 2000 environment. +L+ Here, the environment state is the set of visi- +L+ ble user interface (UI) objects, and object prop- +L+ erties such as label, location, and parent window. +L+ Possible commands include left-click, right-click, +L+ double-click, and type-into, all of which take a UI +L+ object as a parameter; type-into additionally re- +L+ quires a parameter for the input text. +L+ Table 1 lists some of the features we use for this +L+ domain. These features capture various aspects of +L+ the action under consideration, the current Win- +L+ dows UI state, and the input instructions. For ex- +L+ ample, one lexical feature measures the similar- +L+ ity of a word in the sentence to the UI labels of +L+ objects in the environment. Environment-specific +L+ features, such as whether an object is currently in +L+ focus, are useful when selecting the object to ma- +L+ nipulate. In total, there are 4,438 features. +L+ Reward Function Environment feedback can +L+ be used as a reward function in this domain. An +L+ obvious reward would be task completion (e.g., +L+ whether the stated computer problem was fixed). +L+ Unfortunately, verifying task completion is a chal- +L+ lenging system issue in its own right. +L+ Instead, we rely on a noisy method of check- +L+ ing whether execution can proceed from one sen- +L+ tence to the next: at least one word in each sen- +L+ tence has to correspond to an object in the envi- +L+ </SectLabel_bodyText> <SectLabel_page> 86 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 3: Crossblock puzzle with tutorial. For this +L+ level, four squares in a row or column must be re- +L+ moved at once. The first move specified by the +L+ tutorial is greyed in the puzzle. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> ronment.6 For instance, in the sentence from Fig- +L+ ure 2 the word “Run” matches the Run... menu +L+ item. If no words in a sentence match a current +L+ environment object, then one of the previous sen- +L+ tences was analyzed incorrectly. In this case, we +L+ assign the history a reward of -1. This reward is +L+ not guaranteed to penalize all incorrect histories, +L+ because there may be false positive matches be- +L+ tween the sentence and the environment. When +L+ at least one word matches, we assign a positive +L+ reward that linearly increases with the percentage +L+ of words assigned to non-null commands, and lin- +L+ early decreases with the number of output actions. +L+ This reward signal encourages analyses that inter- +L+ pret all of the words without producing spurious +L+ actions. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.2 Crossblock: A Puzzle Game +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our second application is to a puzzle game called +L+ Crossblock, available online as a Flash game.7 +L+ Each of 50 puzzles is played on a grid, where some +L+ grid positions are filled with squares. The object +L+ of the game is to clear the grid by drawing vertical +L+ or horizontal line segments that remove groups of +L+ squares. Each segment must exactly cross a spe- +L+ cific number of squares, ranging from two to seven +L+ depending on the puzzle. Humans players have +L+ found this game challenging and engaging enough +L+ to warrant posting textual tutorials.8 A sample +L+ puzzle and tutorial are shown in Figure 3. +L+ The environment is defined by the state of the +L+ grid. The only command is clear, which takes a +L+ parameter specifying the orientation (row or col- +L+ umn) and grid location of the line segment to be +L+ </SectLabel_bodyText> <SectLabel_footnote> 6We assume that a word maps to an environment object if +L+ the edit distance between the word and the object’s name is +L+ below a threshold value. +L+ 7hexaditidom.deviantart.com/art/Crossblock-108669149 +L+ 8www.jayisgames.com/archives/2009/01/crossblock.php +L+ </SectLabel_footnote> <SectLabel_bodyText> removed. The challenge in this domain is to seg- +L+ ment the text into the phrases describing each ac- +L+ tion, and then correctly identify the line segments +L+ from references such as “the bottom four from the +L+ second column from the left.” +L+ For this domain, we use two sets of binary fea- +L+ tures on state-action pairs (s, a). First, for each +L+ vocabulary word w, we define a feature that is one +L+ if w is the last word of a’s consumed words W'. +L+ These features help identify the proper text seg- +L+ mentation points between actions. Second, we in- +L+ troduce features for pairs of vocabulary word w +L+ and attributes of action a, e.g., the line orientation +L+ and grid locations of the squares that a would re- +L+ move. This set of features enables us to match +L+ words (e.g., “row”) with objects in the environ- +L+ ment (e.g., a move that removes a horizontal series +L+ of squares). In total, there are 8,094 features. +L+ Reward Function For Crossblock it is easy to +L+ directly verify task completion, which we use as +L+ the basis of our reward function. The reward r(h) +L+ is -1 if h ends in a state where the puzzle cannot +L+ be completed. For solved puzzles, the reward is +L+ a positive value proportional to the percentage of +L+ words assigned to non-null commands. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7 Experimental Setup +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Datasets For the Windows domain, our dataset +L+ consists of 128 documents, divided into 70 for +L+ training, 18 for development, and 40 for test. In +L+ the puzzle game domain, we use 50 tutorials, +L+ divided into 40 for training and 10 for test.9 +L+ Statistics for the datasets are shown below. +L+ </SectLabel_bodyText> <SectLabel_table> 	Windows	Puzzle +L+ Total # of documents	128	50 +L+ Total # of words	5562	994 +L+ Vocabulary size	610	46 +L+ Avg. words per sentence	9.93	19.88 +L+ Avg. sentences per document	4.38	1.00 +L+ Avg. actions per document	10.37	5.86 +L+ </SectLabel_table> <SectLabel_bodyText> The data exhibits certain qualities that make +L+ for a challenging learning problem. For instance, +L+ there are a surprising variety of linguistic con- +L+ structs — as Figure 4 shows, in the Windows do- +L+ main even a simple command is expressed in at +L+ least six different ways. +L+ </SectLabel_bodyText> <SectLabel_footnote> 9For Crossblock, because the number of puzzles is lim- +L+ ited, we did not hold out a separate development set, and re- +L+ port averaged results over five training/test splits. +L+ </SectLabel_footnote> <SectLabel_page> 87 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 4: Variations of “click internet options on +L+ the tools menu” present in the Windows corpus. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Experimental Framework To apply our algo- +L+ rithm to the Windows domain, we use the Win32 +L+ application programming interface to simulate hu- +L+ man interactions with the user interface, and to +L+ gather environment state information. The operat- +L+ ing system environment is hosted within a virtual +L+ machine,10 allowing us to rapidly save and reset +L+ system state snapshots. For the puzzle game do- +L+ main, we replicated the game with an implemen- +L+ tation that facilitates automatic play. +L+ As is commonly done in reinforcement learn- +L+ ing, we use a softmax temperature parameter to +L+ smooth the policy distribution (Sutton and Barto, +L+ 1998), set to 0.1 in our experiments. For Windows, +L+ the development set is used to select the best pa- +L+ rameters. For Crossblock, we choose the parame- +L+ ters that produce the highest reward during train- +L+ ing. During evaluation, we use these parameters +L+ to predict mappings for the test documents. +L+ Evaluation Metrics For evaluation, we com- +L+ pare the results to manually constructed sequences +L+ of actions. We measure the number of correct ac- +L+ tions, sentences, and documents. An action is cor- +L+ rect if it matches the annotations in terms of com- +L+ mand and parameters. A sentence is correct if all +L+ of its actions are correctly identified, and analo- +L+ gously for documents.11 Statistical significance is +L+ measured with the sign test. +L+ Additionally, we compute a word alignment +L+ score to investigate the extent to which the input +L+ text is used to construct correct analyses. This +L+ score measures the percentage of words that are +L+ aligned to the corresponding annotated actions in +L+ correctly analyzed documents. +L+ Baselines We consider the following baselines +L+ to characterize the performance of our approach. +L+ </SectLabel_bodyText> <SectLabel_footnote> 10VMware Workstation, available at www.vmware.com +L+ 11In these tasks, each action depends on the correct execu- +L+ tion of all previous actions, so a single error can render the +L+ remainder of that document’s mapping incorrect. In addition, +L+ due to variability in document lengths, overall action accu- +L+ racy is not guaranteed to be higher than document accuracy. +L+ </SectLabel_footnote> <SectLabel_listItem> •	Full Supervision Sequence prediction prob- +L+ </SectLabel_listItem> <SectLabel_bodyText> lems like ours are typically addressed us- +L+ ing supervised techniques. We measure how +L+ a standard supervised approach would per- +L+ form on this task by using a reward signal +L+ based on manual annotations of output ac- +L+ tion sequences, as defined in Section 5.2. As +L+ shown there, policy gradient with this re- +L+ ward is equivalent to stochastic gradient as- +L+ cent with a maximum likelihood objective. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Partial Supervision We consider the case +L+ </SectLabel_listItem> <SectLabel_bodyText> when only a subset of training documents is +L+ annotated, and environment reward is used +L+ for the remainder. Our method seamlessly +L+ combines these two kinds of rewards. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Random and Majority (Windows) We con- +L+ </SectLabel_listItem> <SectLabel_bodyText> sider two naive baselines. Both scan through +L+ each sentence from left to right. A com- +L+ mand c is executed on the object whose name +L+ is encountered first in the sentence. This +L+ command c is either selected randomly, or +L+ set to the majority command, which is left- +L+ click. This procedure is repeated until no +L+ more words match environment objects. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Random (Puzzle) We consider a baseline +L+ that randomly selects among the actions that +L+ are valid in the current game state.12 +L+ </SectLabel_listItem> <SectLabel_sectionHeader> 8 Results +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Table 2 presents evaluation results on the test sets. +L+ There are several indicators of the difficulty of this +L+ task. The random and majority baselines’ poor +L+ performance in both domains indicates that naive +L+ approaches are inadequate for these tasks. The +L+ performance of the fully supervised approach pro- +L+ vides further evidence that the task is challenging. +L+ This difficulty can be attributed in part to the large +L+ branching factor of possible actions at each step — +L+ on average, there are 27.14 choices per action in +L+ the Windows domain, and 9.78 in the Crossblock +L+ domain. +L+ In both domains, the learners relying only +L+ on environment reward perform well. Although +L+ the fully supervised approach performs the best, +L+ adding just a few annotated training examples +L+ to the environment-based learner significantly re- +L+ duces the performance gap. +L+ </SectLabel_bodyText> <SectLabel_footnote> 12 Since action selection is among objects, there is no natu- +L+ ral majority baseline for the puzzle. +L+ </SectLabel_footnote> <SectLabel_page> 88 +L+ </SectLabel_page> <SectLabel_table> 			Windows					Puzzle +L+ 	Action		Sent.		Doc.	Word	Action		Doc.		Word +L+ Random baseline	0.128		0.101		0.000	—–		0.081		0.111	—– +L+ Majority baseline	0.287		0.197		0.100	—–		—–		—–	—– +L+ Environment reward	* 0.647	*	0.590	*	0.375	0.819	*	0.428	*	0.453	0.686 +L+ Partial supervision	*0.723	*	0.702		0.475	0.989		0.575	*	0.523	0.850 +L+ Full supervision	*0.756		0.714		0.525	0.991		0.632		0.630	0.869 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Performance on the test set with different reward signals and baselines. Our evaluation measures +L+ the proportion of correct actions, sentences, and documents. We also report the percentage of correct +L+ word alignments for the successfully completed documents. Note the puzzle domain has only single- +L+ sentence documents, so its sentence and document scores are identical. The partial supervision line +L+ refers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle. Each +L+ result marked with * or o is a statistically significant improvement over the result immediately above it; +L+ * indicates p < 0.01 and o indicates p < 0.05. +L+ </SectLabel_tableCaption> <SectLabel_figureCaption> Figure 5: Comparison of two training scenarios where training is done using a subset of annotated +L+ documents, with and without environment reward for the remaining unannotated documents. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Figure 5 shows the overall tradeoff between an- +L+ notation effort and system performance for the two +L+ domains. The ability to make this tradeoff is one +L+ of the advantages of our approach. The figure also +L+ shows that augmenting annotated documents with +L+ additional environment-reward documents invari- +L+ ably improves performance. +L+ The word alignment results from Table 2 in- +L+ dicate that the learners are mapping the correct +L+ words to actions for documents that are success- +L+ fully completed. For example, the models that per- +L+ form best in the Windows domain achieve nearly +L+ perfect word alignment scores. +L+ To further assess the contribution of the instruc- +L+ tion text, we train a variant of our model without +L+ access to text features. This is possible in the game +L+ domain, where all of the puzzles share a single +L+ goal state that is independent of the instructions. +L+ This variant solves 34% of the puzzles, suggest- +L+ ing that access to the instructions significantly im- +L+ proves performance. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9 Conclusions +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we presented a reinforcement learn- +L+ ing approach for inducing a mapping between in- +L+ structions and actions. This approach is able to use +L+ environment-based rewards, such as task comple- +L+ tion, to learn to analyze text. We showed that hav- +L+ ing access to a suitable reward function can signif- +L+ icantly reduce the need for annotations. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgments +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The authors acknowledge the support of the NSF +L+ (CAREER grant IIS-0448168, grant IIS-0835445, +L+ grant IIS-0835652, and a Graduate Research Fel- +L+ lowship) and the ONR. Thanks to Michael Collins, +L+ Amir Globerson, Tommi Jaakkola, Leslie Pack +L+ Kaelbling, Dina Katabi, Martin Rinard, and mem- +L+ bers of the MIT NLP group for their suggestions +L+ and comments. Any opinions, findings, conclu- +L+ sions, or recommendations expressed in this paper +L+ are those of the authors, and do not necessarily re- +L+ flect the views of the funding organizations. +L+ </SectLabel_bodyText> <SectLabel_page> 89 +L+ </SectLabel_page> <SectLabel_bodyText> Jeffrey Mark Siskind. 2001. Grounding the lexical se- +L+ mantics of verbs in visual perception using force dy- +L+ namics and event logic. J. Artif. Intell. Res. (JAIR), +L+ 15:31–90. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> Kobus Barnard and David A. Forsyth. 2001. Learning +L+ the semantics of words and pictures. In Proceedings +L+ of ICCV. +L+ David L. Chen and Raymond J. Mooney. 2008. Learn- +L+ ing to sportscast: a test of grounded language acqui- +L+ sition. In Proceedings of ICML. +L+ Stephen Della Pietra, Vincent J. Della Pietra, and +L+ John D. Lafferty. 1997. Inducing features of ran- +L+ dom fields. IEEE Trans. Pattern Anal. Mach. Intell., +L+ 19(4):380–393. +L+ Barbara Di Eugenio. 1992. Understanding natural lan- +L+ guage instructions: the case of purpose clauses. In +L+ Proceedings of ACL. +L+ Michael Fleischman and Deb Roy. 2005. Intentional +L+ context in situated language learning. In Proceed- +L+ ings of CoNLL. +L+ John Lafferty, Andrew McCallum, and Fernando +L+ Pereira. 2001. Conditional random fields: Prob- +L+ abilistic models for segmenting and labeling se- +L+ quence data. In Proceedings of ICML. +L+ Diane J. Litman, Michael S. Kearns, Satinder Singh, +L+ and Marilyn A. Walker. 2000. Automatic optimiza- +L+ tion of dialogue management. In Proceedings of +L+ COLING. +L+ Raymond J. Mooney. 2008a. Learning language +L+ from its perceptual context. In Proceedings of +L+ ECML/PKDD. +L+ Raymond J. Mooney. 2008b. Learning to connect lan- +L+ guage and perception. In Proceedings ofAAAI. +L+ Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, and +L+ Shankar Sastry. 2003. Autonomous helicopter flight +L+ via reinforcement learning. In Advances in NIPS. +L+ James Timothy Oates. 2001. Grounding knowledge +L+ in sensors: Unsupervised learning for language and +L+ planning. Ph.D. thesis, University of Massachusetts +L+ Amherst. +L+ Deb K. Roy and Alex P. Pentland. 2002. Learn- +L+ ing words from sights and sounds: a computational +L+ model. Cognitive Science 26, pages 113–146. +L+ Nicholas Roy, Joelle Pineau, and Sebastian Thrun. +L+ 2000. Spoken dialogue management using proba- +L+ bilistic reasoning. In Proceedings of ACL. +L+ Konrad Scheffler and Steve Young. 2002. Automatic +L+ learning of dialogue strategy using dialogue simula- +L+ tion and reinforcement learning. In Proceedings of +L+ HLT. +L+ Satinder P. Singh, Michael J. Kearns, Diane J. Litman, +L+ and Marilyn A. Walker. 1999. Reinforcement learn- +L+ ing for spoken dialogue systems. In Advances in +L+ NIPS. +L+ Richard S. Sutton and Andrew G. Barto. 1998. Re- +L+ inforcement Learning: An Introduction. The MIT +L+ Press. +L+ Richard S. Sutton, David McAllester, Satinder Singh, +L+ and Yishay Mansour. 2000. Policy gradient meth- +L+ ods for reinforcement learning with function approx- +L+ imation. In Advances in NIPS. +L+ Terry Winograd. 1972. Understanding Natural Lan- +L+ guage. Academic Press. +L+ Chen Yu and Dana H. Ballard. 2004. On the integra- +L+ tion of grounding language and learning objects. In +L+ Proceedings ofAAAI. +L+ </SectLabel_reference> <SectLabel_page> 90 +L+ </SectLabel_page>
<SectLabel_title> 2-Source Dispersers for Sub-Polynomial Entropy and +L+ Ramsey Graphs Beating the Frankl-Wilson Construction +L+ </SectLabel_title> <SectLabel_none> ∗ +L+ </SectLabel_none> <SectLabel_author> Boaz Barak +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ Princeton University +L+ </SectLabel_affiliation> <SectLabel_email> boaz@cs.princeton.edu +L+ </SectLabel_email> <SectLabel_author> Ronen Shaltiel ‡ +L+ </SectLabel_author> <SectLabel_affiliation> University of Haifa +L+ </SectLabel_affiliation> <SectLabel_address> Mount Carmel +L+ Haifa, Israel +L+ </SectLabel_address> <SectLabel_email> ronen@cs.haifa.ac.il +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The main result of this paper is an explicit disperser for two +L+ independent sources on n bits, each of entropy k = no(1). +L+ Put differently, setting N = 2n and K = 2k, we construct +L+ explicit N × N Boolean matrices for which no K × K sub- +L+ matrix is monochromatic. Viewed as adjacency matrices of +L+ bipartite graphs, this gives an explicit construction of K- +L+ Ramsey bipartite graphs of size N. +L+ This greatly improves the previous bound of k = o(n) of +L+ Barak, Kindler, Shaltiel, Sudakov and Wigderson [4]. It also +L+ significantly improves the 25-year record of k = ~O(√n) on +L+ the special case of Ramsey graphs, due to Frankl and Wilson +L+ [9]. +L+ The construction uses (besides ”classical” extractor ideas) +L+ almost all of the machinery developed in the last couple of +L+ years for extraction from independent sources, including: +L+ •	Bourgain’s extractor for 2 independent sources of some +L+ entropy rate < 1/2 [5] +L+ •	Raz’s extractor for 2 independent sources, one of which +L+ has any entropy rate > 1/2 [18] +L+ </SectLabel_bodyText> <SectLabel_footnote> ∗Supported by a Princeton University startup grant. +L+ †Most of this work was done while the author was visiting +L+ Princeton University and the Institute for Advanced Study. +L+ Supported in part by an MCD fellowship from UT Austin +L+ and NSF Grant CCR-0310960. +L+ ‡This research was supported by the United States-Israel +L+ Binational Science Foundation (BSF) grant 2004329. +L+ §This research was supported by NSF Grant CCR-0324906. +L+ </SectLabel_footnote> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> STOC’06, May 21–23, 2006, Seattle, Washington, USA. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2006 ACM 1-59593-134-1/06/0005 ...$5.00. +L+ </SectLabel_copyright> <SectLabel_author> Anup Rao † +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ University of Texas at Austin +L+ </SectLabel_affiliation> <SectLabel_email> arao@cs.utexas.edu +L+ </SectLabel_email> <SectLabel_author> Avi Wigderson § +L+ </SectLabel_author> <SectLabel_affiliation> Institute for Advanced Study +L+ </SectLabel_affiliation> <SectLabel_address> Princeton +L+ New Jersey +L+ </SectLabel_address> <SectLabel_email> avi@math.ias.edu +L+ </SectLabel_email> <SectLabel_bodyText> •	Rao’s extractor for 2 independent block-sources of en- +L+ tropy no(1) [17] +L+ •	The “Challenge-Response” mechanism for detecting +L+ “entropy concentration” of [4]. +L+ The main novelty comes in a bootstrap procedure which +L+ allows the Challenge-Response mechanism of [4] to be used +L+ with sources of less and less entropy, using recursive calls +L+ to itself. Subtleties arise since the success of this mecha- +L+ nism depends on restricting the given sources, and so re- +L+ cursion constantly changes the original sources. These are +L+ resolved via a new construct, in between a disperser and +L+ an extractor, which behaves like an extractor on sufficiently +L+ large subsources of the given ones. +L+ </SectLabel_bodyText> <SectLabel_note> This version is only an extended abstract, please see the +L+ full version, available on the authors’ homepages, for more +L+ details. +L+ </SectLabel_note> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> G.2.2 [Mathematics of Computing]: Discrete Mathe- +L+ matics—Graph algorithms +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Theory, Algorithms +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Dispersers, Ramsey Graphs, Independent Sources, Extrac- +L+ tors +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper deals with randomness extraction from weak +L+ random sources. Here a weak random source is a distribu- +L+ tion which contains some entropy. The extraction task is to +L+ design efficient algorithms (called extractors) to convert this +L+ entropy into useful form, namely a sequence of independent +L+ unbiased bits. Beyond the obvious motivations (potential +L+ use of physical sources in pseudorandom generators and in +L+ derandomization), extractors have found applications in a +L+ </SectLabel_bodyText> <SectLabel_page> 671 +L+ </SectLabel_page> <SectLabel_bodyText> variety of areas in theoretical computer science where ran- +L+ domness does not seem an issue, such as in efficient con- +L+ structions of communication networks [24, 7], error correct- +L+ ing codes [22, 12], data structures [14] and more. +L+ Most work in this subject over the last 20 years has fo- +L+ cused on what is now called seeded extraction, in which the +L+ extractor is given as input not only the (sample from the) +L+ defective random source, but also a few truly random bits +L+ (called the seed). A comprehensive survey of much of this +L+ body of work is [21]. +L+ Another direction, which has been mostly dormant till +L+ about two years ago, is (seedless, deterministic) extraction +L+ from a few independent weak sources. This kind of extrac- +L+ tion is important in several applications where it is unrealis- +L+ tic to have a short random seed or deterministically enumer- +L+ ate over its possible values. However, it is easily shown to be +L+ impossible when only one weak source is available. When at +L+ least 2 independent sources are available extraction becomes +L+ possible in principle. The 2-source case is the one we will +L+ focus on in this work. +L+ The rest of the introduction is structured as follows. We’ll +L+ start by describing our main result in the context of Ramsey +L+ graphs. We then move to the context of extractors and dis- +L+ perser, describing the relevant background and stating our +L+ result in this language. Then we give an overview of the +L+ construction of our dispersers, describing the main building +L+ blocks we construct along the way. As the construction is +L+ quite complex and its analysis quite subtle, in this proceed- +L+ ings version we try to abstract away many of the technical +L+ difficulties so that the main ideas, structure and tools used +L+ are highlighted. For that reason we also often state defini- +L+ tions and theorems somewhat informally. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 1.1 Ramsey Graphs +L+ </SectLabel_subsectionHeader> <SectLabel_construct> DefInItIOn 1.1. A graph on N vertices is called a K- +L+ Ramsey Graph if it contains no clique or independent set of +L+ size K. +L+ </SectLabel_construct> <SectLabel_bodyText> In 1947 Erd}os published his paper inaugurating the Prob- +L+ abilistic Method with a few examples, including a proof that +L+ most graphs on N = 2n vertices are 2n-Ramsey. The quest +L+ for constructing such graphs explicitly has existed ever since +L+ and lead to some beautiful mathematics. +L+ The best record to date was obtained in 1981 by Frankl +L+ and Wilson [9], who used intersection theorems for set sys- +L+ tems to construct N-vertex graphs which are 21�n log n-Ramsey. +L+ This bound was matched by Alon [1] using the Polynomial +L+ Method, by Grolmusz [11] using low rank matrices over rings, +L+ and also by Barak [2] boosting Abbot’s method with almost +L+ k-wise independent random variables (a construction that +L+ was independently discovered by others as well). Remark- +L+ ably all of these different approaches got stuck at essentially +L+ the same bound. In recent work, Gopalan [10] showed that +L+ other than the last construction, all of these can be viewed +L+ as coming from low-degree symmetric representations of the +L+ OR function. He also shows that any such symmetric rep- +L+ resentation cannot be used to give a better Ramsey graph, +L+ which gives a good indication of why these constructions +L+ had similar performance. Indeed, as we will discuss in a +L+ later section, the √n entropy bound initially looked like a +L+ natural obstacle even for our techniques, though eventually +L+ we were able to surpass it. +L+ The analogous question for bipartite graphs seemed much +L+ harder. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1.2. A bipartite graph on two sets of N ver- +L+ tices is a K-Ramsey Bipartite Graph if it has no K × K +L+ complete or empty bipartite subgraph. +L+ </SectLabel_construct> <SectLabel_bodyText> While Erd}os’ result on the abundance of 2n-Ramsey graphs +L+ holds as is for bipartite graphs, until recently the best ex- +L+ plicit construction of bipartite Ramsey graphs was 2n/2- +L+ Ramsey, using the Hadamard matrix. This was improved +L+ last year, first to o(2n/2) by Pudlak and R}odl [16] and then +L+ to 2o(n) by Barak, Kindler, Shaltiel, Sudakov and Wigderson +L+ [4] . +L+ It is convenient to view such graphs as functions f : +L+ ({0, 1}n)2 → {0, 1}. This then gives exactly the definition +L+ of a disperser. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1.3. A function f : ({0, 1}n)2 → {0, 1} is +L+ called a 2-source disperser for entropy k if for any two sets +L+ X, Y ⊂ {0, 1}n with | X | = |Y| = 2k, we have that the image +L+ f (X, Y) is {0, 1}. +L+ </SectLabel_construct> <SectLabel_bodyText> This allows for a more formal definition of explicitness: we +L+ simply demand that the function f is computable in polyno- +L+ mial time. Most of the constructions mentioned above are +L+ explicit in this sense.' +L+ Our main result (stated informally) significantly improves +L+ the bounds in both the bipartite and non-bipartite settings: +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 1.4. For every N we construct polynomial time +L+ computable bipartite graphs which are 2n'(1)-Ramsey. A stan- +L+ dard transformation of these graphs also yields polynomial +L+ time computable ordinary Ramsey Graphs with the same pa- +L+ rameters. +L+ </SectLabel_construct> <SectLabel_subsectionHeader> 1.2 Extractors and Dispersers from indepen- +L+ dent sources +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Now we give a brief review of past relevant work (with the +L+ goal of putting this paper in proper context) and describe +L+ some of the tools from these past works that we will use. +L+ We start with the basic definitions of k-sources by Nisan +L+ and Zuckerman [15] and of extractors and dispersers for in- +L+ dependent sources by Santha and Vazirani [20]. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1.5 ([15], See alSO [8]). The min-entropy +L+ of a distribution X is the maximum k such that for every +L+ element x in its support, Pr[X = x] ≤ 2-k. If X is a dis- +L+ tribution on strings with min-entropy at least k, we will call +L+ X a k-source 2. +L+ </SectLabel_construct> <SectLabel_bodyText> To simplify the presentation, in this version of the paper +L+ we will assume that we are working with entropy as opposed +L+ to min-entropy. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1.6 ([20]). A function f : ({0,1}n)c → +L+ {0, 1}m is a c-source (k, ǫ) extractor if for every family of c +L+ independent k-sources X', • • • , Xc, the output f (X', • • • , Xc) +L+ </SectLabel_construct> <SectLabel_footnote> 'The Abbot’s product based Ramsey-graph construction of +L+ [3] and the bipartite Ramsey construction of [16] only satisfy +L+ a weaker notion of explicitness. +L+ 2It is no loss of generality to imagine that X is uniformly +L+ distributed over some (unknown) set of size 2k. +L+ </SectLabel_footnote> <SectLabel_page> 672 +L+ </SectLabel_page> <SectLabel_bodyText> is a ǫ-close 3 to uniformly distributed on m bits. f is a dis- +L+ perser for the same parameters if the output is simply re- +L+ quired to have a support of relative size (1 − ǫ). +L+ To simplify the presentation, in this version of the paper, +L+ we will assume that ǫ = 0 for all of our constructions. +L+ In this language, Erd}os’ theorem says that most functions +L+ f : ({0, 1}n)2 → {0, 1} are dispersers for entropy 1 + logn +L+ (treating f as the characteristic function for the set of edges +L+ of the graph). The proof easily extends to show that indeed +L+ most such functions are in fact extractors. This naturally +L+ challenges us to find explicit functions f that are 2-source +L+ extractors. +L+ Until one year ago, essentially the only known explicit +L+ construction was the Hadamard extractor Had defined by +L+ Had(x,y) +L+ k > n/2 as observed by Chor and Goldreich [8] and can +L+ be extended to give m = Q(n) output bits as observed by +L+ Vazirani [23]. Over 20 years later, a recent breakthrough +L+ of Bourgain [5] broke this “1/2 barrier” and can handle 2 +L+ sources of entropy .4999n, again with linear output length +L+ m = 0(n). This seemingly minor improvement will be cru- +L+ cial for our work! +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 1.7 ([5] ). There is a polynomial time com- +L+ putable 2-source extractor f : ({0, 1}n)2 → {0, 1}m for en- +L+ tropy .4999n and m = 0(n). +L+ </SectLabel_construct> <SectLabel_bodyText> No better bounds are known for 2-source extractors. Now +L+ we turn our attention to 2-source dispersers. It turned out +L+ that progress for building good 2-source dispersers came via +L+ progress on extractors for more than 2 sources, all happening +L+ in fast pace in the last 2 years. The seminal paper of Bour- +L+ gain, Katz and Tao [6] proved the so-called ”sum-product +L+ theorem” in prime fields, a result in arithmetic combina- +L+ torics. This result has already found applications in diverse +L+ areas of mathematics, including analysis, number theory, +L+ group theory and ... extractor theory. Their work implic- +L+ itly contained dispersers for c = O(log(n/k)) independent +L+ sources of entropy k (with output m = Q(k)). The use of +L+ the ”sum-product” theorem was then extended by Barak et +L+ al. [3] to give extractors with similar parameters. Note that +L+ for linear entropy k = 0(n), the number of sources needed +L+ for extraction c is a constant! +L+ Relaxing the independence assumptions via the idea of +L+ repeated condensing, allowed the reduction of the number +L+ of independent sources to c = 3, for extraction from sources +L+ of any linear entropy k = 0(n), by Barak et al. [4] and +L+ independently by Raz [18]. +L+ For 2 sources Barak et al. [4] were able to construct dis- +L+ persers for sources of entropy o(n). To do this, they first +L+ showed that if the sources have extra structure (block-source +L+ structure, defined below), even extraction is possible from 2 +L+ sources. The notion of block-sources, capturing ”semi inde- +L+ pendence” of parts of the source, was introduced by Chor +L+ and Goldreich [8]. It has been fundamental in the develop- +L+ ment of seeded extractors and as we shall see, is essential +L+ for us as well. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1.8 ([8] ). A distribution X = X1, ... , Xc +L+ is a c-block-source of (block) entropy k if every block Xi +L+ has entropy k even conditioned on fixing the previous blocks +L+ X1, • • • , Xi_1 to arbitrary constants. +L+ </SectLabel_construct> <SectLabel_footnote> 3The error is usually measured in terms of ℓ1 distance or +L+ variation distance. +L+ </SectLabel_footnote> <SectLabel_bodyText> This definition allowed Barak et al. [4] to show that their +L+ extractor for 4 independent sources, actually performs as +L+ well with only 2 independent sources, as long as both are +L+ 2-block-sources. +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 1.9 ([4] ). There exists a polynomial time com- +L+ putable extractor f : ({0, 1}n)2 → {0, 1} for 2 independent +L+ 2-block-sources with entropy o(n). +L+ </SectLabel_construct> <SectLabel_bodyText> There is no reason to assume that the given sources are +L+ block-sources, but it is natural to try and reduce to this +L+ case. This approach has been one of the most successful in +L+ the extractor literature. Namely try to partition a source +L+ X into two blocks X = X1, X2 such that X1, X2 form a +L+ 2-block-source. Barak et al. introduced a new technique to +L+ do this reduction called the Challenge-Response mechanism, +L+ which is crucial for this paper. This method gives a way to +L+ “find” how entropy is distributed in a source X, guiding the +L+ choice of such a partition. This method succeeds only with +L+ small probability, dashing the hope for an extractor, but still +L+ yielding a disperser. +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 1.10 ([4] ). There exists a polynomial time +L+ computable 2-source disperser f : ({0, 1}n)2 → {0, 1} for +L+ entropy o(n). +L+ </SectLabel_construct> <SectLabel_bodyText> Reducing the entropy requirement of the above 2-source +L+ disperser, which is what we achieve in this paper, again +L+ needed progress on achieving a similar reduction for extrac- +L+ tors with more independent sources. A few months ago Rao +L+ [?] was able to significantly improve all the above results +L+ for c ≥ 3 sources. Interestingly, his techniques do not use +L+ arithmetic combinatorics, which seemed essential to all the +L+ papers above. He improves the results of Barak et al. [3] to +L+ give c = O((logn)/(logk))-source extractors for entropy k. +L+ Note that now the number c of sources needed for extraction +L+ is constant, even when the entropy is as low as nδ for any +L+ constant δ! +L+ Again, when the input sources are block-sources with suf- +L+ ficiently many blocks, Rao proves that 2 independent sources +L+ suffice (though this result does rely on arithmetic combina- +L+ torics, in particular, on Bourgain’s extractor). +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 1.11 ([?] ). There is a polynomial time com- +L+ putable extractor f : ({0, 1}n)2 → {0, 1}m for 2 independent +L+ c-block-sources with block entropy k and m = 0(k), as long +L+ as c = O((log n)/(log k)). +L+ </SectLabel_construct> <SectLabel_bodyText> In this paper (see Theorem 2.7 below) we improve this +L+ result to hold even when only one of the 2 sources is a c- +L+ block-source. The other source can be an arbitrary source +L+ with sufficient entropy. This is a central building block in +L+ our construction. This extractor, like Rao’s above, critically +L+ uses Bourgain’s extractor mentioned above. In addition it +L+ uses a theorem of Raz [18] allowing seeded extractors to have +L+ ”weak” seeds, namely instead of being completely random +L+ they work as long as the seed has entropy rate > 1/2. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. MAIN NOTIONS AND NEW RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The main result of this paper is a polynomial time com- +L+ putable disperser for 2 sources of entropy no(1), significantly +L+ improving both the results of Barak et al. [4] (o(n) entropy). +L+ It also improves on Frankl and Wilson [9], who only built +L+ Ramsey Graphs and only for entropy ~O(√n). +L+ = (x, y)( mod 2). It is an extractor for entropy +L+ </SectLabel_bodyText> <SectLabel_page> 673 +L+ </SectLabel_page> <SectLabel_construct> ThEOREm 2.1 (MaIn thEOREm, REStatEd). There ex- +L+ ists a polynomial time computable 2-source disperser D : +L+ ({0, 1}n)2 → {0, 1} for entropy no(1). +L+ </SectLabel_construct> <SectLabel_bodyText> The construction of this disperser will involve the con- +L+ struction of an object which in some sense is stronger and +L+ in another weaker than a disperser: a subsource somewhere +L+ extractor. We first define a related object: a somewhere ex- +L+ tractor, which is a function producing several outputs, one of +L+ which must be uniform. Again we will ignore many technical +L+ issues such as error, min-entropy vs. entropy and more, in +L+ definitions and results, which are deferred to the full version +L+ of this paper. +L+ </SectLabel_bodyText> <SectLabel_construct> DEfInItIOn 2.2. A function f : ({0, 1}n)2 → ({0,1}m)ℓ +L+ is a 2-source somewhere extractor with ℓ outputs, for entropy +L+ k, if for every 2 independent k-sources X, Y there exists an +L+ i ∈ [ℓ] such the ith output f (X, Y)i is a uniformly distributed +L+ string of m bits. +L+ </SectLabel_construct> <SectLabel_bodyText> Here is a simple construction of such a somewhere extrac- +L+ tor with ℓ as large as poly(n) (and the p in its name will +L+ stress the fact that indeed the number of outputs is that +L+ large). It will nevertheless be useful to us (though its de- +L+ scription in the next sentence may be safely skipped). Define +L+ pSE(x, y)i = V(E(x, i), E(y, i)) where E is a ”strong” loga- +L+ rithmic seed extractor, and V is the Hadamard/Vazirani 2- +L+ source extractor. Using this construction, it is easy to see +L+ that: +L+ </SectLabel_bodyText> <SectLabel_construct> PROPOSItIOn 2.3. For every n, k there is a polynomial +L+ time computable somewhere extractor pSE : ({0, 1}n)2 → +L+ ({0, 1}m)ℓ with ℓ = poly(n) outputs, for entropy k, and m = +L+ Q(k). +L+ </SectLabel_construct> <SectLabel_bodyText> Before we define subsource somewhere extractor, we must +L+ first define a subsource. +L+ </SectLabel_bodyText> <SectLabel_construct> DEfInItIOn 2.4 (SUBSOURCES). Given random variables +L+ Z and Z^ on {0, 1}n we say that Z^ is a deficiency d subsource +L+ of Z and write Z^ ⊆ Z if there exists a set A ⊆ {0,1}n such +L+ that (Z|Z ∈ A) = Z^ and Pr[Z ∈ A] ≥ 2-d. +L+ </SectLabel_construct> <SectLabel_bodyText> A subsource somewhere extractor guarantees the ”some- +L+ where extractor” property only on subsources X', Y' of the +L+ original input distributions X, Y (respectively). It will be +L+ extremely important for us to make these subsources as large +L+ as possible (i.e. we have to lose as little entropy as possible). +L+ Controlling these entropy deficiencies is a major technical +L+ complication we have to deal with. However we will be in- +L+ formal with it here, mentioning it only qualitatively when +L+ needed. We discuss this issue a little more in Section 6. +L+ </SectLabel_bodyText> <SectLabel_construct> DEfInItIOn 2.5. A function f : ({0, 1}n)2 → ({0,1}m)ℓ +L+ is a 2-source subsource somewhere extractor with ℓ outputs +L+ for entropy k, if for every 2 independent k-sources X, Y there +L+ exists a subsource X^ of X, a subsource Y^ of Y and an i ∈ [ℓ] +L+ such the ith output f (^X, Y^)i is a uniformly distributed string +L+ of m bits. +L+ </SectLabel_construct> <SectLabel_bodyText> A central technical result for us is that with this ”sub- +L+ source” relaxation, we can have much fewer outputs – in- +L+ deed we’ll replace poly(n) outputs in our first construction +L+ above with no(1) outputs. +L+ </SectLabel_bodyText> <SectLabel_construct> ThEOREm 2.6 (SUBSOURCE SOmEWhERE ExtRaCtOR). +L+ For every δ > 0 there is a polynomial time computable sub- +L+ source somewhere extractor SSE : ({0, 1}n)2 → ({0, 1}m)ℓ +L+ with ℓ = no(1) outputs, for entropy k = nδ, with output +L+ m=√k. +L+ </SectLabel_construct> <SectLabel_bodyText> We will describe the ideas used for constructing this im- +L+ portant object and analyzing it in the next section, where +L+ we will also indicate how it is used in the construction of +L+ the final disperser. Here we state a central building block, +L+ mentioned in the previous section (as an improvement of the +L+ work of Rao [?]). We construct an extractor for 2 indepen- +L+ dent sources one of which is a block-sources with sufficient +L+ number of blocks. +L+ </SectLabel_bodyText> <SectLabel_construct> ThEOREm 2.7 (BlOCK SOURCE ExtRaCtOR). There is +L+ a polynomial time computable extractor B : ({0, 1}n)2 → +L+ {0,1}m for 2 independent sources, one of which is a c-block- +L+ sources with block entropy k and the other a source of en- +L+ tropy k, with m = 0(k), and c = O((log n)/(log k)). +L+ </SectLabel_construct> <SectLabel_bodyText> A simple corollary of this block-source extractor B, is the +L+ following weaker (though useful) somewhere block-source +L+ extractor SB. A source Z = Z1, Z2, • • • , Zt is a somewhere +L+ c-block-source of block entropy k if for some c indices i1 < +L+ i2 < • • • < ic the source Zi1, Zi2, • • • , Zic is a c-block-source. +L+ Collecting the outputs of B on every c-subset of blocks re- +L+ sults in that somewhere extractor. +L+ </SectLabel_bodyText> <SectLabel_construct> COROllaRY 2.8. There is a polynomial time computable +L+ somewhere extractorSB : ({0, 1}n)2 → ({0, 1}m)ℓ for2 inde- +L+ pendent sources, one of which is a somewhere c-block-sources +L+ with block entropy k and t blocks total and the other a source +L+ of entropy k, with m = 0(k), c = O((log n)/(log k)), and +L+ ℓ ≤ tc. +L+ </SectLabel_construct> <SectLabel_bodyText> In both the theorem and corollary above, the values of +L+ entropy k we will be interested in are k = no(1). It follows +L+ that a block-source with a constant c = O(1) suffices. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. THE CHALLENGE-RESPONSE MECH- +L+ ANISM +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We now describe abstractly a mechanism which will be +L+ used in the construction of the disperser as well as the sub- +L+ source somewhere extractor. Intuitively, this mechanism al- +L+ lows us to identify parts of a source which contain large +L+ amounts of entropy. One can hope that using such a mech- +L+ anism one can partition a given source into blocks in a way +L+ which make it a block-source, or alternatively focus on a part +L+ of the source which is unusually condensed with entropy - +L+ two cases which may simplify the extraction problem. +L+ The reader may decide, now or in the middle of this +L+ section, to skip ahead to the next section which describes +L+ the construction of the subsource somewhere extractor SSE, +L+ which extensively uses this mechanism. Then this section +L+ may seem less abstract, as it will be clearer where this mech- +L+ anism is used. +L+ This mechanism was introduced by Barak et al. [4], and +L+ was essential in their 2-source disperser. Its use in this paper +L+ is far more involved (in particular it calls itself recursively, +L+ a fact which creates many subtleties). However, at a high +L+ level, the basic idea behind the mechanism is the same: +L+ Let Z be a source and Z' a part of Z (Z projected on a +L+ subset of the coordinates). We know that Z has entropy k, +L+ </SectLabel_bodyText> <SectLabel_page> 674 +L+ </SectLabel_page> <SectLabel_bodyText> and want to distinguish two possibilities: Z′ has no entropy +L+ (it is fixed) or it has at least k′ entropy. Z′ will get a pass +L+ or fail grade, hopefully corresponding to the cases of high or +L+ no entropy in Z′. +L+ Anticipating the use of this mechanism, it is a good idea +L+ to think of Z as a ”parent” of Z′, which wants to check if +L+ this ”child” has sufficient entropy. Moreover, in the context +L+ of the initial 2 sources X, Y we will operate on, think of Z +L+ as a part of X, and thus that Y is independent of Z and Z′. +L+ To execute this ”test” we will compute two sets of strings +L+ (all of length m, say): the Challenge C = C(Z′,Y) and +L+ the Response R = R(Z, Y). Z′ fails if C C R and passes +L+ otherwise. +L+ The key to the usefulness of this mechanism is the follow- +L+ ing lemma, which states that what ”should” happen, indeed +L+ happens after some restriction of the 2 sources Z and Y. +L+ We state it and then explain how the functions C and R are +L+ defined to accommodate its proof. +L+ </SectLabel_bodyText> <SectLabel_construct> Lemma 3.1. Assume Z, Y are sources of entropy k. +L+ </SectLabel_construct> <SectLabel_listItem> 1. If Z′ has entropy k′+O(m), then there are subsources +L+ Z^ of Z and Y^ of Y, such that +L+ Pr[^Z′ passes] = Pr[C(^Z′, Y^) C R(^Z, Y^)] > 1—nO(1)2−m +L+ 2. If Z′ is fixed (namely, has zero entropy), then for some +L+ subsources Z^ of Z and Y^ of Y, we have +L+ Pr[Z′ fails] = Pr[C(^Z′, Y^) C R(^Z, Y^)] = 1 +L+ </SectLabel_listItem> <SectLabel_bodyText> Once we have such a mechanism, we will design our dis- +L+ perser algorithm assuming that the challenge response mech- +L+ anism correctly identifies parts of the source with high or +L+ low levels of entropy. Then in the analysis, we will ensure +L+ that our algorithm succeeds in making the right decisions, +L+ at least on subsources of the original input sources. +L+ Now let us explain how to compute the sets C and R. We +L+ will use some of the constructs above with parameters which +L+ don’t quite fit. +L+ The response set R(Z, Y) = pSE(Z, Y) is chosen to be the +L+ output of the somewhere extractor of Proposition 2.3. The +L+ challenge set C(Z′, Y) = SSE(Z′, Y) is chosen to be the out- +L+ put of the subsource somewhere extractor of Theorem 2.6. +L+ Why does it work? We explain each of the two claims +L+ in the lemma in turn (and after each comment on the im- +L+ portant parameters and how they differ from Barak et al. +L+ [4]). +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Z′ has entropy. We need to show that Z′ passes the +L+ test with high probability. We will point to the out- +L+ put string in C(^Z′, Y^′) which avoids R(^Z, Y^) with high +L+ probability as follows. In the analysis we will use the +L+ union bound on several events, one associated with +L+ each (poly(n) many) string in pSE(^Z, Y^). We note +L+ that by the definition of the response function, if we +L+ want to fix a particular element in the response set to +L+ a particular value, we can do this by fixing E(Z, i) and +L+ E(Y, i). This fixing keeps the restricted sources inde- +L+ pendent and loses only O(m) entropy. In the subsource +L+ of Z′ guaranteed to exist by Theorem 2.6 we can afford +L+ to lose this entropy in Z′. Thus we conclude that one +L+ of its outputs is uniform. The probability that this +L+ output will equal any fixed value is thus 2−m, com- +L+ pleting the argument. We note that we can handle +L+ the polynomial output size of pSE, since the uniform +L+ string has length m = no(1) (something which could +L+ not be done with the technology available to Barak et +L+ al. [4]). +L+ 2. Z′ has no entropy. We now need to guarantee that +L+ in the chosen subsources (which we choose) ^Z, Y^, all +L+ strings in C = C(^Z′, Y^) are in R(^Z, Y^). First notice +L+ that as Z′ is fixed, C is only a function of Y. We +L+ set Y~ to be the subsource of Y that fixes all strings +L+ in C = C(Y) to their most popular values (losing +L+ only ℓm entropy from Y). We take care of includ- +L+ ing these fixed strings in R(Z, Y~) one at a time, by +L+ restricting to subsources assuring that. Let σ be any +L+ m-bit string we want to appear in R(Z, Y~). Recall that +L+ R(z, y) = V(E(z, i), E(y, i)). We pick a ”good” seed i, +L+ and restrict Z, Y~ to subsources with only O(m) less +L+ entropy by fixing E(Z, i) = a and E(Y~, i) = b to values +L+ (a, b) for which V(a, b) = σ. This is repeated suc- +L+ cessively ℓ times, and results in the final subsources +L+ ^Z, Y^ on which ^Z′ fails with probability 1. Note that +L+ we keep reducing the entropy of our sources ℓ times, +L+ which necessitates that this ℓ be tiny (here we could +L+ not tolerate poly(n), and indeed can guarantee no(1), +L+ at least on a subsource – this is one aspect of how cru- +L+ cial the subsource somewhere extractor SSE is to the +L+ construction. +L+ </SectLabel_listItem> <SectLabel_bodyText> We note that initially it seemed like the Challenge-Response +L+ mechanism as used in [4] could not be used to handle en- +L+ tropy that is significantly less than -,/n (which is approxi- +L+ mately the bound that many of the previous constructions +L+ got stuck at). The techniques of [4] involved partitioning +L+ the sources into t pieces of length n/t each, with the hope +L+ that one of those parts would have a significant amount of +L+ entropy, yet there’d be enough entropy left over in the rest +L+ of the source (so that the source can be partitioned into a +L+ block source). +L+ However it is not clear how to do this when the total +L+ entropy is less than -,/n. On the one hand we will have +L+ to partition our sources into blocks of length significantly +L+ more than -,/n (or the adversary could distribute a negligible +L+ fraction of entropy in all blocks). On the other hand, if +L+ our blocks are so large, a single block could contain all the +L+ entropy. Thus it was not clear how to use the challenge +L+ response mechanism to find a block source. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. THE SUBSOURCE SOMEWHERE +L+ EXTRACTOR SSE +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We now explain some of the ideas behind the construction +L+ of the subsource somewhere extractor SSE of Theorem 2.6. +L+ Consider the source X. We are seeking to find in it a some- +L+ where c-block-source, so that we can use it (together with Y) +L+ in the block-source extractor of Theorem 2.8. Like in previ- +L+ ous works in the extractor literature (e.g. [19, 13]) we use a +L+ ”win-win” analysis which shows that either X is already a +L+ somewhere c-block-source, or it has a condensed part which +L+ contains a lot of the entropy of the source. In this case we +L+ proceed recursively on that part. Continuing this way we +L+ eventually reach a source so condensed that it must be a +L+ somewhere block source. Note that in [4], the challenge re- +L+ sponse mechanism was used to find a block source also, but +L+ there the entropy was so high that they could afford to use +L+ </SectLabel_bodyText> <SectLabel_page> 675 +L+ </SectLabel_page> <SectLabel_figure> Not Somewhere block source	n bits total +L+ 		t blocks			Outputs +L+ < k’ +L+ Challenge Challenge +L+ responded responded +L+ X +L+ low +L+ med +L+ high +L+ n/t bits total +L+ t blocks +L+ Challenge Unresponded +L+ SB +L+ Somewhere Block Source! +L+ med +L+ med +L+ low +L+ 0< low < k’/t +L+ k’/t < med < k’/c +L+ k’/c < high < k’ +L+ high +L+ med +L+ Random Row +L+ med +L+ SB +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1: Analysis of the subsource somewhere extractor. +L+ a tree of depth 1. They did not need to recurse or condense +L+ the sources. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Consider the tree of parts of the source X evolved by +L+ such recursion. Each node in the tree corresponds to some +L+ interval of bit locations of the source, with the root node +L+ corresponding to the entire source. A node is a child of an- +L+ other if its interval is a subinterval of the parent. It can be +L+ shown that some node in the tree is ”good”; it corresponds +L+ to a somewhere c-source, but we don’t know which node is +L+ good. Since we only want a somewhere extractor, we can +L+ apply to each node the somewhere block-source extractor of +L+ Corollary 2.8 – this will give us a random output in every +L+ ”good” node of the tree. The usual idea is output all these +L+ values (and in seeded extractors, merge them using the ex- +L+ ternally given random seed). However, we cannot afford to +L+ do that here as there is no external seed and the number of +L+ these outputs (the size of the tree) is far too large. +L+ Our aim then will be to significantly prune this number +L+ of candidates and in fact output only the candidates on one +L+ path to a canonical”good” node. First we will give a very in- +L+ formal description of how to do this (Figure 1). Before call- +L+ ing SSE recursively on a subpart of a current part of X, we’ll +L+ use the ”Challenge-Response” mechanism described above +L+ to check if ”it has entropy”.4 We will recurse only with the +L+ first (in left-to-right order) part which passes the ”entropy +L+ test”. Thus note that we will follow a single path on this +L+ tree. The algorithm SSE will output only the sets of strings +L+ produced by applying the somewhere c-block-extractor SB +L+ on the parts visited along this path. +L+ Now let us describe the algorithm for SSE. SSE will be +L+ initially invoked as SSE(x, y), but will recursively call itself +L+ with different inputs z which will always be substrings of x. +L+ </SectLabel_bodyText> <SectLabel_footnote> 4We note that we ignore the additional complication that +L+ SSE will actually use recursion also to compute the challenge +L+ in the challenge-response mechanism. +L+ </SectLabel_footnote> <SectLabel_construct> Algorithm: SSE(z, y) +L+ </SectLabel_construct> <SectLabel_bodyText> Let pSE(., .) be the somewhere extractor with a polyno- +L+ mial number of outputs of Proposition 2.3. +L+ Let SB be the somewhere block source extractor of Corol- +L+ lary 2.8. +L+ Global Parameters: t, the branching factor of the tree. k +L+ the original entropy of the sources. +L+ Output will be a set of strings. +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. If z is shorter than √k, return the empty set, else +L+ continue. +L+ 2. Partition z into t equal parts z = z1, z2, ... ,zt. +L+ 3. Compute the response set R(z, y) which is the set of +L+ strings output by pSE(z, y). +L+ 4. For i E [t], compute the challenge set C(zi, y), which +L+ is the set of outputs of SSE(zi, y). +L+ 5. Let h be the smallest index for which the challenge set +L+ C(zh, y) is not contained in the response set (set h = t +L+ if no such index exists). +L+ 6. Output SB(z, y) concatenated with SSE(zh, y). +L+ Proving that indeed there are subsources on which SSE +L+ will follow a path to a ”good” (for these subsources) node, +L+ is the heart of the analysis. It is especially complex due +L+ to the fact that the recursive call to SSE on subparts of +L+ the current part is used to generate the Challenges for the +L+ Challenge-Response mechanism. Since SSE works only on +L+ a subsources we have to guarantee that restriction to these +L+ does not hamper the behavior of SSE in past and future calls +L+ to it. +L+ Let us turn to the highlights of the analysis, for the proof +L+ of Theorem 2.6. Let k' be the entropy of the source Z at +L+ some place in this recursion. Either one of its blocks Zi has +L+ </SectLabel_listItem> <SectLabel_page> 676 +L+ </SectLabel_page> <SectLabel_bodyText> entropy k'/c, in which case it is very condensed, since its +L+ size is n/t for t ≫ c), or it must be that c of its blocks form +L+ a c-block source with block entropy k'/t (which is sufficient +L+ for the extractor B used by SB). In the 2nd case the fact +L+ that SB(z, y) is part of the output of of our SSE guarantees +L+ that we are somewhere random. If the 2nd case doesn’t hold, +L+ let Zi be the leftmost condensed block. We want to ensure +L+ that (on appropriate subsources) SSE calls itself on that ith +L+ subpart. To do so, we fix all Zj for j < i to constants zj. We +L+ are now in the position described in the Challenge-Response +L+ mechanism section, that (in each of the first i parts) there +L+ is either no entropy or lots of entropy. We further restrict +L+ to subsources as explained there which make all first i − 1 +L+ blocks fail the ”entropy test”, and the fact that Zi still has +L+ lots of entropy after these restrictions (which we need to +L+ prove) ensures that indeed SSE will be recursively applied +L+ to it. +L+ We note that while the procedure SSE can be described re- +L+ cursively, the formal analysis of fixing subsources is actually +L+ done globally, to ensure that indeed all entropy requirements +L+ are met along the various recursive calls. +L+ Let us remark on the choice of the branching parameter t. +L+ On the one hand, we’d like to keep it small, as it dominates +L+ the number of outputs tc of SB, and thus the total number of +L+ outputs (which is tc logt n). For this purpose, any t = no(1) +L+ will do. On the other hand, t should be large enough so that +L+ condensing is faster than losing entropy. Here note that if +L+ Z is of length n, its child has length n/t, while the entropy +L+ shrinks only from k' to k'/c. A simple calculation shows that +L+ if k(lo9t)/lo9c) > n2 then a c block-source must exist along +L+ such a path before the length shrinks to √k. Note that for +L+ k = nΩ(1) a (large enough) constant t suffices (resulting in +L+ only logarithmic number of outputs of SSE). This analysis +L+ is depicted pictorially in Figure 1. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. THE FINAL DISPERSER D +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Following is a rough description of our disperser D proving +L+ Theorem 2.1. The high level structure of D will resemble the +L+ structure of SSE - we will recursively split the source X and +L+ look for entropy in the parts. However now we must output +L+ a single value (rather than a set) which can take both values +L+ 0 and 1. This was problematic in SSE, even knowing where +L+ the ”good” part (containing a c-block-source) was! How can +L+ we do so now? +L+ We now have at our disposal a much more powerful tool +L+ for generating challenges (and thus detecting entropy), namely +L+ the subsource somewhere disperser SSE. Note that in con- +L+ structing SSE we only had essentially the somewhere c-block- +L+ source extractor SB to (recursively) generate the challenges, +L+ but it depended on a structural property of the block it was +L+ applied on. Now SSE does not assume any structure on its +L+ input sources except sufficient entropy 5. +L+ Let us now give a high level description of the disperser +L+ D. It too will be a recursive procedure. If when processing +L+ some part Z of X it ”realizes” that a subpart Zi of Z has +L+ entropy, but not all the entropy of Z (namely Zi, Z is a +L+ 2-block-source) then we will halt and produce the output +L+ of D. Intuitively, thinking about the Challenge-Response +L+ mechanism described above, the analysis implies that we +L+ </SectLabel_bodyText> <SectLabel_footnote> 5There is a catch – it only works on subsources of them! +L+ This will cause us a lot of head ache; we will elaborate on it +L+ later. +L+ can either pass or fail Zi (on appropriate subsources). But +L+ this means that the outcome of this ”entropy test” is a 1-bit +L+ disperser! +L+ To capitalize on this idea, we want to use SSE to identify +L+ such a block-source in the recursion tree. As before, we scan +L+ the blocks from left to right, and want to distinguish three +L+ possibilities. +L+ low Zi has low entropy. In this case we proceed to i + 1. +L+ medium Zi has ”medium” entropy (Zi, Z is a block-source). +L+ In which case we halt and produce an output (zero or +L+ one). +L+ high Zi has essentially all entropy of Z. In this case we +L+ recurse on the condensed block Zi. +L+ As before, we use the Challenge-Response mechanism (with +L+ a twist). We will compute challenges C(Zi, Y) and responses +L+ R(Z, Y), all strings of length m. The responses are computed +L+ exactly as before, using the somewhere extractor pSE. The +L+ Challenges are computed using our subsource somewhere +L+ extractor SSE. +L+ We really have 4 possibilities to distinguish, since when we +L+ halt we also need to decide which output bit we give. We will +L+ do so by deriving three tests from the above challenges and +L+ responses: (CH, RH), (CM, RM), (CL, RL) for high, medium +L+ and low respectively, as follows. Let m ≥ mH >> mM >> +L+ mL be appropriate integers: then in each of the tests above +L+ we restrict ourselves to prefixes of all strings of the appro- +L+ priate lengths only. So every string in CM will be a prefix +L+ of length mM of some string in CH. Similarly, every string +L+ in RL is the length mL prefix of some string in RH. Now +L+ it is immediately clear that if CM is contained in RM, then +L+ CL is contained in RL. Thus these tests are monotone, if +L+ our sample fails the high test, it will definitely fail all tests. +L+ </SectLabel_footnote> <SectLabel_construct> Algorithm: D(z, y) +L+ </SectLabel_construct> <SectLabel_bodyText> Let pSE(., .) be the somewhere extractor with a polyno- +L+ mial number of outputs of Proposition 2.3. +L+ Let SSE(.,.) be the subsource somewhere extractor of The- +L+ orem 2.6. +L+ Global Parameters: t, the branching factor of the tree. k +L+ the original entropy of the sources. +L+ Local Parameters for recursive level: mL ≪ mM ≪ mH. +L+ Output will be an element of {0, 1}. +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. If z is shorter than √k, return 0. +L+ 2. Partition z into t equal parts z = z1, z2, ... , zt. +L+ 3. Compute three response sets RL, RM, RH using pSE(z, y). +L+ Rj will be the prefixes of length mj of the strings in +L+ pSE(z, y). +L+ 4. For each i ∈ [t], compute three challenge sets CiL, CiM, CiH +L+ using SSE(zi, y). Cij will be the prefixes of length mj +L+ of the strings in SSE(zi, y). +L+ 5. Let h be the smallest index for which the challenge set +L+ CL is not contained in the response set RL, if there is +L+ no such index, output 0 and halt. +L+ 6. If ChH is contained in RH and ChH is contained in RM, +L+ output 0 and halt. If ChH is contained in RH but ChH +L+ is not contained in RM, output 1 and halt. +L+ </SectLabel_listItem> <SectLabel_page> 677 +L+ </SectLabel_page> <SectLabel_figure> t blocks +L+ X +L+ low +L+ fail +L+ fail +L+ fail +L+ X_3 +L+ (X_3)_4 +L+ low +L+ low +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ low +L+ low +L+ pass +L+ pass +L+ pass +L+ high +L+ low +L+ low +L+ t blocks +L+ low +L+ high +L+ t blocks +L+ med +L+ n bits total +L+ n/t bits total +L+ n/t^2 bits total +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ fail +L+ pass +L+ pass +L+ fail +L+ pass +L+ fail +L+ fail +L+ Output 0	Output 1 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: Analysis of the disperser. +L+ </SectLabel_figureCaption> <SectLabel_listItem> 7. Output D(zh, y), +L+ </SectLabel_listItem> <SectLabel_bodyText> First note the obvious monotonicity of the tests. If Zi fails +L+ one of the tests it will certainly fail for shorter strings. Thus +L+ there are only four outcomes to the three tests, written in the +L+ order (low, medium, high): (pass, pass, pass), (pass, pass, fail), +L+ (pass, fail, fail) and (fail, fail, fail). Conceptually, the algo- +L+ rithm is making the following decisions using the four tests: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. (fail, fail, fail): Assume Zi has low entropy and proceed +L+ to block i + 1. +L+ 2. (pass, fail, fail): Assume Zi is medium, halt and output +L+ 0. +L+ 3. (pass, pass, fail): Assume Zi is medium, halt and out- +L+ put 1. +L+ 4. (pass, pass, pass): Assume Zi is high and recurse on Zi. +L+ </SectLabel_listItem> <SectLabel_bodyText> The analysis of this idea (depicted in Figure 2).turns out +L+ to be more complex than it seems. There are two reasons for +L+ that. Now we briefly explain them and the way to overcome +L+ them in the construction and analysis. +L+ The first reason is the fact mentioned above, that SSE +L+ which generates the challenges, works only on a subsources +L+ of the original sources. Restricting to these subsources at +L+ some level of the recursion (as required by the analysis of of +L+ the test) causes entropy loss which affects both definitions +L+ (such as these entropy thresholds for decisions) and correct- +L+ ness of SSE in higher levels of recursion. Controlling this en- +L+ tropy loss is achieved by calling SSE recursively with smaller +L+ and smaller entropy requirements, which in turn limits the +L+ entropy which will be lost by these restrictions. In order not +L+ to lose all the entropy for this reason alone, we must work +L+ with special parameters of SSE, essentially requiring that at +L+ termination it has almost all the entropy it started with. +L+ The second reason is the analysis of the test when we are +L+ in a medium block. In contrast with the above situation, we +L+ cannot consider the value of Zi fixed when we need it to fail +L+ on the Medium and Low tests. We need to show that for +L+ these two tests (given a pass for High), they come up both +L+ (pass, fail) and (fail, fail) each with positive probability. +L+ Since the length of Medium challenges and responses is +L+ mM, the probability of failure is at least exp(−Q(mM)) (this +L+ follows relatively easily from the fact that the responses are +L+ somewhere random). If the Medium test fails so does the +L+ Low test, and thus (fail, fail) has a positive probability and +L+ our disperser D outputs 0 with positive probability. +L+ To bound (pass, fail) we first observe (with a similar +L+ reasoning) that the low test fails with probability at least +L+ exp(−Q(mL)). But we want the medium test to pass at the +L+ same time. This probability is at least the probability that +L+ low fails minus the probability that medium fails. We already +L+ have a bound on the latter: it is at most poly(n)exp(−ℓmM). +L+ Here comes our control of the different length into play - we +L+ can make the mL sufficiently smaller than mM to yield this +L+ difference positive. We conclude that our disperser D out- +L+ puts 1 with positive probability as well. +L+ Finally, we need to take care of termination: we have to +L+ ensure that the recurrence always arrives at a medium sub- +L+ part, but it is easy to chose entropy thresholds for low, medium +L+ and high to ensure that this happens. +L+ </SectLabel_bodyText> <SectLabel_page> 678 +L+ </SectLabel_page> <SectLabel_sectionHeader> 6. RESILIENCY AND DEFICIENCY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we will breifly discuss an issue which arises +L+ in our construction that we glossed over in the previous sec- +L+ tions. Recall our definition of subsources: +L+ </SectLabel_bodyText> <SectLabel_construct> DEfInItIOn 6.1 (SUBSOURCES). Given random variables +L+ Z and Zˆ on {0,1}n we say that Zˆ is a deficiency d subsource +L+ of Z and write Zˆ ⊆ Z if there exists a set A ⊆ {0,1}n such +L+ that (Z|A) = Zˆ and Pr[Z ∈ A] ≥ 2—d. +L+ </SectLabel_construct> <SectLabel_bodyText> Recall that we were able to guarantee that our algorithms +L+ made the right decisions only on subsources of the original +L+ source. For example, in the construction of our final dis- +L+ perser, to ensure that our algorithms correctly identify the +L+ right high block to recurse on, we were only able to guar- +L+ antee that there are subsources of the original sources in +L+ which our algorithm makes the correct decision with high +L+ probability. Then, later in the analysis we had to further +L+ restrict the source to even smaller subsources. This leads to +L+ complications, since the original event of picking the correct +L+ high block, which occurred with high probability, may be- +L+ come an event which does not occur with high probability +L+ in the current subsource. To handle these kinds of issues, +L+ we will need to be very careful in measuring how small our +L+ subsources are. +L+ In the formal analysis we introduce the concept of re- +L+ siliency to deal with this. To give an idea of how this works, +L+ here is the actual definition of somewhere subsource extrac- +L+ tor that we use in the formal analysis. +L+ </SectLabel_bodyText> <SectLabel_construct> DEfInItIOn 6.2 (SUBSOURCE SOmEWhERE ExtRaCtOR). +L+ </SectLabel_construct> <SectLabel_bodyText> A function SSE : {0, 1}n × {0, 1}n → ({0, 1}m)ℓ is a sub- +L+ source somewhere extractor with nrows output rows, entropy +L+ threshold k, deficiency def, resiliency res and error ǫ if for +L+ every (n, k)-sources X, Y there exist a deficiency def sub- +L+ source Xgood of X and a deficiency def subsource Ygood of +L+ Y such that for every deficiency res subsource X' of Xgood +L+ and deficiency res subsource Y' of Ygood, the random vari- +L+ able SSE(X',Y') is ǫ-close to a ℓ × m somewhere random +L+ distribution. +L+ It turns out that our subsource somewhere extractor does +L+ satisfy this stronger definition. The advantage of this defi- +L+ nition is that it says that once we restrict our attention to +L+ the good subsources Xgood, Ygood, we have the freedom to fur- +L+ ther restrict these subsources to smaller subsources, as long +L+ as our final subsources do not lose more entropy than the +L+ resiliency permits. +L+ This issue of managing the resiliency for the various ob- +L+ jects that we construct is one of the major technical chal- +L+ lenges that we had to overcome in our construction. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. OPEN PROBLEMS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Better Independent Source Extractors A bottleneck to +L+ improving our disperser is the block versus general +L+ source extractor of Theorem 2.7. A good next step +L+ would be to try to build an extractor for one block +L+ source (with only a constant number of blocks) and +L+ one other independent source which works for polylog- +L+ arithmic entropy, or even an extractor for a constant +L+ number of sources that works for sub-polynomial en- +L+ tropy. +L+ Simple Dispersers While our disperser is polynomial time +L+ computable, it is not as explicit as one might have +L+ hoped. For instance the Ramsey Graph construction +L+ of Frankl-Wilson is extremely simple: For a prime p, +L+ let the vertices of the graph be all subsets of [p3] of +L+ size p2 − 1. Two vertices S, T are adjacent if and only +L+ if |S ∩ T | ≡ −1 mod p. It would be nice to find a good +L+ disperser that beats the Frankl-Wilson construction, +L+ yet is comparable in simplicity. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] N. Alon. The shannon capacity of a union. +L+ Combinatorica, 18, 1998. +L+ [2] B. Barak. A simple explicit construction of an +L+ n˜o(logn )-ramsey graph. Technical report, Arxiv, 2006. +L+ http://arxiv.org/abs/math.CO/0601651. +L+ [3] B. Barak, R. Impagliazzo, and A. Wigderson. +L+ Extracting randomness using few independent sources. +L+ In Proceedings of the 45th Annual IEEE Symposium +L+ on Foundations of Computer Science, pages 384–393, +L+ 2004. +L+ [4] B. Barak, G. Kindler, R. Shaltiel, B. Sudakov, and +L+ A. Wigderson. Simulating independence: New +L+ constructions of condensers, Ramsey graphs, +L+ dispersers, and extractors. In Proceedings of the 37th +L+ Annual ACM Symposium on Theory of Computing, +L+ pages 1–10, 2005. +L+ [5] J. Bourgain. More on the sum-product phenomenon in +L+ prime fields and its applications. International Journal +L+ of Number Theory, 1:1–32, 2005. +L+ [6] J. Bourgain, N. Katz, and T. Tao. A sum-product +L+ estimate in finite fields, and applications. Geometric +L+ and Functional Analysis, 14:27–57, 2004. +L+ [7] M. Capalbo, O. Reingold, S. Vadhan, and +L+ A. Wigderson. Randomness conductors and +L+ constant-degree lossless expanders. In Proceedings of +L+ the 34th Annual ACM Symposium on Theory of +L+ Computing, pages 659–668, 2002. +L+ [8] B. Chor and O. Goldreich. Unbiased bits from sources +L+ of weak randomness and probabilistic communication +L+ complexity. SIAM Journal on Computing, +L+ 17(2):230–261, 1988. +L+ [9] P. Frankl and R. M. Wilson. Intersection theorems +L+ with geometric consequences. Combinatorica, +L+ 1(4):357–368, 1981. +L+ [10] P. Gopalan. Constructing ramsey graphs from boolean +L+ function representations. In Proceedings of the 21th +L+ Annual IEEE Conference on Computational +L+ Complexity, 2006. +L+ [11] V. Grolmusz. Low rank co-diagonal matrices and +L+ ramsey graphs. Electr. J. Comb, 7, 2000. +L+ [12] V. Guruswami. Better extractors for better codes? +L+ Electronic Colloquium on Computational Complexity +L+ (ECCC), (080), 2003. +L+ [13] C. J. Lu, O. Reingold, S. Vadhan, and A. Wigderson. +L+ Extractors: Optimal up to constant factors. In +L+ Proceedings of the 35th Annual ACM Symposium on +L+ Theory of Computing, pages 602–611, 2003. +L+ [14] P. Miltersen, N. Nisan, S. Safra, and A. Wigderson. +L+ On data structures and asymmetric communication +L+ complexity. Journal of Computer and System +L+ Sciences, 57:37–49, 1 1998. +L+ </SectLabel_reference> <SectLabel_page> 679 +L+ </SectLabel_page> <SectLabel_reference> [15] N. Nisan and D. Zuckerman. More deterministic +L+ simulation in logspace. In Proceedings of the 25th +L+ Annual ACM Symposium on Theory of Computing, +L+ pages 235–244, 1993. +L+ [16] P. Pudlak and V. Rodl. Pseudorandom sets and +L+ explicit constructions of ramsey graphs. Submitted for +L+ publication, 2004. +L+ [17] A. Rao. Extractors for a constant number of +L+ polynomially small min-entropy independent sources. +L+ In Proceedings of the 38th Annual ACM Symposium +L+ on Theory of Computing, 2006. +L+ [18] R. Raz. Extractors with weak random seeds. In +L+ Proceedings of the 37th Annual ACM Symposium on +L+ Theory of Computing, pages 11–20, 2005. +L+ [19] O. Reingold, R. Shaltiel, and A. Wigderson. +L+ Extracting randomness via repeated condensing. In +L+ Proceedings of the 41st Annual IEEE Symposium on +L+ Foundations of Computer Science, pages 22–31, 2000. +L+ [20] M. Santha and U. V. Vazirani. Generating +L+ quasi-random sequences from semi-random sources. +L+ Journal of Computer and System Sciences, 33:75–87, +L+ 1986. +L+ [21] R. Shaltiel. Recent developments in explicit +L+ constructions of extractors. Bulletin of the European +L+ Association for Theoretical Computer Science, +L+ 77:67–95, 2002. +L+ [22] A. Ta-Shma and D. Zuckerman. Extractor codes. +L+ IEEE Transactions on Information Theory, 50, 2004. +L+ [23] U. Vazirani. Towards a strong communication +L+ complexity theory or generating quasi-random +L+ sequences from two communicating slightly-random +L+ sources (extended abstract). In Proceedings of the 17th +L+ Annual ACM Symposium on Theory of Computing, +L+ pages 366–378, 1985. +L+ [24] A. Wigderson and D. Zuckerman. Expanders that +L+ beat the eigenvalue bound: Explicit construction and +L+ applications. Combinatorica, 19(1):125–138, 1999. +L+ </SectLabel_reference> <SectLabel_page> 680 +L+ </SectLabel_page>
<SectLabel_title> A Frequency-based and a Poisson-based Definition of the +L+ Probability of Being Informative +L+ </SectLabel_title> <SectLabel_author> Thomas Roelleke +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ Queen Mary University of London +L+ </SectLabel_affiliation> <SectLabel_email> thor@dcs.qmul.ac.uk +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper reports on theoretical investigations about the +L+ assumptions underlying the inverse document frequency (idf ). +L+ We show that an intuitive idf-based probability function for +L+ the probability of a term being informative assumes disjoint +L+ document events. By assuming documents to be indepen- +L+ dent rather than disjoint, we arrive at a Poisson-based prob- +L+ ability of being informative. The framework is useful for +L+ understanding and deciding the parameter estimation and +L+ combination in probabilistic retrieval models. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.3.3 [Information Search and Retrieval]: Retrieval +L+ models +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Theory +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Probabilistic information retrieval, inverse document fre- +L+ quency (idf), Poisson distribution, information theory, in- +L+ dependence assumption +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION AND BACKGROUND +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The inverse document frequency (idf) is one of the most +L+ successful parameters for a relevance-based ranking of re- +L+ trieved objects. With N being the total number of docu- +L+ ments, and n(t) being the number of documents in which +L+ term t occurs, the idf is defined as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> idf(t) := −log n(tt) , 0 <= idf(t) < ∞ +L+ </SectLabel_equation> <SectLabel_bodyText> Ranking based on the sum of the idf-values of the query +L+ terms that occur in the retrieved documents works well, this +L+ has been shown in numerous applications. Also, it is well +L+ known that the combination of a document-specific term +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> SIGIR’03, July 28–August 1, 2003, Toronto, Canada. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2003 ACM 1-58113-646-3/03/0007 ...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> weight and idf works better than idf alone. This approach +L+ is known as tf-idf, where tf(t, d) (0 <= tf(t, d) <= 1) is +L+ the so-called term frequency of term t in document d. The +L+ idf reflects the discriminating power (informativeness) of a +L+ term, whereas the tf reflects the occurrence of a term. +L+ The idf alone works better than the tf alone does. An ex- +L+ planation might be the problem of tf with terms that occur +L+ in many documents; let us refer to those terms as “noisy” +L+ terms. We use the notion of “noisy” terms rather than “fre- +L+ quent” terms since frequent terms leaves open whether we +L+ refer to the document frequency of a term in a collection or +L+ to the so-called term frequency (also referred to as within- +L+ document frequency) of a term in a document. We asso- +L+ ciate “noise” with the document frequency of a term in a +L+ collection, and we associate “occurrence” with the within- +L+ document frequency of a term. The tf of a noisy term might +L+ be high in a document, but noisy terms are not good candi- +L+ dates for representing a document. Therefore, the removal +L+ of noisy terms (known as “stopword removal”) is essential +L+ when applying tf. In a tf-idf approach, the removal of stop- +L+ words is conceptually obsolete, if stopwords are just words +L+ with a low idf. +L+ From a probabilistic point of view, tf is a value with a +L+ frequency-based probabilistic interpretation whereas idf has +L+ an “informative” rather than a probabilistic interpretation. +L+ The missing probabilistic interpretation of idf is a problem +L+ in probabilistic retrieval models where we combine uncertain +L+ knowledge of different dimensions (e.g.: informativeness of +L+ terms, structure of documents, quality of documents, age +L+ of documents, etc.) such that a good estimate of the prob- +L+ ability of relevance is achieved. An intuitive solution is a +L+ normalisation of idf such that we obtain values in the inter- +L+ val [0; 1]. For example, consider a normalisation based on +L+ the maximal idf-value. Let T be the set of terms occurring +L+ in a collection. +L+ </SectLabel_bodyText> <SectLabel_equation> Pf,Q (t is informative) :=  idf(t) +L+ maxidf +L+ maxidf := max({idf(t)|t ∈ T}), maxidf <= −log(1/N) +L+ minidf := min({idf(t)|t ∈ T}), minidf >= 0 +L+ minidf < Pf,Q (t is informative) ≤ 1.0 +L+ maxidf — +L+ </SectLabel_equation> <SectLabel_bodyText> This frequency-based probability function covers the interval +L+ [0; 1] if the minimal idf is equal to zero, which is the case +L+ if we have at least one term that occurs in all documents. +L+ Can we interpret Pf� Q , the normalised idf, as the probability +L+ that the term is informative? +L+ When investigating the probabilistic interpretation of the +L+ </SectLabel_bodyText> <SectLabel_page> 227 +L+ </SectLabel_page> <SectLabel_bodyText> normalised idf, we made several observations related to dis- +L+ jointness and independence of document events. These ob- +L+ servations are reported in section 3. We show in section 3.1 +L+ that the frequency-based noise probability n(t) Nused in the +L+ classic idf-definition can be explained by three assumptions: +L+ binary term occurrence, constant document containment and +L+ disjointness of document containment events. In section 3.2 +L+ we show that by assuming independence of documents, we +L+ obtain 1 — e-1 Pz� 1 — 0.37 as the upper bound of the noise +L+ probability of a term. The value e−1 is related to the loga- +L+ rithm and we investigate in section 3.3 the link to informa- +L+ tion theory. In section 4, we link the results of the previous +L+ sections to probability theory. We show the steps from possi- +L+ ble worlds to binomial distribution and Poisson distribution. +L+ In section 5, we emphasise that the theoretical framework +L+ of this paper is applicable for both idf and tf. Finally, in +L+ section 6, we base the definition of the probability of be- +L+ ing informative on the results of the previous sections and +L+ compare frequency-based and Poisson-based definitions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. BACKGROUND +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The relationship between frequencies, probabilities and +L+ information theory (entropy) has been the focus of many +L+ researchers. In this background section, we focus on work +L+ that investigates the application of the Poisson distribution +L+ in IR since a main part of the work presented in this paper +L+ addresses the underlying assumptions of Poisson. +L+ [4] proposes a 2-Poisson model that takes into account +L+ the different nature of relevant and non-relevant documents, +L+ rare terms (content words) and frequent terms (noisy terms, +L+ function words, stopwords). [9] shows experimentally that +L+ most of the terms (words) in a collection are distributed +L+ according to a low dimension n-Poisson model. [10] uses a +L+ 2-Poisson model for including term frequency-based proba- +L+ bilities in the probabilistic retrieval model. The non-linear +L+ scaling of the Poisson function showed significant improve- +L+ ment compared to a linear frequency-based probability. The +L+ Poisson model was here applied to the term frequency of a +L+ term in a document. We will generalise the discussion by +L+ pointing out that document frequency and term frequency +L+ are dual parameters in the collection space and the docu- +L+ ment space, respectively. Our discussion of the Poisson dis- +L+ tribution focuses on the document frequency in a collection +L+ rather than on the term frequency in a document. +L+ [7] and [6] address the deviation of idf and Poisson, and +L+ apply Poisson mixtures to achieve better Poisson-based esti- +L+ mates. The results proved again experimentally that a one- +L+ dimensional Poisson does not work for rare terms, therefore +L+ Poisson mixtures and additional parameters are proposed. +L+ [3], section 3.3, illustrates and summarises comprehen- +L+ sively the relationships between frequencies, probabilities +L+ and Poisson. Different definitions of idf are put into con- +L+ text and a notion of “noise” is defined, where noise is viewed +L+ as the complement of idf. We use in our paper a different +L+ notion of noise: we consider a frequency-based noise that +L+ corresponds to the document frequency, and we consider a +L+ term noise that is based on the independence of document +L+ events. +L+ [11], [12], [8] and [1] link frequencies and probability esti- +L+ mation to information theory. [12] establishes a framework +L+ in which information retrieval models are formalised based +L+ on probabilistic inference. A key component is the use of a +L+ space of disjoint events, where the framework mainly uses +L+ terms as disjoint events. The probability of being informa- +L+ tive defined in our paper can be viewed as the probability +L+ of the disjoint terms in the term space of [12]. +L+ [8] address entropy and bibliometric distributions. En- +L+ tropy is maximal if all events are equiprobable and the fre- +L+ quency-based Lotka law (N/iλ is the number of scientists +L+ that have written i publications, where N and λ are distri- +L+ bution parameters), Zipf and the Pareto distribution are re- +L+ lated. The Pareto distribution is the continuous case of the +L+ Lotka and Lotka and Zipf show equivalences. The Pareto +L+ distribution is used by [2] for term frequency normalisation. +L+ The Pareto distribution compares to the Poisson distribu- +L+ tion in the sense that Pareto is “fat-tailed”, i. e. Pareto as- +L+ signs larger probabilities to large numbers of events than +L+ Poisson distributions do. This makes Pareto interesting +L+ since Poisson is felt to be too radical on frequent events. +L+ We restrict in this paper to the discussion of Poisson, how- +L+ ever, our results show that indeed a smoother distribution +L+ than Poisson promises to be a good candidate for improving +L+ the estimation of probabilities in information retrieval. +L+ [1] establishes a theoretical link between tf-idf and infor- +L+ mation theory and the theoretical research on the meaning +L+ of tf-idf “clarifies the statistical model on which the different +L+ measures are commonly based”. This motivation matches +L+ the motivation of our paper: We investigate theoretically +L+ the assumptions of classical idf and Poisson for a better +L+ understanding of parameter estimation and combination. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. FROM DISJOINT TO INDEPENDENT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We define and discuss in this section three probabilities: +L+ The frequency-based noise probability (definition 1), the to- +L+ tal noise probability for disjoint documents (definition 2). +L+ and the noise probability for independent documents (defi- +L+ nition 3). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Binary occurrence, constant containment +L+ and disjointness of documents +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We show in this section, that the frequency-based noise +L+ probability nN(t) in the idf definition can be explained as +L+ a total probability with binary term occurrence, constant +L+ document containment and disjointness of document con- +L+ tainments. +L+ We refer to a probability function as binary if for all events +L+ the probability is either 1.0 or 0.0. The occurrence proba- +L+ bility P(t1d) is binary, if P(t1d) is equal to 1.0 if t E d, and +L+ P(t1d) is equal to 0.0, otherwise. +L+ </SectLabel_bodyText> <SectLabel_equation> P(t1d) is binary: P(t1d) = 1.0 V P(t1d) = 0.0 +L+ </SectLabel_equation> <SectLabel_bodyText> We refer to a probability function as constant if for all +L+ events the probability is equal. The document containment +L+ probability reflect the chance that a document occurs in a +L+ collection. This containment probability is constant if we +L+ have no information about the document containment or +L+ we ignore that documents differ in containment. Contain- +L+ ment could be derived, for example, from the size, quality, +L+ age, links, etc. of a document. For a constant containment +L+ in a collection with N documents, N1 is often assumed as +L+ the containment probability. We generalise this definition +L+ and introduce the constant λ where 0 < λ < N. The con- +L+ tainment of a document d depends on the collection c, this +L+ is reflected by the notation P(d1c) used for the containment +L+ </SectLabel_bodyText> <SectLabel_page> 228 +L+ </SectLabel_page> <SectLabel_bodyText> of a document. +L+ </SectLabel_bodyText> <SectLabel_equation> P(d1c) is constant:	Vd : P(d1c) = λ +L+ N +L+ </SectLabel_equation> <SectLabel_bodyText> For disjoint documents that cover the whole event space, +L+ we set λ = 1 and obtain Ed P(d1c) = 1.0. Next, we define +L+ the frequency-based noise probability and the total noise +L+ probability for disjoint documents. We introduce the event +L+ notation t is noisy and t occurs for making the difference +L+ between the noise probability P(t is noisy1c) in a collection +L+ and the occurrence probability P(t occurs 1d) in a document +L+ more explicit, thereby keeping in mind that the noise prob- +L+ ability corresponds to the occurrence probability of a term +L+ in a collection. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1. The frequency-based term noise prob- +L+ ability: +L+ </SectLabel_construct> <SectLabel_equation> Pfr q(t is noisy1c) := n(t) +L+ N +L+ </SectLabel_equation> <SectLabel_construct> DefInItIOn 2. The total term noise probability for +L+ disjoint documents: +L+ </SectLabel_construct> <SectLabel_equation> Pdi9(t is noisy1c) := E P(t occurs1d) • P(d1c) +L+ d +L+ </SectLabel_equation> <SectLabel_bodyText> Now, we can formulate a theorem that makes assumptions +L+ explicit that explain the classical idf. +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 1. IDF assumptions: If the occurrence prob- +L+ ability P(t1d) of term t over documents d is binary, and +L+ the containment probability P(d1c) of documents d is con- +L+ stant, and document containments are disjoint events, then +L+ the noise probability for disjoint documents is equal to the +L+ frequency-based noise probability. +L+ </SectLabel_construct> <SectLabel_equation> Pdi9(t is noisy1c) = Pfr q(t is noisy1c) +L+ </SectLabel_equation> <SectLabel_bodyText> PrOOf. The assumptions are: +L+ </SectLabel_bodyText> <SectLabel_equation> Vd : (P(t occurs1d) = 1 V P(t occurs1d) = 0) n +L+ P(d1c) = N n +L+ E P(d1c) = 1.0 +L+ </SectLabel_equation> <SectLabel_bodyText> d +L+ the containment for small documents tends to be smaller +L+ than for large documents. From that point of view, idf +L+ means that P(t n d1c) is constant for all d in which t occurs, +L+ and P(t n d1c) is zero otherwise. The occurrence and con- +L+ tainment can be term specific. For example, set P(t nd1c) = +L+ 1/ND(c) if t occurs in d, where ND(c) is the number of doc- +L+ uments in collection c (we used before just N). We choose a +L+ document-dependent occurrence P(t1d) := 1/NT(d), i. e. the +L+ occurrence probability is equal to the inverse of NT (d), which +L+ is the total number of terms in document d. Next, we choose +L+ the containment P(d1c) := NT(d)/NT(c)•NT(c)/ND(c) where +L+ NT(d)/NT(c) is a document length normalisation (number +L+ of terms in document d divided by the number of terms in +L+ collection c), and NT (c)/ND (c) is a constant factor of the +L+ collection (number of terms in collection c divided by the +L+ number of documents in collection c). We obtain P(tnd1c) = +L+ 1/ND (c). +L+ In a tf-idf-retrieval function, the tf-component reflects +L+ the occurrence probability of a term in a document. This is +L+ a further explanation why we can estimate the idf with a +L+ simple P(t1d), since the combined tf-idf contains the occur- +L+ rence probability. The containment probability corresponds +L+ to a document normalisation (document length normalisa- +L+ tion, pivoted document length) and is normally attached to +L+ the tf-component or the tf-idf-product. +L+ The disjointness assumption is typical for frequency-based +L+ probabilities. From a probability theory point of view, we +L+ can consider documents as disjoint events, in order to achieve +L+ a sound theoretical model for explaining the classical idf. +L+ But does disjointness reflect the real world where the con- +L+ tainment of a document appears to be independent of the +L+ containment of another document? In the next section, we +L+ replace the disjointness assumption by the independence as- +L+ sumption. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 The upper bound of the noise probability +L+ for independent documents +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> For independent documents, we compute the probability +L+ of a disjunction as usual, namely as the complement of the +L+ probability of the conjunction of the negated events: +L+ </SectLabel_bodyText> <SectLabel_equation> P(di V ... V dN) = 1 — P(-di n ... n �dN) +L+ (1 — P(d)) +L+ = 1—rl +L+ d +L+ 1 +L+ N +L+ = +L+ n(t) +L+ N +L+ = Pfr q(t is noisy1c) +L+ We obtain: +L+ Pdi9(t is noisy1c) = E +L+ dt∈d +L+ </SectLabel_equation> <SectLabel_bodyText> The noise probability can be considered as the conjunction +L+ of the term occurrence and the document containment. +L+ The above result is not a surprise but it is a mathemati- +L+ cal formulation of assumptions that can be used to explain +L+ the classical idf. The assumptions make explicit that the +L+ different types of term occurrence in documents (frequency +L+ of a term, importance of a term, position of a term, doc- +L+ ument part where the term occurs, etc.) and the different +L+ types of document containment (size, quality, age, etc.) are +L+ ignored, and document containments are considered as dis- +L+ joint events. +L+ From the assumptions, we can conclude that idf (frequency- +L+ based noise, respectively) is a relatively simple but strict +L+ estimate. Still, idf works well. This could be explained +L+ by a leverage effect that justifies the binary occurrence and +L+ constant containment: The term occurrence for small docu- +L+ ments tends to be larger than for large documents, whereas +L+ </SectLabel_bodyText> <SectLabel_equation> P(t is noisy1c) := P(t occurs n (dl V ... V dN)1c) +L+ </SectLabel_equation> <SectLabel_bodyText> For disjoint documents, this view of the noise probability +L+ led to definition 2. For independent documents, we use now +L+ the conjunction of negated events. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 3. The term noise probability for inde- +L+ pendent documents: +L+ Pin(t is noisy1c) := rl (1 — P(t occurs1d) • P(d1c)) +L+ d +L+ </SectLabel_construct> <SectLabel_bodyText> With binary occurrence and a constant containment P(d1c) := +L+ λ/N, we obtain the term noise of a term t that occurs in n(t) +L+ documents: +L+ </SectLabel_bodyText> <SectLabel_equation> Pin(t is noisy1c) = 1 — (1 — λN1n(t) +L+ </SectLabel_equation> <SectLabel_page> 229 +L+ </SectLabel_page> <SectLabel_bodyText> For binary occurrence and disjoint documents, the contain- +L+ ment probability was 1/N. Now, with independent docu- +L+ ments, we can use λ as a collection parameter that controls +L+ the average containment probability. We show through the +L+ next theorem that the upper bound of the noise probability +L+ depends on λ. +L+ </SectLabel_bodyText> <SectLabel_construct> TheOrem 2. The upper bound of being noisy: If the +L+ occurrence P(t|d) is binary, and the containment P(d|c) +L+ is constant, and document containments are independent +L+ events, then 1 − e−λ is the upper bound of the noise proba- +L+ bility. +L+ </SectLabel_construct> <SectLabel_equation> ∀t : Pin (t is noisy|c) < 1 − e−λ +L+ </SectLabel_equation> <SectLabel_bodyText> PrOOf. The upper bound of the independent noise prob- +L+ ability follows from the limit limN→∞(1 + xN)N = ex (see +L+ any comprehensive math book, for example, [5], for the con- +L+ vergence equation of the Euler function). With x = −λ, we +L+ obtain: +L+ </SectLabel_bodyText> <SectLabel_equation> N +L+ lim C1 − λN) = e−λ +L+ N→ +L+ </SectLabel_equation> <SectLabel_bodyText> For the term noise, we have: +L+ </SectLabel_bodyText> <SectLabel_equation> Pin(t is noisy|c) = 1 − C1 − λN)n(t) +L+ </SectLabel_equation> <SectLabel_bodyText> Pin (t is noisy |c) is strictly monotonous: The noise of a term +L+ tn is less than the noise of a term tn+1, where tn occurs in +L+ n documents and tn+1 occurs in n + 1 documents. There- +L+ fore, a term with n = N has the largest noise probability. +L+ For a collection with infinite many documents, the upper +L+ bound of the noise probability for terms tN that occur in all +L+ documents becomes: +L+ </SectLabel_bodyText> <SectLabel_equation> 1−C1−λN)N +L+ = 1−e−λ +L+ </SectLabel_equation> <SectLabel_bodyText> By applying an independence rather a disjointness assump- +L+ tion, we obtain the probability e−1 that a term is not noisy +L+ even if the term does occur in all documents. In the disjoint +L+ case, the noise probability is one for a term that occurs in +L+ all documents. +L+ If we view P(d|c) := λ/N as the average containment, +L+ then λ is large for a term that occurs mostly in large docu- +L+ ments, and λ is small for a term that occurs mostly in small +L+ documents. Thus, the noise of a term t is large if t occurs in +L+ n(t) large documents and the noise is smaller if t occurs in +L+ small documents. Alternatively, we can assume a constant +L+ containment and a term-dependent occurrence. If we as- +L+ sume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as +L+ the average probability that t represents a document. The +L+ common assumption is that the average containment or oc- +L+ currence probability is proportional to n(t). However, here +L+ is additional potential: The statistical laws (see [3] on Luhn +L+ and Zipf) indicate that the average probability could follow +L+ a normal distribution, i. e. small probabilities for small n(t) +L+ and large n(t), and larger probabilities for medium n(t). +L+ For the monotonous case we investigate here, the noise of +L+ a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and +L+ the noise of a term with n(t) = N is close to 1− e−λ. In the +L+ next section, we relate the value e−λ to information theory. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 The probability of a maximal informative +L+ signal +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The probability e−1 is special in the sense that a signal +L+ with that probability is a signal with maximal information as +L+ derived from the entropy definition. Consider the definition +L+ of the entropy contribution H(t) of a signal t. +L+ </SectLabel_bodyText> <SectLabel_equation> H(t) := P(t) · − ln P(t) +L+ </SectLabel_equation> <SectLabel_bodyText> We form the first derivation for computing the optimum. +L+ </SectLabel_bodyText> <SectLabel_equation> − ln P(t) + P(t) · P(t) +L+ = −(1+lnP(t)) +L+ For obtaining optima, we use: +L+ 0 = −(1 + ln P(t)) +L+ </SectLabel_equation> <SectLabel_bodyText> The entropy contribution H(t) is maximal for P(t) = e−1. +L+ This result does not depend on the base of the logarithm as +L+ we see next: +L+ </SectLabel_bodyText> <SectLabel_equation> −1 +L+ P(t) · ln b ·P(t) +L+ 1	1 + ln P(t) +L+ ln b + log P(t)	ln b	) +L+ </SectLabel_equation> <SectLabel_bodyText> We summarise this result in the following theorem: +L+ TheOrem 3. The probability of a maximal informa- +L+ tive signal: The probability Pmax = e−1 ≈ 0.37 is the prob- +L+ ability of a maximal informative signal. The entropy of a +L+ maximal informative signal is Hmax = e−1. +L+ PrOOf. The probability and entropy follow from the deriva- +L+ tion above. +L+ The complement of the maximal noise probability is e−λ +L+ and we are looking now for a generalisation of the entropy +L+ definition such that e−λ is the probability of a maximal in- +L+ formative signal. We can generalise the entropy definition +L+ by computing the integral of λ+ln P(t), i. e. this derivation +L+ is zero for e−λ. We obtain a generalised entropy: +L+ </SectLabel_bodyText> <SectLabel_equation> J −(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t)) +L+ </SectLabel_equation> <SectLabel_bodyText> The generalised entropy corresponds for λ = 1 to the classi- +L+ cal entropy. By moving from disjoint to independent docu- +L+ ments, we have established a link between the complement +L+ of the noise probability of a term that occurs in all docu- +L+ ments and information theory. Next, we link independent +L+ documents to probability theory. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. THE LINK TO PROBABILITY THEORY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We review for independent documents three concepts of +L+ probability theory: possible worlds, binomial distribution +L+ and Poisson distribution. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Possible Worlds +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Each conjunction of document events (for each document, +L+ we consider two document events: the document can be +L+ true or false) is associated with a so-called possible world. +L+ For example, consider the eight possible worlds for three +L+ documents (N = 3). +L+ </SectLabel_bodyText> <SectLabel_equation> Pin (tN is noisy) = lim +L+ N→∞ +L+ lim +L+ N→∞ +L+ ∂H(t) +L+ ∂P(t) +L+ ∂H(t) +L+ ∂P(t) +L+ = − logb P(t)+ +L+ = −C +L+ </SectLabel_equation> <SectLabel_page> 230 +L+ </SectLabel_page> <SectLabel_table> world w	conjunction +L+ w7	d1 ∧ d2 ∧ d3 +L+ ws	d1 ∧ d2 ∧ ¬d3 +L+ w5	d1 ∧ ¬d2 ∧ d3 +L+ w4	d1 ∧ ¬d2 ∧ ¬d3 +L+ w3	¬d1 ∧ d2 ∧ d3 +L+ w2	¬d1 ∧ d2 ∧ ¬d3 +L+ w1	¬d1 ∧ ¬d2 ∧ d3 +L+ w0	¬d1 ∧ ¬d2 ∧ ¬d3 +L+ </SectLabel_table> <SectLabel_bodyText> With each world w, we associate a probability µ(w), which +L+ is equal to the product of the single probabilities of the doc- +L+ ument events. +L+ </SectLabel_bodyText> <SectLabel_table> world w	µ(w) +L+ 	probability +L+ w7	(λ·N)0 +L+ 	(N)3 +L+ ws	·	(1 − N)1 +L+ 	rrN)2 +L+ w5	\N)2·	(1 − N)1 +L+ w4	(N)1 ·	2 +L+ 		(1 − N) +L+ w3	(N)2·	(1 − N)1 +L+ w2	(λ	(1 − N)2 +L+ w1	(N)1 · +L+ w0	(λ	N)3 +L+ </SectLabel_table> <SectLabel_bodyText> The sum over the possible worlds in which k documents are +L+ true and N−k documents are false is equal to the probabil- +L+ ity function of the binomial distribution, since the binomial +L+ coefficient yields the number of possible worlds in which k +L+ documents are true. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Binomial distribution +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The binomial probability function yields the probability +L+ that k of N events are true where each event is true with +L+ the single event probability p. +L+ </SectLabel_bodyText> <SectLabel_equation> P(k) := binom(N, k, p) := (N k / pk (1 − p)N− k +L+ </SectLabel_equation> <SectLabel_bodyText> The single event probability is usually defined as p := λ/N, +L+ i. e. p is inversely proportional to N, the total number of +L+ events. With this definition of p, we obtain for an infinite +L+ number of documents the following limit for the product of +L+ the binomial coefficient and p k : +L+ </SectLabel_bodyText> <SectLabel_equation> lim N k +L+ N→∞k p +L+ N · (N−1) · ... · (N−k +1) +L+ k! +L+ </SectLabel_equation> <SectLabel_bodyText> The limit is close to the actual value for k << N. For large +L+ k, the actual value is smaller than the limit. +L+ The limit of (1−p)N−k follows from the limit limN→∞(1+ W= +L+ ex. +L+ </SectLabel_bodyText> <SectLabel_equation> N−k +L+ lim (1 −p)N −k = lim (1 − λN) +L+ N→∞	N→ +L+ </SectLabel_equation> <SectLabel_bodyText> Again, the limit is close to the actual value for k << N. For +L+ large k, the actual value is larger than the limit. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Poisson distribution +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> For an infinite number of events, the Poisson probability +L+ function is the limit of the binomial probability function. +L+ </SectLabel_bodyText> <SectLabel_equation> Ak +L+ binom(N, k, p) = k! · e−λ +L+ P(k) = poisson(k, λ) := k! · e−λ +L+ </SectLabel_equation> <SectLabel_bodyText> The probability poisson(0, 1) is equal to e−1, which is the +L+ probability of a maximal informative signal. This shows +L+ the relationship of the Poisson distribution and information +L+ theory. +L+ After seeing the convergence of the binomial distribution, +L+ we can choose the Poisson distribution as an approximation +L+ of the independent term noise probability. First, we define +L+ the Poisson noise probability: +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 4. The Poisson term noise probability: +L+ P,oi(t is noisy|c) := e−λ · +L+ </SectLabel_construct> <SectLabel_bodyText> For independent documents, the Poisson distribution ap- +L+ proximates the probability of the disjunction for large n(t), +L+ since the independent term noise probability is equal to the +L+ sum over the binomial probabilities where at least one of +L+ n(t) document containment events is true. +L+ </SectLabel_bodyText> <SectLabel_equation> Pin (t is noisy | c) = n(t) (n(t)1 pk (1 − p)N−k +L+ 1. k J +L+ 1 +L+ Pin (t is noisy | c) ≈ P,oi (t is noisy | c) +L+ </SectLabel_equation> <SectLabel_bodyText> We have defined a frequency-based and a Poisson-based prob- +L+ ability of being noisy, where the latter is the limit of the +L+ independence-based probability of being noisy. Before we +L+ present in the final section the usage of the noise proba- +L+ bility for defining the probability of being informative, we +L+ emphasise in the next section that the results apply to the +L+ collection space as well as to the the document space. +L+ </SectLabel_bodyText> <SectLabel_equation> k +L+  N) = e—λ +L+ </SectLabel_equation> <SectLabel_sectionHeader> 5. THE COLLECTION SPACE AND THE +L+ DOCUMENT SPACE +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Consider the dual definitions of retrieval parameters in +L+ table 1. We associate a collection space D × T with a col- +L+ lection c where D is the set of documents and T is the set +L+ of terms in the collection. Let ND := |D| and NT := |T| +L+ be the number of documents and terms, respectively. We +L+ consider a document as a subset of T and a term as a subset +L+ of D. Let nT(d) := |{t|d ∈ t}| be the number of terms that +L+ occur in the document d, and let nD(t) := | {d|t ∈ d}| be the +L+ number of documents that contain the term t. +L+ In a dual way, we associate a document space L × T with +L+ a document d where L is the set of locations (also referred +L+ to as positions, however, we use the letters L and l and not +L+ P and p for avoiding confusion with probabilities) and T is +L+ the set of terms in the document. The document dimension +L+ in a collection space corresponds to the location (position) +L+ dimension in a document space. +L+ The definition makes explicit that the classical notion of +L+ term frequency of a term in a document (also referred to as +L+ the within-document term frequency) actually corresponds +L+ to the location frequency of a term in a document. For the +L+ </SectLabel_bodyText> <SectLabel_equation> lime +L+ — +L+ λ(1− +L+ N→ +L+ =lim +L+ N→∞ +L+ (λ)k = λk +L+ N	k! +L+ lim +L+ N→∞ +L+ λk +L+ k! +L+ n(t)� +L+ k=1 +L+ </SectLabel_equation> <SectLabel_page> 231 +L+ </SectLabel_page> <SectLabel_table> space	collection	document +L+ dimensions	documents and terms	locations and terms +L+ document/location frequency	nD(t, c): Number of documents in which term t occurs in collection c	nL(t, d): Number of locations (positions) at which term t occurs in document d +L+ 	ND(c): Number of documents in collection c	NL(d): Number of locations (positions) in docu-ment d +L+ term frequency	nT(d,c): Number of terms that document d con- tains in collection c	nT(l,d): Number of terms that location l contains in document d +L+ 	NT(c): Number of terms in collection c	NT(d): Number of terms in document d +L+ noise/occurrence containment	P(t1c) (term noise) P(d1c) (document)	P(t1d) (term occurrence) P(l1d) (location) +L+ informativeness conciseness	— ln P(t1c) — ln P(d1c)	— ln P(t1d) — ln P(l 1d) +L+ P(informative) P(concise)	ln(P(t1c))/ ln(P(tm in, c)) ln(P(d1c))/ ln(P(dmin1c))	ln(P(t1d))/ ln(P(tm in, d)) ln(P(l1d))/ln(P(lm in1d)) +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Retrieval parameters +L+ </SectLabel_tableCaption> <SectLabel_bodyText> actual term frequency value, it is common to use the max- +L+ imal occurrence (number of locations; let lf be the location +L+ frequency). +L+ </SectLabel_bodyText> <SectLabel_equation> tf(t, d) := lf(t, d) :=  Pf, , (t occurs 1d) =  nL (t, d) +L+ Pf,,(t. occurs1d) nL(t,, ,d) +L+ </SectLabel_equation> <SectLabel_bodyText> A further duality is between informativeness and concise- +L+ ness (shortness of documents or locations): informativeness +L+ is based on occurrence (noise), conciseness is based on con- +L+ tainment. +L+ We have highlighted in this section the duality between +L+ the collection space and the document space. We concen- +L+ trate in this paper on the probability of a term to be noisy +L+ and informative. Those probabilities are defined in the col- +L+ lection space. However, the results regarding the term noise +L+ and informativeness apply to their dual counterparts: term +L+ occurrence and informativeness in a document. Also, the +L+ results can be applied to containment of documents and lo- +L+ cations. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. THE PROBABILITY OF BEING INFOR- +L+ MATIVE +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We showed in the previous sections that the disjointness +L+ assumption leads to frequency-based probabilities and that +L+ the independence assumption leads to Poisson probabilities. +L+ In this section, we formulate a frequency-based definition +L+ and a Poisson-based definition of the probability of being +L+ informative and then we compare the two definitions. +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 5. The frequency-based probability of be- +L+ ing informative: +L+ </SectLabel_construct> <SectLabel_bodyText> We define the Poisson-based probability of being informa- +L+ tive analogously to the frequency-based probability of being +L+ informative (see definition 5). +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 6. The Poisson-based probability of be- +L+ ing informative: +L+ </SectLabel_construct> <SectLabel_bodyText> For λ >> 1, we can alter the noise and informativeness Pois- +L+ son by starting the sum from 0, since eλ >> 1. Then, the +L+ minimal Poisson informativeness is poisson(0, λ) = e−λ. We +L+ obtain a simplified Poisson probability of being informative: +L+ </SectLabel_bodyText> <SectLabel_equation> λ — ln En (to \k +L+ Ppoj(t is informative1c) � +L+ ln En(t) λk +L+ k=0 k! +L+ λ +L+ </SectLabel_equation> <SectLabel_bodyText> The computation of the Poisson sum requires an optimi- +L+ sation for large n(t). The implementation for this paper +L+ exploits the nature of the Poisson density: The Poisson den- +L+ sity yields only values significantly greater than zero in an +L+ interval around λ. +L+ Consider the illustration of the noise and informative- +L+ ness definitions in figure 1. The probability functions dis- +L+ played are summarised in figure 2 where the simplified Pois- +L+ son is used in the noise and informativeness graphs. The +L+ frequency-based noise corresponds to the linear solid curve +L+ in the noise figure. With an independence assumption, we +L+ obtain the curve in the lower triangle of the noise figure. By +L+ changing the parameter p := λ/N of the independence prob- +L+ ability, we can lift or lower the independence curve. The +L+ noise figure shows the lifting for the value λ := ln N � +L+ 9.2. The setting λ = ln N is special in the sense that the +L+ frequency-based and the Poisson-based informativeness have +L+ the same denominator, namely ln N, and the Poisson sum +L+ converges to λ. Whether we can draw more conclusions from +L+ this setting is an open question. +L+ We can conclude, that the lifting is desirable if we know +L+ for a collection that terms that occur in relatively few doc- +L+ </SectLabel_bodyText> <SectLabel_equation> — ln (e−λ	n(t) λk I +L+ Ek=1 k! +L+ — ln(e−λ • λ) +L+ λ —ln Ek=1 kk +L+ = +L+ λ — lnλ +L+ </SectLabel_equation> <SectLabel_bodyText> For the sum expression, the following limit holds: +L+ lim	n(t)�	λk	=eλ—1 +L+ n(t)→∞	k=1	k! +L+ Ppoj(t is informative1c) := +L+ — ln n(t) +L+ N +L+ —ln 1 +L+ N +L+ — logN	N)= 1 — logN n(t) = 1 — ln In N +L+ N	N +L+ Pf, ,(t is informative1c) := +L+ λ +L+ = 1 +L+ </SectLabel_bodyText> <SectLabel_page> 232 +L+ </SectLabel_page> <SectLabel_figure> 0.8 +L+ 0.6 +L+ 0.4 +L+ 0.2 +L+ 0 +L+ 1 +L+ frequency +L+ independence: 1/N +L+ independence: ln(N)/N +L+ poisson: 1000 +L+ poisson: 2000 +L+ poisson: 1 000,2000 +L+ frequency +L+ independence: 1/N +L+ 0.8	independence: ln(N)/N +L+ poisson: 1000 +L+ poisson: 2000 +L+ poisson: 1000,2000 +L+ 0.6 +L+ 0.4 +L+ 0.2 +L+ 0 +L+ 1 +L+ 0	2000 4000 6000 8000 10000 +L+ n(t): Number of documents with term t +L+ 0	2000 4000 6000 8000 10000 +L+ n(t): Number of documents with term t +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1: Noise and Informativeness +L+ </SectLabel_figureCaption> <SectLabel_figure> Probability function		Noise	Informativeness +L+ Frequency PfreQ	Def Interval	n(t)/N	ln(n(t)/N)/ln(1/N) +L+ 		1/N < PfreQ < 1.0	0.0 < PfreQ < 1.0 +L+ Independence Pin	Def Interval	1 — (1 — p)n (t)	ln(1 — (1 — p)n(t))/ ln(p) +L+ 		p < Pin < 1 — e—λ	ln(p) < Pin < 1.0 +L+ Poisson Ppoi	Def Interval Def	e—λEn(t) λk	(λ — ln En(t) \k )/(λ — ln λ) +L+ Poisson Ppoi simplified	Interval	k=1 k!	k=1 +L+ 		e—λ • λ < Ppoi < 1 — e—λ	(λ — ln(eλ — 1))/(λ — ln λ) < Ppoi < 1.0 +L+ 		e—λ Ek=0 kk	(λ — ln Ek=0 kk )/λ +L+ 		e—λ < Ppoi < 1.0	0.0 < Ppoi < 1.0 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: Probability functions +L+ </SectLabel_figureCaption> <SectLabel_bodyText> uments are no guarantee for finding relevant documents, +L+ i. e. we assume that rare terms are still relatively noisy. On +L+ the opposite, we could lower the curve when assuming that +L+ frequent terms are not too noisy, i. e. they are considered as +L+ being still significantly discriminative. +L+ The Poisson probabilities approximate the independence +L+ probabilities for large n(t); the approximation is better for +L+ larger λ. For n(t) < λ, the noise is zero whereas for n(t) > λ +L+ the noise is one. This radical behaviour can be smoothened +L+ by using a multi-dimensional Poisson distribution. Figure 1 +L+ shows a Poisson noise based on a two-dimensional Poisson: +L+ </SectLabel_bodyText> <SectLabel_equation> λkλk +L+ poisson(k, λ1, λ2) := π • e—λ1•k� +(1—π)•e- λ2 • 2 +L+ k! +L+ </SectLabel_equation> <SectLabel_bodyText> The two dimensional Poisson shows a plateau between λ1 = +L+ 1000 and λ2 = 2000, we used here π = 0.5. The idea be- +L+ hind this setting is that terms that occur in less than 1000 +L+ documents are considered to be not noisy (i.e. they are in- +L+ formative), that terms between 1000 and 2000 are half noisy, +L+ and that terms with more than 2000 are definitely noisy. +L+ For the informativeness, we observe that the radical be- +L+ haviour of Poisson is preserved. The plateau here is ap- +L+ proximately at 1/6, and it is important to realise that this +L+ plateau is not obtained with the multi-dimensional Poisson +L+ noise using π = 0.5. The logarithm of the noise is nor- +L+ malised by the logarithm of a very small number, namely +L+ 0.5 • e—1000 + 0.5 • e—2000. That is why the informativeness +L+ will be only close to one for very little noise, whereas for a +L+ bit of noise, informativeness will drop to zero. This effect +L+ can be controlled by using small values for π such that the +L+ noise in the interval [λ1; λ2] is still very little. The setting +L+ π = e—2000/6 leads to noise values of approximately e —2000/6 +L+ in the interval [λ1; λ2], the logarithms lead then to 1/6 for +L+ the informativeness. +L+ The indepence-based and frequency-based informativeness +L+ functions do not differ as much as the noise functions do. +L+ However, for the indepence-based probability of being infor- +L+ mative, we can control the average informativeness by the +L+ definition p := λ/N whereas the control on the frequency- +L+ based is limited as we address next. +L+ For the frequency-based idf, the gradient is monotonously +L+ decreasing and we obtain for different collections the same +L+ distances of idf-values, i. e. the parameter N does not affect +L+ the distance. For an illustration, consider the distance be- +L+ tween the value idf(tn+1) of a term tn+1 that occurs in n+1 +L+ documents, and the value idf(tn) of a term tn that occurs in +L+ n documents. +L+ </SectLabel_bodyText> <SectLabel_equation> idf(tn+1) — idf(tn) = ln  n +L+ n+ 1 +L+ </SectLabel_equation> <SectLabel_bodyText> The first three values of the distance function are: +L+ </SectLabel_bodyText> <SectLabel_equation> idf(t2) — idf(t1) = ln(1/(1	+ 1))	=	0.69 +L+ idf(t3) — idf(t2) = ln(1/(2	+ 1))	=	0.41 +L+ idf(t4) — idf(t3) = ln(1/(3	+ 1))	=	0.29 +L+ </SectLabel_equation> <SectLabel_bodyText> For the Poisson-based informativeness, the gradient decreases +L+ first slowly for small n(t), then rapidly near n(t) R� λ and +L+ then it grows again slowly for large n(t). +L+ In conclusion, we have seen that the Poisson-based defini- +L+ tion provides more control and parameter possibilities than +L+ </SectLabel_bodyText> <SectLabel_page> 233 +L+ </SectLabel_page> <SectLabel_bodyText> the frequency-based definition does. Whereas more control +L+ and parameter promises to be positive for the personalisa- +L+ tion of retrieval systems, it bears at the same time the dan- +L+ ger of just too many parameters. The framework presented +L+ in this paper raises the awareness about the probabilistic +L+ and information-theoretic meanings of the parameters. The +L+ parallel definitions of the frequency-based probability and +L+ the Poisson-based probability of being informative made +L+ the underlying assumptions explicit. The frequency-based +L+ probability can be explained by binary occurrence, constant +L+ containment and disjointness of documents. Independence +L+ of documents leads to Poisson, where we have to be aware +L+ that Poisson approximates the probability of a disjunction +L+ for a large number of events, but not for a small number. +L+ This theoretical result explains why experimental investiga- +L+ tions on Poisson (see [7]) show that a Poisson estimation +L+ does work better for frequent (bad, noisy) terms than for +L+ rare (good, informative) terms. +L+ In addition to the collection-wide parameter setting, the +L+ framework presented here allows for document-dependent +L+ settings, as explained for the independence probability. This +L+ is in particular interesting for heterogeneous and structured +L+ collections, since documents are different in nature (size, +L+ quality, root document, sub document), and therefore, bi- +L+ nary occurrence and constant containment are less appro- +L+ priate than in relatively homogeneous collections. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. SUMMARY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The definition of the probability of being informative trans- +L+ forms the informative interpretation of the idf into a proba- +L+ bilistic interpretation, and we can use the idf -based proba- +L+ bility in probabilistic retrieval approaches. We showed that +L+ the classical definition of the noise (document frequency) in +L+ the inverse document frequency can be explained by three +L+ assumptions: the term within-document occurrence prob- +L+ ability is binary, the document containment probability is +L+ constant, and the document containment events are disjoint. +L+ By explicitly and mathematically formulating the assump- +L+ tions, we showed that the classical definition of idf does not +L+ take into account parameters such as the different nature +L+ (size, quality, structure, etc.) of documents in a collection, +L+ or the different nature of terms (coverage, importance, po- +L+ sition, etc.) in a document. We discussed that the absence +L+ of those parameters is compensated by a leverage effect of +L+ the within-document term occurrence probability and the +L+ document containment probability. +L+ By applying an independence rather a disjointness as- +L+ sumption for the document containment, we could estab- +L+ lish a link between the noise probability (term occurrence +L+ in a collection), information theory and Poisson. From the +L+ frequency-based and the Poisson-based probabilities of be- +L+ ing noisy, we derived the frequency-based and Poisson-based +L+ probabilities of being informative. The frequency-based prob- +L+ ability is relatively smooth whereas the Poisson probability +L+ is radical in distinguishing between noisy or not noisy, and +L+ informative or not informative, respectively. We showed how +L+ to smoothen the radical behaviour of Poisson with a multi- +L+ dimensional Poisson. +L+ The explicit and mathematical formulation of idf- and +L+ Poisson-assumptions is the main result of this paper. Also, +L+ the paper emphasises the duality of idf and tf, collection +L+ space and document space, respectively. Thus, the result +L+ applies to term occurrence and document containment in a +L+ collection, and it applies to term occurrence and position +L+ containment in a document. This theoretical framework is +L+ useful for understanding and deciding the parameter estima- +L+ tion and combination in probabilistic retrieval models. The +L+ links between indepence-based noise as document frequency, +L+ probabilistic interpretation of idf, information theory and +L+ Poisson described in this paper may lead to variable proba- +L+ bilistic idf and tf definitions and combinations as required +L+ in advanced and personalised information retrieval systems. +L+ Acknowledgment: I would like to thank Mounia Lalmas, +L+ Gabriella Kazai and Theodora Tsikrika for their comments +L+ on the as they said “heavy” pieces. My thanks also go to the +L+ meta-reviewer who advised me to improve the presentation +L+ to make it less “formidable” and more accessible for those +L+ “without a theoretic bent”. This work was funded by a +L+ research fellowship from Queen Mary University of London. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] A. Aizawa. An information-theoretic perspective of +L+ tf-idf measures. Information Processing and +L+ Management, 39:45–65, January 2003. +L+ [2] G. Amati and C. J. Rijsbergen. Term frequency +L+ normalization via Pareto distributions. In 24th +L+ BCS-IRSG European Colloquium on IR Research, +L+ Glasgow, Scotland, 2002. +L+ [3] R. K. Belew. Finding out about. Cambridge University +L+ Press, 2000. +L+ [4] A. Bookstein and D. Swanson. Probabilistic models +L+ for automatic indexing. Journal of the American +L+ Society for Information Science, 25:312–318, 1974. +L+ [5] I. N. Bronstein. Taschenbuch der Mathematik. Harri +L+ Deutsch, Thun, Frankfurt am Main, 1987. +L+ [6] K. Church and W. Gale. Poisson mixtures. Natural +L+ Language Engineering, 1(2):163–190, 1995. +L+ [7] K. W. Church and W. A. Gale. Inverse document +L+ frequency: A measure of deviations from poisson. In +L+ Third Workshop on Very Large Corpora, ACL +L+ Anthology, 1995. +L+ [8] T. Lafouge and C. Michel. Links between information +L+ construction and information gain: Entropy and +L+ bibliometric distribution. Journal of Information +L+ Science, 27(1):39–49, 2001. +L+ [9] E. Margulis. N-poisson document modelling. In +L+ Proceedings of the 15th Annual International ACM +L+ SIGIR Conference on Research and Development in +L+ Information Retrieval, pages 177–189, 1992. +L+ [10] S. E. Robertson and S. Walker. Some simple effective +L+ approximations to the 2-poisson model for +L+ probabilistic weighted retrieval. In Proceedings of the +L+ 17th Annual International ACM SIGIR Conference on +L+ Research and Development in Information Retrieval, +L+ pages 232–241, London, et al., 1994. Springer-Verlag. +L+ [11] S. Wong and Y. Yao. An information-theoric measure +L+ of term specificity. Journal of the American Society +L+ for Information Science, 43(1):54–61, 1992. +L+ [12] S. Wong and Y. Yao. On modeling information +L+ retrieval with probabilistic inference. ACM +L+ Transactions on Information Systems, 13(1):38–68, +L+ 1995. +L+ </SectLabel_reference> <SectLabel_page> 234 +L+ </SectLabel_page>
<SectLabel_title> A Geometric Constraint Library for +L+ 3D Graphical Applications +L+ </SectLabel_title> <SectLabel_author> Hiroshi Hosobe +L+ </SectLabel_author> <SectLabel_affiliation> National Institute of Informatics +L+ </SectLabel_affiliation> <SectLabel_address> 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan +L+ </SectLabel_address> <SectLabel_email> hosobe@nii.ac.jp +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Recent computer technologies have enabled fast high-quality +L+ 3D graphics on personal computers, and also have made +L+ the development of 3D graphical applications easier. How- +L+ ever, most of such technologies do not sufficiently support +L+ layout and behavior aspects of 3D graphics. Geometric con- +L+ straints are, in general, a powerful tool for specifying layouts +L+ and behaviors of graphical objects, and have been applied +L+ to 2D graphical user interfaces and specialized 3D graph- +L+ ics packages. In this paper, we present Chorus3D, a geo- +L+ metric constraint library for 3D graphical applications. It +L+ enables programmers to use geometric constraints for vari- +L+ ous purposes such as geometric layout, constrained dragging, +L+ and inverse kinematics. Its novel feature is to handle scene +L+ graphs by processing coordinate transformations in geomet- +L+ ric constraint satisfaction. We demonstrate the usefulness of +L+ Chorus3D by presenting sample constraint-based 3D graph- +L+ ical applications. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> geometric constraints, constraint satisfaction, geometric lay- +L+ out, 3D graphics, scene graphs +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Recent advances in commodity hardware have enabled fast +L+ high-quality 3D graphics on personal computers. Also, soft- +L+ ware technologies such as VRML and Java 3D have made the +L+ development of 3D graphical applications easier. However, +L+ most of such technologies mainly focus on rendering aspects +L+ of 3D graphics, and do not sufficiently support layout and +L+ behavior aspects. +L+ Constraints are, in general, a powerful tool for specifying +L+ layouts and behaviors of graphical objects. It is widely +L+ recognized that constraints facilitate describing geometric +L+ layouts and behaviors of diagrams in 2D graphical user in- +L+ terfaces such as drawing editors, and therefore constraint +L+ solvers for this purpose have been extensively studied [3, 7, +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to distribute to lists, requires prior specific +L+ permission and/or fee. +L+ </SectLabel_copyright> <SectLabel_note> Int. Symp. on Smart Graphics, June 11-13, 2002, Hawthorne, NY, USA. +L+ Copyright 2002 ACM 1-58113-555-6/02/0600...$5.00 +L+ </SectLabel_note> <SectLabel_bodyText> 8, 9, 11, 12, 13, 17, 18]. Also, many specialized 3D graph- +L+ ics packages enable the specification of object layouts and +L+ behaviors by using constraints or similar functions. +L+ It is natural to consider that various 3D graphical applica- +L+ tions can also be enhanced by incorporating constraints. It +L+ might seem sufficient for this purpose to modify existing 2D +L+ geometric constraint solvers to support 3D geometry. It is, +L+ however, insufficient in reality because of the essential dif- +L+ ference between the ways of specifying 2D and 3D graphics; +L+ typical 2D graphics handles only simple coordinate systems, +L+ whereas most 3D graphics requires multiple coordinate sys- +L+ tems with complex relations such as rotations to treat scene +L+ graphs. It means that we need to additionally support coor- +L+ dinate transformations in 3D geometric constraint solvers. +L+ In this paper, we present Chorus3D, a geometric constraint +L+ library for 3D graphical applications. The novel feature of +L+ Chorus3D is to handle scene graphs by processing coordi- +L+ nate transformations in geometric constraint satisfaction. +L+ We have realized Chorus3D by adding this feature to our +L+ previous 2D geometric constraint library Chorus [13]. +L+ Another important point of Chorus3D is that it inherits from +L+ Chorus the capability to handle “soft” constraints with hier- +L+ archical strengths or preferences (i.e., constraint hierarchies +L+ [7]), which are useful for specifying default layouts and be- +L+ haviors of graphical objects. It determines solutions so that +L+ they satisfy as many strong constraints as possible, leaving +L+ weaker inconsistent constraints unsatisfied. +L+ Chorus3D also inherits from Chorus a module mechanism +L+ which allows user-defined kinds of geometric constraints. +L+ This feature enables programmers to use geometric con- +L+ straints for various purposes including the following: +L+ </SectLabel_bodyText> <SectLabel_listItem> Geometric layout: A typical use of Chorus3D is to lay +L+ out graphical objects. For example, it allows putting +L+ objects parallel or perpendicular to others without re- +L+ quiring predetermined positioning parameters. Also, it +L+ provides constraint-based general graph layout based +L+ on the spring model [14]. +L+ Constrained dragging: Chorus3D enables dragging ob- +L+ jects with positioning constraints. For example, it +L+ can constrain a dragged object to be on the surface +L+ of a sphere. Constrained dragging is important for 3D +L+ graphics because it provides a sophisticated way to ac- +L+ </SectLabel_listItem> <SectLabel_page> 94 +L+ </SectLabel_page> <SectLabel_listItem> Translation: A translation transformation is characterized +L+ with three variables tx, tr, and tz, and specifies the +L+ translation of vector (tx,tr, tz). +L+ Rotation: A rotation transformation is parameterized with +L+ four variables rx, rr, rz, and rw, and specifies the ro- +L+ tation of angle rw about the axis (rx, rr, rz). +L+ Scale: A scale transformation is represented with three +L+ variables sx, sr, and sz, and specifies the axis-wise +L+ scale (sx, sr, sz) about the origin. +L+ </SectLabel_listItem> <SectLabel_bodyText> We can express many practically useful transformations by +L+ using such elemental ones. In fact, any transformations rep- +L+ resented with Transform nodes in VRML can be realized by +L+ combining these kinds of transformations [4]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. CONSTRAINT FRAMEWORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we briefly describe our framework for han- +L+ dling constraints. We base it on the framework for the 2D +L+ version of the Chorus constraint solver. See [13] for further +L+ detail. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Problem Formulation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We first present the mathematical formulation for modeling +L+ constraints and constraint systems. In the following, we +L+ write x to represent a variable vector (x1, x2, ... , xn) of +L+ n variables, and also v to indicate a variable value vector +L+ (v1, v2, ... , vn) of n real numbers (vi expresses the value of +L+ xi). +L+ To support various geometric constraints in a uniform man- +L+ ner, we adopt error functions as a means of expressing con- +L+ straints. An error function e(x) is typically associated with +L+ a single arithmetic constraint, and is defined as a func- +L+ tion from variable value vectors to errors expressed as non- +L+ negative real numbers; that is, e(v) gives the error of the +L+ associated constraint for v. An error function returns a zero +L+ if and only if the constraint is exactly satisfied. For example, +L+ e(x) = (xi — xj)2 can be used for the constraint xi = xj. +L+ We assume that, for each e(x), its gradient is known: +L+ </SectLabel_bodyText> <SectLabel_equation> De(x) =	�ae(x) ae(x) ae(x) ax1,ax2 ,...,axn ) +L+ </SectLabel_equation> <SectLabel_bodyText> In the same way as constraint hierarchies [7], constraint sys- +L+ tems in our framework can be divided into levels consisting +L+ of constraints with equal strengths. Constraints with the +L+ strongest preference are said to be required (or hard), and +L+ are guaranteed to be always satisfied (if it is impossible, +L+ there will be no solution). By contrast, constraints with +L+ weaker preferences are said to be preferential (or soft), and +L+ may be relaxed if they conflict with stronger constraints. +L+ Solutions to constraint systems are defined as follows: let +L+ ei,j(x) be the error function of the j-th constraint (1 < j < +L+ mi) at strength level i (0 < i < l); then solutions v are +L+ determined with the optimization problem +L+ minimize	E(v) subject to e0,j (v) = 0 (1 < j < m0) +L+ </SectLabel_bodyText> <SectLabel_none> v +L+ </SectLabel_none> <SectLabel_bodyText> commodate ordinary mouse dragging to 3D spaces. +L+ Inverse kinematics: Chorus3D is applicable to inverse +L+ kinematics, which is a problem of finding desired con- +L+ figurations of “articulated” objects [1, 20]. It allows +L+ the specification of articulated objects by using coor- +L+ dinate transformations, and can automatically calcu- +L+ late the parameters of the transformations that satisfy +L+ constraints. This method is also applicable to camera +L+ control by aiming at a possibly moving target object. +L+ In this paper, we demonstrate the usefulness of Chorus3D +L+ by presenting sample constraint-based 3D graphical appli- +L+ cations. +L+ This paper is organized as follows: We first present our ap- +L+ proach to the use of constraints for 3D graphics. Second, +L+ we describe our basic framework of constraints. Next, we +L+ present a method for processing coordinate transformations +L+ in our framework. We then provide the implementation of +L+ Chorus3D, and demonstrate examples of using constraints +L+ in 3D graphics. After giving related work and discussion, we +L+ mention the conclusions and future work of this research. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. OUR APPROACH +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this research, we integrate geometric constraints with 3D +L+ graphics. Basically, we realize this by extending our previ- +L+ ous 2D geometric constraint solver Chorus [13] to support +L+ 3D geometry. However, as already mentioned, it is not a +L+ straightforward task because 3D graphics typically requires +L+ handling scene graphs with hierarchical structures of coor- +L+ dinate systems, which is not covered by the 2D version of +L+ the Chorus constraint solver. +L+ To support hierarchies of coordinate systems, we introduce +L+ the following new model of constraints: +L+ </SectLabel_bodyText> <SectLabel_listItem> Point variables: Each point variable (which consists of +L+ three real-valued constrainable variables) is associated +L+ with one coordinate system, and its value is expressed +L+ as local coordinates. +L+ Geometric constraints: Geometric constraints on point +L+ variables are evaluated by using the world coordinates +L+ of the point variables (they can also refer to 1D vari- +L+ ables for, e.g., distances and angles by using their val- +L+ ues directly). A single constraint can refer to point +L+ variables belonging to different coordinate systems. +L+ Coordinate transformations: Parameters of coordinate +L+ transformations are provided as constrainable vari- +L+ ables, and the solver is allowed to change the param- +L+ eters of transformations to appropriately satisfy given +L+ constraints. +L+ </SectLabel_listItem> <SectLabel_bodyText> With this model, we can gain the benefit of the easy mainte- +L+ nance of geometric relations by using constraints, as well as +L+ the convenience of modeling geometric objects by employing +L+ scene graphs. +L+ In our actual implementation, we provide the following three +L+ elemental kinds of coordinate transformations: +L+ </SectLabel_bodyText> <SectLabel_page> 95 +L+ </SectLabel_page> <SectLabel_bodyText> where E is an objective function defined as +L+ </SectLabel_bodyText> <SectLabel_equation> wiei,j (x) +L+ </SectLabel_equation> <SectLabel_bodyText> in which wi indicates the weight associated with strength i, +L+ and the relation w1 » w2 » . . . » wl holds. In this formu- +L+ lation, level 0 corresponds to required constraints, and the +L+ others to preferential ones. Intuitively, more weighted (or +L+ stronger) preferential constraints should be more satisfied. +L+ Our framework simulates constraint hierarchies. Particu- +L+ larly, if the squares of constraint violations are used to com- +L+ pute error functions, a system in our framework will obtain +L+ approximate solutions to the similar hierarchy solved with +L+ the criterion least-squares-better [3, 17]. The largest differ- +L+ ence is that a system in our framework slightly considers a +L+ weak constraint inconsistent with a stronger satisfiable one +L+ in computing its solutions, while the similar hierarchy would +L+ discard such a weak one. +L+ Our actual implementation of the Chorus3D constraint +L+ solver provides four external strengths required, strong, +L+ medium, and weak as well as two internal strengths very +L+ strong (used to approximately handle required nonlinear +L+ or inequality constraints) and very weak (exploited to make +L+ new solutions as close to previous ones as possible). It typ- +L+ ically assigns weights 324, 323, 322, 321, and 1 to strengths +L+ very strong, strong, medium, weak, and very weak respec- +L+ tively. These weights were determined according to the pre- +L+ cision of the actual numerical algorithm (described in the +L+ next subsection). To know how much these weights affect +L+ solutions, suppose a system of strong constraint x = 0 and +L+ medium one x = 100. Then the unique solution will be ob- +L+ tained as x = 3.0303 . . . (= 100/33). Thus the difference of +L+ strengths is obvious. According to our actual experience, +L+ this precision allows us to discriminate constraint strengths +L+ in most graphical applications. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To actually find solutions to constraint systems presented +L+ above, we need to solve their corresponding optimization +L+ problems. For this purpose, we designed a constraint sat- +L+ isfaction algorithm by combining a numerical optimization +L+ technique with a genetic algorithm. It uses numerical op- +L+ timization to find local solutions, while it adopts a genetic +L+ algorithm to search for global solutions. +L+ For numerical optimization, we mainly use the quasi-Newton +L+ method based on Broyden-Fletcher-Goldfarb-Sahnno updat- +L+ ing formula [2, 6], which is a fast iterative technique that +L+ exhibits superlinear convergence. Since it excludes fruit- +L+ less searches by utilizing its history, it is usually faster than +L+ straightforward Newton’s method. +L+ We introduced a genetic algorithm to alleviate the problem +L+ that some kinds of geometric constraints suffer from local op- +L+ timal but global non-optimal solutions [11, 16]. Generally, +L+ a genetic algorithm is a stochastic search method that re- +L+ peatedly transforms a population of potential solutions into +L+ another next-generation population [10, 15]. We typically +L+ necessitate it only for computing initial solutions; in other +L+ words, we can usually re-solve modified constraint systems +L+ without the genetic algorithm, only by applying numerical +L+ optimization to previous solutions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. PROCESSING COORDINATE +L+ TRANSFORMATIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we propose a method for integrating coordi- +L+ nate transformations with our constraint framework. +L+ As already mentioned, we use world coordinates of points +L+ to evaluate 3D geometric constraints. A naive method for +L+ this is to duplicate point variables in all ancestor coordinate +L+ systems, and then to impose required constraints that rep- +L+ resent coordinate transformations between the point vari- +L+ ables. However, this method requires an optimization rou- +L+ tine supporting required nonlinear constraints, which lim- +L+ its the availability of actual techniques (in fact, we cannot +L+ use the quasi-Newton method for this purpose). Also, this +L+ method tends to yield many variables and constraints, and +L+ therefore requires an extra amount of memory. +L+ Below we propose a more widely applicable method for han- +L+ dling coordinate transformations. Its characteristic is to +L+ hide transformations from optimization routines, which is +L+ realized by embedding transformations in error functions. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Model +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To begin with, we introduce another variable vector x' = +L+ (x'1, x'2, ... ,x' n), which is created by replacing variables for +L+ local coordinates of 3D points in x with the corresponding +L+ ones for world coordinates (1D variables remain the same). +L+ We can mathematically model this process as follows: Con- +L+ sider the sequence of the s transformations +L+ </SectLabel_bodyText> <SectLabel_equation> t0	t1	s 2	ts 1 +L+ y0 (= x)� y1 �... t y3-1 —� y3 (= x') +L+ </SectLabel_equation> <SectLabel_bodyText> where y0 and y3 are equal to x and x' respectively, each +L+ yk (1 < k < s — 1) is an “intermediate” vector, and each tk +L+ (0 < k < s — 1) is a function that transforms yk into yk+1. +L+ Intuitively, tk corresponds to a coordinate transformation, +L+ and transforms related point variables from its source co- +L+ ordinate system into its destination system. It should be +L+ noted that, although transformations are, in general, hier- +L+ archical (or tree-structured), we can always find such a linear +L+ sequence by “serializing” them in an appropriate order. +L+ By using such transformations, we can compute x' as fol- +L+ lows: +L+ </SectLabel_bodyText> <SectLabel_equation> x' = t3-1(t3-2(...(t1(t0(x))) ... )) = t(x) +L+ </SectLabel_equation> <SectLabel_bodyText> where t is defined as the composition of all the elemental +L+ transformations. In the following description, we write yk,i +L+ to denote the i-th element of yk, and also tk,i to represent +L+ the i-th element of tk; that is, +L+ </SectLabel_bodyText> <SectLabel_equation> yk+1 = (yk+1,1 , yk+1,2, ... , yk+1,n) +L+ = (tk,1(yk),tk,2(yk), ..., tk,n(yk)) = tk(yk). +L+ </SectLabel_equation> <SectLabel_subsectionHeader> 4.2 Method +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Geometric constraints are evaluated by using world coordi- +L+ nates of points, which means that their error functions are +L+ </SectLabel_bodyText> <SectLabel_equation> Mi +L+ E +L+ j=1 +L+ E(x) = +L+ �l +L+ i=1 +L+ </SectLabel_equation> <SectLabel_page> 96 +L+ </SectLabel_page> <SectLabel_bodyText> defined as e(x'). Using the composed transformations, we	parameter of the coordinate transformation), we have +L+ can evaluate them as	yk,i = xi, which means that we have atk,j(yk)/axi. +L+ e(x') = e(t(x)).	Therefore, we can compute ae(x')/axi immediately. +L+ Importantly, we can efficiently realize this computation by +L+ applying only necessary transformations to actually used +L+ variables. +L+ We also need to compute the gradient of e(t(x)), i.e., +L+ </SectLabel_bodyText> <SectLabel_equation> �ae(t(x)) ae(t(x)) ae(t(x)) +L+ ax1 , ax2 , ..., axn ) . +L+ </SectLabel_equation> <SectLabel_bodyText> Basically, we can decompose each partial derivative +L+ ae(t(x))/axi into primitive expressions by repeatedly us- +L+ ing the chain rule. However, we should avoid the simple +L+ application of the chain rule since it would result in a large +L+ number of expressions. +L+ Instead, we perform a controlled way of decomposing such +L+ partial derivatives; it appropriately arranges the chain rule +L+ to restrict the computation to only necessary components. +L+ First, we decompose ae(t(x))/axi as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> ats-1,j,(ys-1) +L+ axi +L+ ae(x')	ats-1,j,(ys-1)ats-2,js_1(ys-2) +L+ axj,Eays-1,js_1	axi +L+ js_1 +L+ ats-2,js_1(ys-2) +L+ axi +L+ ae(x') ats-2,js_1(ys-2) +L+ ays-1,js_1	axi	. +L+ </SectLabel_equation> <SectLabel_bodyText> Note that each ae(x')/ax'j, is given by the defini- +L+ tion of the geometric constraint, and also that each +L+ ats-1,j,(ys-1)/ays-1,js_1 is a partial derivative in the gra- +L+ dient of a single coordinate transformation ts-1. Thus we +L+ can obtain each ae(x')/ays-1 ,js_1. Also, by repeating this +L+ process, we can compute, for each k, +L+ </SectLabel_bodyText> <SectLabel_equation> atk-1,jk (yk-1) +L+ </SectLabel_equation> <SectLabel_bodyText> and finally achieve +L+ </SectLabel_bodyText> <SectLabel_equation> ae(t(x)) +L+ axi +L+ where each at0,j1(x)/axi is a component of the gradient of +L+ </SectLabel_equation> <SectLabel_bodyText> t0. Therefore, ae(t(x))/axi is now determined. +L+ Furthermore, we can considerably reduce the number of the +L+ computations of ae(x')/ayk,jk in practice. We can make the +L+ following observations about the above computation: +L+ 9 For each variable xj,, ae(x')/ax'j, can be non-zero only +L+ if xj,is actually needed to evaluate the designated con- +L+ straint. +L+ 9 If xi is originated in the coordinate system associated +L+ with tk (that is, xi is either a local coordinate or a +L+ These observations reveal that we need to transfer a partial +L+ derivative ae(x')/ayk,j to the next step only when xj rep- +L+ resents a really necessary coordinate that has not reached +L+ its local coordinate system. Also, since we can handle each +L+ necessary point independently, we can implement this pro- +L+ cess with a linear recursive function that hands over only +L+ three derivatives ae(x')/ayk,j at each recursive call. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. IMPLEMENTATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We implemented the proposed method by developing a con- +L+ straint solver called Chorus3D, which is a 3D extension to +L+ our previous 2D geometric constraint solver Chorus [13]. We +L+ constructed Chorus3D as a C++ class library, and also de- +L+ veloped a native method interface to make it available to +L+ Java programs. +L+ Chorus3D allows programmers to add a new kind of arith- +L+ metic constraints (e.g., Euclidean geometric constraints) by +L+ constructing a new constraint class with a method that eval- +L+ uates their error functions. Also, programmers can intro- +L+ duce a new kind of non-arithmetic (or pseudo) constraints +L+ (for, e.g., general graph layout) by developing a new evalua- +L+ tion module which computes an “aggregate” error function +L+ for a given set of constraints. +L+ Chorus3D currently provides linear equality, linear inequal- +L+ ity, edit (update a variable value), stay (fix a variable value), +L+ Euclidean geometric constraints (for, e.g., parallelism, per- +L+ pendicularity, and distance equality), and graph layout con- +L+ straints based on the spring model [14]. Linear equality/ +L+ inequality constraints can refer to only 1D variables (includ- +L+ ing elements of 3D point variables), while edit and stay con- +L+ straints can be associated with 1D and 3D point variables. +L+ Euclidean geometric constraints typically refer to point vari- +L+ ables although they sometimes require 1D variables for an- +L+ gles and distances. Each graph layout constraint represents +L+ a graph edge, and refers to two point variables as its asso- +L+ ciated graph nodes. As stated earlier, constraints on such +L+ point variables are evaluated by using world coordinates of +L+ the points. Also, a single constraint can refer to point vari- +L+ ables belonging to different coordinate systems. +L+ The application programming interface of Chorus3D is a +L+ natural extension to that of Chorus, which provides a certain +L+ compatibility with a recent linear solver called Cassowary +L+ [3]; in a similar way to Cassowary and Chorus, Chorus3D +L+ allows programmers to process constraint systems by cre- +L+ ating variables and constraints as objects, and by adding/ +L+ removing constraint objects to/from the solver object. In +L+ addition, Chorus3D handles coordinate transformations as +L+ objects, and presents an interface for arranging them hier- +L+ archically. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. EXAMPLES +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we present three examples to demonstrate +L+ how to incorporate geometric constraints into 3D graphics +L+ by using the Chorus3D constraint solver. All the examples +L+ are implemented in Java by using Java 3D as a graphics +L+ </SectLabel_bodyText> <SectLabel_equation> De(t(x)) = +L+ ae(t(x)) +L+ axi +L+ =E +L+ j, +L+ ae(x') +L+ ax'j, +L+ =E +L+ j, +L+ = E +L+ js_1 +L+ = E +L+ js_1 +L+ I +L+ ats1,j, (ys-1) +L+ ae(x') +L+ E +L+ ays-1,js_1 +L+ j, +L+ ax'j, +L+ I +L+ ae(t(x)) +L+ axi +L+ =E	ae(x') +L+ jk +L+ 	ayk,jk +L+ =E	ae(x') +L+ j1 +L+ 	ay1,j1 +L+ axi +L+ at0,j1(x) +L+ axi +L+ </SectLabel_equation> <SectLabel_page> 97 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: A 3D geometric layout of a general graph +L+ structure. +L+ Figure 2: Dragging an object constrained to be on +L+ a sphere. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> programming interface as well as the native method interface +L+ with Chorus3D. We also provide computation times taken +L+ for constraint satisfaction in these examples. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.1 Graph Layout +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The first example is an application which lays out a set +L+ of points with a general graph structure in a 3D space as +L+ shown in Figure 1. This application also allows a user to +L+ drag graph nodes with a mouse.' The used graph layout +L+ technique is based on a 3D extension to the spring model +L+ [14]. This kind of 3D graph layout is practically useful to +L+ information visualization, and has actually been adopted in +L+ a certain system [19]. +L+ The constraint system of this graph layout consists of 26 +L+ point variables (i.e., 78 real-valued variables), 31 graph lay- +L+ out constraints, and three linear equality constraints for fix- +L+ ing one of the point variables at the origin. When executed +L+ on an 866 MHz Pentium III processor running Linux 2.2.16, +L+ Chorus3D obtained an initial solution in 456 milliseconds. It +L+ performed constraint satisfaction typically within 250 mil- +L+ liseconds to reflect the user’s dragging a graph node. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.2 Constrained Dragging +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The second example is an application which allows a user +L+ to drag an object constrained to be on another spherical +L+ object. Figure 2 depicts this application, where the smaller +L+ solid spherical object is constrained to be on the surface of +L+ the larger wireframe one. The application declares a strong +L+ Euclidean geometric constraint which specifies a constant +L+ distance between the centers of these objects. When the +L+ user tries to drag the smaller object with a mouse, the appli- +L+ cation imposes another medium Euclidean constraint which +L+ collinearly locates the viewpoint, the 3D position of the +L+ mouse cursor (which is considered to be on the screen), and +L+ </SectLabel_bodyText> <SectLabel_footnote> 'Unlike constrained dragging in the next example, this +L+ mouse operation is simply implemented with Java 3D’s +L+ PickMouseBehavior classes. +L+ Sphere +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 3: Implementation of constrained dragging. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the center of the dragged object as shown in Figure 3. This +L+ collinearity constraint reflects the motion of the mouse in +L+ the position of the dragged object. Since the collinearity +L+ constraint is weaker than the first Euclidean constraint, the +L+ user cannot drag the smaller object to the outside of the +L+ larger sphere. +L+ The application initially declares one Euclidean geometric +L+ constraint on two point variables, and solved it in 1 mil- +L+ lisecond on the same computer as the first example. When +L+ the user tries to drag the smaller object, it adds another +L+ Euclidean constraint as well as two edit constraints for the +L+ viewpoint and mouse position. The solver maintained this +L+ constraint system usually within 2 milliseconds. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.3 Inverse Kinematics +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The final example applies inverse kinematics to a virtual +L+ robot arm by using constraints. Unlike the previous ex- +L+ amples, it takes advantage of coordinate transformations to +L+ express its constraint system. +L+ </SectLabel_bodyText> <SectLabel_figure> Mouse cursor which +L+ is on the screen +L+ Viewpoint +L+ Distance +L+ constraint +L+ Collinearity +L+ constraint +L+ Object which is on +L+ the sphere surface +L+ Screen +L+ </SectLabel_figure> <SectLabel_page> 98 +L+ </SectLabel_page> <SectLabel_figure> (a)	(b)	(c) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: A robot arm application which performs inverse kinematics. +L+ </SectLabel_figureCaption> <SectLabel_figure> (d)	(e)	(f) +L+ </SectLabel_figure> <SectLabel_bodyText> As illustrated in Figure 4(a), the robot arm consists of four +L+ parts called a base, a shoulder, an upper arm, and a forearm. +L+ Constraint satisfaction for inverse kinematics is performed +L+ to position its hand (the end of the forearm) at the target +L+ object if possible, or otherwise to make it maximally close +L+ to the target. Figures 4(b)–(f) show the movement of the +L+ robot arm. In Figures 4(b)–(e), its hand is positioned at +L+ the exact location of the target by using appropriate angles +L+ of its joints. By contrast, in Figure 4(f), the hand cannot +L+ reach the target, and therefore the arm is extended toward +L+ the target instead. +L+ Figure 5 describes the constraint program used in the robot +L+ arm application. After constructing a constraint solver +L+ s, it creates six coordinate transformations shldrTTfm, +L+ shldrRTfm, uarmTTfm, uarmRTfm, farmTTfm, and farmRTfm. +L+ Here the rotation angle parameters of the rotation trans- +L+ formations shldrRTfm, uarmRTfm, and farmRTfm will actu- +L+ ally work as variables that can be altered by the solver. +L+ Next, it generates a point variable handPos to represent +L+ the position of the hand, and then suggests the target po- +L+ sition to the hand by using a preferential edit constraint +L+ editHandPos. Finally, executing the solver, it obtains the +L+ desired angles shldrAngle, uarmAngle, and farmAngle of +L+ the rotation transformations. These angles will be passed +L+ to the Java 3D library to render the properly configured +L+ robot arm. +L+ This program generates a constraint system which contains +L+ three translation and three rotation transformations, one ex- +L+ plicit point variable as well as six point variables and three +L+ 1D variables for coordinate transformations, and one edit +L+ constraint. The solver found an initial solution to this sys- +L+ tem in 18 milliseconds, and obtained each new solution for +L+ a frame update typically within 10 milliseconds. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. RELATED WORK AND DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> There has been work on integrating constraints or similar +L+ functions with 3D graphics languages to facilitate the spec- +L+ ification of graphical objects. For example, we can view the +L+ event routing mechanism in VRML [4] as a limited form of +L+ one-way propagation constraints. Also, there is an attempt +L+ to extend VRML by introducing one-way propagation and +L+ finite-domain combinatorial constraints [5]. However, they +L+ cannot handle more powerful simultaneous nonlinear con- +L+ straints such as Euclidean geometric constraints. +L+ Although many constraint solvers have been developed in +L+ </SectLabel_bodyText> <SectLabel_page> 99 +L+ </SectLabel_page> <SectLabel_figure> // constraint solver +L+ s = new C3Solver(); +L+ // translation transformation for the shoulder: fixed to (0, .1, 0) +L+ shldrTTfm = new C3TranslateTransform(new C3Domain3D(0, .1, 0)); +L+ s.add(shldrTTfm); // shldrTTfm is parented by the world coordinate system +L+ // rotation transformation for the shoulder: axis fixed to (0, 1, 0); angle ranging over [-10000, 10000] +L+ shldrRTfm = new C3RotateTransform(new C3Domain3D(0, 1, 0), new C3Domain(-10000, 10000)); +L+ s.add(shldrRTfm, shldrTTfm); // shldrRTfm is parented by shldrTTfm +L+ // translation transformation for the upper arm: fixed to (0, .1, 0) +L+ uarmTTfm = new C3TranslateTransform(new C3Domain3D(0, .1, 0)); +L+ s.add(uarmTTfm, shldrRTfm); // uarmTTfm is parented by shldrRTfm +L+ // rotation transformation for the upper arm: axis fixed to (0, 0, 1); angle ranging over [-1.57,1.57] +L+ uarmRTfm = new C3RotateTransform(new C3Domain3D(0, 0, 1), new C3Domain(-1.57, 1.57)); +L+ s.add(uarmRTfm, uarmTTfm); // uarmRTfm is parented by uarmTTfm +L+ // translation transformation for the forearm: fixed to (0, .5, 0) +L+ farmTTfm = new C3TranslateTransform(new C3Domain3D(0, .5, 0)); +L+ s.add(farmTTfm, uarmRTfm); // farmTTfm is parented by uarmRTfm +L+ // rotation transformation for the forearm: axis fixed to (0, 0, 1); angle ranging over [-3.14, 0] +L+ farmRTfm = new C3RotateTransform(new C3Domain3D(0, 0, 1), new C3Domain(-3.14, 0)); +L+ s.add(farmRTfm, farmTTfm); // farmRTfm is parented by farmTTfm +L+ // variable for the hand’s position, associated with farmRTfm and fixed to (0, .5, 0) +L+ handPos = new C3Variable3D(farmRTfm, new C3Domain3D(0, .5, 0)); +L+ // medium-strength edit constraint for the hand’s position +L+ editHandPos = new C3EditConstraint(handPos, C3.MEDIUM); +L+ s.add(editHandPos); +L+ // suggest the hand being located at the target’s position +L+ editHandPos.set(getTargetWorldCoordinates() ); +L+ // solve the constraint system +L+ s.solve(); +L+ // get solutions +L+ double shldrAngle = shldrRTfm.rotationAngle().value() ; +L+ double uarmAngle = uarmRTfm.rotationAngle().value(); +L+ double farmAngle = farmRTfm.rotationAngle().value(); +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: Constraint program for the robot arm application. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the field of graphical user interfaces [3, 7, 11, 12, 13, 17, 18], +L+ most of them do not provide special treatment for 3D graph- +L+ ics. In general, the role of nonlinear geometric constraints +L+ is more important in 3D applications than in 2D interfaces. +L+ Most importantly, 3D graphics usually requires rotations of +L+ objects which are rarely used in 2D interfaces. The main +L+ reason is that we often equally treat all “horizontal” direc- +L+ tions in a 3D space even if we may clearly distinguish them +L+ from “vertical” directions. Therefore, nonlinear constraint +L+ solvers are appropriate for 3D applications. In addition, co- +L+ ordinate transformations should be supported since they are +L+ typically used to handle rotations of objects. +L+ Gleicher proposed the differential approach [8, 9], which sup- +L+ ports 3D geometric constraints and coordinate transforma- +L+ tions. In a sense, it shares a motivation with Chorus3D; in +L+ addition to support for 3D graphics, it allows user-defined +L+ kinds of geometric constraints. However, it is based on a dif- +L+ ferent solution method from Chorus3D; it realizes constraint +L+ satisfaction by running virtual dynamic simulations. This +L+ difference results in a quite different behavior of solutions as +L+ well as an interface for controlling solutions. By contrast, +L+ Chorus3D provides a much more compatible interface with +L+ recent successful solvers such as Cassowary [3]. +L+ Much research on inverse kinematics has been conducted in +L+ the fields of computer graphics and robotics [1, 20]. How- +L+ ever, inverse kinematics is typically implemented as special- +L+ ized software which only provides limited kinds of geometric +L+ constraints. +L+ Chorus3D has two limitations in its algorithm: one is on the +L+ precision of solutions determined by preferential constraints; +L+ the other is on the speed of the satisfaction of large con- +L+ straint systems. These limitations are mainly caused by the +L+ treatment of multi-level preferences of constraints in addi- +L+ tion to required constraints (i.e., constraint hierarchies). Al- +L+ though many numerical optimization techniques have been +L+ proposed and implemented in the field of mathematical pro- +L+ gramming [2, 6], most of them do not handle preferential +L+ constraints. To alleviate the limitations of Chorus3D, we +L+ are pursuing a more sophisticated method for processing +L+ multi-level preferential constraints. +L+ We implemented Chorus3D as a class library which can +L+ be exploited in C++ and Java programs. However, more +L+ high-level authoring tools will also be useful for declarative +L+ approaches to 3D design. One possible direction is to ex- +L+ tend VRML [4] to support geometric constraints. Standard +L+ VRML requires scripts in Java or JavaScript to realize com- +L+ plex layouts and behaviors. By contrast, constraint-enabled +L+ VRML will cover a wider range of applications without such +L+ additional scripts. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. CONCLUSIONS AND FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we presented Chorus3D, a geometric con- +L+ straint library for 3D graphical applications. It enables pro- +L+ grammers to use geometric constraints for various purposes +L+ </SectLabel_bodyText> <SectLabel_page> 100 +L+ </SectLabel_page> <SectLabel_bodyText> such as geometric layout, constrained dragging, and inverse +L+ kinematics. Its novel feature is to handle scene graphs +L+ by processing coordinate transformations in geometric con- +L+ straint satisfaction. +L+ Our future work includes the development of other kinds of +L+ geometric constraints to further prove the usefulness of our +L+ approach. In particular, we are planning to implement non- +L+ overlapping constraints [13] in Chorus3D so that we can use +L+ it for the collision resolution of graphical objects. Another +L+ future direction is to improve Chorus3D in the scalability +L+ and accuracy of constraint satisfaction. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] Badler, N. I., Phillips, C. B., and Webber, B. L. +L+ Simulating Humans: Computer Graphics, Animation, +L+ and Control. Oxford University Press, Oxford, 1993. +L+ [2] Bertsekas, D. P. Nonlinear Programming, 2nd ed. +L+ Athena Scientific, 1999. +L+ [3] Borning, A., Marriott, K., Stuckey, P., and Xiao, Y. +L+ Solving linear arithmetic constraints for user interface +L+ applications. In Proc. ACM UIST, 1997, 87–96. +L+ [4] Carey, R., Bell, G., and Marrin, C. The Virtual +L+ Reality Modeling Language (VRML97). ISO/IEC +L+ 14772-1:1997, The VRML Consortium Inc., 1997. +L+ [5] Diehl, S., and Keller, J. VRML with constraints. In +L+ Proc. Web3D-VRML, ACM, 2000, 81–86. +L+ [6] Fletcher, R. Practical Methods of Optimization, +L+ 2nd ed. John Wiley & Sons, 1987. +L+ [7] Freeman-Benson, B. N., Maloney, J., and Borning, A. +L+ An incremental constraint solver. Commun. ACM 33, +L+ 1 (1990), 54–63. +L+ [8] Gleicher, M. A graphical toolkit based on differential +L+ constraints. In Proc. ACM UIST, 1993, 109–120. +L+ [9] Gleicher, M. A differential approach to graphical +L+ manipulation (Ph.D. thesis). Tech. Rep. +L+ CMU-CS-94-217, Sch. Comput. Sci. Carnegie Mellon +L+ Univ., 1994. +L+ [10] Herrera, F., Lozano, M., and Verdegay, J. L. Tackling +L+ real-coded genetic algorithms: Operators and tools for +L+ behavioural analysis. Artif. Intell. Rev. 12, 4 (1998), +L+ 265–319. +L+ [11] Heydon, A., and Nelson, G. The Juno-2 +L+ constraint-based drawing editor. Research Report +L+ 131a, Digital Systems Research Center, 1994. +L+ [12] Hosobe, H. A scalable linear constraint solver for user +L+ interface construction. In Principles and Practice of +L+ Constraint Programming—CP2000, vol. 1894 of +L+ LNCS, Springer, 2000, 218–232. +L+ [13] Hosobe, H. A modular geometric constraint solver for +L+ user interface applications. In Proc. ACM UIST, 2001, +L+ 91–100. +L+ [14] Kamada, T., and Kawai, S. An algorithm for drawing +L+ general undirected graphs. Inf. Process. Lett. 31, 1 +L+ (1989), 7–15. +L+ [15] Kitano, H., Ed. Genetic Algorithms. Sangyo-Tosho, +L+ 1993. In Japanese. +L+ [16] Kramer, G. A. A geometric constraint engine. Artif. +L+ Intell. 58, 1–3 (1992), 327–360. +L+ [17] Marriott, K., Chok, S. S., and Finlay, A. A tableau +L+ based constraint solving toolkit for interactive +L+ graphical applications. In Principles and Practice of +L+ Constraint Programming—CP98, vol. 1520 of LNCS, +L+ Springer, 1998, 340–354. +L+ [18] Sannella, M. Skyblue: A multi-way local propagation +L+ constraint solver for user interface construction. In +L+ Proc. ACM UIST, 1994,137–146. +L+ [19] Takahashi, S. Visualizing constraints in visualization +L+ rules. In Proc. CP2000 Workshop on Analysis and +L+ Visualization of Constraint Programs and Solvers, +L+ 2000. +L+ [20] Zhao, J., and Badler, N. I. Inverse kinematics +L+ positioning using nonlinear programming for highly +L+ articulated figures. ACM Trans. Gr. 13, 4 (1994), +L+ 313–336. +L+ </SectLabel_reference> <SectLabel_page> 101 +L+ </SectLabel_page>
<SectLabel_title> A Machine Learning Based Approach for Table Detection +L+ on The Web +L+ </SectLabel_title> <SectLabel_author> Yalin Wang +L+ </SectLabel_author> <SectLabel_affiliation> Intelligent Systems Laboratory +L+ Dept. of Electrical Engineering +L+ Univ. of Washington +L+ </SectLabel_affiliation> <SectLabel_address> Seattle, WA 98195 US +L+ </SectLabel_address> <SectLabel_email> ylwang@u.washington.edu +L+ </SectLabel_email> <SectLabel_author> Jianying Hu +L+ </SectLabel_author> <SectLabel_affiliation> Avaya Labs Research +L+ </SectLabel_affiliation> <SectLabel_address> 233, Mount Airy Road +L+ Basking Ridge, NJ 07920 US +L+ </SectLabel_address> <SectLabel_email> jianhu@avaya.com +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Table is a commonly used presentation scheme, especially +L+ for describing relational information. However, table under- +L+ standing remains an open problem. In this paper, we con- +L+ sider the problem of table detection in web documents. Its +L+ potential applications include web mining, knowledge man- +L+ agement, and web content summarization and delivery to +L+ narrow-bandwidth devices. We describe a machine learning +L+ based approach to classify each given table entity as either +L+ genuine or non-genuine. Various features reflecting the lay- +L+ out as well as content characteristics of tables are studied. +L+ In order to facilitate the training and evaluation of our +L+ table classifier, we designed a novel web document table +L+ ground truthing protocol and used it to build a large ta- +L+ ble ground truth database. The database consists of 1,393 +L+ HTML files collected from hundreds of different web sites +L+ and contains 11,477 leaf <TABLE> elements, out of which +L+ 1,740 are genuine tables. Experiments were conducted us- +L+ ing the cross validation method and an F-measure of 95.89% +L+ was achieved. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.4.3 [Information Systems Applications]: Communi- +L+ cations Applications Information browsers +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Algorithms +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Table Detection, Layout Analysis, Machine Learning, Deci- +L+ sion tree, Support Vector Machine, Information Retrieval +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The increasing ubiquity of the Internet has brought about +L+ a constantly increasing amount of online publications. As +L+ a compact and efficient way to present relational informa- +L+ tion, tables are used frequently in web documents. Since +L+ tables are inherently concise as well as information rich, the +L+ automatic understanding of tables has many applications in- +L+ cluding knowledge management, information retrieval, web +L+ </SectLabel_bodyText> <SectLabel_copyright> Copyright is held by the author/owner(s). +L+ </SectLabel_copyright> <SectLabel_note> WWW2002, May 7–11,2002, Honolulu, Hawaii, USA. +L+ ACM 1-58113-449-5/02/0005. +L+ </SectLabel_note> <SectLabel_bodyText> mining, summarization, and content delivery to mobile de- +L+ vices. The processes of table understanding in web doc- +L+ uments include table detection, functional and structural +L+ analysis and finally table interpretation [6]. In this paper, +L+ we concentrate on the problem of table detection. The web +L+ provides users with great possibilities to use their own style +L+ of communication and expressions. In particular, people use +L+ the <TABLE> tag not only for relational information display +L+ but also to create any type of multiple-column layout to +L+ facilitate easy viewing, thus the presence of the <TABLE> +L+ tag does not necessarily indicate the presence of a relational +L+ table. In this paper, we define genuine tables to be docu- +L+ ment entities where a two dimensional grid is semantically +L+ significant in conveying the logical relations among the cells +L+ [10]. Conversely, Non-genuine tables are document entities +L+ where <TABLE> tags are used as a mechanism for grouping +L+ contents into clusters for easy viewing only. Figure 1 gives +L+ a few examples of genuine and non-genuine tables. While +L+ genuine tables in web documents could also be created with- +L+ out the use of <TABLE> tags at all, we do not consider such +L+ cases in this article as they seem very rare from our ex- +L+ perience. Thus, in this study, Table detection refers to the +L+ technique which classifies a document entity enclosed by the +L+ <TABLE></TABLE> tags as genuine or non-genuine tables. +L+ Several researchers have reported their work on web table +L+ detection [2, 10, 6, 14]. In [2], Chen et al. used heuris- +L+ tic rules and cell similarities to identify tables. They tested +L+ their table detection algorithm on 918 tables from airline in- +L+ formation web pages and achieved an F-measure of 86.50%. +L+ Penn et al. proposed a set of rules for identifying genuinely +L+ tabular information and news links in HTML documents +L+ [10]. They tested their algorithm on 75 web site front-pages +L+ and achieved an F-measure of 88.05%. Yoshida et al. pro- +L+ posed a method to integrate WWW tables according to the +L+ category of objects presented in each table [14]. Their data +L+ set contains 35,232 table tags gathered from the web. They +L+ estimated their algorithm parameters using all of table data +L+ and then evaluated algorithm accuracy on 175 of the tables. +L+ The average F-measure reported in their paper is 82.65%. +L+ These previous methods all relied on heuristic rules and were +L+ only tested on a database that is either very small [10], or +L+ highly domain specific [2]. Hurst mentioned that a Naive +L+ Bayes classifier algorithm produced adequate results but no +L+ detailed algorithm and experimental information was pro- +L+ vided [6]. +L+ We propose a new machine learning based approach for +L+ </SectLabel_bodyText> <SectLabel_page> 242 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: Examples of genuine and non-genuine tables. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> table detection from generic web documents. In particu- +L+ lar, we introduce a set of novel features which reflect the +L+ layout as well as content characteristics of tables. These +L+ features are used in classifiers trained on thousands of ex- +L+ amples. To facilitate the training and evaluation of the table +L+ classifiers, we designed a novel web document table ground +L+ truthing protocol and used it to build a large table ground +L+ truth database. The database consists of 1,393 HTML files +L+ collected from hundreds of different web sites and contains +L+ 11,477 leaf <TABLE> elements, out of which 1,740 are gen- +L+ uine tables. Experiments on this database using the cross +L+ validation method demonstrate significant performance im- +L+ provements over previous methods. +L+ The rest of the paper is organized as follows. We describe +L+ our feature set in Section 2, followed by a brief discussion +L+ of the classifiers we experimented with in Section 3. In Sec- +L+ tion 4, we present a novel table ground truthing protocol +L+ and explain how we built our database. Experimental re- +L+ sults are then reported in Section 5 and we conclude with +L+ future directions in Section 6. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. FEATURES FOR WEB TABLE +L+ DETECTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Feature selection is a crucial step in any machine learning +L+ based methods. In our case, we need to find a combination +L+ of features that together provide significant separation be- +L+ tween genuine and non-genuine tables while at the same time +L+ constrain the total number of features to avoid the curse of +L+ dimensionality. Past research has clearly indicated that lay- +L+ out and content are two important aspects in table under- +L+ standing [6]. Our features were designed to capture both of +L+ these aspects. In particular, we developed 16 features which +L+ can be categorized into three groups: seven layout features, +L+ eight content type features and one word group feature. In +L+ the first two groups, we attempt to capture the global com- +L+ position of tables as well as the consistency within the whole +L+ table and across rows and columns. The last feature looks at +L+ words used in tables and is derived directly from the vector +L+ space model commonly used in Information Retrieval. +L+ Before feature extraction, each HTML document is first +L+ parsed into a document hierarchy tree using Java Swing +L+ XML parser with W3C HTML 3.2 DTD [10]. A <TABLE> +L+ node is said to be a leaf table if and only if there are no +L+ <TABLE> nodes among its children [10]. Our experience in- +L+ dicates that almost all genuine tables are leaf tables. Thus +L+ in this study only leaf tables are considered candidates for +L+ genuine tables and are passed on to the feature extraction +L+ stage. In the following we describe each feature in detail. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.1 Layout Features +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In HTML documents, although tags like <TR> and <TD> +L+ (or <TH>) may be assumed to delimit table rows and table +L+ cells, they are not always reliable indicators of the number +L+ of rows and columns in a table. Variations can be caused +L+ by spanning cells created using <ROWSPAN> and <COLSPAN> +L+ tags. Other tags such as <BR> could be used to move con- +L+ tent into the next row. Therefore to extract layout features +L+ reliably one can not simply count the number of <TR>'s and +L+ <TD>'s. For this purpose, we maintain a matrix to record all +L+ </SectLabel_bodyText> <SectLabel_page> 243 +L+ </SectLabel_page> <SectLabel_bodyText> the cell spanning information and serve as a pseudo render- +L+ ing of the table. Layout features based on row or column +L+ numbers are then computed from this matrix. +L+ Given a table T, assuming its numbers of rows and columns +L+ are rn and cn respectively, we compute the following layout +L+ features: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Average number of columns, computed as the average +L+ number of cells per row: +L+ </SectLabel_listItem> <SectLabel_bodyText> Here LCcl is defined as: LCcl = 0.5 — D, where D = +L+ min{lcl — mil�mi,1.0}. Intuitively, LCcl measures the +L+ degree of consistency between cl and the mean cell +L+ length, with —0.5 indicating extreme inconsistency and +L+ 0.5 indicating extreme consistency. When most cells +L+ within Ri are consistent, the cumulative measure CLCi +L+ is positive, indicating a more or less consistent row. +L+ 3. Take the average across all rows: +L+ </SectLabel_bodyText> <SectLabel_equation> ci, +L+ c = +L+ 1 +L+ rn +L+ Xrn +L+ i�1 +L+ CLCr = 1 +L+ r +L+ Xr CLCi . +L+ i�1 +L+ </SectLabel_equation> <SectLabel_bodyText> where ci is the number of cells in row i, i = 1, ..., rn, +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Standard deviation of number of columns: +L+ </SectLabel_listItem> <SectLabel_equation> (ci — c) x (ci — c); +L+ </SectLabel_equation> <SectLabel_listItem> •	Average number of rows, computed as the average +L+ number of cells per column: +L+ </SectLabel_listItem> <SectLabel_bodyText> where ri is the number of cells in column i, i = 1, ..., cn, +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Standard deviation of number of rows: +L+ </SectLabel_listItem> <SectLabel_equation> (ri — r) x (ri — r). +L+ </SectLabel_equation> <SectLabel_bodyText> Since the majority of tables in web documents contain +L+ characters, we compute three more layout features based on +L+ cell length in terms of number of characters: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Average overall cell length: cl = en Pin1 cli, where en +L+ is the total number of cells in a given table and cli is +L+ the length of cell i, i = 1, ... , en, +L+ •	Standard deviation of cell length: +L+ </SectLabel_listItem> <SectLabel_equation> (cli — cl) x (cli — cl)� +L+ </SectLabel_equation> <SectLabel_listItem> •	Average Cumulative length consistency, CLC. +L+ </SectLabel_listItem> <SectLabel_bodyText> The last feature is designed to measure the cell length con- +L+ sistency along either row or column directions. It is inspired +L+ by the fact that most genuine tables demonstrate certain +L+ consistency either along the row or the column direction, +L+ but usually not both, while non-genuine tables often show +L+ no consistency in either direction. First, the average cumu- +L+ lative within-row length consistency, CLCr, is computed as +L+ follows. Let the set of cell lengths of the cells from row i be +L+ Ri, i = 1, ... , r (considering only non-spanning cells): +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Compute the mean cell length, mi, for row Ri. +L+ 2. Compute cumulative length consistency within each +L+ Ri: +L+ </SectLabel_listItem> <SectLabel_equation> CLCi = X LCcl . +L+ clERi +L+ </SectLabel_equation> <SectLabel_bodyText> After the within-row length consistency CLCr is com- +L+ puted, the within-column length consistency CLCc is com- +L+ puted in a similar manner. Finally, the overall cumulative +L+ length consistency is computed as CLC = max(CLCr, CLCc). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Content Type Features +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Web documents are inherently multi-media and has more +L+ types of content than any traditional documents. For ex- +L+ ample, the content within a <TABLE> element could include +L+ hyperlinks, images, forms, alphabetical or numerical strings, +L+ etc. Because of the relational information it needs to convey, +L+ a genuine table is more likely to contain alpha or numeri- +L+ cal strings than, say, images. The content type feature was +L+ designed to reflect such characteristics. +L+ We define the set of content types T = {Image, Form, +L+ Hyperlink, Alphabetical, Digit, Empty, Others}. Our content +L+ type features include: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	The histogram of content type for a given table. This +L+ contributes 7 features to the feature set, +L+ •	Average content type consistency, CTC. +L+ </SectLabel_listItem> <SectLabel_bodyText> The last feature is similar to the cell length consistency fea- +L+ ture. First, within-row content type consistency CTCr is +L+ computed as follows. Let the set of cell type of the cells +L+ from row i as Ti, i = 1,... , r (again, considering only non- +L+ spanning cells): +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Find the dominant type, DTi, for Ti. +L+ 2. Compute the cumulative type consistency with each +L+ row Ri, i = 1,... ,r: +L+ </SectLabel_listItem> <SectLabel_equation> CTCi = X D, +L+ ctERi +L+ </SectLabel_equation> <SectLabel_bodyText> where D = 1 if ct is equal to DTi and D = —1, other- +L+ wise. +L+ 3. Take the average across all rows: +L+ </SectLabel_bodyText> <SectLabel_equation> CTCr = 1 +L+ r +L+ </SectLabel_equation> <SectLabel_bodyText> The within-column type consistency is then computed in +L+ a similar manner. Finally, the overall cumulative type con- +L+ sistency is computed as: CTC = max(CTCr, CTCc). +L+ </SectLabel_bodyText> <SectLabel_equation> tiv +L+ dC = +L+ 1 +L+ rn +L+ Xrn +L+ i�1 +L+ ri, +L+ r= +L+ 1 +L+ rn +L+ Xcn +L+ i�1 +L+ vt uu +L+ dR = +L+ 1 +L+ cn +L+ Xcn +L+ i�1 +L+ tuuv +L+ dCL = +L+ 1 +L+ en +L+ Xen +L+ i�1 +L+ Xr CT Ci +L+ i�1 +L+ </SectLabel_equation> <SectLabel_page> 244 +L+ </SectLabel_page> <SectLabel_subsectionHeader> 2.3 Word Group Feature +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> If we treat each table as a "mini-document" by itself, ta- +L+ ble classification can be viewed as a document categoriza- +L+ tion problem with two broad categories: genuine tables and +L+ non-genuine tables. We designed the word group feature to +L+ incorporate word content for table classification based on +L+ techniques developed in information retrieval [7, 13]. +L+ After morphing [11] and removing the infrequent words, +L+ we obtain the set of words found in the training data, W. +L+ We then construct weight vectors representing genuine and +L+ non-genuine tables and compare that against the frequency +L+ vector from each new incoming table. +L+ Let 3 represent the non-negative integer set. The follow- +L+ ing functions are defined on set W. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	dfG : W —> 3, where dfG (wi) is the number of genuine +L+ tables which include word wi, i = 1, ..., 1W1; +L+ •	t f G : W —> 3, where t f G (wi) is the number of times +L+ word wi, i =1,...,1W1, appears in genuine tables; +L+ •	dfN : W —> 3, where dfN(wi) is the number of non- +L+ genuine tables which include word wi, i =1,...,1W1; +L+ •	t f N : W —> 3, where t f N (wi) is the number of times +L+ word wi, i =1,...,1W1, appears in non-genuine tables. +L+ •	t fT : W —> 3, where t fT (wi) is the number of times +L+ word wi, wi 2 W appears in a new test table. +L+ </SectLabel_listItem> <SectLabel_bodyText> To simplify the notations, in the following discussion, we +L+ will use dfGi, t fGi , df N i and t f Ni to represent dfG (wi), t f G (wi), +L+ df N (wi) and t f N (wi), respectively. +L+ Let NG, NN be the number of genuine tables and non- +L+ genuine tables in the training collection, respectively and let +L+ C = max(NG, NN). Without loss of generality, we assume +L+ NG =� 0 and NN =� 0. For each word wi in W, i = 1, ...,1W1, +L+ two weights, pGi and pNi are computed: +L+ </SectLabel_bodyText> <SectLabel_equation> N +L+ tfGilog(N� fN +1), when df Ni :A 0 +L+ tfGilog(Ni C+1), when df i = 0 +L+ tfiNlog(NNN fG +L+ G+1), when dfGi00 +L+ tfNilog(NNC+1),	when dfG=0 +L+ </SectLabel_equation> <SectLabel_bodyText> As can be seen from the formulas, the definitions of these +L+ weights were derived from the traditional t f * idf measures +L+ used in informational retrieval, with some adjustments made +L+ for the particular problem at hand. +L+ Given a new incoming table, let us denote the set includ- +L+ ing all the words in it as Wn. Since W is constructed using +L+ thousands of tables, the words that are present in both W +L+ and Wn are only a small subset of W. Based on the vector +L+ space model, we define the similarity between weight vec- +L+ tors representing genuine and non-genuine tables and the +L+ frequency vector representing the incoming table as the cor- +L+ responding dot products. Since we only need to consider the +L+ words that are present in both W and Wn, we first compute +L+ the effective word set: We = W n Wn. Let the words in +L+ We be represented as wmk, where mk,k = 1, ..., 1We1, are +L+ indexes to the words from set W = fw1, w2, ..., wIWI g. we +L+ define the following vectors: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Weight vector representing the genuine table group: +L+ </SectLabel_listItem> <SectLabel_equation> i	pGmJ +L+ GS= +L+ U +L+ </SectLabel_equation> <SectLabel_bodyText> where U is the cosine normalization term: +L+ where V is the cosine normalization term: +L+ </SectLabel_bodyText> <SectLabel_equation> NN +L+ pmk X pmk . +L+ </SectLabel_equation> <SectLabel_listItem> •	Frequency vector representing the new incoming table: +L+ </SectLabel_listItem> <SectLabel_equation> 'i	T T	T +L+ I	(tfml,tfmt,... tfT Wel I . +L+ </SectLabel_equation> <SectLabel_bodyText> Finally, the word group feature is defined as the ratio of +L+ the two dot products: +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. CLASSIFICATION SCHEMES +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Various classification schemes have been widely used in +L+ document categorization as well as web information retrieval +L+ [13, 8]. For the table detection task, the decision tree classi- +L+ fier is particularly attractive as our features are highly non- +L+ homogeneous. We also experimented with Support Vector +L+ Machines (SVM), a relatively new learning approach which +L+ has achieved one of the best performances in text catego- +L+ rization [13]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Decision Tree +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Decision tree learning is one of the most widely used and +L+ practical methods for inductive inference. It is a method +L+ for approximating discrete-valued functions that is robust +L+ to noisy data. +L+ Decision trees classify an instance by sorting it down the +L+ tree from the root to some leaf node, which provides the clas- +L+ sification of the instance. Each node in a discrete-valued de- +L+ cision tree specifies a test of some attribute of the instance, +L+ and each branch descending from that node corresponds to +L+ one of the possible values for this attribute. Continuous- +L+ valued decision attributes can be incorporated by dynami- +L+ cally defining new discrete-valued attributes that partition +L+ the continuous attribute value into a discrete set of intervals +L+ [9]. +L+ An implementation of the continuous-valued decision tree +L+ described in [4] was used for our experiments. The decision +L+ tree is constructed using a training set of feature vectors with +L+ true class labels. At each node, a discriminant threshold +L+ </SectLabel_bodyText> <SectLabel_equation> tuuv +L+ IWeI +L+ X +L+ k=1 +L+ U= +L+ pGmk X pGmk. +L+ �, +L+ pmt +L+ V +L+ , +L+ N +L+ pmlWel +L+ V +L+ iNS= pNm� +L+ V +L+ </SectLabel_equation> <SectLabel_listItem> •	Weight vector representing the non-genuine table group: +L+ </SectLabel_listItem> <SectLabel_equation> i i +L+ ,when IT . NS�= 0 +L+ i i +L+ 	1,	when IT . GS= 0 and +L+ i +L+ 	10,	when IT . +L+ i i +L+ IT . NS= 0 +L+ i +L+ NS= 0 +L+ � � +L+ IT� GS +L+ � � +L+ IT' NS +L+ i	i +L+ GS�=0and IT . +L+ ����� +L+ ���� +L+ wg = +L+ �� +L+ � +L+ G +L+ pi = +L+ I +L+ N +L+ pi = +L+ , +L+ G +L+ pmlWel +L+ U +L+ pmt +L+ U +L+ �, +L+ tuuv +L+ V= +L+ IWeI +L+ X +L+ k=1 +L+ </SectLabel_equation> <SectLabel_page> 245 +L+ </SectLabel_page> <SectLabel_bodyText> is chosen such that it minimizes an impurity value. The +L+ learned discriminant function splits the training subset into +L+ two subsets and generates two child nodes. The process is +L+ repeated at each newly generated child node until a stopping +L+ condition is satisfied, and the node is declared as a terminal +L+ node based on a majority vote. The maximum impurity +L+ reduction, the maximum depth of the tree, and minimum +L+ number of samples are used as stopping conditions. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 SVM +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Support Vector Machines (SVM) are based on the Struc- +L+ tural Risk Management principle from computational learn- +L+ ing theory [12]. The idea of structural risk minimization +L+ is to find a hypothesis h for which the lowest true error is +L+ guaranteed. The true error of h is the probability that h +L+ will make an error on an unseen and randomly selected test +L+ example. +L+ The SVM method is defined over a vector space where the +L+ goal is to find a decision surface that best separates the data +L+ points in two classes. More precisely, the decision surface by +L+ SVM for linearly separable space is a hyperplane which can +L+ be written as +L+ </SectLabel_bodyText> <SectLabel_equation> w�•x�—b=0 +L+ </SectLabel_equation> <SectLabel_bodyText> where x� is an arbitrary data point and the vector w" and +L+ the constant b are learned from training data. Let D = +L+ (yz, �xz) denote the training set, and yz E {+1, —1} be the +L+ classification for �xz, the SVM problem is to find w� and b +L+ that satisfies the following constraints: +L+ </SectLabel_bodyText> <SectLabel_equation> w� •�xz—b>+1 for yz=+1 +L+ w�•�xz—b<—1 for yz=—1 +L+ </SectLabel_equation> <SectLabel_bodyText> while minimizing the vector 2-norm of �w. +L+ The SVM problem in linearly separable cases can be effi- +L+ ciently solved using quadratic programming techniques, while +L+ the non-linearly separable cases can be solved by either in- +L+ troducing soft margin hyperplanes, or by mapping the orig- +L+ inal data vectors to a higher dimensional space where the +L+ data points become linearly separable [12, 3]. +L+ One reason why SVMs are very powerful is that they are +L+ very universal learners. In their basic form, SVMs learn lin- +L+ ear threshold functions. Nevertheless, by a simple "plug-in" +L+ of an appropriate kernel function, they can be used to learn +L+ polynomial classifiers, radial basis function (RBF) networks, +L+ three-layer sigmoid neural nets, etc. [3]. +L+ For our experiments, we used the SVMlzght system im- +L+ plemented by Thorsten Joachims.1 +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. DATA COLLECTION AND TRUTHING +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Since there are no publicly available web table ground +L+ truth database, researchers tested their algorithms in differ- +L+ ent data sets in the past [2, 10, 14]. However, their data +L+ sets either had limited manually annotated table data (e.g., +L+ 918 table tags in [2], 75 HTML pages in [10], 175 manually +L+ annotated table tags in [14]), or were collected from some +L+ specific domains (e.g., a set of tables selected from airline +L+ information pages were used in [2]). To develop our machine +L+ learning based table detection algorithm, we needed to build +L+ a general web table ground truth database of significant size. +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 http://svmlight.joachims.org +L+ </SectLabel_footnote> <SectLabel_subsectionHeader> 4.1 Data Collection +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Instead of working within a specific domain, our goal of +L+ data collection was to get tables of as many different varieties +L+ as possible from the web. To accomplish this, we composed +L+ a set of key words likely to indicate documents containing +L+ tables and used those key words to retrieve and download +L+ web pages using the Google search engine. Three directo- +L+ ries on Google were searched: the business directory and +L+ news directory using key words: {table, stock, bonds, +L+ figure, schedule, weather, score, service, results, +L+ value}, and the science directory using key words {table, +L+ results, value}. A total of 2,851 web pages were down- +L+ loaded in this manner and we ground truthed 1,393 HTML +L+ pages out of these (chosen randomly among all the HTML +L+ pages). These 1,393 HTML pages from around 200 web sites +L+ comprise our database. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Ground Truthing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> There has been no previous report on how to systemati- +L+ cally generate web table ground truth data. To build a large +L+ web table ground truth database, a simple, flexible and com- +L+ plete ground truth protocol is required. Figure 4.2(a) shows +L+ the diagram of our ground truthing procedure. We created +L+ a new Document Type Definition(DTD) which is a super- +L+ set of W3C HTML 3.2 DTD. We added three attributes for +L+ <TABLE> element, which are "tabid", "genuine table" and +L+ "table title". The possible value of the second attribute is +L+ yes or no and the value of the first and third attributes is a +L+ string. We used these three attributes to record the ground +L+ truth of each leaf <TABLE> node. The benefit of this design +L+ is that the ground truth data is inside HTML file format. +L+ We can use exactly the same parser to process the ground +L+ truth data. +L+ We developed a graphical user interface for web table +L+ ground truthing using the Java [1] language. Figure 4.2(b) +L+ is a snapshot of the interface. There are two windows. Af- +L+ ter reading an HTML file, the hierarchy of the HTML file is +L+ shown in the left window. When an item is selected in the +L+ hierarchy, the HTML source for the selected item is shown +L+ in the right window. There is a panel below the menu bar. +L+ The user can use the radio button to select either genuine +L+ table or non-genuine table. The text window is used to input +L+ table title. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Database Description +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our final table ground truth database consists of 1,393 +L+ HTML pages collected from around 200 web sites. There +L+ are a total of 14,609 <TABLE> nodes, including 11,477 leaf +L+ <TABLE> nodes. Out of the 11,477 leaf <TABLE> nodes, +L+ 1,740 are genuine tables and 9,737 are non-genuine tables. +L+ Not every genuine table has its title and only 1,308 genuine +L+ tables have table titles. We also found at least 253 HTML +L+ files have unmatched <TABLE>, </TABLE> pairs or wrong +L+ hierarchy, which demonstrates the noisy nature of web doc- +L+ uments. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. EXPERIMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A hold-out method is used to evaluate our table classi- +L+ fier. We randomly divided the data set into nine parts. +L+ Each classifier was trained on eight parts and then tested +L+ on the remaining one part. This procedure was repeated +L+ nine times, each time with a different choice for the test +L+ </SectLabel_bodyText> <SectLabel_page> 246 +L+ </SectLabel_page> <SectLabel_figure> HTML File +L+ Hierarchy +L+ Adding attributes +L+ Parser +L+ HTML with attributes and unique +L+ index to each table(ground truth) +L+ Validation +L+ (a)	(b) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: (a) The diagram of ground truthing procedure; (b) A snapshot of the ground truthing software. +L+ part. Then the combined nine part results are averaged to +L+ arrive at the overall performance measures [4]. +L+ For the layout and content type features, this procedure +L+ is straightforward. However it is more complicated for the +L+ word group feature training. To compute wg for training +L+ samples, we need to further divide the training set into two +L+ groups, a larger one (7 parts) for the computation of the +L+ weights pGi and pNi, i =1�...�jWj, and a smaller one (1 +L+ </SectLabel_figureCaption> <SectLabel_none> i i i +L+ </SectLabel_none> <SectLabel_bodyText> part) for the computation of the vectors GS, NS, and IT. +L+ This partition is again rotated to compute wg for each table +L+ in the training set. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 1: Possible true- and detected-state combina- +L+ tions for two classes. +L+ </SectLabel_tableCaption> <SectLabel_table> True Class	Assigned Class +L+ 	genuine table	non-genuine table +L+ genuine table	Ngg	Ngn +L+ non-genuine table	Nng	Nnn +L+ lows: +L+ Ngg	Ngg	R + P +L+ R	P 	 F Ngg + Ng'	Ngg + Nng	= 2 +L+ </SectLabel_table> <SectLabel_bodyText> For comparison among different features and learning al- +L+ gorithms we report the performance measures when the best +L+ F-measure is achieved. First, the performance of various fea- +L+ ture groups and their combinations were evaluated using the +L+ decision tree classifier. The results are given in Table 2. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 2: Experimental results using various feature +L+ groups and the decision tree classifier. +L+ </SectLabel_tableCaption> <SectLabel_table> 	L	T	LT	LTW +L+ R (%)	87.24	90.80	94.20	94.25 +L+ P (%)	88.15	95.70	97.27	97.50 +L+ F (%)	87.70	93.25	95.73	95.88 +L+ L: Layout only. +L+ T: Content type only. +L+ LT: Layout and content type. +L+ LTW: Layout, content type and word group. +L+ </SectLabel_table> <SectLabel_bodyText> The output of each classifier is compared with the ground +L+ truth and a contingency table is computed to indicate the +L+ number of a particular class label that are identified as mem- +L+ bers of one of two classes. The rows of the contingency table +L+ represent the true classes and the columns represent the as- +L+ signed classes. The cell at row r and column c is the number +L+ of tables whose true class is r while its assigned class is c. +L+ The possible true- and detected-state combination is shown +L+ in Table 1. Three performance measures Recall Rate(R), +L+ Precision Rate(P) and F-measure(F) are computed as fol- +L+ As seen from the table, content type features performed +L+ better than layout features as a single group, achieving an +L+ F-measure of 93.25%. However, when the two groups were +L+ combined the F-measure was improved substantially to 95.73%, +L+ reconfirming the importance of combining layout and con- +L+ tent features in table detection. The addition of the word +L+ group feature improved the F-measure slightly more to 95.88%. +L+ Table 3 compares the performances of different learning +L+ algorithms using the full feature set. The leaning algorithms +L+ tested include the decision tree classifier and the SVM al- +L+ </SectLabel_bodyText> <SectLabel_page> 247 +L+ </SectLabel_page> <SectLabel_bodyText> gorithm with two different kernels — linear and radial basis +L+ function (RBF). +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 3: Experimental results using different learn- +L+ ing algorithms. +L+ </SectLabel_tableCaption> <SectLabel_table> 	Tree	SVM (linear)	SVM (RBF) +L+ R (%)	94.25	93.91	95.98 +L+ P (%)	97.50	91.39	95.81 +L+ F (%)	95.88	92.65	95.89 +L+ </SectLabel_table> <SectLabel_bodyText> As seen from the table, for this application the SVM with +L+ radial basis function kernel performed much better than the +L+ one with linear kernel. It achieved an F measure of 95.89%, +L+ comparable to the 95.88% achieved by the decision tree clas- +L+ sifier. +L+ Figure 3 shows two examples of correctly classified tables, +L+ where Figure 3(a) is a genuine table and Figure 3(b) is a +L+ non-genuine table. +L+ Figure 4 shows a few examples where our algorithm failed. +L+ Figure 4(a) was misclassified as a non-genuine table, likely +L+ because its cell lengths are highly inconsistent and it has +L+ many hyperlinks which is unusual for genuine tables. The +L+ reason why Figure 4(b) was misclassified as non-genuine is +L+ more interesting. When we looked at its HTML source code, +L+ we found it contains only two <TR> tags. All text strings +L+ in one rectangular box are within one <TD> tag. Its author +L+ used <p> tags to put them in different rows. This points +L+ to the need for a more carefully designed pseudo-rendering +L+ process. Figure 4(c) shows a non-genuine table misclassi- +L+ fied as genuine. A close examination reveals that it indeed +L+ has good consistency along the row direction. In fact, one +L+ could even argue that this is indeed a genuine table, with +L+ implicit row headers of Title, Name, Company Affiliation +L+ and Phone Number. This example demonstrates one of the +L+ most difficult challenges in table understanding, namely the +L+ ambiguous nature of many table instances (see [5] for a more +L+ detailed analysis on that). Figure 4(d) was also misclassi- +L+ fied as a genuine table. This is a case where layout features +L+ and the kind of shallow content features we used are not +L+ enough deeper semantic analysis would be needed in or- +L+ der to identify the lack of logical coherence which makes it +L+ a non-genuine table. +L+ For comparison, we tested the previously developed rule- +L+ based system [10] on the same database. The initial re- +L+ sults (shown in Table 4 under "Original Rule Based") were +L+ very poor. After carefully studying the results from the +L+ initial experiment we realized that most of the errors were +L+ caused by a rule imposing a hard limit on cell lengths in gen- +L+ uine tables. After deleting that rule the rule-based system +L+ achieved much improved results (shown in Table 4 under +L+ "Modified Rule Based"). However, the proposed machine +L+ learning based method still performs considerably better in +L+ comparison. This demonstrates that systems based on hand- +L+ crafted rules tend to be brittle and do not generalize well. +L+ In this case, even after careful manual adjustment in a new +L+ database, it still does not work as well as an automatically +L+ trained classifier. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 3: Examples of correctly classified tables. +L+ (a): a genuine table; (b): a non-genuine table. +L+ </SectLabel_figureCaption> <SectLabel_tableCaption> Table 4: Experimental results of a previously devel- +L+ oped rule based system. +L+ </SectLabel_tableCaption> <SectLabel_table> 	Original Rule Based	Modified Rule Based +L+ R (%)	48.16	95.80 +L+ P (%)	75.70	79.46 +L+ F (%)	61.93	87.63 +L+ </SectLabel_table> <SectLabel_page> 248 +L+ </SectLabel_page> <SectLabel_figure> (a)	(b) +L+ (c)	(d) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: Examples of misclassified tables. (a) and (b): Genuine tables misclassified as non-genuine; (c) and +L+ (d): Non-genuine tables misclassified as genuine. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> A direct comparison to other previous results [2, 14] is +L+ not possible currently because of the lack of access to their +L+ system. However, our test database is clearly more general +L+ and far larger than the ones used in [2] and [14], while our +L+ precision and recall rates are both higher. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. CONCLUSION AND FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Table detection in web documents is an interesting and +L+ challenging problem with many applications. We present a +L+ machine learning based table detection algorithm for HTML +L+ documents. Layout features, content type features and word +L+ group features were used to construct a novel feature set. +L+ Decision tree and SVM classifiers were then implemented +L+ and tested in this feature space. We also designed a novel ta- +L+ ble ground truthing protocol and used it to construct a large +L+ web table ground truth database for training and testing. +L+ Experiments on this large database yielded very promising +L+ results. +L+ Our future work includes handling more different HTML +L+ styles in pseudo-rendering, detecting table titles of the rec- +L+ ognized genuine tables and developing a machine learning +L+ based table interpretation algorithm. We would also like to +L+ investigate ways to incorporate deeper language analysis for +L+ both table detection and interpretation. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. ACKNOWLEDGMENT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We would like to thank Kathie Shipley for her help in +L+ collecting the web pages, and Amit Bagga for discussions on +L+ vector space models. +L+ 8. REFERENCES +L+ </SectLabel_bodyText> <SectLabel_reference> [1] M. Campione, K. Walrath, and A. Huml. The +L+ java(tm) tutorial: A short course on the basics (the +L+ java(tm) series). +L+ [2] H.-H. Chen, S.-C. Tsai, and J.-H. Tsai. Mining tables +L+ from large scale html texts. In Proc. 18th +L+ International Conference on Computational +L+ Linguistics, Saabrucken, Germany, July 2000. +L+ [3] C. Cortes and V. Vapnik. Support-vector networks. +L+ Machine Learning, 20:273{296, August 1995. +L+ [4] R. Haralick and L. Shapiro. Computer and Robot +L+ Vision, volume 1. Addison Wesley, 1992. +L+ [5] J. Hu, R. Kashi, D. Lopresti, G. Nagy, and +L+ G. Wilfong. Why table ground-truthing is hard. In +L+ Proc. 6th International Conference on Document +L+ Analysis and Recognition (ICDAR01), pages 129{133, +L+ Seattle, WA, USA, September 2001. +L+ [6] M. Hurst. Layout and language: Challenges for table +L+ understanding on the web. In Proc. 1st International +L+ Workshop on Web Document Analysis, pages 27{30, +L+ Seattle, WA, USA, September 2001. +L+ [7] T. Joachims. A probabilistic analysis of the rocchio +L+ algorithm with tfidf for text categorization. In Proc. +L+ 14th International Conference on Machine Learning, +L+ pages 143{151, Morgan Kaufmann, 1997. +L+ [8] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. +L+ Automating the construction of internet portals with +L+ machine learning. In Information Retrieval Journal, +L+ volume 3, pages 127{163, Kluwer, 2000. +L+ </SectLabel_reference> <SectLabel_page> 249 +L+ </SectLabel_page> <SectLabel_reference> [9] T. M. Mitchell. Machine Learning. McGraw-Hill, 1997. +L+ [10] G. Penn, J. Hu, H. Luo, and R. McDonald. Flexible +L+ web document analysis for delivery to narrow- +L+ bandwidth devices. In Proc. 6th International +L+ Conference on Document Analysis and Recognition +L+ (ICDAR01), pages 1074{1078, Seattle, WA, USA, +L+ September 2001. +L+ [11] M. F. Porter. An algorithm for suffix stripping. +L+ Program, 14(3):130-137, 1980. +L+ [12] V. N. Vapnik. The Nature of Statistical Learning +L+ Theory, volume 1. Springer, New York, 1995. +L+ [13] Y. Yang and X. Liu. A re-examination of text +L+ categorization methods. In Proc. SIGIR'99, pages +L+ 42{49, Berkeley, California, USA, August 1999. +L+ [14] M. Yoshida, K. Torisawa, and J. Tsujii. A method to +L+ integrate tables of the world wide web. In Proc. 1st +L+ International Workshop on Web Document Analysis, +L+ pages 31{34, Seattle, WA, USA, September 2001. +L+ </SectLabel_reference> <SectLabel_page> 250 +L+ </SectLabel_page>
<SectLabel_title> A New Approach to Intranet Search +L+ Based on Information Extraction +L+ </SectLabel_title> <SectLabel_author> Hang Li, Yunbo Cao +L+ </SectLabel_author> <SectLabel_affiliation> Microsoft Research Asia +L+ </SectLabel_affiliation> <SectLabel_address> 5F Sigma Center +L+ No.49 Zhichun Road, +L+ Haidian, Beijing, China, 100080 +L+ </SectLabel_address> <SectLabel_email> {hangli, yucao}@microsoft.com +L+ </SectLabel_email> <SectLabel_author> Jun Xu* +L+ </SectLabel_author> <SectLabel_affiliation> College of Software +L+ Nankai University +L+ </SectLabel_affiliation> <SectLabel_address> No.94 Weijin Road, +L+ Tianjin, China, 300071 +L+ </SectLabel_address> <SectLabel_email> nkxj@yahoo.com.cn +L+ </SectLabel_email> <SectLabel_author> Yunhua Hu* +L+ </SectLabel_author> <SectLabel_affiliation> Dept. of Computer Science +L+ Xi’an Jiaotong University +L+ </SectLabel_affiliation> <SectLabel_address> No 28, West Xianning Road, +L+ Xi'an, China, 710049 +L+ </SectLabel_address> <SectLabel_email> yunhuahu@mail.xjtu.edu.cn +L+ </SectLabel_email> <SectLabel_author> Shenjie Li* +L+ </SectLabel_author> <SectLabel_affiliation> Dept. of Computer Science +L+ Hong Kong University of Science and Technology +L+ Kowloon, Hong Kong, China +L+ </SectLabel_affiliation> <SectLabel_email> lisj@cs.ust.hk +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper is concerned with ‘intranet search’. By intranet search, we +L+ mean searching for information on an intranet within an organization. +L+ We have found that search needs on an intranet can be categorized into +L+ types, through an analysis of survey results and an analysis of search +L+ log data. The types include searching for definitions, persons, experts, +L+ and homepages. Traditional information retrieval only focuses on +L+ search of relevant documents, but not on search of special types of +L+ information. We propose a new approach to intranet search in which +L+ we search for information in each of the special types, in addition to +L+ the traditional relevance search. Information extraction technologies +L+ can play key roles in such kind of ‘search by type’ approach, because +L+ we must first extract from the documents the necessary information in +L+ each type. We have developed an intranet search system called +L+ ‘Information Desk’. In the system, we try to address the most +L+ important types of search first - finding term definitions, homepages of +L+ groups or topics, employees’ personal information and experts on +L+ topics. For each type of search, we use information extraction +L+ technologies to extract, fuse, and summarize information in advance. +L+ The system is in operation on the intranet of Microsoft and receives +L+ accesses from about 500 employees per month. Feedbacks from users +L+ and system logs show that users consider the approach useful and the +L+ system can really help people to find information. This paper describes +L+ the architecture, features, component technologies, and evaluation +L+ results of the system. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.3.3 [Information Storage and Retrieval]: Information Search +L+ and Retrieval – search process; I.7.m [Document and Text +L+ Processing]: Miscellaneous +L+ </SectLabel_category> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CIKM’05, October 31-November 5, 2005, Bremen, Germany. +L+ Copyright 2005 ACM 1-59593-140-6/05/0010...$5.00. +L+ </SectLabel_note> <SectLabel_author> Dmitriy Meyerzon +L+ </SectLabel_author> <SectLabel_affiliation> Microsoft Corporation +L+ One Microsoft Way, +L+ </SectLabel_affiliation> <SectLabel_address> Redmond, WA, USA, 98052 +L+ </SectLabel_address> <SectLabel_email> dmitriym@microsoft.com +L+ </SectLabel_email> <SectLabel_sectionHeader> General Terms: Algorithms, Experimentation, Human +L+ Factors +L+ Keywords: Intranet search, information extraction, metadata +L+ extraction, expert finding, definition search +L+ 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Internet search has made significant progress in recent years. In +L+ contrast, intranet search does not seem to be so successful. The +L+ IDC white paper entitled “The high cost of not finding +L+ information” [13] reports that information workers spend from +L+ 15% to 35% of their work time on searching for information and +L+ 40% of information workers complain that they cannot find the +L+ information they need to do their jobs on their company intranets. +L+ Many commercial systems [35, 36, 37, 38, 39] have been +L+ developed for intranet search. However, most of them view +L+ intranet search as a problem of conventional relevance search. In +L+ relevance search, when a user types a query, the system returns a +L+ list of ranked documents with the most relevant documents on the +L+ top. +L+ Relevance search can only serve average needs well. It cannot, +L+ however, help users to find information in a specific type, e.g., +L+ definitions of a term and experts on a topic. The characteristic of +L+ intranet search does not seem to be sufficiently leveraged in the +L+ commercial systems. +L+ In this paper, we try to address intranet search in a novel approach. +L+ We assume that the needs of information access on intranets can +L+ be categorized into searches for information in different types. An +L+ analysis on search log data on the intranet of Microsoft and an +L+ analysis on the results of a survey conducted at Microsoft have +L+ verified the correctness of the assumption. +L+ Our proposal then is to take a strategy of ‘divide-and-conquer’. +L+ We first figure out the most important types of search, e.g., +L+ definition search, expert search. For each type, we employ +L+ information extraction technologies to extract, fuse, and +L+ summarize search results in advance. Finally, we combine all the +L+ types of searches together, including the traditional relevance +L+ </SectLabel_bodyText> <SectLabel_page> 460 +L+ </SectLabel_page> <SectLabel_bodyText> search, in a unified system. In this paper, we refer to the approach +L+ as ‘search by type’. Search by type can also be viewed as a +L+ simplified version of Question Answering, adapted to intranet. +L+ The advantage of the new search approach lies in that it can help +L+ people find the types of information which relevance search +L+ cannot easily find. The approach is particularly reasonable on +L+ intranets, because in such space users are information workers and +L+ search needs are business oriented. +L+ We have developed a system based on the approach, which is +L+ called ‘Information Desk’. Information Desk can help users to +L+ find term definitions, homepages of groups or topics, employees’ +L+ personal information and experts on topics, on their company +L+ intranets. +L+ The system has been put into practical use since November 24th, +L+ 2004. Each month, about 500 Microsoft employees make access +L+ to the system. Both the results of an analysis on a survey and the +L+ results of an analysis on system log show that the features of +L+ definition search and homepage search are really helpful. The +L+ results also show that search by type is necessary at enterprise. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. RELATED WORK +L+ 2.1 Intranet Search +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The needs on search on intranets are huge. It is estimated that +L+ intranets at enterprises have tens or even hundreds of times larger +L+ data collections (both structured and unstructured) than internet. +L+ As explained above, however, many users are not satisfied with +L+ the current intranet search systems. How to help people access +L+ information on intranet is a big challenge in information retrieval. +L+ Much effort has been made recently on solutions both in industry +L+ and in academia. +L+ Many commercial systems [35, 36, 37, 38, 39] dedicated to +L+ intranet search have been developed. Most of the systems view +L+ intranet search as a problem of conventional relevance search. +L+ In the research community, ground designs, fundamental +L+ approaches, and evaluation methodologies on intranet search have +L+ been proposed. +L+ Hawking et al [17] made ten suggestions on how to conduct high +L+ quality intranet search. Fagin et al [12] made a comparison +L+ between internet search and intranet search. Recently, Hawking +L+ [16] conducted a survey on previous work and made an analysis +L+ on the intranet search problem. Seven open problems on intranet +L+ search were raised in their paper. +L+ Chen et al [3] developed a system named ‘Cha-Cha’, which can +L+ organize intranet search results in a novel way such that the +L+ underlying structure of the intranet is reflected. Fagin et al [12] +L+ proposed a new ranking method for intranet search, which +L+ combine various ranking heuristics. Mattox et al [25] and +L+ Craswell et al [7] addressed the issue of expert finding on a +L+ company intranet. They developed methods that can automatically +L+ identify experts in an area using documents on the intranet. +L+ Stenmark [30] proposed a method for analyzing and evaluating +L+ intranet search tools. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Question Answering +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Question Answering (QA) particularly that in TREC +L+ (http://trec.nist.gov/) is an application in which users type +L+ questions in natural language and the system returns short and +L+ usually single answers to the questions. +L+ When the answer is a personal name, a time expression, or a place +L+ name, the QA task is called ‘Factoid QA’. Many QA systems have +L+ been developed, [2, 4, 18, 20, 22, 27]. Factoid QA usually +L+ consists of the following steps: question type identification, +L+ question expansion, passage retrieval, answer ranking, and answer +L+ creation. +L+ TREC also has a task of ‘Definitional QA’. In the task, “what is +L+ <term>” and “who is <person>” questions are answered in a +L+ single combined text [1, 11, 15, 33, 34]. A typical system consists +L+ of question type identification, document retrieval, key sentence +L+ matching, kernel fact finding, kernel fact ranking, and answer +L+ generation. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. OUR APPROACH TO INTRANET +L+ SEARCH +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Search is nothing but collecting information based on users’ +L+ information access requests. If we can correctly gather +L+ information on the basis of users’ requests, then the problem is +L+ solved. Current intranet search is not designed along this +L+ direction. Relevance search can help create a list of ranked +L+ documents that serve only average needs well. The limitation of +L+ this approach is clear. That is, it cannot help users to find +L+ information of a specific type, e.g., definitions of a term. On the +L+ other hand, Question Answering (QA) is an ideal form for +L+ information access. When a user inputs a natural language +L+ question or a query (a combination of keywords) as a description +L+ of his search need, it is ideal to have the machine ‘understand’ the +L+ input and return only the necessary information based on the +L+ request. However, there are still lots of research work to do before +L+ putting QA into practical uses. In short term, we need consider +L+ adopting a different approach. +L+ One question arises here: can we take a hybrid approach? +L+ Specifically, on one hand, we adopt the traditional approach for +L+ search, and on the other hand, we realize some of the most +L+ frequently asked types of search with QA. Finally, we integrate +L+ them in a single system. For the QA part, we can employ +L+ information extraction technologies to extract, fuse, and +L+ summarize the results in advance. This is exactly the proposal we +L+ make to intranet search. +L+ Can we categorize users’ search needs easily? We have found that +L+ we can create a hierarchy of search needs for intranet search, as +L+ will be explained in section 4. +L+ On intranets, users are information workers and their motivations +L+ for conducting search are business oriented. We think, therefore, +L+ that our approach may be relatively easily realized on intranets +L+ first. (There is no reason why we cannot apply the same approach +L+ to the internet, however.) +L+ To verify the correctness of the proposal, we have developed a +L+ system and made it available internally at Microsoft. The system +L+ called Information Desk is in operation on the intranet of +L+ Microsoft and receives accesses from about 500 employees per +L+ month. +L+ At Information Desk, we try to solve the most important types of +L+ search first - find term definitions, homepages of groups or topics, +L+ experts on topics, and employees’ personal information. We are +L+ </SectLabel_bodyText> <SectLabel_page> 461 +L+ </SectLabel_page> <SectLabel_bodyText> also trying to increase the number of search types, and integrate +L+ them with the conventional relevance search. We will explain the +L+ working of Information Desk in section 5. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. ANALYSIS OF SEARCH NEEDS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we describe our analyses on intranet search needs +L+ using search query logs and survey results. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Categorization of Search Needs +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In order to understand the underlying needs of search queries, we +L+ would need to ask the users about their search intentions. +L+ Obviously, this is not feasible. We conducted an analysis by using +L+ query log data. Here query log data means the records on queries +L+ typed by users, and documents clicked by the users after sending +L+ the queries. +L+ Our work was inspirited by those of Rose and Levinson [28]. In +L+ their work, they categorized the search needs of users on internet +L+ by analyzing search query logs. +L+ We tried to understand users’ search needs on intranet by +L+ identifying and organizing a manageable number of categories of +L+ the needs. The categories encompass the majority of actual +L+ requests users may have when conducting search on an intranet. +L+ We used a sample of queries from the search engine of the +L+ intranet of Microsoft. First, we brainstormed a number of +L+ categories, based on our own experiences and previous work. +L+ Then, we modified the categories, including adding, deleting, and +L+ merging categories, by assigning queries to the categories. +L+ Given a query, we used the following information to deduce the +L+ underlying search need: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	the query itself +L+ •	the documents returned by the search engine +L+ •	the documents clicked on by the user +L+ </SectLabel_listItem> <SectLabel_bodyText> For example, if a user typed a keyword of ‘.net’ and clicked a +L+ homepage of .net, then we judged that the user was looking for a +L+ homepage of .net. +L+ As we repeated the process, we gradually reached the conclusion +L+ that search needs on intranet can be categorized as a hierarchical +L+ structure shown in Figure 1. In fact, the top level of the hierarchy +L+ resembles that in the taxonomy proposed by Rose and Levinson +L+ for internet [28]. However, the second level differs. On intranet, +L+ users’ search needs are less diverse than those on internet, because +L+ the users are information workers and their motivations of +L+ conducting search are business oriented. +L+ There is a special need called ‘tell me about’ here. It is similar to +L+ the traditional relevance search. Many search needs are by nature +L+ difficult to be categorized, for example, “I want to find documents +L+ related to both .net and SQL Server”. We can put them into the +L+ category. +L+ We think that the search needs are not Microsoft specific; one can +L+ image that similar needs exist in other companies as well. +L+ </SectLabel_bodyText> <SectLabel_figure> 	When (time) +L+ 	Where +L+ 	(place) +L+ 	Why (reason) +L+ Informational	What is +L+ 	(definition) knows +L+ 	Who	about (expert) +L+ 	Who is +L+ 	(person) +L+ 	How +L+ 	to (manual) +L+ 	Tell +L+ 	me about (relevance) +L+ 	Group +L+ 	Person +L+ 	Product +L+ Navigational +L+ 	Technology +L+ 	Services +L+ Transactional +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1. Categories of search needs +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 4.2 Analysis on Search Needs – by Query Log +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We have randomly selected 200 unique queries and tried to assign +L+ the queries to the categories of search needs described above. +L+ Table 1 shows the distribution. We have also picked up the top +L+ 350 frequently submitted queries and assigned them to the +L+ categories. Table 2 shows the distribution. (There is no result for +L+ ‘why’, ‘what is’, and ‘who knows about’, because it is nearly +L+ impossible to guess users’ search intensions by only looking at +L+ query logs.) +L+ For random queries, informational needs are dominating. For high +L+ frequency queries, navigational needs are dominating. The most +L+ important types for random queries are relevance search, personal +L+ information search, and manual search. The most important types +L+ for high frequency queries are home page search and relevance +L+ search. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Analysis on Search Needs – by Survey +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We can use query log data to analyze users’ search needs, as +L+ described above. However, there are two shortcomings in the +L+ approach. First, sometimes it is difficult to guess the search +L+ intensions of users by only looking at query logs. This is +L+ especially true for the categories of ‘why’ and ‘what’. Usually it is +L+ hard to distinguish them from ‘relevance search’. Second, query +L+ log data cannot reveal users’ potential search needs. For example, +L+ many employees report that they have needs of searching for +L+ experts on specific topics. However, it is difficult to find expert +L+ searches from query log at a conventional search engine, because +L+ users understand that such search is not supported and they do not +L+ conduct the search. +L+ To alleviate the negative effect, we have conducted another +L+ analysis through a survey. Although a survey also has limitation +L+ (i.e., it only asks people to answer pre-defined questions and thus +L+ can be biased), it can help to understand the problem from a +L+ different perspective. +L+ </SectLabel_bodyText> <SectLabel_page> 462 +L+ </SectLabel_page> <SectLabel_tableCaption> Table 1. Distribution of search needs for random queries +L+ </SectLabel_tableCaption> <SectLabel_table> Category of Search Needs	Percentage +L+ When	0.02 +L+ Where	0.02 +L+ Why	NA +L+ What is	NA +L+ Who knows about	NA +L+ Who is	0.23 +L+ How to	0.105 +L+ Tell me about	0.46 +L+ Informational total	0.835 +L+ Groups	0.03 +L+ Persons	0.005 +L+ Products	0.02 +L+ Technologies	0.02 +L+ Services	0.06 +L+ Navigational total	0.135 +L+ Transactional	0.025 +L+ Other	0.005 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2. Distribution of search needs for high frequency queries +L+ </SectLabel_tableCaption> <SectLabel_table> Category of Search Needs	Relative Prevalence +L+ When	0.0057 +L+ Where	0.0143 +L+ Why	NA +L+ What is	NA +L+ Who knows about	NA +L+ Who is	0.0314 +L+ How to	0.0429 +L+ Tell me about	0.2143 +L+ Informational total	0.3086 +L+ Groups	0.0571 +L+ Persons	0.0057 +L+ Products	0.26 +L+ Technologies	0.0829 +L+ Services	0.2371 +L+ Navigational total	0.6428 +L+ Transactional	0.0086 +L+ Other	0.04 +L+ </SectLabel_table> <SectLabel_figureCaption> Figure 2. Survey results on search needs +L+ </SectLabel_figureCaption> <SectLabel_bodyText> In the survey, we have asked questions regarding to search needs +L+ at enterprise. 35 Microsoft employees have taken part in the +L+ survey. Figure 2 shows the questions and the corresponding +L+ results. +L+ We see from the answers that definition search, manual search, +L+ expert finding, personal information search, and time schedule +L+ search are requested by the users. Homepage finding on +L+ technologies and products are important as well. Search for a +L+ download site is also a common request. +L+ I have experiences of conducting search at Microsoft intranet in +L+ order to (multiple choice) +L+ </SectLabel_bodyText> <SectLabel_figure> •	download a software, a document, or a picture. E.g., "getting +L+ MSN logo" +L+ 71 % +L+ •	make use of a service. E.g., "getting a serial number of +L+ Windows" +L+ 	53 % +L+ •	none of the above +L+ 18 % +L+ I have experiences of conducting search at Microsoft intranet to +L+ look for the web sites (or homepages) of (multiple choice) +L+ •	technologies +L+ 74 % +L+ •	products +L+ 	74 % +L+ •	services +L+ 	68 % +L+ •	projects +L+ 	68 % +L+ •	groups +L+ 	60 % +L+ •	persons +L+ 42 % +L+ •	none of the above +L+ 11 % +L+ I have experiences of conducting search at Microsoft intranet in +L+ which the needs can be translated into questions like? (multiple +L+ choice) +L+ •	‘what is’ - e.g., "what is blaster" +L+ 77 % +L+ •	‘how to’ - "how to submit expense report" +L+ 54 % +L+ •	‘where’ - e.g., "where is the company store" +L+ 51 % +L+ •	‘who knows about’ - e.g., "who knows about data mining" +L+ 51 % +L+ •	‘who is’ - e.g., "who is Rick Rashid" +L+ 45 % +L+ •	‘when’ - e.g., "when is TechFest'05 " +L+ 42 % +L+ •	‘why’ - e.g., "why do Windows NT device drivers contain +L+ trusted code" +L+ 28 % +L+ •	none of the above +L+ 14 % +L+ </SectLabel_figure> <SectLabel_page> 463 +L+ </SectLabel_page> <SectLabel_figure> Longhorn	Go +L+ What is	Who is	Where is homepage of	Who knows about +L+ 	What is	Who isWhere is homepage of	Who knows about +L+ 	Definition of Longhorn +L+ 	Longhorn is the codename for the next release of the Windows operating system, planned for release in FY 2005. Longhorn will further Microsoft’s long term vision for ... +L+ 	http://url1 +L+ 	Longhorn is a platform that enables incredible user experiences that are unlike anything possible with OS releases to date. This session describes our approach and philosophy that... +L+ 	http://url2 +L+ 	Longhorn is the platform in which significant improvements in the overall manageability of the system by providing the necessary infrastructure to enable standardized configuration/change management, structured eventing and monitoring, and a unified software distribution mechanism will be made. In order to achieve this management with each Longhorn... +L+ 	http://url3 +L+ 	Longhorn is the evolution of the .NET Framework on the client and the biggest investment that Microsoft has made in the Windows client development platform in years. Longhorn is the platform for smart , connected... +L+ 	http://url4 +L+ 	Longhorn is the platform for smart, connected applications, combining the best features of the Web, such as ease of deployment and rich content with the power of the Win32 development platform, enabling developers to build a new breed of applications that take real advantage of the connectivity, storage, and graphical capabilities of the modern personal +L+ 	computer . +L+ 	http//url5 +L+ 	Office	Go +L+ 	What is	Who is	Where is homepage of	Who knows about +L+ 	What is	Who is	Where is homepage of	Who knows about +L+ 	Homepages of Office +L+ 	Office Portal Site +L+ 	This is the internal site for Office +L+ 	http://url1 +L+ 	Office Site (external) +L+ 	Microsoft.com site offering information on the various Office products. Links include FAQs, downloads, support, and more. +L+ 	http:/url2 +L+ 	Office +L+ 	New Office Site +L+ 	http://url3 +L+ 	Office Office +L+ 	http://url4 +L+ 	Data Mining	Go +L+ 	What is	Who is	Where is homepage of	Who knows about +L+ 	What is	Who is	Where is homepage of	Who knows about +L+ 	People Associated with Data mining +L+ 	Jamie MacLennan	DEVELOPMENT LEAD +L+ 	US-SQL Data Warehouse +1 (425) XXXXXXX XXXXXX Associated documents(4): +L+ 	•	is author of document entitled Data Mining Tutorial +L+ 	http://url1 +L+ 	•	is author of document entitled Solving Business Problems Using Data Mining +L+ 	http://url2 +L+ 	Jim Gray	DISTINGUISHED ENGINEER +L+ 	US-WAT MSR San Francisco +XXXXXXXXXXX +L+ 	Associated documents(2): +L+ 	•	is author of document entitled Mainlining Data Mining +L+ 	http://url3 +L+ 	•	is author of document entitled Data Mining the SDSS SkyServer Database +L+ 	http://url4 +L+ Bill Gates	Go +L+ What is	Who is	Where is homepage of	Who knows about +L+ What is	Who is	Where is homepage of	Who knows about +L+ 	Bill Gates	CHRMN & CHIEF SFTWR ARCHITECT +L+ 	US-Executive-Chairman +L+ 	+1 (425) XXXXXXX XXXXXX +L+ 	Documents of Bill Gates(118) +L+ 	•	My advice to students: Education counts +L+ 	http://url1 +L+ 	•	Evento NET Reviewers – Seattle –7/8 Novembro +L+ 	http://url2 +L+ 	•	A Vision for Life Long Learning – Year 2020 +L+ 	http://url3 +L+ 	•	Bill Gates answers most frequently asked questions. +L+ 	http://url4 +L+ 	>>more +L+ 	Top 10 terms appearing in documents of Bill Gates +L+ 	Term 1 (984.4443) Term 2 (816.4247) Term 3 (595.0771) Term 4 (578.5604) Term 5 (565.7299) Term 6 (435.5366) Term 7 (412.4467) Term 8 (385.446) Term 9 (346.5993) Term 10 (345.3285) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: Information Desk system +L+ </SectLabel_figureCaption> <SectLabel_sectionHeader> 5. INFORMATION DESK +L+ 5.1 Features +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Currently Information Desk provides four types of search. The +L+ four types are: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. ‘what is’ – search of definitions and acronyms. Given a term, +L+ it returns a list of definitions of the term. Given an acronym, it +L+ returns a list of possible expansions of the acronym. +L+ 2. ‘who is’ – search of employees’ personal information. Given +L+ the name of a person, it returns his/her profile information, +L+ authored documents and associated key terms. +L+ 3. ‘where is homepage of’ – search of homepages. Given the +L+ name of a group, a product, or a technology, it returns a list of +L+ its related home pages. +L+ 4. ‘who knows about’ – search of experts. Given a term on a +L+ technology or a product, it returns a list of persons who might +L+ be experts on the technology or the product. +L+ </SectLabel_listItem> <SectLabel_figureCaption> Figure 4. Workflow of Information Desk +L+ </SectLabel_figureCaption> <SectLabel_bodyText> There are check boxes on the UI, and each represents one search +L+ type. In search, users can designate search types by checking the +L+ corresponding boxes and then submit queries. By default, all the +L+ boxes are checked. +L+ For example, when users type ‘longhorn’ with the ‘what is’ box +L+ checked, they get a list of definitions of ‘Longhorn’ (the first +L+ snapshot in figure 3). Users can also search for homepages (team +L+ web sites) related to ‘Office’, using the ‘where is homepage’ +L+ feature (the second snapshot in figure 3). Users can search for +L+ experts on, for example, ‘data mining’ by asking ‘who knows +L+ about data mining’ (the third snapshot in figure 3). Users can also +L+ get a list of documents that are automatically identified as being +L+ authored by ‘Bill Gates’, for example, with the ‘who is’ feature +L+ (the last snapshot in figure 3). The top ten key terms found in his +L+ documents are also given. +L+ Links to the original documents, from which the information has +L+ been extracted, are also available on the search result UIs. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Technologies +L+ 5.2.1 Architecture +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Information Desk makes use of information extraction +L+ technologies to support the search by type feaatures. The +L+ technologies include automatically extracting document metadata +L+ and domain specific knowledge from a web site using information +L+ extraction technologies. The domain specific knowledge includes +L+ definition, acronym, and expert. The document metadata includes +L+ title, author, key term, homepage. Documents are in the form of +L+ Word, PowerPoint, or HTML. Information Desk stores all the +L+ data in Microsoft SQL Server and provides search using web +L+ </SectLabel_bodyText> <SectLabel_figure> homepage +L+ term +L+ Where is homepage of +L+ Crawler & +L+ Extractor +L+ definition +L+ acronym +L+ document +L+ key term +L+ person +L+ document +L+ what is +L+ who is +L+ who knows about +L+ MS Web +L+ Information Desk +L+ Web Server +L+ term +L+ person +L+ term +L+ </SectLabel_figure> <SectLabel_page> 464 +L+ </SectLabel_page> <SectLabel_bodyText> services. Figure 4 shows the workflow of Information Desk. +L+ Currently, there are 4 million documents crawled from the +L+ Microsoft intranet. +L+ Below we explain each feature in details. Table 3 shows which +L+ feature employs what kind of mining technology. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 3. Information extraction technologies employed +L+ </SectLabel_tableCaption> <SectLabel_table> 	`What is'	`Who is'	`Who knows	`Where is homepage' +L+ 			about' +L+ Definition extraction	Yes +L+ Acronym extraction	Yes +L+ Homepage				Yes +L+ finding +L+ Title		Yes	Yes +L+ extraction +L+ Author extraction		Yes	Yes +L+ Key term extraction		Yes	Yes +L+ Expert mining			Yes +L+ </SectLabel_table> <SectLabel_subsubsectionHeader> 5.2.2 `What is' +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> There are two parts in the feature: definition finding and acronym +L+ recognition. +L+ In definition finding, we extract from the entire collection of +L+ documents <term, definition, score> triples. They are respectively +L+ a term, a definitional excerpt of the term, and its score +L+ representing its likelihood of being a good definition. We assign +L+ the scores using a statistical model. Both paragraphs and +L+ sentences can be considered as definition excerpts in our approach. +L+ Currently, we only consider the use of paragraphs. +L+ As model, we employ SVM (Support Vector Machines) [31], +L+ which identifies whether a given paragraph is a definition of the +L+ first noun phrase (term) in the paragraph. There are positive +L+ features in the SVM model. For example, if the term appears at +L+ the beginning of the paragraph or repeatedly occurs in the +L+ paragraph, then it is likely the paragraph is a (good) definition on +L+ the term. There are also negative features. If words like `she', `he', +L+ or `said' occurs in the paragraph, or many adjectives occur in the +L+ paragraph, then it is likely the paragraph is not a (good) definition. +L+ In search, given a query term, we retrieve all the triples matched +L+ against the query term and present the corresponding definitions +L+ in descending order of the scores. +L+ The top 1 and top 3 precision of our approach in definition +L+ ranking are 0.550 and 0.887 respectively. They are much better +L+ than the baseline method of employing relevance search. +L+ Methods for extracting definitions from documents have been +L+ proposed [1, 10, 11, 15, 21, 24, 33]. All of the methods resorted to +L+ human-defined rules for the extraction and did not consider +L+ ranking of definitions. In Information Desk, we rank definitions +L+ according to their likelihoods of being good definitions, +L+ represented by SVM scores. See [32] for details. +L+ In acronym recognition, we find candidate acronym and candidate +L+ expansion pairs from text using pattern matching. There are ten +L+ types of patterns. For example, one of them is `<expansion> +L+ (<acronym>)' in which <expansion> denotes a phrase with the +L+ first letters in the words capitalized and <acronym> denotes a +L+ sequence of the capitalized letters in the same order. The pattern +L+ matches sentences such as "Active Directory is implemented +L+ using the Lightweight Directory Access Protocol (LDAP)". We +L+ then store all the acronyms, their expansions, and the numbers of +L+ occurrences of the expansions. +L+ In search, given an acronym, we retrieve all the expansions +L+ against the acronym and present the corresponding expansions in +L+ descending order of their numbers of occurrences. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 5.2.3 `Who is' +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> We first harvest all the employees' personal information from a +L+ database. It includes name, alias, title, and contact information. +L+ We next automatically extract titles and authors from all the Word +L+ and PowerPoint documents on the intranet. With the extracted +L+ titles and authors we bring together all the documents to each +L+ person, which are thought authored by him/her. Finally, we +L+ extract key terms from the documents for each person and pick up +L+ the top ten key terms in terms of TF-IDF. This feature lies mainly +L+ on document metadata extraction. +L+ Metadata of documents such as title and author is useful for +L+ document processing. However, people seldom define document +L+ metadata by themselves. We collected 6,000 Word and 6,000 +L+ PowerPoint documents and examined how many titles and authors +L+ in the file properties are correct. We found that the accuracies +L+ were only 0.265 and 0.126 respectively. +L+ We take a machine learning approach to automatically extract +L+ titles and authors from the bodies of Office documents, as shown +L+ in Figure 5. We annotate titles in sample documents (for Word +L+ and PowerPoint respectively) and take them as training data, train +L+ statistical models, and perform title extraction using the trained +L+ models. In the models, we mainly utilize format information such +L+ as font size as features. As models, we employ Perceptron with +L+ Uneven Margins [23]. +L+ Experimental results indicate that our approach works well for +L+ title extraction from general documents. Our method can +L+ significantly outperform the baselines: one that always uses the +L+ first lines as titles and the other that always uses the lines in the +L+ largest font sizes as titles. Precision and recall for title extraction +L+ from Word are 0.875 and 0.899 respectively, and precision and +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5. Title and author extraction from four example +L+ </SectLabel_figureCaption> <SectLabel_figure> PowerPoint documents +L+ Microsoft Project 2002 Project Guide +L+ Architecture and Extensibi(ity +L+ White Paper +L+ DRAFT +L+ </SectLabel_figure> <SectLabel_page> 465 +L+ </SectLabel_page> <SectLabel_bodyText> recall for title extraction from PowerPoint are 0.907 and 0.951 +L+ respectively. +L+ Metadata extraction has been intensively studied. For instance, +L+ Han et al [14] proposed a method for metadata extraction from +L+ research papers. They considered the problem as that of +L+ classification based on SVM. They mainly used linguistic +L+ information as features. To the best of our knowledge, no +L+ previous work has been done on metadata extraction from general +L+ documents. We report our title extraction work in details in [19]. +L+ The feature of ‘who is’ can help find documents authored by a +L+ person, but existing in different team web sites. Information +L+ extraction (specifically metadata extraction) makes the aggregation +L+ of information possible. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 5.2.4 ‘Who knows about’ +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The basic idea for the feature is that if a person has authored many +L+ documents on an issue (term), then it is very likely that he/she is an +L+ expert on the issue, or if the person’s name co-occurs in many times +L+ with the issue, then it is likely that he/she is an expert on the issue. +L+ As described above, we can extract titles, authors, and key terms +L+ from all the documents. In this way, we know how many times each +L+ person is associated with each topic in the extracted titles and in the +L+ extracted key terms. We also go through all the documents and see +L+ how many times each person’s name co-occurs with each topic in +L+ text segments within a pre-determined window size. +L+ In search, we use the three types of information: topic in title, topic +L+ in key term, and topic in text segment to rank persons, five persons +L+ for each type. We rank persons with a heuristic method and return +L+ the list of ranked persons. A person who has several documents with +L+ titles containing the topic will be ranked higher than a person whose +L+ name co-occurs with the topic in many documents. +L+ It appears that the results of the feature largely depend on the size of +L+ document collection we crawl. Users’ feedbacks on the results show +L+ that sometimes the results are very accurate, however, sometimes +L+ they are not (due to the lack of information). +L+ Craswell et al. developed a system called ‘P@NOPTIC’, which can +L+ automatically find experts using documents on an intranet [7]. The +L+ system took documents as plain texts and did not utilize metadata of +L+ documents as we do at Information Desk. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 5.2.5 ‘Where is homepage of’ +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> We identify homepages (team web sites) using several rules. Most of +L+ the homepages at the intranet of Microsoft are created by +L+ SharePoint, a product of Microsoft. From SharePoint, we can obtain +L+ a property of each page called ‘ContentClass’. It tells exactly +L+ whether a web page corresponds to a homepage or a team site. So +L+ we know it is a homepage (obviously, this does not apply in +L+ general). Next we use several patterns to pull out titles from the +L+ homepages. The precision of home page identification is nearly +L+ 100%. +L+ In search, we rank the discovered home pages related to a query +L+ term using the URL lengths of the home pages. A home page with a +L+ shorter URL will be ranked higher. +L+ TREC has a task called ‘home/named page finding’ [8, 9], which is +L+ to find home pages talking about a topic. Many methods have been +L+ developed for pursuing the task [5, 6, 26, 29]. Since we can identify +L+ homepages by using special properties on our domain, we do not +L+ consider employing a similar method. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Usually it is hard to conduct evaluation on a practical system. We +L+ evaluated the usefulness of Information Desk by conducting a +L+ survey and by recording system logs. +L+ We have found from analysis results that the ‘what is’ and ‘where is +L+ homepage of’ features are very useful. The ‘who is’ feature works +L+ well, but the ‘who knows about’ feature still needs improvements. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.1 Survey Result Analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The survey described in section 4.3 also includes feedbacks on +L+ Information Desk. +L+ Figure 6 shows a question on the usefulness of the features and a +L+ summary on the answers. We see that the features ‘where is +L+ homepage of’ and ‘what is’ are regarded useful by the responders in +L+ the survey. +L+ Figure 7 shows a question on new features and a summary on the +L+ answers. We see that the users want to use the features of ‘how to’, +L+ ‘when’, ‘where’ and ‘why’ in the future. This also justifies the +L+ correctness of our claim on intranet search made in section 4. +L+ Figure 8 shows a question on purposes of use and a digest on the +L+ results. About 50% of the responders really want to use Information +L+ Desk to search for information. +L+ There is also an open-ended question asking people to make +L+ comments freely. Figure 9 gives some typical answers from the +L+ responders. The first and second answers are very positive, while the +L+ third and fourth point out the necessity of increasing the coverage of +L+ the system. +L+ Figure 6. Users’ evaluation on Information Desk +L+ Figure 7. New features expected by users +L+ </SectLabel_bodyText> <SectLabel_figure> Which feature of Information Desk has helped you in finding +L+ information? +L+ •	‘where is homepage of’ - finding homepages +L+ 54 % +L+ •	‘what is’ - finding definitions/acronyms +L+ 25 % +L+ •	‘who is’ - finding information about people +L+ 18 % +L+ •	‘who knows about’ - finding experts +L+ 3 % +L+ What kind of new feature do you want to use at Information +L+ Desk? (multiple choice) +L+ •	‘how to’ - e.g., "how to activate Windows" +L+ 57 % +L+ •	‘when’ - e.g., "when is Yukon RTM" +L+ 57 % +L+ •	‘where’ - e.g., "where can I find an ATM" +L+ 39 % +L+ •	‘why’ - e.g., "why doesn't my printer work" +L+ 28 % +L+ •	others +L+ 9 % +L+ </SectLabel_figure> <SectLabel_page> 466 +L+ </SectLabel_page> <SectLabel_figure> I visited Information Desk today to +L+ •	conduct testing on Information Desk +L+ 	54 % +L+ •	search for information related to my work +L+ 	46 % +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 8. Motivation of using Information Desk +L+ </SectLabel_figureCaption> <SectLabel_figure> Please provide any additional comments, thanks! +L+ •	This is a terrific tool! Including ‘how to’ and ‘when’ +L+ capabilities will put this in the ‘can’t live without it’ +L+ category. +L+ •	Extremely successful searching so far! Very nice product +L+ with great potential. +L+ •	I would like to see more ‘Microsoftese’ definitions. There is +L+ a lot of cultural/tribal knowledge here that is not explained +L+ anywhere. +L+ •	Typing in my team our website doesn’t come up in the +L+ results, is there any way we can provide content for the +L+ search tool e.g., out group sharepoint URL? +L+ •	... +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 9. Typical user comments to Information Desk +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 6.2 System Log Analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We have made log during the running of Information Desk. The +L+ log includes user IP addresses, queries and clicked documents +L+ (recall that links to the original documents, from which +L+ information has been extraction, are given in search). The log data +L+ was collected from 1,303 unique users during the period from +L+ November 26th, 2004 to February 22nd, 2005. The users were +L+ Microsoft employees. +L+ In the log, there are 9,076 query submission records. The records +L+ include 4,384 unique query terms. About 40% of the queries are +L+ related to the ‘what is’ feature, 29% related to ‘where is homepage +L+ of’, 30% related to ‘who knows about’ and 22% related to ‘who +L+ is’. A query can be related to more than one feature. +L+ In the log, there are 2,316 clicks on documents after query +L+ submissions. The numbers of clicks for the ‘what is’, ‘where is +L+ homepage of’, ‘who knows about’, and ‘who is’ features are 694, +L+ 1041, 200 and 372, respectively. Note that for ‘what is’, ‘where is +L+ home page of’, and ‘who knows about’ we conduct ranking on +L+ retrieved information. The top ranked results are considered to be +L+ the best. If a user has clicked a top ranked document, then it +L+ means that he is interested in the document, and thus it is very +L+ likely he has found the information he looks for. Thus a system +L+ which has higher average rank of clicks is better than the other +L+ that does not. We used average rank of clicked documents to +L+ evaluate the performances of the features. The average ranks of +L+ clicks for ‘what is’, ‘where is homepage of’ and ‘who knows +L+ about’ are 2.4, 1.4 and 4.7 respectively. The results indicate that +L+ for the first two features, users usually can find information they +L+ look for on the top three answers. Thus it seems safe to say that +L+ the system have achieved practically acceptable performances for +L+ the two features. As for ‘who is’, ranking of a person’s documents +L+ does not seem to be necessary and the performance should be +L+ evaluated in a different way. (For example, precision and recall of +L+ metadata extraction as we have already reported in section 5). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we have investigated the problem of intranet search +L+ using information extraction. +L+ •	Through an analysis of survey results and an analysis of +L+ search log data, we have found that search needs on intranet +L+ can be categorized into a hierarchy. +L+ •	Based on the finding, we propose a new approach to intranet +L+ search in which we conduct search for each special type of +L+ information. +L+ •	We have developed a system called ‘Information Desk’, +L+ based on the idea. In Information Desk, we provide search on +L+ four types of information - finding term definitions, +L+ homepages of groups or topics, employees’ personal +L+ information and experts on topics. Information Desk has +L+ been deployed to the intranet of Microsoft and has received +L+ accesses from about 500 employees per month. Feedbacks +L+ from users show that the proposed approach is effective and +L+ the system can really help employees to find information. +L+ •	For each type of search, information extraction technologies +L+ have been used to extract, fuse, and summarize information +L+ in advance. High performance component technologies for +L+ the mining have been developed. +L+ As future work, we plan to increase the number of search types +L+ and combine them with conventional relevance search. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We thank Jin Jiang, Ming Zhou, Avi Shmueli, Kyle Peltonen, +L+ Drew DeBruyne, Lauri Ellis, Mark Swenson, and Mark Davies for +L+ their supports to the project. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] S. Blair-Goldensohn, K.R. McKeown, A.H. Schlaikjer. A +L+ Hybrid Approach for QA Track Definitional Questions. In +L+ Proc. of Twelfth Annual Text Retrieval Conference (TREC- +L+ 12), NIST, Nov., 2003. +L+ [2] E. Brill, S. Dumais, and M. Banko, An Analysis of the +L+ AskMSR Question-Answering System, EMNLP 2002 +L+ [3] M. Chen, A. Hearst, A. Marti, J. Hong, and J. Lin, Cha-Cha: +L+ A System for Organizing Intranet Results. Proceedings of the +L+ 2nd USENIX Symposium on Internet Technologies and +L+ Systems. Boulder, CO. Oct. 1999. +L+ [4] C. L. A. Clarke, G. V. Cormack, T. R. Lynam, C. M. Li, and +L+ G. L. McLearn, Web Reinforced Question Answering +L+ (MultiText Experiments for TREC 2001). TREC 2001 +L+ [5] N. Craswell, D. Hawking, and S.E. Robertson. Effective site +L+ finding using link anchor information. In Proc. of the 24th +L+ annual international ACM SIGIR conference on research +L+ and development in information retrieval, pages 250--257, +L+ 2001. +L+ [6] N. Craswell, D. Hawking, and T. Upstill. TREC12 Web and +L+ Interactive Tracks at CSIRO. In TREC12 Proceedings, 2004. +L+ [7] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins. +L+ P@noptic expert: Searching for experts not just for +L+ documents. Poster Proceedings of AusWeb'01, +L+ </SectLabel_reference> <SectLabel_page> 467 +L+ </SectLabel_page> <SectLabel_reference> 2001b./urlausweb.scu.edu.au/aw01/papers/edited/vercoustre/ +L+ paper.htm. +L+ [8] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. +L+ Overview of the TREC-2003 Web Track. In NIST Special +L+ Publication: 500-255, The Twelfth Text REtrieval +L+ Conference (TREC 2003), Gaithersburg, MD, 2003. +L+ [9] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. Task +L+ Descriptions: Web Track 2003. In TREC12 Proceedings, +L+ 2004. +L+ [10] H. Cui, M-Y. Kan, and T-S. Chua. Unsupervised Learning of +L+ Soft Patterns for Definitional Question Answering, +L+ Proceedings of the Thirteenth World Wide Web conference +L+ (WWW 2004), New York, May 17-22, 2004. +L+ [11] A. Echihabi, U.Hermjakob, E. Hovy, D. Marcu, E. Melz, D. +L+ Ravichandran. Multiple-Engine Question Answering in +L+ TextMap. In Proc. of Twelfth Annual Text Retrieval +L+ Conference (TREC-12), NIST, Nov., 2003. +L+ [12] R. Fagin, R. Kumar, K. S. McCurley, J. Novak, D. +L+ Sivakumar, J. A. Tomlin, and D. P. Williamson. Searching +L+ the workplace web. Proc. 12th World Wide Web Conference, +L+ Budapest, 2003. +L+ [13] S. Feldman and C. Sherman. The high cost of not finding +L+ information. Technical Report #29127, IDC, April 2003. +L+ [14] H. Han, C. L. Giles, E. Manavoglu, H. Zha, Z. Zhang, and E. +L+ A. Fox. Automatic Document Metadata Extraction using +L+ Support Vector Machines. In Proceedings of the third +L+ ACM/IEEE-CS joint conference on Digital libraries, 2003 +L+ [15] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. +L+ Williams, J. Bensley. Answer Mining by Combining +L+ Extraction Techniques with Abductive Reasoning. In Proc. +L+ of Twelfth Annual Text Retrieval Conference (TREC-12), +L+ NIST, Nov., 2003. +L+ [16] D. Hawking. Challenges in Intranet search. Proceedings of +L+ the fifteenth conference on Australasian database. Dunedin, +L+ New Zealand, 2004. +L+ [17] D. Hawking, N. Craswell, F. Crimmins, and T. Upstill. +L+ Intranet search: What works and what doesn't. Proceedings +L+ of the Infonortics Search Engines Meeting, San Francisco, +L+ April 2002. +L+ [18] E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C. Y. Lin. +L+ Question Answering in Webclopedia. TREC 2000 +L+ [19] Y. Hu, H. Li, Y. Cao, D. Meyerzon, and Q. Zheng. +L+ Automatic Extraction of Titles from General Documents +L+ using Machine Learning. To appear at Proc. of Joint +L+ Conference on Digital Libraries (JCDL), 2005. Denver, +L+ Colorado, USA. 2005. +L+ [20] A. Ittycheriah and S. Roukos, IBM's Statistical Question +L+ Answering System-TREC 11. TREC 2002 +L+ [21] J. Klavans and S. Muresan. DEFINDER: Rule-Based +L+ Methods for the Extraction of Medical Terminology and +L+ their Associated Definitions from On-line Text. In +L+ Proceedings of AMIA Symposium 2000. +L+ [22] C. C. T. Kwok, O. Etzioni, and D. S. Weld, Scaling question +L+ answering to the Web. WWW-2001: 150-161 +L+ [23] Y. Li, H Zaragoza, R Herbrich, J Shawe-Taylor, and J. S. +L+ Kandola. The Perceptron Algorithm with Uneven Margins. +L+ in Proceedings of ICML'02. +L+ [24] B. Liu, C. W. Chin, and H. T. Ng. Mining Topic-Specific +L+ Concepts and Definitions on the Web. In Proceedings of the +L+ twelfth international World Wide Web conference (WWW- +L+ 2003), 20-24 May 2003, Budapest, HUNGARY. +L+ [25] D. Mattox, M. Maybury and D. Morey. Enterprise Expert +L+ and Knowledge Discovery. Proceedings of the HCI +L+ International '99 (the 8th International Conference on +L+ Human-Computer Interaction) on Human-Computer +L+ Interaction: Communication, Cooperation, and Application +L+ Design-Volume 2 - Volume 2. 1999. +L+ [26] P. Ogilvie and J. Callan. Combining Structural Information +L+ and the Use of Priors in Mixed Named-Page and Homepage +L+ Finding. In TREC12 Proceedings, 2004. +L+ [27] D. R. Radev, W. Fan, H. Qi, H. Wu, and A. Grewal. +L+ Probabilistic question answering on the web. WWW 2002: +L+ 408-419 +L+ [28] D. E. Rose and D. Levinson. Understanding user goals in +L+ web search. Proceedings of the 13th international World +L+ Wide Web conference on Alternate track papers & posters, +L+ 2004 New York, USA. +L+ [29] J. Savoy, Y. Rasolofo, and L. Perret, L. Report on the TREC- +L+ 2003 Experiment: Genomic and Web Searches. In TREC12 +L+ Proceedings, 2004. +L+ [30] D. Stenmark. A Methodology for Intranet Search Engine +L+ Evaluations. Proceedings of IRIS22, Department of CS/IS, +L+ University of Jyväskylä, Finland, August 1999. +L+ [31 ] V. N. Vapnik. The Nature of Statistical Learning Theory. +L+ Springer, 1995. +L+ [32] J. Xu, Y. Cao, H. Li, and M. Zhao. Ranking Definitions with +L+ Supervised Learning Methods. In Proc. of 14th International +L+ World Wide Web Conference (WWW05), Industrial and +L+ Practical Experience Track, Chiba, Japan, pp.811-819, 2005. +L+ [33] J. Xu, A. Licuanan, R. Weischedel. TREC 2003 QA at BBN: +L+ Answering Definitional Questions. In Proc. of 12th Annual +L+ Text Retrieval Conference (TREC-12), NIST, Nov., 2003. +L+ [34] H. Yang, H. Cui, M. Maslennikov, L. Qiu, M-Y. Kan, and T- +L+ S. Chua, QUALIFIER in TREC-12 QA Main Task. TREC +L+ 2003: 480-488 +L+ [35] Intellectual capital management products. Verity, +L+ http://www.verity.com/ +L+ [36] IDOL server. Autonomy, +L+ http://www.autonomy.com/content/home/ +L+ [37] Fast data search. Fast Search & Transfer, +L+ http://www.fastsearch.com/ +L+ [38] Atomz intranet search. Atomz, http://www.atomz.com/ +L+ [39] Google Search Appliance. Google, +L+ http://www.google.com/enterprise/ +L+ </SectLabel_reference> <SectLabel_page> 468 +L+ </SectLabel_page>
<SectLabel_title> A New Statistical Formula for Chinese Text Segmentation +L+ Incorporating Contextual Information +L+ </SectLabel_title> <SectLabel_author> Yubin Dai +L+ Christopher S.G. Khoo +L+ </SectLabel_author> <SectLabel_affiliation> Division of Information Studies +L+ School of Applied Science +L+ Nanyang Technological University +L+ </SectLabel_affiliation> <SectLabel_address> Singapore 639798 +L+ (65) 790-4602 +L+ </SectLabel_address> <SectLabel_email> dyb_lte@hotmail.com +L+ assgkhoo@ntu.edu.sg +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A new statistical formula for identifying 2-character words in +L+ Chinese text, called the contextual information formula, was +L+ developed empirically by performing stepwise logistic regression +L+ using a sample of sentences that had been manually segmented. +L+ Contextual information in the form of the frequency of characters +L+ that are adjacent to the bigram being processed as well as the +L+ weighted document frequency of the overlapping bigrams were +L+ found to be significant factors for predicting the probablity that +L+ the bigram constitutes a word. Local information (the number of +L+ times the bigram occurs in the document being segmented) and +L+ the position of the bigram in the sentence were not found to be +L+ useful in determining words. The contextual information formula +L+ was found to be significantly and substantially better than the +L+ mutual information formula in identifying 2-character words. +L+ The method can also be used for identifying multi-word terms in +L+ English text. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Chinese text segmentation, word boundary identification, logistic +L+ regression, multi-word terms +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Chinese text is different from English text in that there is no +L+ explicit word boundary. In English text, words are separated by +L+ spaces. Chinese text (as well as text of other Oriental languages) +L+ is made up of ideographic characters, and a word can comprise +L+ one, two or more such characters, without explicit indication +L+ where one word ends and another begins. +L+ This has implications for natural language processing and +L+ information retrieval with Chinese text. Text processing +L+ techniques that have been developed for Western languages deal +L+ with words as meaningful text units and assume that words are +L+ easy to identify. These techniques may not work well for Chinese +L+ text without some adjustments. To apply these techniques to +L+ </SectLabel_bodyText> <SectLabel_author> Teck Ee Loh +L+ </SectLabel_author> <SectLabel_address> 10 Kent Ridge Crescent +L+ </SectLabel_address> <SectLabel_affiliation> Data Storage Institute +L+ </SectLabel_affiliation> <SectLabel_address> Singapore 119260 +L+ (65) 874-8413 +L+ </SectLabel_address> <SectLabel_email> dsilohte@dsi.nus.edu.sg +L+ </SectLabel_email> <SectLabel_bodyText> Chinese text, automatic methods for identifying word boundaries +L+ accurately have to be developed. The process of identifying word +L+ boundaries has been referred to as text segmentation or, more +L+ accurately, word segmentation. +L+ Several techniques have been developed for Chinese text +L+ segmentation. They can be divided into: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. statistical methods, based on statistical properties and +L+ frequencies of characters and character strings in a corpus +L+ (e.g. [13] and [16]). +L+ 2. dictionary-based methods, often complemented with +L+ grammar rules. This approach uses a dictionary of words to +L+ identify word boundaries. Grammar rules are often used to +L+ resolve conflicts (choose between alternative segmentations) +L+ and to improve the segmentation (e.g. [4], [8], [19] and [20]). +L+ 3. syntax-based methods, which integrate the word +L+ segmentation process with syntactic parsing or part-of-speech +L+ tagging (e.g. [1]). +L+ 4. conceptual methods, that make use of some kind of semantic +L+ processing to extract information and store it in a knowledge +L+ representation scheme. Domain knowledge is used for +L+ disambiguation (e.g. [9]). +L+ </SectLabel_listItem> <SectLabel_bodyText> Many researchers use a combination of methods (e.g. [14]). +L+ The objective of this study was to empirically develop a +L+ statistical formula for Chinese text segmentation. Researchers +L+ have used different statistical methods in segmentation, most of +L+ which were based on theoretical considerations or adopted from +L+ other fields. In this study, we developed a statistical formula +L+ empirically by performing stepwise logistic regression using a +L+ sample of sentences that had been manually segmented. This +L+ paper reports the new formula developed for identifying 2- +L+ character words, and the effectiveness of this formula compared +L+ with the mutual information formula. +L+ This study has the following novel aspects: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	The statistical formula was derived empirically using +L+ regression analysis. +L+ •	The manual segmentation was performed to identify +L+ </SectLabel_listItem> <SectLabel_bodyText> meaningful	words rather than simple words. +L+ Meaningful words include phrasal words and multi- +L+ word terms. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	In addition to the relative frequencies of bigrams and +L+ </SectLabel_listItem> <SectLabel_bodyText> characters often used in other studies, our study also +L+ investigated the use of document frequencies and weighted +L+ </SectLabel_bodyText> <SectLabel_page> 82 +L+ </SectLabel_page> <SectLabel_bodyText> document frequencies. Weighted document frequencies are +L+ similar to document frequencies but each document is +L+ weighted by the square of the number of times the character +L+ or bigram occurs in the document. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Contextual information was included in the study. To predict +L+ whether the bigram BC in the character string	A B C D +L+ constitutes a word, we investigated whether the +L+ frequencies for AB, CD, A and D should be included in the +L+ formula. +L+ •	Local frequencies were included in the study. We +L+ investigated character and bigram frequencies within the +L+ document in which the sentence occurs (i.e. the number of +L+ times the character or bigram appears in the document being +L+ segmented). +L+ •	We investigated whether the position of the bigram (at the +L+ beginning of the sentence, before a punctuation mark, or after +L+ a punctuation mark) had a significant effect. +L+ •	We developed a segmentation algorithm to apply the +L+ statistical formula to segment sentences and resolve conflicts. +L+ </SectLabel_listItem> <SectLabel_bodyText> In this study, our objective was to segment text into +L+ meaningful words rather than simple words . A simple +L+ word is the smallest independent unit of a sentence that has +L+ meaning on its own. A meaningful word can be a simple word or +L+ a compound word comprising 2 or more simple words – +L+ depending on the context. In many cases, the meaning of a +L+ compound word is more than just a combination of the meanings +L+ of the constituent simple words, i.e. some meaning is lost when +L+ the compound word is segmented into simple words. +L+ Furthermore, some phrases are used so often that native speakers +L+ perceive them and use them as a unit. Admittedly, there is some +L+ subjectivity in the manual segmentation of text. But the fact that +L+ statistical models can be developed to predict the manually +L+ segmented words substantially better than chance indicates some +L+ level of consistency in the manual segmentation. +L+ The problem of identifying meaningful words is not limited to +L+ Chinese and oriental languages. Identifying multi-word terms is +L+ also a problem in text processing with English and other Western +L+ languages, and researchers have used the mutual information +L+ formula and other statistical approaches for identifying such +L+ terms (e.g. [3], [6] and [7]). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. PREVIOUS STUDIES +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> There are few studies using a purely statistical approach to +L+ Chinese text segmentation. One statistical formula that has been +L+ used by other researchers (e.g. [11] and [16]) is the mutual +L+ information formula. Given a character string A B C D +L+ , the mutual information for the bigram BC is given by the +L+ formula: +L+ </SectLabel_bodyText> <SectLabel_equation> freq(BC) +L+ log2 freq(B) * freq(C) +L+ = log2 freq(BC) – log2 freq(B) – log2 freq(C) +L+ </SectLabel_equation> <SectLabel_bodyText> where freq refers to the relative frequency of the character or +L+ bigram in the corpus (i.e. the number of times the character or +L+ bigram occurs in the corpus divided by the number of characters +L+ in the corpus). +L+ Mutual information is a measure of how strongly the two +L+ characters are associated, and can be used as a measure of how +L+ likely the pair of characters constitutes a word. Sproat & Shih +L+ [16] obtained recall and precision values of 94% using mutual +L+ information to identify words. This study probably segmented +L+ text into simple words rather than meaningful words. In our +L+ study, text was segmented into meaningful words and we +L+ obtained much poorer results for the mutual information +L+ formula. +L+ Lua [12] and Lua & Gan [13] applied information theory to the +L+ problem of Chinese text segmentation. They calculated the +L+ information content of characters and words using the +L+ information entropy formula I = - log2 P, where P is the +L+ probability of occurrence of the character or word. If the +L+ information content of a character string is less than the sum of +L+ the information content of the constituent characters, then the +L+ character string is likely to constitute a word. The formula for +L+ calculating this loss of information content when a word is +L+ formed is identical to the mutual information formula. Lua & +L+ Gan [13] obtained an accuracy of 99% (measured in terms of the +L+ number of errors per 100 characters). +L+ Tung & Lee [18] also used information entropy to identify +L+ unknown words in a corpus. However, instead of calculating the +L+ entropy value for the character string that is hypothesized to be a +L+ word (i.e. the candidate word), they identified all the characters +L+ that occurred to the left of the candidate word in the corpus. For +L+ each left character, they calculated the probability and entropy +L+ value for that character given that it occurs to the left of the +L+ candidate word. The same is done for the characters to the right +L+ of the candidate word. If the sum of the entropy values for the +L+ left characters and the sum of the entropy values for the right +L+ characters are both high, than the candidate word is considered +L+ likely to be a word. In other words, a character string is likely to +L+ be a word if it has several different characters to the left and to +L+ the right of it in the corpus, and none of the left and right +L+ characters predominate (i.e. not strongly associated with the +L+ character string). +L+ Ogawa & Matsuda [15] developed a statistical method to +L+ segment Japanese text. Instead of attempting to identify words +L+ directly, they developed a formula to estimate the probability that +L+ a bigram straddles a word boundary. They referred to this as the +L+ segmentation probability. This was complemented with some +L+ syntactic information about which class of characters could be +L+ combined with which other class. +L+ All the above mathematical formulas used for identifying words +L+ and word boundaries were developed based on theoretical +L+ considerations and not derived empirically. +L+ Other researchers have developed statistical methods to find the +L+ best segmentation for the whole sentence rather than focusing on +L+ identifying individual words. Sproat et al. [17] developed a +L+ stochastic finite state model for segmenting text. In their model, +L+ a word dictionary is represented as a weighted finite state +L+ transducer. Each weight represents the estimated cost of the +L+ word (calculated using the negative log probability). Basically, +L+ the system selects the sentence segmentation that has the +L+ smallest total cost. Chang & Chen [1] developed a method for +L+ word segmentation and part-of-speech tagging based on a first- +L+ order hidden Markov model. +L+ </SectLabel_bodyText> <SectLabel_equation> MI(BC) = +L+ </SectLabel_equation> <SectLabel_page> 83 +L+ </SectLabel_page> <SectLabel_sectionHeader> 3. RESEARCH METHOD +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The purpose of this study was to empirically develop a statistical +L+ formula for identifying 2-character words as well as to +L+ investigate the usefulness of various factors for identifying the +L+ words. A sample of 400 sentences was randomly selected from 2 +L+ months (August and September 1995) of news articles from the +L+ Xin Hua News Agency, comprising around 2.3 million characters. +L+ The sample sentences were manually segmented. The +L+ segmentation rules described in [10] were followed fairly closely. +L+ More details of the manual segmentation process, especially with +L+ regard to identifying meaningful words will be given in [5]. +L+ 300 sentences were used for model building, i.e. using regression +L+ analysis to develop a statistical formula. 100 sentences were set +L+ aside for model validation to evaluate the formula developed in +L+ the regression analysis. The sample sentences were broken up +L+ into overlapping bigrams. In the regression analysis, the +L+ dependent variable was whether a bigram was a two-character +L+ word according to the manual segmentation. The independent +L+ variables were various corpus statistics derived from the corpus +L+ (2 months of news articles). +L+ The types of frequency information investigated were: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Relative frequency of individual characters and bigrams +L+ (character pairs) in the corpus, i.e. the number of times the +L+ character or bigram occurs in the corpus divided by the total +L+ number of characters in the corpus. +L+ 2. Document frequency of characters and bigrams, i.e. the +L+ number of documents in the corpus containing the character +L+ or bigram divided by the total number of documents in the +L+ corpus. +L+ 3. Weighted document frequency of characters and bigrams. To +L+ calculate the weighted document frequency of a character +L+ string, each document containing the character string is +L+ assigned a score equal to the square of the number of times +L+ the character string occurs in the document. The scores for all +L+ the documents containing the character string are then +L+ summed and divided by the total number of documents in the +L+ corpus to obtain the weighted document frequency for the +L+ character string. The rationale is that if a character string +L+ occurs several times within the same document, this is +L+ stronger evidence that the character string constitutes a word, +L+ than if the character string occurs once in several documents. +L+ Two or more characters can occur together by chance in +L+ several different documents. It is less likely for two +L+ characters to occur together several times within the same +L+ document by chance. +L+ 4. Local frequency in the form of within-document frequency of +L+ characters and bigrams, i.e. the number of times the character +L+ or bigram occurs in the document being segmented. +L+ 5. Contextual information. Frequency information of characters +L+ adjacent to a bigram is used to help determine whether the +L+ bigram is a word. For the character string	A B C D +L+ , to determine whether the bigram BC is a word, +L+ frequency information for the adjacent characters A and D, as +L+ well as the overlapping bigrams AB and BC were considered. +L+ 6. Positional information. We studied whether the position of a +L+ character string (at the beginning, middle or end of a +L+ sentence) gave some indication of whether the character +L+ string was a word. +L+ </SectLabel_listItem> <SectLabel_bodyText> The statistical model was developed using forward stepwise +L+ logistic regression, using the Proc Logistic function in the SAS +L+ v.6.12 statistical package for Windows. Logistic regression is an +L+ appropriate regression technique when the dependent variable is +L+ binary valued (takes the value 0 or 1). The formula developed +L+ using logistic regression predicts the probability (more +L+ accurately, the log of the odds) that a bigram is a meaningful +L+ word. +L+ In the stepwise regression, the threshold for a variable to enter +L+ the model was set at the 0.001 significance level and the +L+ threshold for retaining a variable in the model was set at 0.01. In +L+ addition, preference was given to relative frequencies and local +L+ frequencies because they are easier to calculate than document +L+ frequencies and weighted document frequencies. Also, relative +L+ frequencies are commonly used in previous studies. +L+ Furthermore, a variable was entered in a model only if it gave a +L+ noticeable improvement to the effectiveness of the model. During +L+ regression analysis, the effectiveness of the model was estimated +L+ using the measure of concordance that was automatically output +L+ by the SAS statistical program. A variable was accepted into the +L+ model only if the measure of concordance improved by at least +L+ 2% when the variable was entered into the model. +L+ We evaluated the accuracy of the segmentation using measures of +L+ recall and precision. Recall and precision in this context are +L+ defined as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> Recall = No. of 2-character words identified in the automatic +L+ segmentation that are correct +L+ No. of 2-character words identified in the manual +L+ segmentation +L+ Precision = No. of 2-character words identified in the automatic +L+ segmentation that are correct +L+ No. of 2-character words identified in the automatic +L+ segmentation +L+ </SectLabel_equation> <SectLabel_sectionHeader> 4. STATISTICAL FORMULAS +L+ DEVELOPED +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 4.1 The Contextual Information Formula +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The formula that was developed for 2-character words is as +L+ follows. Given a character string A B C D , the +L+ association strength for bigram BC is: +L+ </SectLabel_bodyText> <SectLabel_equation> Assoc(BC) = 0.35 * log2 freq(BC) + 0.37 * log2 freq(A) + +L+ 0.32 log2 freq(D) – 0.36 * log2 docfreqwt(AB) – +L+ 0.29 * log2 docfreqwt(CD) + 5.91 +L+ </SectLabel_equation> <SectLabel_bodyText> where freq refers to the relative frequency in the corpus and +L+ docfreqwt refers to the weighted document frequency. We refer to +L+ this formula as the contextual information formula. More details +L+ of the regression model are given in Table 1. +L+ The formula indicates that contextual information is helpful in +L+ identifying word boundaries. A in the formula refers to the +L+ character preceding the bigram that is being processed, whereas +L+ D is the character following the bigram. The formula indicates +L+ that if the character preceding and the character following the +L+ bigram have high relative frequencies, then the bigram is more +L+ likely to be a word. +L+ </SectLabel_bodyText> <SectLabel_page> 84 +L+ </SectLabel_page> <SectLabel_table> 		Parameter	Standard	Wald		Pr >	Standardized +L+ Variable	DF	Estimate	Error	Chi-Square		Chi-Square	Estimate +L+ INTERCPT	1	5.9144	0.1719	1184.0532	0.0001	. +L+ Log freq(BC)	1	0.3502	0.0106	1088.7291	0.0001	0.638740 +L+ Log freq(A)	1	0.3730	0.0113	1092.1382	0.0001	0.709621 +L+ Log freq(D)	1	0.3171	0.0107	886.4446	0.0001	0.607326 +L+ Log docfreqwt(AB)	1	-0.3580	0.0111	1034.0948	0.0001	-0.800520 +L+ Log docfreqwt(CD)	1	-0.2867	0.0104	754.2276	0.0001	-0.635704 +L+ Note: freq refers to the relative frequency, and docfreqwt refers to the +L+ weighted document frequency. +L+ Association of Predicted Probabilities and Observed Responses +L+ Somers' D = 0.803 +L+ Gamma	= 0.803 +L+ Tau-a	= 0.295 +L+ (23875432 pairs)	c	= 0.901 +L+ Concordant	=	90.1% +L+ Discordant	=	9.8% +L+ Tied	=	0.1% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1. Final regression model for 2-character words +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Contextual information involving the weighted document +L+ frequency was also found to be significant. The formula indicates +L+ that if the overlapping bigrams AB and CD have high weighted +L+ document frequencies, then the bigram BC is less likely to be a +L+ word. We tried replacing the weighted document frequencies +L+ with the unweighted document frequencies as well as the relative +L+ frequencies. These were found to give a lower concordance score. +L+ Even with docfreq (AB) and docfreq (CD) in the model, docfreqwt +L+ (AB) and docfreqwt (CD) were found to improve the model +L+ significantly. However, local frequencies were surprisingly not +L+ found to be useful in predicting 2-character words. +L+ We investigated whether the position of the bigram in the +L+ sentence was a significant factor. We included a variable to +L+ indicate whether the bigram occurred just after a punctuation +L+ mark or at the beginning of the sentence, and another variable to +L+ indicate whether the bigram occurred just before a punctuation +L+ mark or at the end of a sentence. The interaction between each of +L+ the position variables and the various relative frequencies +L+ were not significant. However, it was found that whether or not +L+ the bigram was at the end of a sentence or just before a +L+ punctuation mark was a significant factor. Bigrams at the end of +L+ a sentence or just before a punctuation mark tend to be words. +L+ However, since this factor did not improve the concordance score +L+ by 2%, the effect was deemed too small to be included in the +L+ model. +L+ It should be noted that the contextual information used in the +L+ study already incorporates some positional information. The +L+ frequency of character A (the character preceding the bigram) +L+ was given the value 0 if the bigram was preceded by a +L+ punctuation mark or was at the beginning of a sentence. +L+ Similarly, the frequency of character D (the character following +L+ the bigram) was given the value 0 if the bigram preceded a +L+ punctuation mark. +L+ We also investigated whether the model would be different for +L+ high and low frequency words. We included in the regression +L+ analysis the interaction between the relative frequency of the +L+ bigram and the other relative frequencies. The interaction terms +L+ were not found to be significant. Finally, it is noted that the +L+ coefficients for the various factors are nearly the same, hovering +L+ around 0.34. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Improved Mutual Information Formula +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this study, the contextual information formula (CIF) was +L+ evaluated by comparing it with the mutual information formula +L+ (MIF). We wanted to find out whether the segmentation results +L+ using the CIF was better than the segmentation results using the +L+ MIF. +L+ In the CIF model, the coefficients of the variables were +L+ determined using regression analysis. If CIF was found to give +L+ better results than MIF, it could be because the coefficients for +L+ the variables in CIF had been determined empirically – and not +L+ because of the types of variables in the formula. To reject this +L+ explanation, regression analysis was used to determine the +L+ coefficients for the factors in the mutual information formula. +L+ We refer to this new version of the formula as the improved +L+ mutual information formula. +L+ Given a character string	A B C D	, the improved +L+ mutual information formula is: +L+ </SectLabel_bodyText> <SectLabel_equation> Improved MI(BC) = 0.39 * log2 freq(BC) - 0.28 * log2 freq(B) - +L+ 0.23 log2 freq(C) - 0.32 +L+ </SectLabel_equation> <SectLabel_bodyText> The coefficients are all close to 0.3. The formula is thus quite +L+ similar to the mutual information formula, except for a +L+ multiplier of 0.3. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. SEGMENTATION ALGORITHMS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The automatic segmentation process has the following steps: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. The statistical formula is used to calculate a score for each +L+ bigram to indicate its association strength (or how likely the +L+ bigram is a word). +L+ 2. A threshold value is then set and used to decide which +L+ bigram is a word. If a bigram obtains a score above the +L+ threshold value, then it is selected as a word. Different +L+ threshold values can be used, depending on whether the user +L+ prefers high recall or high precision. +L+ 3. A segmentation algorithm is used to resolve conflict. If two +L+ overlapping bigrams both have association scores above the +L+ </SectLabel_listItem> <SectLabel_page> 85 +L+ </SectLabel_page> <SectLabel_table> 	Precision +L+ Recall	Comparative Forward Match	Forward Match	Improvement +L+ Mutual Information		-	- +L+ 90%	51% +L+ 80%	52%	47%	5% +L+ 70%	53%	51%	2% +L+ 60%	54%	52%	2% +L+ Improved Mutual Information +L+ 90%	51%		-	- +L+ 80%	53%	46%	7% +L+ 70%	54%	52%	2% +L+ 60%	55%	54%	1% +L+ Contextual Information Formula +L+ 90%	55%	54%	1% +L+ 80%	62%	62%	0% +L+ 70%	65%	65%	0% +L+ 60%	68%	68%	0% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2. Recall and precision values for the comparative +L+ forward match segmentation algorithm vs. forward match +L+ </SectLabel_tableCaption> <SectLabel_bodyText> threshold value, then there is conflict or ambiguity. The +L+ frequency of such conflicts will rise as the threshold value is +L+ lowered. The segmentation algorithm resolves the conflict +L+ and selects one of the bigrams as a word. +L+ One simple segmentation algorithm is the forward match +L+ algorithm. Consider the sentence A B C D E . The +L+ segmentation process proceeds from the beginning of the +L+ sentence to the end. First the bigram AB is considered. If the +L+ association score is above the threshold, then AB is taken as a +L+ word, and the bigram CD is next considered. If the association +L+ score of AB is below the threshold, the character A is taken as a +L+ 1-character word. And the bigram BC is next considered. In +L+ effect, if the association score of both AB and BC are above +L+ threshold, the forward match algorithm selects AB as a word and +L+ not BC. +L+ The forward match method for resolving ambiguity is somewhat +L+ arbitrary and not satisfactory. When overlapping bigrams exceed +L+ the threshold value, it simply decides in favour of the earlier +L+ bigram. Another segmentation algorithm was developed in this +L+ study which we refer to as the comparative forward match +L+ algorithm. This has an additional step: +L+ If 2 overlapping bigrams AB and BC both have scores above +L+ the threshold value then their scores are compared. If AB has a +L+ higher value, then it is selected as a word, and the program +L+ next considers the bigrams CD and DE. On the other hand, if +L+ AB has a lower value, then character A is selected as a 1- +L+ character word, and the program next considers bigrams BC +L+ and CD. +L+ The comparative forward match method (CFM) was compared +L+ with the forward match method (FM) by applying them to the 3 +L+ statistical formulas (the contextual information formula, the +L+ mutual information formula and the improved mutual +L+ information formula). One way to compare the effectiveness of +L+ the 2 segmentation algorithms is by comparing their precision +L+ figures at the same recall levels. The precision figures for +L+ </SectLabel_bodyText> <SectLabel_table> Precision +L+ Recall	Mutual	Improved Mutual Contextual +L+ Information	Information	Information +L+ 90%	57%	(0.0)	57%	(-2.5)	61%	(-1.5) +L+ 80%	59%	(3.7)	59%	(-1.5)	66%	(-0.8) +L+ 70%	59%	(4.7)	60%	(-1.0)	70%	(-0.3) +L+ 60%	60%	(5.6)	62%	(-0.7)	74%	(0.0) +L+ * Threshold values are given in parenthesis. +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3. Recall and precision for three statistical formulas +L+ </SectLabel_tableCaption> <SectLabel_bodyText> selected recall levels are given in Table 2. The results are based +L+ on the sample of 300 sentences. +L+ The comparative forward match algorithm gave better results for +L+ the mutual information and improved mutual information +L+ formulas – especially at low threshold values when a large +L+ number of conflicts are likely. Furthermore, for the forward +L+ match method, the recall didn t go substantially higher than +L+ 80% even at low threshold values. +L+ For the contextual information formula, the comparative forward +L+ match method did not perform better than forward match, except +L+ at very low threshold values when the recall was above 90%. +L+ This was expected because the contextual information formula +L+ already incorporates information about neighboring characters +L+ within the formula. The formula gave very few conflicting +L+ segmentations. There were very few cases of overlapping +L+ bigrams both having association scores above the threshold – +L+ except when threshold values were below –1.5. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 6.1 Comparing the Contextual Information +L+ Formula with the Mutual Information +L+ Formula +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this section we compare the effectiveness of the contextual +L+ information formula with the mutual information formula and +L+ the improved mutual information formula using the 100 +L+ sentences that had been set aside for evaluation purposes. For the +L+ contextual information formula, the forward match segmentation +L+ algorithm was used. The comparative forward match algorithm +L+ was used for the mutual information and the improved mutual +L+ information formulas. +L+ The three statistical formulas were compared by comparing their +L+ precision figures at 4 recall levels – at 60%, 70%, 80% and 90%. +L+ For each of the three statistical formulas, we identified the +L+ threshold values that would give a recall of 60%, 70%, 80% and +L+ 90%. We then determined the precision values at these threshold +L+ values to find out whether the contextual information formula +L+ gave better precision than the other two formulas at 60%, 70%, +L+ 80% and 90% recall. These recall levels were selected because a +L+ recall of 50% or less is probably unacceptable for most +L+ applications. +L+ The precision figures for the 4 recall levels are given in Table 3. +L+ The recall-precision graphs for the 3 formulas are given in Fig. 1. +L+ The contextual information formula substantially outperforms +L+ the mutual information and the improved mutual information +L+ formulas. At the 90% recall level, the contextual information +L+ </SectLabel_bodyText> <SectLabel_page> 86 +L+ </SectLabel_page> <SectLabel_table> Avg Precision +L+ Avg	Mutual	Improved Mutual Contextual +L+ Recall Information	Information	Information +L+ 90%	57%	(1.0)	58%	(-2.3)	61%	(-1.5) +L+ 80%	60%	(3.8)	60%	(-1.4)	67%	(-0.7) +L+ 70%	59%	(4.8)	60%	(-1.0)	70%	(-0.3) +L+ 60%	60%	(5.6)	63%	(-0.6)	73%	(0.0) +L+ * Threshold values are given in parenthesis. +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4. Average recall and average precision for the three +L+ statistical formulas +L+ </SectLabel_tableCaption> <SectLabel_figure> 60	65	70	75	80	85	90	95 +L+ Recall(%) +L+ </SectLabel_figure> <SectLabel_figureCaption> Fig. 1. Recall-precision graph for the three statistical +L+ </SectLabel_figureCaption> <SectLabel_bodyText> formula was better by about 4%. At the 60% recall level, it +L+ outperformed the mutual information formula by 14% (giving a +L+ relative improvement of 23%). The results also indicate that the +L+ improved mutual information formula does not perform better +L+ than the mutual information formula. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.2 Statistical Test of Significance +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In order to perform a statistical test, recall and precision figures +L+ were calculated for each of the 100 sentences used in the +L+ evaluation. The average recall and the average precision across +L+ the 100 sentences were then calculated for the three statistical +L+ formulas. In the previous section, recall and precision were +L+ calculated for all the 100 sentences combined. Here, recall and +L+ precision were obtained for individual sentences and then the +L+ average across the 100 sentences was calculated. The average +L+ precision for 60%, 70%, 80% and 90% average recall are given +L+ in Table 4. +L+ For each recall level, an analysis of variance with repeated +L+ measures was carried out to find out whether the differences in +L+ precision were significant. Pairwise comparisons using Tukey s +L+ HSD test was also carried out. The contextual information +L+ formula was significantly better (a=0.001) than the mutual +L+ information and the improved mutual information formulas at all +L+ 4 recall levels. The improved mutual information formula was +L+ not found to be significantly better than mutual information. +L+ </SectLabel_bodyText> <SectLabel_table> Association Score>1.0 (definite errors) +L+ (	)	university (agricultural +L+ university) +L+ (	)	geology (geologic age) +L+ (	)	plant (upland plant) +L+ (	)	sovereignty (sovereign state) +L+ Association Score Between –1.0 and 1.0 +L+ (borderline errors) +L+ (	)	statistics (statistical data) +L+ (	)	calamity (natural calamity) +L+ (	)	resources (manpower resources) +L+ (	)	professor (associate professor) +L+ (	)	poor (pauperization) +L+ (	)	fourteen (the 14th day) +L+ (	)	twenty (twenty pieces) +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5. Simple words that are part of a longer +L+ meaningful word +L+ </SectLabel_tableCaption> <SectLabel_table> Association Score >1.0 (definite errors) +L+ will through +L+ telegraph [on the] day [31 July] +L+ Association Score Between –1.0 and 1.0 +L+ (borderline errors) +L+ still	to +L+ will be +L+ people etc. +L+ I want +L+ Person's name +L+ (	)	Wan Wen Ju +L+ Place name +L+ (	)	a village name in China +L+ (	)	Canada +L+ Name of an organization/institution +L+ (	)	Xin Hua Agency +L+ (	)	The State Department +L+ </SectLabel_table> <SectLabel_tableCaption> Table 6. Bigrams incorrectly identified as words +L+ </SectLabel_tableCaption> <SectLabel_sectionHeader> 7. ANALYSIS OF ERRORS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The errors that arose from using the contextual information +L+ formula were analyzed to gain insights into the weaknesses of +L+ the model and how the model can be improved. There are two +L+ types of errors: errors of commission and errors of omission. +L+ Errors of commission are bigrams that are identified by the +L+ automatic segmentation to be words when in fact they are not +L+ (according to the manual segmentation). Errors of omission are +L+ bigrams that are not identified by the automatic segmentation to +L+ be words but in fact they are. +L+ The errors depend of course on the threshold values used. A high +L+ threshold (e.g. 1.0) emphasizes precision and a low threshold +L+ (e.g. –1.0) emphasizes recall. 50 sentences were selected from +L+ the 100 sample sentences to find the distribution of errors at +L+ different regions of threshold values. +L+ </SectLabel_bodyText> <SectLabel_figure> Contextual information +L+ Mutual information +L+ Improved mutual +L+ information +L+ 	75 70 65 60 55 +L+ </SectLabel_figure> <SectLabel_page> 87 +L+ </SectLabel_page> <SectLabel_table> Association Score between -1.0 and -2.0 +L+ the northern section of a construction project +L+ fragments of ancient books +L+ Association Score < -2.0 +L+ September +L+ 3rd day +L+ (name of a district in China ) +L+ (name of an institution) +L+ the Book of Changes +L+ </SectLabel_table> <SectLabel_tableCaption> Table 7. 2-character words with association score +L+ below -1.0 +L+ </SectLabel_tableCaption> <SectLabel_bodyText> We divide the errors of commission (bigrams that are incorrectly +L+ identified as words by the automatic segmentation) into 2 groups: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Definite errors: bigrams with association scores above 1.0 but +L+ are not words +L+ 2. Borderline errors: bigrams with association scores between – +L+ 1.0 and 1.0 and are not words +L+ </SectLabel_listItem> <SectLabel_bodyText> We also divide the errors of omission (bigrams that are words +L+ but are not identified by the automatic segmentation) into 2 +L+ groups: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Definite errors: bigrams with association scores below –1.0 +L+ but are words +L+ 2. Borderline errors: bigrams with association scores between – +L+ 1.0 and 1.0 and are words. +L+ </SectLabel_listItem> <SectLabel_subsectionHeader> 7.1 Errors of Commission +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Errors of commission can be divided into 2 types: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. The bigram is a simple word that is part of a longer +L+ meaningful word. +L+ 2. The bigram is not a word (neither simple word nor +L+ meaningful word). +L+ </SectLabel_listItem> <SectLabel_bodyText> Errors of the first type are illustrated in Table 5. The words +L+ within parenthesis are actually meaningful words but segmented +L+ as simple words (words on the left). The words lose part of the +L+ meaning when segmented as simple words. These errors +L+ occurred mostly with 3 or 4-character meaningful words. +L+ Errors of the second type are illustrated in Table 6. Many of the +L+ errors are caused by incorrectly linking a character with a +L+ function word or pronoun. Some of the errors can easily be +L+ removed by using a list of function words and pronouns to +L+ identify these characters. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 7.2 Errors of Omission +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Examples of definite errors of omission (bigrams with +L+ association scores below –1.0 but are words) are given in Table +L+ 7. Most of the errors are rare words and time words. Some are +L+ ancient names, rare and unknown place names, as well as +L+ technical terms. Since our corpus comprises general news +L+ articles, these types of words are not frequent in the corpus. Time +L+ words like dates usually have low association values because +L+ they change everyday! These errors can be reduced by +L+ incorporating a separate algorithm for recognizing them. +L+ The proportion of errors of the various types are given in Table 8. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A new statistical formula for identifying 2-character words in +L+ Chinese text, called the contextual information formula, was +L+ developed empirically using regression analysis. The focus was +L+ on identifying meaningful words (including multi-word terms +L+ and idioms) rather than simple words. The formula was found to +L+ give significantly and substantially better results than the mutual +L+ information formula. +L+ Contextual information in the form of the frequency of characters +L+ that are adjacent to the bigram being processed as well as the +L+ weighted document frequency of the overlapping bigrams were +L+ found to be significant factors for predicting the probablity that +L+ the bigram constitutes a word. Local information (e.g. the +L+ number of times the bigram occurs in the document being +L+ segmented) and the position of the bigram in the sentence were +L+ not found to be useful in determining words. +L+ Of the bigrams that the formula erroneously identified as words, +L+ about 80% of them were actually simple words. Of the rest, +L+ many involved incorrect linking with a function words. Of the +L+ words that the formula failed to identify as words, more than a +L+ third of them were rare words or time words. The proportion of +L+ rare words increased as the threshold value used was lowered. +L+ These rare words cannot be identified using statistical +L+ techniques. +L+ This study investigated a purely statistical approach to text +L+ </SectLabel_bodyText> <SectLabel_table> Errors of Commission	Borderline Cases	Errors of Omission +L+ Association score > 1.0	Association score: –1.0 to1.0	Association score < –1.0 +L+ (No. of errors=34)	(No. of cases: 210) +L+ Simple words	Not words	Simple words	Not words	Meaning- ful words	Association score:	Association score +L+ 82.3%	17.7%	55.2%	20.5%	24.3%	–1.0 to –2.0	< –2.0 +L+ 					(No. of errors=43)	(No. of errors=22) +L+ 					Rare words	Others	Rare words	Others +L+ 					& time	76.8%	& time	36.4% +L+ 					words		words +L+ 					23.2%		63.6% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 8. Proportion of errors of different types +L+ </SectLabel_tableCaption> <SectLabel_page> 88 +L+ </SectLabel_page> <SectLabel_bodyText> segmentation. The advantage of the statistical approach is that it +L+ can be applied to any domain, provided that the document +L+ collection is sufficiently large to provide frequency information. +L+ A domain-specific dictionary of words is not required. In fact, the +L+ statistical formula can be used to generate a shortlist of candidate +L+ words for such a dictionary. On the other hand, the statistical +L+ method cannot identify rare words and proper names. It is also +L+ fooled by combinations of function words that occur frequently +L+ and by function words that co-occur with other words. +L+ It is well-known that a combination of methods is needed to give +L+ the best segmentation results. The segmentation quality in this +L+ study can be improved by using a list of function words and +L+ segmenting the function words as single character words. A +L+ dictionary of common and well-known names (including names +L+ of persons, places, institutions, government bodies and classic +L+ books) could be used by the system to identify proper names that +L+ occur infrequently in the corpus. Chang et al. [2] developed a +L+ method for recognizing proper nouns using a dictionary of family +L+ names in combination with a statistical method for identifying +L+ the end of the name. An algorithm for identifying time and dates +L+ would also be helpful. It is not clear whether syntactic processing +L+ can be used to improve the segmentation results substantially. +L+ Our current work includes developing statistical formulas for +L+ identifying 3 and 4-character words, as well as investigating +L+ whether the statistical formula developed here can be used with +L+ other corpora. The approach adopted in this study can also be +L+ used to develop statistical models for identifying multi-word +L+ terms in English text. It would be interesting to see whether the +L+ regression model developed for English text is similar to the one +L+ developed in this study for Chinese text. Frantzi, Ananiadou & +L+ Tsujii [7], using a different statistical approach, found that +L+ contextual information could be used to improve the +L+ identification of multi-word terms in English text. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] Chang, C.-H., and Chen, C.-D. A study of integrating +L+ Chinese word segmentation and part-of-speech tagging. +L+ Communications of COLIPS, 3, 1 (1993), 69-77. +L+ [2] Chang, J.-S., Chen, S.-D., Ker, S.-J., Chen, Y., and Liu, J.S. +L+ A multiple-corpus approach to recognition of proper names +L+ in Chinese texts. Computer Processing of Chinese and +L+ Oriental Languages, 8, 1 (June 1994), 75-85. +L+ [3] Church, K.W., and Hanks, P. Word association norms, +L+ mutual information and lexicography. In Proceedings of the +L+ 27th Annual Meeting of the Association for Computational +L+ Linguistics (Vancouver, June 1989), 76-83. +L+ [4] Dai, J.C., and Lee, H.J. A generalized unification-based LR +L+ parser for Chinese. Computer Processing of Chinese and +L+ Oriental Languages, 8, 1 (1994), 1-18. +L+ [5] Dai, Y. Developing a new statistical method for Chinese +L+ text segmentation. (Master s thesis in preparation) +L+ [6] Damerau, F.J. Generating and evaluating domain-oriented +L+ multi-word terms from texts. Information Processing & +L+ Management, 29, 4 (1993), 433-447. +L+ [7] Frantzi, K.T., Ananiadou, S., and Tsujii, J. The C- +L+ value/NC-value method of automatic recognition for multi- +L+ word terms. In C. Nikolaou and C. Stephanidis (eds.), +L+ Research and Advanced Technology for Digital Libraries, +L+ 2nd European Conference, ECDL 98 (Heraklion, Crete, +L+ September 1998), Springer-Verlag, 585-604. +L+ [8] Liang, N.Y. The knowledge of Chinese words segmentation +L+ [in Chinese]. Journal of Chinese Information Processing, 4, +L+ 2 (1990), 42-49. +L+ [9] Liu, I.M. Descriptive-unit analysis of sentences: Toward a +L+ model natural language processing. Computer Processing of +L+ Chinese & Oriental Languages, 4, 4 (1990), 314-355. +L+ [10] Liu, Y., Tan, Q., and Shen, X.K. Xin xi chu li yong xian dai +L+ han yu fen ci gui fan ji zi dong fen ci fang fa [ Modern +L+ Chinese Word Segmentation Rules and Automatic Word +L+ Segmentation Methods for Information Processing ]. Qing +L+ Hua University Press, Beijing, 1994. +L+ [11] Lua, K.T. Experiments on the use of bigram mutual +L+ information in Chinese natural language processing. +L+ Presented at the 1995 International Conference on Computer +L+ Processing of Oriental Languages (ICCPOL) (Hawaii, +L+ November 1995). Available: http://137.132.89.143/luakt/ +L+ publication.html +L+ [12] Lua, K.T. From character to word - An application of +L+ information theory. Computer Processing of Chinese & +L+ Oriental Languages, 4, 4 (1990), 304-312. +L+ [13] Lua, K.T., and Gan, G.W. An application of information +L+ theory in Chinese word segmentation. Computer Processing +L+ of Chinese & Oriental Languages, 8, 1 (1994), 115-124. +L+ [14] Nie, J.Y., Hannan, M.L., and Jin, W.Y. Unknown word +L+ detection and segmentation of Chinese using statistical and +L+ heuristic knowledge. Communications of COLIPS, 5, 1&2 +L+ (1995), 47-57. +L+ [15] Ogawa, Y., and Matsuda, T. Overlapping statistical word +L+ indexing: A new indexing method for Japanese text. In +L+ Proceedings of the 20th Annual International ACM SIGIR +L+ Conference on Research and Development in Information +L+ Retrieval (Philadelphia, July 1997), ACM, 226-234. +L+ [16] Sproat, R., and Shih, C.L. A statistical method for finding +L+ word boundaries in Chinese text. Computer Processing of +L+ Chinese & Oriental Languages, 4, 4 (1990), 336-351. +L+ [17] Sproat, R., Shih, C., Gale, W., and Chang, N. A stochastic +L+ finite-state word-segmentation algorithm for Chinese. +L+ Computational Lingustics, 22, 3 (1996), 377-404. +L+ [18] Tung, C.-H., and Lee, H.-J. Identification of unknown words +L+ from a corpus. Computer Processing of Chinese and +L+ Oriental Languages, 8 (Supplement, Dec. 1994), 131-145. +L+ [19] Wu, Z., and Tseng, G. ACTS: An automatic Chinese text +L+ segmentation system for full text retrieval. Journal of the +L+ American Society for Information Science, 46, 2 (1995), 83- +L+ 96. +L+ [20] Yeh, C.L., and Lee, H.J. Rule-based word identification for +L+ mandarin Chinese sentences: A unification approach. +L+ Computer Processing of Chinese and Oriental Languages, 5, +L+ 2 (1991), 97-118. +L+ </SectLabel_reference> <SectLabel_page> 89 +L+ </SectLabel_page>
<SectLabel_title> A Pseudo Random Coordinated Scheduling Algorithm for +L+ Bluetooth Scatternets +L+ </SectLabel_title> <SectLabel_author> Andr´as R´acz, Gy¨orgy Mikl´os, Ferenc Kubinszky, Andr´as Valk´o +L+ </SectLabel_author> <SectLabel_affiliation> Traffic Analysis and Network Performance Lab., Ericsson Research +L+ </SectLabel_affiliation> <SectLabel_address> Laborc 1, 1037 Budapest, Hungary +L+ Ph: +36-1-4377621, Fax: +36-1-4377767 +L+ </SectLabel_address> <SectLabel_email> Andras.Racz@eth.ericsson.se +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The emergence of Bluetooth as a default radio interface allows +L+ handheld devices to be rapidly interconnected into ad hoc networks. +L+ Bluetooth allows large numbers of piconets to form a scatternet us- +L+ ing designated nodes that participate in multiple piconets. A unit +L+ that participates in multiple piconets can serve as a bridge and for- +L+ wards traffic between neighbouring piconets. Since a Bluetooth +L+ unit can transmit or receive in only one piconet at a time, a bridging +L+ unit has to share its time among the different piconets. To sched- +L+ ule communication with bridging nodes one must take into account +L+ their availability in the different piconets, which represents a dif- +L+ ficult, scatternet wide coordination problem and can be an impor- +L+ tant performance bottleneck in building scatternets. In this paper +L+ we propose the Pseudo-Random Coordinated Scatternet Schedul- +L+ ing (PCSS) algorithm to perform the scheduling of both intra and +L+ inter-piconet communication. In this algorithm Bluetooth nodes +L+ assign meeting points with their peers such that the sequence of +L+ meeting points follows a pseudo random process that is different +L+ for each pair of nodes. The uniqueness of the pseudo random se- +L+ quence guarantees that the meeting points with different peers of +L+ the node will collide only occasionally. This removes the need +L+ for explicit information exchange between peer devices, which is +L+ a major advantage of the algorithm. The lack of explicit signaling +L+ between Bluetooth nodes makes it easy to deploy the PCSS algo- +L+ rithm in Bluetooth devices, while conformance to the current Blue- +L+ tooth specification is also maintained. To assess the performance of +L+ the algorithm we define two reference case schedulers and perform +L+ simulations in a number of scenarios where we compare the perfor- +L+ mance of PCSS to the performance of the reference schedulers. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Bluetooth, scheduling, inter-piconet communication, scatternet +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Short range radio technologies enable users to rapidly interconnect +L+ handheld electronic devices such as cellular phones, palm devices +L+ or notebook computers. The emergence of Bluetooth [1] as de- +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of part or all of this work or +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers, or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> MobiHOC 2001, Long Beach, CA, USA +L+ </SectLabel_note> <SectLabel_copyright> © ACM 2001 1-58113-390-1/01/10...$5.00 +L+ </SectLabel_copyright> <SectLabel_bodyText> fault radio interface in these devices provides an opportunity to turn +L+ them from stand-alone tools into networked equipment. Building +L+ Bluetooth ad hoc networks also represents, however, a number of +L+ new challenges, partly stemming from the fact that Bluetooth was +L+ originally developed for single hop wireless connections. In this +L+ paper we study the scheduling problems of inter-piconet commu- +L+ nication and propose a lightweight scheduling algorithm that Blue- +L+ tooth nodes can employ to perform the scheduling of both intra and +L+ inter-piconet communication. +L+ Bluetooth is a short range radio technology operating in the unli- +L+ censed ISM (Industrial-Scientific-Medical) band using a frequency +L+ hopping scheme. Bluetooth (BT) units are organized into piconets. +L+ There is one Bluetooth device in each piconet that acts as the mas- +L+ ter, which can have any number of slaves out of which up to seven +L+ can be active simultaneously. The communication within a piconet +L+ is organized by the master which polls each slave according to some +L+ polling scheme. A slave is only allowed to transmit in a slave- +L+ to-master slot if it has been polled by the master in the previous +L+ master-to-slave slot. In Section 3 we present a brief overview of +L+ the Bluetooth technology. +L+ A Bluetooth unit can participate in more than one piconet at any +L+ time but it can be a master in only one piconet. A unit that par- +L+ ticipates in multiple piconets can serve as a bridge thus allowing +L+ the piconets to form a larger network. We define bridging degree +L+ as the number of piconets a bridging node is member of. A set +L+ of piconets that are all interconnected by such bridging units is re- +L+ ferred to as a scatternet network (Figure 1). Since a Bluetooth unit +L+ can transmit or receive in only one piconet at a time, bridging units +L+ must switch between piconets on a time division basis. Due to the +L+ fact that different piconets are not synchronized in time a bridging +L+ unit necessarily loses some time while switching from one piconet +L+ to the other. Furthermore, the temporal unavailability of bridging +L+ nodes in the different piconets makes it difficult to coordinate the +L+ communication with them, which impacts throughput and can be +L+ an important performance constraint in building scatternets. +L+ There are two important phenomena that can reduce the efficiency +L+ of the polling based communication in Bluetooth scatternets: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	slaves that have no data to transmit may be unnecessarily +L+ polled, while other slaves with data to transmit may have to +L+ wait to be polled; and +L+ •	at the time of an expected poll one of the nodes of a master- +L+ slave node pair may not be present in the piconet (the slave +L+ </SectLabel_listItem> <SectLabel_figureCaption> Figure 1: Example scatternet +L+ </SectLabel_figureCaption> <SectLabel_bodyText> that is being polled is not listening or the master that is ex- +L+ pected to poll is not polling). +L+ The first problem applies to polling based schemes in general, while +L+ the second one is specific to the Bluetooth environment. In or- +L+ der to improve the efficiency of inter-piconet communication the +L+ scheduling algorithm has to coordinate the presence of bridging +L+ nodes in the different piconets such that the effect of the second +L+ phenomenon be minimized. +L+ However, the scheduling of inter-piconet communication expands +L+ to a scatternet wide coordination problem. Each node that has more +L+ than one Bluetooth links have to schedule the order in which it com- +L+ municates with its respective neighbours. A node with multiple +L+ Bluetooth links can be either a piconet master or a bridging node or +L+ both. The scheduling order of two nodes will mutually depend on +L+ each other if they have a direct Bluetooth link in which case they +L+ have to schedule the communication on their common link for the +L+ same time slots. This necessitates some coordination between the +L+ respective schedulers. For instance in Figure 1 the scheduling order +L+ of node A and the scheduling order of its bridging neighbours, B, +L+ C, D and E mutually depend on each other, while nodes D and E +L+ further effects nodes F, G and H as well. Furthermore, the possi- +L+ ble loops in a scatternet (e.g., A-E-G-H-F-D) makes it even more +L+ complicated to resolve scheduling conflicts. +L+ In case of bursty traffic in the scatternet the scheduling problem +L+ is further augmented by the need to adjust scheduling order in re- +L+ sponse to dynamic variation of traffic intensity. In a bursty traffic +L+ environment it is desirable that a node spends most of its time on +L+ those links that have a backlogged burst of data. +L+ One way to address the coordination problem of inter-piconet +L+ scheduling is to explicitly allocate, in advance, time slots for com- +L+ munication in each pair of nodes. Such a hard coordination ap- +L+ proach eliminates ambiguity with regards to a node’s presence in +L+ piconets, but it implies a complex, scatternet wide coordination +L+ problem and requires explicit signaling between nodes of a scat- +L+ ternet. In the case of bursty traffic, hard coordination schemes +L+ generate a significant computation and signaling overhead as the +L+ communication slots have to be reallocated in response to changes +L+ in traffic intensity and each time when a new connection is estab- +L+ lished or released. +L+ In this paper we propose the Pseudo-Random Coordinated Scatter- +L+ net Scheduling algorithm which falls in the category of soft coor- +L+ dination schemes. In soft coordination schemes nodes decide their +L+ presence in piconets based on local information. By nature, soft co- +L+ ordination schemes cannot guarantee conflict-free participation of +L+ bridging nodes in the different piconets, however, they have a sig- +L+ nificantly reduced complexity. In the PCSS algorithm coordination +L+ is achieved by implicit rules in the communication without the need +L+ of exchanging explicit control information. The low complexity of +L+ the algorithm and its conformance to the current Bluetooth specifi- +L+ cation allow easy implementation and deployment. +L+ The first key component of the algorithm is the notion of check- +L+ points which are defined in relation to each pair of nodes that +L+ are connected by a Bluetooth link and which represent predictable +L+ points in time when packet transmission can be initiated on the par- +L+ ticular link. In other words, checkpoints serve as regular meeting +L+ points for neighboring nodes when they can exchange packets. In +L+ order to avoid systematic collision of checkpoints on different links +L+ of a node the position of checkpoints follows a pseudo random se- +L+ quence that is specific to the particular link the checkpoints belong +L+ to. +L+ The second key component of the algorithm is the dynamic adjust- +L+ ment of checking intensity, which is necessary in order to effec- +L+ tively support bursty data traffic. Bandwidth can be allocated and +L+ deallocated to a particular link by increasing and decreasing check- +L+ point intensity, respectively. +L+ To assess the performance of the algorithm we define two reference +L+ schedulers and relate the performance of the PCSS scheme to these +L+ reference algorithms in a number of simulation scenarios. +L+ The remainder of the paper is structured as follows. In Section 2 we +L+ give an overview of related work focusing on Bluetooth scheduling +L+ related studies available in the literature. Section 3 gives a brief +L+ overview of the Bluetooth technology. In Section 4 and 5 we intro- +L+ duce the proposed algorithm. In Section 6 we define the reference +L+ schedulers. Finally, in Section 7 we present simulation results. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A number of researchers have addressed the issue of scheduling in +L+ Bluetooth. Most of these studies have been restricted, however, to +L+ the single piconet environment, where the fundamental question is +L+ the polling discipline used by the piconet master to poll its slaves. +L+ These algorithms are often referred to as intra-piconet scheduling +L+ schemes. In [7] the authors assume a simple round robin polling +L+ scheme and investigate queueing delays in master and slave units +L+ depending on the length of the Bluetooth packets used. In [5] Jo- +L+ hansson et al. analyze and compare the behavior of three differ- +L+ ent polling algorithms. They conclude that the simple round robin +L+ scheme may perform poorly in Bluetooth systems and they propose +L+ a scheme called Fair Exhaustive Polling. The authors demonstrate +L+ the strength of this scheme and argue in favor of using multi-slot +L+ packets. Similar conclusions are drawn by Kalia et al. who argue +L+ that the traditional round robin scheme may result in waste and un- +L+ fairness [8]. The authors propose two new scheduling disciplines +L+ that utilize information about the status of master and slave queues. +L+ In [9, 10] the authors concentrate on scheduling policies designed +L+ with the aim of low power consumption. A number of schedul- +L+ ing policies are proposed which exploit either the park or sniff low +L+ power modes of Bluetooth. +L+ </SectLabel_bodyText> <SectLabel_figure> B +L+ E +L+ A +L+ G +L+ C +L+ D +L+ H +L+ F +L+ master +L+ slave +L+ slave in two piconets +L+ slave in one piconet and master in another +L+ </SectLabel_figure> <SectLabel_bodyText> Although the above studies have revealed a number of important +L+ performance aspects of scheduling in Bluetooth piconets, the algo- +L+ rithms developed therein are not applicable for inter-piconet com- +L+ munication. In [6] the authors have shown that constructing an op- +L+ timal link schedule that maximizes total throughput in a Bluetooth +L+ scatternet is an NP hard problem even if scheduling is performed +L+ by a central entity. The authors also propose a scheduling algo- +L+ rithm referred to as Distributed Scatternet Scheduling Algorithm +L+ (DSSA), which falls in the category of distributed, hard coordina- +L+ tion schemes. Although the DSSA algorithm provides a solution +L+ for scheduling communication in a scatternet, some of its idealized +L+ properties (e.g., nodes are aware of the traffic requirements of their +L+ neighbours) and its relatively high complexity make it difficult to +L+ apply it in a real life environment. +L+ There is an ongoing work in the Personal Area Networking (PAN) +L+ working group of the Bluetooth Special Interest Group (SIG) [2] to +L+ define an appropriate scheduling algorithm for Bluetooth scatter- +L+ nets. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. BLUETOOTH BACKGROUND +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Bluetooth is a short range radio technology that uses frequency +L+ hopping scheme, where hopping is performed on 79 RF channels +L+ spaced 1 MHz apart. Communication in Bluetooth is always be- +L+ tween master and slave nodes. Being a master or a slave is only +L+ a logical state: any Bluetooth unit can be a master or a slave. +L+ The Bluetooth system provides full-duplex transmission based on +L+ slotted Time Division Duplex (TDD) scheme, where each slot is +L+ 0.625 ms long. Master-to-slave transmission always starts in an +L+ even-numbered time slot, while slave-to-master transmission al- +L+ ways starts in an odd-numbered time slot. A pair of master-to-slave +L+ and slave-to-master slots are often referred to as a frame. The com- +L+ munication within a piconet is organized by the master which polls +L+ each slave according to some polling scheme. A slave is only al- +L+ lowed to transmit in a slave-to-master slot if it has been polled by +L+ the master in the previous master-to-slave slot. The master may +L+ or may not include data in the packet used to poll a slave. Blue- +L+ tooth packets can carry synchronous data (e.g., real-time traffic) on +L+ Synchronous Connection Oriented (SCO) links or asynchronous +L+ data (e.g., elastic data traffic, which is the case in our study) on +L+ Asynchronous Connectionless (ACL) links. Bluetooth packets on +L+ an ACL link can be 1, 3 or 5 slot long and they can carry differ- +L+ ent amount of user data depending on whether the payload is FEC +L+ coded or not. Accordingly, the Bluetooth packet types DH 1, DH3 +L+ and DH5 denote 1, 3 and 5 slot packets, respectively, where the +L+ payload is not FEC encoded, while in case of packet types DM1, +L+ DM3 and DM5 the payload is protected with FEC encoding. There +L+ are two other types of packets, the POLL and NULL packets that do +L+ not carry user data. The POLL packet is used by the master when +L+ it has no user data to the slave but it still wants to poll it. Similarly, +L+ the NULL packet is used by the slave to respond to the master if it +L+ has no user data. For further information regarding the Bluetooth +L+ technology the reader is referred to [1, 3]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. OVERVIEW OF THE PCSS ALGO- +L+ RITHM +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Coordination in the PCSS algorithm is achieved by the unique +L+ pseudo random sequence of checkpoints that is specific to each +L+ master-slave node pair and by implicit information exchange be- +L+ tween peer devices. A checkpoint is a designated Bluetooth frame. +L+ The activity of being present at a checkpoint is referred to as to +L+ check. A master node actively checks its slave by sending a packet +L+ to the slave at the corresponding checkpoint and waiting for a re- +L+ sponse from the slave. The slave node passively checks its master +L+ by listening to the master at the checkpoint and sending a response +L+ packet in case of being addressed. +L+ The expected behaviour of nodes is that they show up at each +L+ checkpoint on all of their links and check their peers for available +L+ user data. The exchange of user data packets started at a check- +L+ point can be continued in the slots following the checkpoint. A +L+ node remains active on the current link until there is user data in +L+ either the master-to-slave or slave-to-master directions or until it +L+ has to leave for a next checkpoint on one of its other links. In +L+ the PCSS scheme we exploit the concept of randomness in assign- +L+ ing the position of checkpoints, which excludes the possibility that +L+ checkpoints on different links of a node will collide systematically, +L+ thus giving the node an equal chance to visit all of its checkpoints. +L+ The pseudo random procedure is similar to the one used to derive +L+ the pseudo random frequency hopping sequence. In particular, the +L+ PCSS scheme assigns the positions of checkpoints on a given link +L+ following a pseudo random sequence that is generated based on the +L+ Bluetooth clock of the master and the MAC address of the slave. +L+ This scheme guarantees that the same pseudo random sequence +L+ will be generated by both nodes of a master-slave pair, while the se- +L+ quences belonging to different node pairs will be different. Figure +L+ 2 shows an example for the pseudo random arrangement of check- +L+ points in case of a node pair A and B. The length of the current base +L+ checking interval is denoted by T(h k and the current checking in- +L+ tensity is defined accordingly as e9 . There is one checkpoint +L+ T(i) heck +L+ within each base checking interval and the position of the check- +L+ point within this window is changing from one time window to the +L+ other in a pseudo random manner. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: Pseudo-random positioning of checkpoints +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Since the pseudo random sequence is different from one link to an- +L+ other, checkpoints on different links of a node will collide only oc- +L+ casionally. In case of collision the node can attend only one of the +L+ colliding checkpoints, which implies that the corresponding neigh- +L+ bours have to be prepared for a non-present peer. That is, the mas- +L+ ter might not poll and the slave might not listen at a checkpoint. +L+ We note that a collision occurs either if there are more than one +L+ checkpoints scheduled for the same time slot or if the checkpoints +L+ are so close to each other that a packet transmission started at the +L+ first checkpoint necessarily overlaps the second one. Furthermore, +L+ if the colliding checkpoints belong to links in different piconets, +L+ the necessary time to perform the switch must be also taken into +L+ account. +L+ During the communication there is the possibility to increase or +L+ decrease the intensity of checkpoints depending on the amount of +L+ user data to be transmitted and on the available capacity of the +L+ node. According to the PCSS algorithm a node performs certain +L+ traffic measurements at the checkpoints and increases or decreases +L+ the current checking intensity based on these measurements. Since +L+ </SectLabel_bodyText> <SectLabel_figure> T(Z) +L+ check +L+ 1 frame +L+ checkpoints of A toward B	checkpoints of B toward A +L+ </SectLabel_figure> <SectLabel_bodyText> nodes decide independently about the current checking intensity +L+ without explicit coordination, two nodes on a given link may select +L+ different base checking periods. In order to ensure that two nodes +L+ with different checking intensities on the same link can still com- +L+ municate we require the pseudo random generation of checkpoints +L+ to be such that the set of checkpoint positions at a lower checking +L+ intensity is a subset of checkpoint positions at any higher checking +L+ intensities. In the Appendix we are going to present a pseudo ran- +L+ dom scheme for generating the position of checkpoints, which has +L+ the desired properties. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. OPERATION OF PCSS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In what follows, we describe the procedures of the PCSS algorithm. +L+ We start by the initialization process which ensures that two nodes +L+ can start communication as soon as a new link has been established +L+ or the connection has been reset. Next, we describe the rules that +L+ define how nodes calculate their checkpoints, decide upon their +L+ presence at checkpoints and exchange packets. Finally, we present +L+ the way neighboring nodes can dynamically increase and decrease +L+ of checkpoint intensity. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.1 Initialization +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the PCSS algorithm there is no need for a separate initialization +L+ procedure to start communication, since the pseudo random gener- +L+ ation of checkpoints is defined such that once a master slave node +L+ pair share the same master’s clock and slave’s MAC address infor- +L+ mation, it is guaranteed that the same pseudo random sequence will +L+ be produced at each node. That is, it is guaranteed that two nodes +L+ starting checkpoint generation at different time instants with differ- +L+ ent checking intensities will be able to communicate. It is the own +L+ decision of the nodes to select an appropriate initial checking in- +L+ tensity, which may depend for example on the free capacities of the +L+ node or on the amount of data to transmit. Once the communication +L+ is established the increase and decrease procedures will adjust the +L+ possibly different initial checking intensities to a common value. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Communication +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A pair of nodes can start exchanging user data packets at a check- +L+ point, which can expand through the slots following the checkpoint. +L+ The nodes remain active on the current link following a check- +L+ point until there is user data to be transmitted or one of them has to +L+ leave in order to attend a checkpoint on one of its other links. Af- +L+ ter a POLL/NULL packet pair has been exchanged indicating that +L+ there is no more user data left the nodes switch off their transmit- +L+ ters/receivers and remain idle until a next checkpoint comes on one +L+ of their links. However, during the communication any of the nodes +L+ can leave in order to attend a coming checkpoint on one of its other +L+ links. After one of the nodes has left the remaining peer will realize +L+ the absence of the node and will go idle until the time of its next +L+ checkpoint. If the master has left earlier the slave will realize the +L+ absence of the master at the next master-to-slave slot by not receiv- +L+ ing the expected poll. In the worst case the master has left before +L+ receiving the last packet response from the slave, which can be a 5 +L+ slot packet in which case the slave wastes 5+1 slots before realiz- +L+ ing the absence of the master. Similarly, if the master does not get +L+ a response from the slave it assumes that the slave has already left +L+ the checkpoint and goes idle until its next checkpoint. Note that the +L+ master may also waste 5+1 slots in the worst case before realizing +L+ the absence of the slave. +L+ A node stores the current length of the base checking interval and +L+ the time of the next checkpoint for each of its Bluetooth links sep- +L+ arately. For its ith link a node maintains the variable Tcheck to +L+ store the length of the current base checking period in number of +L+ frames and the variable t(i) +L+ check, which stores the Bluetooth clock +L+ of the master at the next checkpoint. After passing a checkpoint +L+ the variable thheck is updated to the next checkpoint by running +L+ the pseudo random generator (PseudoChkGen) with the current +L+ value of the master’s clock t(i) and the length of the base checking +L+ period Tcheck and with the MAC address of the slave A(i) +L+ slave as in- +L+ put parameters; tcizeck = PseudoChkGen(Tcheck, A(i)slave, t(i)). +L+ The procedure PseudoChkGen is described in the Appendix. +L+ There is a maximum and minimum checking interval Tmax = +L+ 2fmax and Tmin = 2fmin, respectively. The length of the check- +L+ ing period must be a power of 2 number of frames and it must take +L+ a value from the interval [2fmin, 2fmax]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.3 Increasing and Decreasing Checking In- +L+ tensity +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The increase and decrease procedures are used to adjust the check- +L+ ing intensity of a node according to the traffic intensity and to the +L+ availability of the peer device. Each node decides independently +L+ about the current checking intensity based on traffic measurements +L+ at checkpoints. +L+ Since the time spent by a node on a link is proportional to the ratio +L+ of the number of checkpoints on that link and the number of check- +L+ points on all links of the node, the bandwidth allocated to a link can +L+ be controlled by the intensity of checkpoints on that link. This can +L+ be shown by the following simple calculation. +L+ Let us assume that the node has L number of links and assume +L+ further that for the base checking periods on all links of the node +L+ it holds that Tmin :5 T(i) +L+ check < Tmax, Vi = 1, ... , L. Then the +L+ average number of checkpoints within an interval of length Tmaxis +L+ </SectLabel_bodyText> <SectLabel_equation> L +L+ N = 1 T ,)ax  , and the average time between two consecutive +L+ Tcheck +L+ checkpoints is +L+ t=	Tmax	=		1 +L+ 	N +L+ 					, +L+ 			L	1  Th +L+ 				check +L+ 			i=1 +L+ </SectLabel_equation> <SectLabel_bodyText> provided that the pseudo random generator produces a uniformly +L+ distributed sequence of checkpoints. Then, the share of link j from +L+ the total capacity of the node is +L+ A node has to measure the utilization of checkpoints on each of +L+ its links separately in order to provide input to the checking inten- +L+ sity increase and decrease procedures. According to the algorithm +L+ a given checkpoint is considered to be utilized if both nodes have +L+ shown up at the checkpoint and at least one Bluetooth packet carry- +L+ ing user data has been transmitted or received. If there has not been +L+ a successful poll at the checkpoint due to the unavailability of any +L+ of the nodes or if there has been only a POLL/NULL packet pair +L+ exchange but no user data has been transmitted, the checkpoint is +L+ considered to be unutilized. We note that due to packet losses the +L+ utilization of a given checkpoint might be interpreted differently by +L+ the nodes. However, this does not impact correct operation of the +L+ algorithm. +L+ </SectLabel_bodyText> <SectLabel_equation> . +L+ rj = +L+ 1/T(j) check +L+ L +L+ 1 +L+ T(i) +L+ i=1 check +L+ </SectLabel_equation> <SectLabel_bodyText> To measure the utilization of checkpoints p(i) on the ith link of the +L+ node we employ the moving average method as follows. The uti- +L+ lization of a checkpoint equals to 1 if it has been utilized, otherwise +L+ it equals to 0. If the checkpoint has been utilized the variable p(i) +L+ is updated as, +L+ </SectLabel_bodyText> <SectLabel_equation> p(i) = quti • p(i) + (1 — quti) • 1; +L+ </SectLabel_equation> <SectLabel_bodyText> if the checkpoint has not been utilized it is updated as, +L+ p(i) = quti • p(i) + (1 — quti) • 0, +L+ where 0 < quti < 1 is the time scale parameter of the moving +L+ average method. A further parameter of the utilization measure- +L+ ment is the minimum number of samples that have to be observed +L+ before the measured utilization value is considered to be confident +L+ and can be used as input to decide about increase and decrease of +L+ checking intensity. This minimum number of samples is a denoted +L+ by Nsample,min. +L+ Finally, a node also has to measure its total utilization, which is +L+ defined as the fraction of time slots where the node has been active +L+ (transmitted or received) over the total number of time slots. To +L+ measure the total utilization of a node we employ the moving aver- +L+ age method again. Each node measures its own utilization p(node) +L+ and updates the p(node) variable after each Nuti,win number of +L+ slots as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> (node) _ (node) (node) (node) +L+ p —quti•p +(1— +L+ quti )•p (win ), +L+ </SectLabel_equation> <SectLabel_bodyText> where p(win) is the fraction of time slots in the past time window +L+ of length Nuti,win where the node has been active over the total +L+ number of time slots Nuti,win. +L+ If the utilization of checkpoints on link i falls below the lower +L+ threshold plower, the current base checking period T(i) +L+ check will be +L+ doubled. Having a low checkpoint utilization can be either because +L+ one or both of the nodes have not shown up at all of the checkpoints +L+ or because there is not enough user data to be transmitted. In either +L+ cases the intensity of checkpoints has to be decreased. Whenever a +L+ decrease or increase is performed on link i the measured utilization +L+ p(i) must be reset. +L+ Since the parameter T (i)k is one of the inputs to the pseudo ran- +L+ chec +L+ dom checkpoint generation process, PseudoChkGen the check- +L+ points after the decrease will be generated according to the new +L+ period. Furthermore, due to the special characteristic of the check- +L+ point generation scheme the remaining checkpoints after the de- +L+ crease will be a subset of the original checkpoints, which guaran- +L+ tees that the two nodes can sustain communication independent of +L+ local changes in checking intensities. +L+ An example for the checking intensity decrease in case of a node +L+ pair A and B is shown in Figure 3. First, node A decreases check- +L+ ing intensity by doubling its current base checking period in re- +L+ sponse to the measured low utilization. As a consequence node B +L+ will find node A on average only at every second checkpoint and +L+ its measured utilization will decrease rapidly. When the measured +L+ utilization at node B falls below the threshold plower, B realizes +L+ that its peer has a lower checking intensity and follows the de- +L+ crease by doubling its current base checking period. Although we +L+ have not explicitly indicated in the Figure, it is assumed that there +L+ has been user data exchanged at each checkpoint where both nodes +L+ were present. +L+ </SectLabel_bodyText> <SectLabel_figure> checkpoints of A toward B +L+ node A reduces the checking +L+ intensity, by doubling its base period	checkpoints of B toward A +L+ ρ=0.35<ρlower	ρ=0.2	ρ=0.35	ρ=0.5	ρ=0.65 ρ=0.7 +L+ node B realizes the decrease and +L+ doubles its base period +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: Checking intensity decrease +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Recall from the utilization measurement procedure that there is a +L+ minimum number of checkpoints Nsample,min that has to be sam- +L+ pled before the measured utilization is considered to be confident +L+ and can be used to decide about checking intensity decrease. The +L+ parameter Nsample,min together with the parameter of the mov- +L+ ing average method quti determine the time scale over which the +L+ utilization of checkpoints has to be above the threshold plower, +L+ otherwise the node decreases checking intensity. It might be also +L+ reasonable to allow that the parameter Nsample,min and the mov- +L+ ing average parameter quti can be changed after each decrease or +L+ increase taking into account for example the current checking in- +L+ tensity, the available resources of the node or the amount of user +L+ data to be transmitted, etc. However, in the current implementation +L+ we apply fixed parameter values. +L+ After a checkpoint where user data has been exchanged (not only a +L+ POLL/NULL packet pair) checking intensity can be increased pro- +L+ vided that the measured utilization of checkpoints exceeds the up- +L+ per threshold pupper and the node has available capacity. Formally +L+ a checking intensity increase is performed on link i if the following +L+ two conditions are satisfied: p(i) > pupper and p(node) < piinn T), +L+ where pupil r is the upper threshold of the total utilization of the +L+ node. This last condition ensures that the intensity of checkpoints +L+ will not increase unbounded. The intensity of checkpoints is dou- +L+ bled at each increase by dividing the current length of the base +L+ checking period T(n,eck by 2. For typical values of pupper we rec- +L+ ommend 0.8 < pupper < 0.9 in which case the respective plower +L+ value should be plower < 0.4 in order to avoid oscillation of in- +L+ creases and decreases. +L+ Figure 4 shows an example where node A and B communicate and +L+ after exchanging user data at the second checkpoint both nodes +L+ double the checking intensity. In the Figure we have explicitly in- +L+ dicated whether there has been user data exchanged at a checkpoint +L+ or not. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4: Checking intensity increase +L+ </SectLabel_figureCaption> <SectLabel_figure> ρ=0.6ρ=0.5 ρ=0.58ρ=0.48ρ=0.56 ρ=0.46ρ=0.36<ρlower ρ=0.2	ρ=0.35 ρ=0.5 +L+ ρ=0.7	ρ=0.8>ρupper ρ=0.2 ρ=0.4	ρ=0.3 ρ=0.55 +L+ checking intensity +L+ ρ=0.3 +L+ ρ=0.55 +L+ ρ=0.8>ρupper +L+ ρ=0.2 ρ=0.4 +L+ checkpoints of A toward B +L+ checkpoints of B toward A +L+ ρ=0.7 +L+ user data +L+ both node A and B double +L+ user data +L+ user data	user data +L+ </SectLabel_figure> <SectLabel_sectionHeader> 6. REFERENCE ALGORITHMS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we define the Ideal Coordinated Scatternet Sched- +L+ uler (ICSS) and the Uncoordinated Greedy Scatternet Scheduler +L+ (UGSS) reference algorithms. The ICSS algorithm represents +L+ the “ideal” case where nodes exploit all extra information when +L+ scheduling packet transmissions, which would not be available in a +L+ realistic scenario. The UGSS algorithm represents the greedy case +L+ where nodes continuously switch among their Bluetooth links in a +L+ random order. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.1 The ICSS Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The ICSS algorithm is a hypothetical, ideal scheduling algorithm +L+ that we use as a reference case in the evaluation of the PCSS +L+ scheme. In the ICSS algorithm a node has the following extra +L+ information about its neighbours, which represents the idealized +L+ property of the algorithm: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	a node is aware of the already pre-scheduled transmissions +L+ of its neighbours; and +L+ •	a node is aware of the content of the transmission buffers of +L+ its neighbours. +L+ </SectLabel_listItem> <SectLabel_bodyText> According to the ICSS algorithm each node maintains a scheduling +L+ list, which contains the already pre-scheduled tasks of the node. A +L+ task always corresponds to one packet pair exchange with a given +L+ peer of the node. Knowing the scheduling list of the neighbours +L+ allows the node to schedule communication with its neighbours +L+ without overlapping their other communication, such that the ca- +L+ pacity of the nodes is utilized as much as possible. Furthermore +L+ being aware of the content of the transmission buffers of neigh- +L+ bours eliminates the inefficiencies of the polling based scheme, +L+ since there will be no unnecessary polls and the system will be +L+ work-conserving. +L+ In the scheduling list of a node there is at most one packet pair +L+ exchange scheduled in relation to each of its peers, provided that +L+ there is a Bluetooth packet carrying user data either in the trans- +L+ mission buffer of the node or in the transmission buffer of the peer +L+ or in both. After completing a packet exchange on a given link the +L+ two nodes schedule the next packet exchange, provided that there +L+ is user data to be transmitted in at least one of the directions. If +L+ there is user data in only one of the directions, a POLL or NULL +L+ packet is assumed for the reverse direction depending on whether +L+ it is the master-to-slave or slave-to-master direction, respectively. +L+ The new task is fitted into the scheduling lists of the nodes using +L+ a first fit strategy. According to this strategy the task is fitted into +L+ the first time interval that is available in both of the scheduling lists +L+ and that is long enough to accommodate the new task. Note that the +L+ algorithm strives for maximal utilization of node capacity by trying +L+ to fill in the unused gaps in the scheduling lists. +L+ If there is no more user data to be transmitted on a previously busy +L+ link, the link goes to idle in which case no tasks corresponding to +L+ the given link will be scheduled until there is user data again in at +L+ least one of the directions. +L+ An example for the scheduling lists of a node pair A and B is shown +L+ in Figure 5. The tasks are labeled with the name of the correspond- +L+ ing peers the different tasks belong to. Each node has as many +L+ pre-scheduled tasks in its scheduling list as the number of its ac- +L+ tive Bluetooth links. A link is considered to be active if there is +L+ current time +L+ schedule the next packet pair +L+ exchange for node A and B +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5: Example for the scheduling lists of a node pair in case +L+ of the ICSS algorithm +L+ </SectLabel_figureCaption> <SectLabel_bodyText> user data packet in at least one of the directions. Node A has active +L+ peers B and C, while node B has active peers A, D and E. After +L+ node A and B have finished the transmission of a packet pair they +L+ schedule the next task for the nearest time slots that are available +L+ in both of their scheduling lists and the number of consecutive free +L+ time slots is greater than or equal to the length of the task. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.2 The UGSS Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the UGSS algorithm Bluetooth nodes do not attempt to coordi- +L+ nate their meeting points, instead each node visits its neighbours +L+ in a random order. Nodes switch continuously among their Blue- +L+ tooth links in a greedy manner. If the node has n number of links it +L+ chooses each of them with a probability of 1/n. The greedy nature +L+ of the algorithm results in high power consumption of Bluetooth +L+ devices. +L+ If the node is the master on the visited link it polls the slave by +L+ sending a packet on the given link. The type of Bluetooth packet +L+ sent can be a 1, 3 or 5 slot packet carrying useful data or an empty +L+ POLL packet depending on whether there is user data to be trans- +L+ mitted or not. After the packet has been sent the master remains +L+ active on the link in order to receive any response from the slave. +L+ If the slave has not been active on the given link at the time when +L+ the master has sent the packet it could not have received the packet +L+ and consequently it will not send a response to the master. After +L+ the master has received the response of the slave or if it has sensed +L+ the link to be idle indicating that no response from the salve can be +L+ expected, it selects the next link to visit randomly. +L+ Similar procedure is followed when the node is the slave on the +L+ visited link. The slave tunes its receiver to the master and listens +L+ for a packet transmission from the master in the current master- +L+ to-slave slot. If the slave has not been addressed by the master +L+ in the actual master-to-slave slot it immediately goes to the next +L+ link. However, if the slave has been addressed it remains active on +L+ the current link and receives the packet. After having received the +L+ packet of the master the slave responds with its own packet in the +L+ following slave-to-master slot. After the slave has sent its response +L+ it selects the next link to visit randomly. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. SIMULATION RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> First, we evaluate the algorithm in a realistic usage scenario, which +L+ is the Network Access Point (NAP) scenario. Next we investigate +L+ theoretical configurations and obtain asymptotical results that re- +L+ veals the scaling properties of the algorithm. For instance we in- +L+ vestigate the carried traffic in function of the number of forwarding +L+ </SectLabel_bodyText> <SectLabel_figure> peer B +L+ Dee, C	pear C +L+ t +L+ peer A +L+ ✑peer E	peer E +L+ t +L+ scheduling list of +L+ node A +L+ Deer B +L+ scheduling list of +L+ node B +L+ Deer A p +L+ eer D +L+ </SectLabel_figure> <SectLabel_bodyText> hops along the path and in function of bridging degree. Both in the +L+ realistic and theoretical configurations we relate the performance of +L+ the PCSS scheme to the performance of the ICSS and UGSS ref- +L+ erence algorithms. Before presenting the scenarios and simulation +L+ results we shortly describe the simulation environment and define +L+ the performance metrics that are going to be measured during the +L+ simulations. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 7.1 Simulation Environment +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We have developed a Bluetooth packet level simulator, which is +L+ based on the Plasma simulation environment [4]. The simulator +L+ has a detailed model of all the packet transmission, reception pro- +L+ cedures in the Bluetooth Baseband including packet buffering, up- +L+ per layer packet segmentation/reassemble, the ARQ mechanism, +L+ etc. The simulator supports all Bluetooth packet types and follows +L+ the same master-slave slot structure as in Bluetooth. For the physi- +L+ cal layer we employ a simplified analytical model that captures the +L+ frequency collision effect of interfering piconets. +L+ In the current simulations the connection establishment procedures, +L+ e.g., the inquiry and page procedures are not simulated in detail and +L+ we do not consider dynamic scatternet formation either. Instead we +L+ perform simulations in static scatternet configurations where the +L+ scatternet topology is kept constant during one particular run of +L+ simulation. +L+ In the current simulations we run IP directly on top of the Blue- +L+ tooth link layer and we apply AODV as the routing protocol in the +L+ IP layer. The simulator also includes various implementations of +L+ the TCP protocol (we employed RenoPlus) and supports different +L+ TCP/IP applications, from which we used TCP bulk data transfer +L+ in the current simulations. +L+ One of the most important user perceived performance measures is +L+ the achieved throughput. We are going to investigate the throughput +L+ in case of bulk TCP data transfer and in case of Constant Bit Rate +L+ (CBR) sources. +L+ In order to take into account the power consumption of nodes we +L+ define activity ratio of a node, ract as the fraction of time when +L+ the node has been active over the total elapsed time; and power +L+ efficiency, pep p as the fraction of the number of user bytes success- +L+ fully communicated (transmitted and received) over the total time +L+ the node has been active. The power efficiency shows the num- +L+ ber of user bytes that can be communicated by the node during an +L+ active period of length 1 sec. Power efficiency can be measured +L+ in [kbit/sec], or assuming that being active for 1 sec consumes 1 +L+ unit of energy we can get a more straightforward dimension of +L+ [kbit/energy unit], which is interpreted as the number of bits that +L+ can be transmitted while consuming one unit of energy. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 7.2 Network Access Point Scenario +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this scenario we have a NAP that is assumed to be connected to +L+ a wired network infrastructure and it provides network access via +L+ its Bluetooth radio interface. The NAP acts as a master and up to 7 +L+ laptops, all acting as slaves, can connect to the NAP. Furthermore +L+ we assume that each laptop has a Bluetooth enabled mouse and +L+ each laptop connects to its mouse by forming a new piconet as it is +L+ shown in Figure 6. +L+ We simulate a bulk TCP data transfer from the NAP towards each +L+ laptop separately. Regarding the traffic generated by the mouse we +L+ assume that the mouse produces a 16 byte long packet each 50 ms, +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 6: Network Access Point Scenario +L+ </SectLabel_figureCaption> <SectLabel_bodyText> periodically. In the NAP-laptop communication we are interested +L+ in the achieved throughput while in the laptop-mouse communi- +L+ cation we are concerned with the delay perceived by the mouse. +L+ In the current scenario we switched off the dynamic checkperiod +L+ adjustment capability of the PCSS algorithm and we set the base +L+ checking period to 32 frames (40 ms), which is in accordance with +L+ the delay requirement of a mouse. Note that this same base check- +L+ ing period is applied also on the NAP-laptop links, although, there +L+ is no delay requirement for the TCP traffic. However, the current +L+ implementation in the simulator does not yet support the setting of +L+ the base checking periods for each link separately. The dynamic +L+ checking period adjustment would definitely improve the through- +L+ put of NAP-laptop communication as we are going to see later in +L+ case of other configurations. +L+ The simulation results are shown in Figure 7. In plot (a) the aver- +L+ aged throughput of NAP-laptop communications are shown in the +L+ function of number of laptops for the different algorithms, respec- +L+ tively. Graph (b) plots the sum of the throughputs between the +L+ NAP and all laptops. As we expect, the individual laptop through- +L+ put decreases as the number of laptops increases. However, it is +L+ important to notice that the sum of laptop throughputs do not de- +L+ crease with increasing number of laptops in case of the PCSS and +L+ ICSS algorithms. As the number of laptops increases the efficient +L+ coordination becomes more important and the total carried traffic +L+ will decrease with the uncoordinated UGSS scheme. The increase +L+ of the total throughput in case of the PCSS algorithm is the conse- +L+ quence of the fixed checking intensities, which allocates one half +L+ of a laptop capacity to the mouse and the other half to the NAP. In +L+ case of small number of laptops this prevents the laptops to fully +L+ utilize the NAP capacity, which improves as the number of laptops +L+ increases. +L+ The 99% percentile of the delay seen by mouse packets is shown in +L+ plot (c). The delay that can be provided with the PCSS algorithm +L+ is determined by the base checking period that we use. Recall, that +L+ in the current setup the base checking period of the PCSS scheme +L+ was set to 32 frames, which implies that the delay has to be in the +L+ order of 32 frames, as shown in the figure. The low delay with the +L+ UGSS algorithm is due to the continuous switching among the links +L+ of a node, which ensures high polling intensity within a piconet +L+ and frequent switching between piconets. The UGSS algorithm +L+ provides an unnecessarily low delay, which is less than the delay +L+ requirement at the expense of higher power consumption. +L+ Plots (d) and (e) show the averaged activity ratio over all lap- +L+ tops and mice, respectively. The considerably higher throughput +L+ achieved for small number of laptops by the ICSS scheme explains +L+ its higher activity ratio. On graph (f) the averaged power efficiency +L+ of laptops is shown, which relates the number of bytes transmit- +L+ ted to the total time of activity. The power efficiency of the PCSS +L+ </SectLabel_bodyText> <SectLabel_figure> mouse +L+ mouse +L+ NAP +L+ laptop	max 7	laptop +L+ </SectLabel_figure> <SectLabel_bodyText> scheme decreases with increasing number of laptops, which is a +L+ consequence of the fixed checking intensities. Since the NAP has +L+ to share its capacity among the laptops, with an increasing number +L+ of laptops there will be an increasing number of checkpoints where +L+ the NAP cannot show up. In such cases the dynamic checking in- +L+ tensity adjustment procedure could help by decreasing checking +L+ intensity on the NAP-laptop links. Recall that in the current sce- +L+ nario we employed fixed checking intensities in order to satisfy the +L+ mouse delay requirement. It is also important to notice that with the +L+ uncoordinated UGSS scheme the activity ratio of a mouse is rela- +L+ tively high, which is an important drawback considering the low +L+ power capabilities of such devices. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 7.3 Impact of Number of Forwarding Hops +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In what follows, we investigate the performance impact of the num- +L+ ber of forwarding hops along the communication path in the scat- +L+ ternet configuration shown in Figure 8. The configuration consists +L+ of a chain of S/M forwarding nodes (Fi) and a certain number of +L+ additional node pairs connected to each forwarding node in order to +L+ generate background traffic. The number of S/M forwarding nodes +L+ is denoted by NF. There are NB number of background node pairs +L+ connected to each forwarding node as masters. The background +L+ traffic flows from each source node Bz) to its destination pair +L+ B(P) through the corresponding forwarding node Fi. The traffic +L+ that we are interested in is a bulk TCP data transfer between node +L+ S and D. The background traffic is a CBR source, which generates +L+ 512 byte long IP packets with a period of length 0.05 sec. +L+ </SectLabel_bodyText> <SectLabel_none> D +L+ </SectLabel_none> <SectLabel_bodyText> During the simulations we vary the number of forwarding hops NF +L+ and the number of background node pairs NB connected to each +L+ forwarding node. As one would expect, with increasing number of +L+ forwarding hops and background node pairs the coordinated algo- +L+ rithms will perform significantly better than the one without any +L+ coordination (UGSS). +L+ The throughput of the S-D traffic as a function of the number of +L+ forwarding nodes (NF) without background traffic (NB = 0) and +L+ with two pairs of background nodes (NB = 2) are shown in Fig- +L+ ure 9 (a) and (b), respectively. The throughput in case of no cross +L+ traffic drops roughly by half when we introduce the first forward- +L+ ing node. Adding additional forwarding hops continuously reduces +L+ the throughput, however, the decrease at each step is less drasti- +L+ cal. We note that in case of the ICSS scheme one would expect +L+ that for NF > 1 the throughput should not decrease by adding +L+ additional forwarding hops. However, there are a number of other +L+ effects besides the number of forwarding hops that decrease the +L+ throughput. For instance, with an increasing number of forward- +L+ ing hops the number of piconets in the same area increases, which, +L+ in turn, causes an increasing number of lost packets over the radio +L+ interface due to frequency collisions. Furthermore with increasing +L+ number of hops the end-to-end delay suffered by the TCP flow in- +L+ creases, which makes the TCP connection less reactive to recover +L+ from packet losses. +L+ In the no background traffic case the PCSS scheme performs close +L+ to the UGSS algorithm in terms of throughput. However, as we +L+ introduce two pairs of background nodes the UGSS algorithm fails +L+ completely, while the PCSS scheme still achieves approximately 20 +L+ kbit/sec throughput. Furthermore, the power efficiency of the PCSS +L+ scheme is an order of magnitude higher than that of the UGSS algo- +L+ rithm in both cases, which indicates that the PCSS algorithm con- +L+ sumes significantly less power to transmit the same amount of data +L+ than the UGSS scheme. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 7.4 Impact of Bridging Degree +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Next we investigate the performance of scheduling algorithms as +L+ the number of piconets that a bridging node participates in is in- +L+ creased. The scatternet setup that we consider is shown in Figure +L+ 10, where we are interested in the performance of the bridging node +L+ C. Node C is an all slave bridging node and it is connected to mas- +L+ ter nodes Pi, where the number of these master nodes is denoted +L+ by NP. To each master node Pi we connect NL number of leaf +L+ nodes as slaves in order to generate additional background load in +L+ the piconets. We introduce bulk TCP data transfer from node C +L+ towards each of its master node Pi and CBR background traffic +L+ on each Lij — Pi link. The packet generation interval for back- +L+ ground sources was set to 0.25 sec, which corresponds to a 16 +L+ kbit/sec stream. During the simulation we vary the number of pi- +L+ conets NP participated by node C and investigate the performance +L+ of the PCSS algorithm with and without dynamic checkpoint inten- +L+ sity changes. The number of background nodes NL connected to +L+ each master node Pi was set to NL = 3 and it was kept constant in +L+ the simulations. +L+ </SectLabel_bodyText> <SectLabel_none> LmNL +L+ </SectLabel_none> <SectLabel_bodyText> The throughputs of TCP flows between node C and each Pi are av- +L+ eraged and it is shown in Figure 10 (a). The sum of TCP through- +L+ puts are plotted in graph (b) and the power efficiency of the central +L+ node is shown in graph (c). The PCSS algorithm has been tested +L+ both with fixed base checking periods equal to 32 frames (“PCSS- +L+ 32”) and with dynamic checking intensity changes as well (“PCSS- +L+ dyn”). The parameter settings of the dynamic case is shown in Ta- +L+ ble 1. +L+ </SectLabel_bodyText> <SectLabel_table> quti = 0.7	Nsample,min = 4 +L+ Plower = 0.3	Pupper = 0.7 +L+ quiode)=0.7	Nuti,win = 10	maxe) = 0.8 +L+ Tmin = 8	Tmax = 256 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Parameter setting of the dynamic PCSS scheme +L+ </SectLabel_tableCaption> <SectLabel_figure> Lmi +L+ LNPm +L+ Pm +L+ LNPNL +L+ PNP +L+ C +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 10: Impact of number of participated piconets +L+ </SectLabel_figureCaption> <SectLabel_figure> 	B(D)	B(D)	B(D). +L+ 	1i	2i	NF� +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 8: Impact of number of forwarding nodes +L+ </SectLabel_figureCaption> <SectLabel_figure> B(S) +L+ B(S) +L+ F1	F2	FNF +L+ S +L+ B(S) +L+ N. +L+ F� +L+ (d)	(e)	(f) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 7: Throughput, delay and power measures in the function of number of laptops connected to the NAP +L+ </SectLabel_figureCaption> <SectLabel_figure> Activity Ratio of laptops +L+ Activity Ratio of mice +L+ Power efficiency of laptops +L+ TCP throughput per laptop +L+ Sum TCP throughput of laptops +L+ 0.99 percentile of mouse dealy +L+ 1	2	3	4	5	6	7 +L+ Number of laptops +L+ 1	2	3	4	5	6	7 +L+ Number of laptops +L+ 1	2	3	4	5	6	7 +L+ Number of laptops +L+ (a)	(b)	(c) +L+ PCSS +L+ UGSS +L+ ICSS +L+ PCSS +L+ UGSS +L+ ICSS +L+ 500 +L+ 450 +L+ 400 +L+ 350 +L+ 300 +L+ 250 +L+ 200 +L+ 150 +L+ 100 +L+ 50 +L+ 0 +L+ 0.06 +L+ 0.05 +L+ 0.04 +L+ 0.03 +L+ 0.02 +L+ 0.01 +L+ 0 +L+ 500 +L+ 450 +L+ 400 +L+ 350 +L+ 300 +L+ 250 +L+ 200 +L+ 150 +L+ 100 +L+ 50 +L+ 0 +L+ PCSS +L+ UGSS +L+ ICSS +L+ 1	2	3	4	5	6	7 +L+ Number of laptops +L+ 1	2	3	4	5	6	7 +L+ Number of laptops +L+ 1	2	3	4	5	6	7 +L+ Number of laptops +L+ 600 +L+ 0.9 +L+ 0.8 +L+ 0.7 +L+ 0.6 +L+ 0.5 +L+ 0.4 +L+ 0.3 +L+ 0.2 +L+ 0.1 +L+ 500 +L+ PCSS +L+ UGSS +L+ ICSS +L+ 0.9 +L+ 0.8 +L+ 0.7 +L+ 0.6 +L+ 0.5 +L+ 0.4 +L+ 0.3 +L+ 0.2 +L+ 0.1 +L+ 400 +L+ 300 +L+ 200 +L+ 100 +L+ 0 +L+ 0 +L+ 0 +L+ PCSS +L+ UGSS +L+ ICSS +L+ PCSS +L+ UGSS +L+ ICSS +L+ TCP throughput without background nodes (N_B=0) +L+ PCSS +L+ UGSS +L+ ICSS +L+ 0	1	2	3	4	5	6	7	8 +L+ Number of forwarding nodes (N_F) +L+ 500 +L+ 450 +L+ 400 +L+ 350 +L+ 300 +L+ 250 +L+ 200 +L+ 150 +L+ 100 +L+ 50 +L+ 0 +L+ TCP throughput with 2 pairs of background nodes (N_B=2) +L+ Power efficiency of forwarding nodes (N_B=2) +L+ 0	1	2	3	4	5	6	7	8 +L+ Number of forwarding nodes (N_F) +L+ 0	1	2	3	4	5	6	7	8 +L+ Number of forwarding nodes (N_F) +L+ 0 +L+ 300 +L+ 200 +L+ 100 +L+ 500 +L+ 450 +L+ 400 +L+ PCSS +L+ UGSS +L+ ICSS +L+ 350 +L+ 300 +L+ 250 +L+ 200 +L+ 150 +L+ 100 +L+ 50 +L+ 0 +L+ 600 +L+ 500 +L+ 400 +L+ PCSS +L+ UGSS +L+ ICSS +L+ (a)	(b)	(c) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 9: Throughput and power efficiency in function of number of forwarding hops +L+ </SectLabel_figureCaption> <SectLabel_bodyText> It is important to notice that the per flow TCP throughputs in case +L+ of the dynamic PCSS scheme matches quite closely the through- +L+ put achieved by the ICSS algorithm and it significantly exceeds the +L+ throughput that has been achieved by the fixed PCSS. This large +L+ difference is due to the relatively low background traffic in the +L+ neighbouring piconets of node C, in which case the dynamic PCSS +L+ automatically reduces checkpoint intensity on the lightly loaded +L+ links and allocates more bandwidth to the highly loaded ones by +L+ increasing checking intensity. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have presented Pseudo Random Coordinated Scatternet +L+ Scheduling, an algorithm that can efficiently control communica- +L+ tion in Bluetooth scatternets without exchange of control informa- +L+ tion between Bluetooth devices. The algorithm relies on two key +L+ components, namely the use of pseudo random sequences of meet- +L+ ing points, that eliminate systematic collisions, and a set of rules +L+ that govern the increase and decrease of meeting point intensity +L+ without explicit coordination. +L+ We have evaluated the performance of PCSS in a number of sim- +L+ ulation scenarios, where we have compared throughput and power +L+ measures achieved by PCSS to those achieved by two reference +L+ schedulers. The first reference scheduler is an uncoordinated +L+ greedy algorithm, while the other is a hypothetical “ideal” sched- +L+ uler. +L+ In all the scenarios investigated we have found that PCSS achieves +L+ higher throughput than the uncoordinated reference algorithm. +L+ Moreover, with the traffic dependent meeting point intensity adjust- +L+ ments the throughput and power measures of PCSS quite closely +L+ match the results of the “ideal” reference algorithm. At the same +L+ time PCSS consumes approximately the same amount of power as +L+ the ideal scheduler to achieve the same throughput, which is sig- +L+ nificantly less than the power consumption of the uncoordinated +L+ reference scheduler. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] Bluetooth Special Interest Group. Bluetooth Baseband +L+ Specification Version 1.0 B. http://www.bluetooth.com/. +L+ </SectLabel_reference> <SectLabel_figureCaption> Figure 11: Throughput and power efficiency in function of the bridging degree of node C +L+ </SectLabel_figureCaption> <SectLabel_figure> Averaged TCP throughput between central node and master nodes +L+ Sum of TCP throughputs at the central node +L+ Effective power of central node +L+ 1	2	3	4	5	6 +L+ Number of piconets participated by the central node (N_P) +L+ 1	2	3	4	5	6 +L+ Number of piconets participated by the central node (N_P) +L+ 1	2	3	4	5	6 +L+ Number of piconets participated by the central node (N_P) +L+ (a)	(b)	(c) +L+ PCSS-3y2 +L+ PCSUGSS +L+ ICSS +L+ PCSS-32 +L+ PCSS-dyn +L+ UGSS +L+ ICSS +L+ 450 +L+ 400 +L+ 350 +L+ 300 +L+ 250 +L+ 200 +L+ 150 +L+ 100 +L+ 50 +L+ 0 +L+ 450 +L+ 400 +L+ 350 +L+ 300 +L+ 250 +L+ 200 +L+ 150 +L+ 100 +L+ 50 +L+ 0 +L+ 600 +L+ 500 +L+ 400 +L+ 300 +L+ 200 +L+ 100 +L+ 0 +L+ PCSS-32 +L+ P	y UGG +L+ ICSS +L+ </SectLabel_figure> <SectLabel_reference> [2] Bluetooth Special Interest Group. +L+ http://www.bluetooth.com/. +L+ [3] J. Haartsen. BLUETOOTH- the universal radio interface for +L+ ad-hoc, wireless connectivity. Ericsson Review, (3), 1998. +L+ [4] Z. Haraszti, I. Dahlquist, A. Farag´o, and T. Henk. Plasma - +L+ an integrated tool for ATM network operation. In Proc. +L+ International Switching Symposium, 1995. +L+ [5] N. Johansson, U. K¨orner, and P. Johansson. Performance +L+ evaluation of scheduling algorithms for Bluetooth. In IFIP +L+ TC6 WG6.2 Fifth International Conference on Broadband +L+ Communications (BC’99), Hong Kong, November 1999. +L+ [6] N. Johansson, U. K¨orner, and L. Tassiulas. A distributed +L+ scheduling algorithm for a Bluetooth scatternet. In Proc. of +L+ The Seventeenth International Teletraffic Congress, ITC’17, +L+ Salvador da Bahia, Brazil, September 2001. +L+ [7] P. Johansson, N. Johansson, U. K¨orner, J. Elgg, and +L+ G. Svennarp. Short range radio based ad hoc networking: +L+ Performance and properties. In Proc. ofICC’99, Vancouver, +L+ 1999. +L+ [8] M. Kalia, D. Bansal, and R. Shorey. MAC scheduling and +L+ SAR policies for Bluetooth: A master driven TDD +L+ pico-cellular wireless system. In IEEE Mobile Multimedia +L+ Communications Conference MOMUC’99, San Diego, +L+ November 1999. +L+ [9] M. Kalia, D. Bansal, and R. Shorey. MAC scheduling +L+ policies for power optimization in Bluetooth: A master +L+ driven TDD wireless system. In IEEE Vehicular Technology +L+ Conference 2000, Tokyo, 2000. +L+ [10] M. Kalia, S. Garg, and R. Shorey. Efficient policies for +L+ increasing capacity in Bluetooth: An indoor pico-cellular +L+ wireless system. In IEEE Vehicular Technology Conference +L+ 2000, Tokyo, 2000. +L+ </SectLabel_reference> <SectLabel_sectionHeader> APPENDIX +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Here, we present the procedure for generating the pseudo ran- +L+ dom sequence of checkpoints, where we reuse the elements of +L+ the pseudo random frequency hop generation procedure available +L+ in Bluetooth. The inputs to the checkpoint generation procedure +L+ PseudoChkGen are the current checking period T(h ck, the Blue- +L+ tooth MAC address of the slave As`ave and the current value of the +L+ master’s clock t(i). A node can perform checkpoint generation us- +L+ ing the PseudoChkGen procedure at any point in time, it is al- +L+ ways guaranteed that the position of checkpoint generated by the +L+ two nodes will be the same, as it has been pointed out in Section +L+ 5.1. Nevertheless the typical case will be that whenever a node ar- +L+ rives to a checkpoint it generates the position of the next checkpoint +L+ on the given link. The variable t(i)cysstores the master’s +L+ chek +L+ clock at the next checkpoint, thus it needs to be updated every time +L+ a checkpoint is passed. Here we note that the Bluetooth clock of a +L+ device is a 28 bit counter, where the LSB changes at every half slot. +L+ Let us assume that the base period of checkpoints on the it h link of +L+ the node is Tcheck = 2j-2, j > 2 number of frames, which means +L+ that there is one pseudo randomly positioned checkpoint in each +L+ consecutive time interval of length T(h ck and the jth bit of the +L+ Bluetooth clock changes at every Tcheck. Upon arrival to a check- +L+ point the variable t(i) +L+ check equals to the current value of the master’s +L+ clock on that link. After the checkpoint generation procedure has +L+ been executed the variable tcheck will store the master’s clock at +L+ the time of the next checkpoint on that link. +L+ Before starting the procedure the variable tch)eck is set to the cur- +L+ rent value of the master’s clock t(i) in order to cover the general +L+ case when at the time of generating the next checkpoint the value +L+ of t(i) +L+ check does not necessarily equals to the current value of the +L+ master’s clock t(i). The position of the next checkpoint is ob- +L+ tained such that the node first adds the current value of Tcheck +L+ to the variable tclzeck, clears the bits [j — 1, ... , 0] of tch)eck and +L+ then generates the bits [j — 1, ... , 2] one by one using the pro- +L+ cedure PseudoBitGen(X, WctT`). When generating the kth bit +L+ (j-1 < k < 2) the clock bits X = tcheck [k+1, ... , k+5] are fed +L+ as inputs to the PseudoBitGen procedure, while the control word +L+ WctT` is derived from tcheck including the bits already generated +L+ and from the MAC address of the slave As`ave. The schematic view +L+ of generating the clock bits of the next checkpoint is illustrated in +L+ Figure 12. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 12: Generating the clock bits of the next checkpoint +L+ </SectLabel_figureCaption> <SectLabel_figure> 28. +L+ 27. +L+ Wctrl +L+ k+5. +L+ PseudoBitGen +L+ X +L+ k+1. +L+ k. +L+ 2. +L+ 1. +L+ 0. +L+ </SectLabel_figure> <SectLabel_bodyText> The PseudoBitGen procedure is based on the pseudo random +L+ scheme used for frequency hop selection in Bluetooth. How- +L+ ever, before presenting the PseudoBitGen procedure we give the +L+ pseudo-code of the PseudoChkGen procedure. +L+ PseudoChkGen procedure: +L+ t(i): the current value of the master’s clock; +L+ Tcheck = 2j-2, j > 2: current length of the base checkperiod +L+ in terms of number of frames. +L+ </SectLabel_bodyText> <SectLabel_equation> (i)	(i) +L+ tcheck = t +L+ (i)tcheck [j — 1, . . . , 0] = 0; +L+ t(i)= t(i)	+T(i) +L+ check	check	check; +L+ k=j—1; +L+ while (k > 2) +L+ X[0, ... , 4] = t(i) +L+ check [k + 1,... , k + 5]; +L+ tcheck[k] =PseudoBitGen(X,WctTd); +L+ k=k-1; +L+ end +L+ </SectLabel_equation> <SectLabel_bodyText> Finally, we discuss the PseudoBitGen procedure, which is illus- +L+ trated in Figure 13. +L+ </SectLabel_bodyText> <SectLabel_figure> Z[0] +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 13: The PseudoBitGen procedure +L+ </SectLabel_figureCaption> <SectLabel_figure> P[13,12] P[11,10] P[9,8]	P[7,6]	P[5,4]	P[3,2] P[1,0] +L+ B +L+ A +L+ D +L+ 5 +L+ C +L+ 5 +L+ 5 +L+ 9 +L+ Add +L+ mod 32 +L+ 5 +L+ Y +L+ PERM5 +L+ bit selector +L+ V[k mod 5] +L+ X +L+ O +L+ R +L+ Z +L+ X  5 +L+ 5 +L+ 5 +L+ V +L+ 1 +L+ O +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 14: Butterfly permutation +L+ </SectLabel_figureCaption> <SectLabel_bodyText> The control words of the PseudoBitGen procedure +L+ WctTd = {A, B, C, D} are the same as the control words of +L+ the frequency hop selection scheme in Bluetooth and they are +L+ shown in Table 2. However, the input X and the additional +L+ bit selection operator at the end are different. As it has been +L+ discussed above the input X is changing depending on which +L+ bit of the checkpoint is going to be generated. When gener- +L+ ating the kth clock bit of the next checkpoint the clock bits +L+ X = tcheck [k + 1,... , k + 5] are fed as inputs and the bit +L+ selection operator at the end selects the (k mod 5)th bit of the 5 +L+ bits long output V. +L+ </SectLabel_bodyText> <SectLabel_table> A	Asdave [27 — 23] ® tcheck [25 — 21] +L+ B	B[0 — 3] = Asdave[22 — 19], B[4] =		0 +L+ C	Asdave [8, 6, 4, 2, 0] ® t(i)	— 16] +L+ 	check [20 +L+ D	Asdave[18 — 10] ® t(i)	— 7] +L+ 	check [15 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Control words +L+ </SectLabel_tableCaption> <SectLabel_bodyText> The operation PERM5 is a butterfly permutation, which is the +L+ same as in the frequency hop selection scheme of Bluetooth and +L+ it is described in Figure 14. Each bit of the control word P is +L+ associated with a given bit exchange in the input word. If the +L+ given bit of the control word equals to 1 the corresponding bit ex- +L+ change is performed otherwise skipped. The control word P is +L+ obtained from C and D, such that P[i] = D[i], i = 0... 8 and +L+ P[j + 9] = C[j], j = 0 ... 4. +L+ </SectLabel_bodyText>
<SectLabel_title> A Resilient Packet-Forwarding Scheme against Maliciously +L+ Packet-Dropping Nodes in Sensor Networks +L+ </SectLabel_title> <SectLabel_author> Suk-Bok Lee and Yoon-Hwa Choi +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Engineering +L+ Hongik University +L+ </SectLabel_affiliation> <SectLabel_address> 121-791 Seoul, Korea +L+ </SectLabel_address> <SectLabel_email> {sblee, yhchoi}@cs.hongik.ac.kr +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper focuses on defending against compromised nodes’ +L+ dropping of legitimate reports and investigates the misbe- +L+ havior of a maliciously packet-dropping node in sensor net- +L+ works. We present a resilient packet-forwarding scheme us- +L+ ing Neighbor Watch System (NWS), specifically designed +L+ for hop-by-hop reliable delivery in face of malicious nodes +L+ that drop relaying packets, as well as faulty nodes that +L+ fail to relay packets. Unlike previous work with multipath +L+ data forwarding, our scheme basically employs single-path +L+ data forwarding, which consumes less power than multipath +L+ schemes. As the packet is forwarded along the single-path +L+ toward the base station, our scheme, however, converts into +L+ multipath data forwarding at the location where NWS de- +L+ tects relaying nodes’ misbehavior. Simulation experiments +L+ show that, with the help of NWS, our forwarding scheme +L+ achieves a high success ratio in face of a large number of +L+ packet-dropping nodes, and effectively adjusts its forwarding +L+ style, depending on the number of packet-dropping nodes +L+ en-route to the base station. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> C.2.0 [Computer-Communication Networks]: General— +L+ Security and protection +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Security, Algorithm, Reliability +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Sensor Network Security, Reliable Delivery, Packet-dropping +L+ Attacks, Secure Routing +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Wireless sensor networks consist of hundreds or even thou- +L+ sands of small devices each with sensing, processing, and +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> SASN’06, October 30, 2006, Alexandria, Virginia, USA. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2006 ACM 1-59593-554-1/06/0010 ...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> communicating capabilities to monitor the real-world envi- +L+ ronment. They are envisioned to play an important role +L+ in a wide variety of areas ranging from critical military- +L+ surveillance applications to forest fire monitoring and the +L+ building security monitoring in the near future. In such a +L+ network, a large number of sensor nodes are distributed to +L+ monitor a vast field where the operational conditions are +L+ harsh or even hostile. To operate in such environments, se- +L+ curity is an important aspect for sensor networks and secu- +L+ rity mechanisms should be provided against various attacks +L+ such as node capture, physical tampering, eavesdropping, +L+ denial of service, etc [23, 33, 38]. +L+ Previous research efforts against outsider attacks in key- +L+ management schemes [4, 13, 32] and secure node-to-node +L+ communication mechanisms [24, 32] in sensor networks are +L+ well-defined. Those security protections, however, break +L+ down when even a single legitimate node is compromised. +L+ It turns out to be relatively easy to compromise a legiti- +L+ mate node [14], which is to extract all the security infor- +L+ mation from the captured node and to make malicious code +L+ running for the attacker’s purpose. +L+ Even a small number of compromised nodes can pose +L+ severe security threats on the entire part of the network, +L+ launching several attacks such as dropping legitimate re- +L+ ports, injecting bogus sensing reports, advertising inconsis- +L+ tent routing information, eavesdropping in-network commu- +L+ nication using exposed keys, etc. Such disruption by the +L+ insider attacks can be devastating unless proper security +L+ countermeasures against each type of attacks are provided. +L+ In reality, detecting all of the compromised nodes in the +L+ network is not always possible, so we should pursue grace- +L+ ful degradation [35], with a small number of compromised +L+ nodes. The fundamental principle for defense against the +L+ insider attacks is to restrict the security impact of a node +L+ compromise as close to the vicinity of the compromised node +L+ as possible. +L+ When the attacker compromises a legitimate node, it may +L+ first try to replicate the captured node indefinitely with the +L+ same ID and spread them over the network. Against such +L+ attacks, a distributed detection mechanism (based on emer- +L+ gent properties [11]) has been proposed by Parno et al. [31]. +L+ In addition, Newsome et al. [30] have presented the tech- +L+ niques that prevent the adversary from arbitrarily creating +L+ new IDs for nodes. +L+ Using cryptographic information obtained from a cap- +L+ tured node, attackers can establish pairwise keys with any +L+ legitimate nodes in order to eavesdrop communication any- +L+ </SectLabel_bodyText> <SectLabel_page> 59 +L+ </SectLabel_page> <SectLabel_bodyText> where in the network. Localized key-establishment scheme +L+ by Zhu et al. [46] is a good solution against such an in- +L+ sider attack. Since the scheme does not allow a cloned node +L+ (by inside-attackers) to establish pairwise keys with any le- +L+ gitimate nodes except the neighbors of the compromised +L+ nodes, the cryptographic keys extracted from the compro- +L+ mised node are of no use for attackers. +L+ Compromised nodes can also inject false sensing reports +L+ to the network (i.e. report fabrication attacks [39]), which +L+ causes false alarms at the base station or the aggregation +L+ result to far deviate from the true measurement. Proposed +L+ en-route filtering mechanisms [8, 39, 41, 44, 47] that de- +L+ tect and drop such false reports effectively limit the impact +L+ of this type of attacks. Also, proposed secure aggregation +L+ protocols [34, 40] have addressed the problem of false data +L+ injection, and they ensure that the aggregated result is a +L+ good approximation to the true value in the presence of a +L+ small number of compromised nodes. +L+ Advertising inconsistent routing information by compro- +L+ mised nodes can disrupt the whole network topology. Hu et +L+ al. [19, 20] have proposed SEAD, a secure ad-hoc network +L+ routing protocol that uses efficient one-way hash functions +L+ to prevent any inside attackers from injecting inconsistent +L+ route updates. A few secure routing protocols [6, 27] in sen- +L+ sor networks have been proposed to detect and exclude the +L+ compromised nodes injecting inconsistent route updates. +L+ Compromised nodes also can silently drop legitimate re- +L+ ports (i.e. selective forwarding attacks [23]), instead of for- +L+ warding them to the next-hop toward the base station. Since +L+ data reports are delivered over multihop wireless paths to +L+ the base station, even a small number of strategically-placed +L+ packet-dropping nodes can deteriorate the network through- +L+ put significantly. In order to bypass such nodes, most work +L+ on secure routing and reliable delivery in sensor networks re- +L+ lies on multipath forwarding scheme [5, 6, 7, 10], or interleaved- +L+ mesh forwarding scheme [26, 29, 39, 42]. +L+ Among the insider attacks described above, this paper fo- +L+ cuses on defense against compromised nodes’ dropping of le- +L+ gitimate reports and we present a resilient packet-forwarding +L+ scheme using Neighbor Watch System (NWS) against ma- +L+ liciously packet-dropping nodes in sensor networks. We in- +L+ vestigate the misbehavior of a maliciously packet-dropping +L+ node and show that an acknowledgement (ACK) that its +L+ packets were correctly received at the next-hop node does +L+ not guarantee reliable delivery from the security perspective. +L+ NWS is specifically designed for hop-by-hop reliable de- +L+ livery in face of malicious nodes that drop relaying packets, +L+ as well as faulty nodes that fail to relay packets. Unlike pre- +L+ vious work [10, 29, 42] with multipath data forwarding, our +L+ scheme basically employs single-path data forwarding, which +L+ consumes less power than multipath schemes. As the packet +L+ is forwarded along the single-path toward the base station, +L+ our scheme, however, converts into multipath data forward- +L+ ing at the location where NWS detects relaying nodes’ mis- +L+ behavior. +L+ NWS exploits the dense deployment of large-scale static +L+ sensor networks and the broadcast nature of communication +L+ pattern to overhear neighbors’ communication for free. +L+ The contribution of this paper is two-fold. First, we in- +L+ vestigate the misbehavior of a maliciously packet-dropping +L+ node and propose a resilient packet-forwarding scheme, which +L+ basically employs single-path data forwarding, in face of +L+ such nodes, as well as faulty nodes. Second, our scheme +L+ can work with any existing routing protocols. Since it is +L+ designed not for securing specific protocols but for universal +L+ protocols, it can be applied to any existing routing protocols +L+ as a security complement. +L+ The rest of paper is organized as follows. Background is +L+ given in Section 2. We present our resilient packet-forwarding +L+ scheme in Section 3. An evaluation of the scheme is given +L+ and discussed in Section 4. We present conclusions and fu- +L+ ture work in Section 5. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. BACKGROUND +L+ 2.1 Network Model +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Sensor networks typically comprise one or multiple base +L+ stations and hundreds or thousands of inexpensive, small, +L+ static, and resource-constrained nodes scattered over a wide +L+ area. An inexpensive sensor node cannot afford tamper- +L+ resistant packaging. We assume that a large number of sen- +L+ sor nodes are deployed in high density over a vast field, such +L+ that the expected degree of a node is high; each sensor has +L+ multiple neighbors within its communication range. Sensing +L+ data or aggregated data are sent along the multihop route +L+ to the base station. We assume that each sensor node has +L+ a constant transmission range, and communication links are +L+ bidirectional. +L+ Our sensor network model employs a key-establishment +L+ scheme that extends the one in LEAP [46] where the im- +L+ pact of a node compromise is localized in the immediate +L+ neighborhood of the compromised node, and our scheme is +L+ based on it. To evolve from LEAP, we will describe it briefly +L+ in Section 2.4. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Threat Model +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The attacks launched from outsiders hardly cause much +L+ damage to the network, since the rouge node, which does not +L+ possesses the legitimate credentials (e.g. the predistributed +L+ key ring from the key pool [13]), fails to participate in the +L+ network. On the other hand, there may be multiple attacks +L+ from insiders (e.g. dropping legitimate reports, injecting +L+ false sensing reports, advertising inconsistent route infor- +L+ mation, and eavesdropping in-network communication us- +L+ ing exposed keys, etc), and the combination of such attacks +L+ can lead to disruption of the whole network. Thus, proper +L+ security countermeasures (specifically designed to protect +L+ against each type of the attacks) should be provided. +L+ Among them, in this paper, we focus on defending against +L+ compromised nodes’ dropping of legitimate reports; Other +L+ attacks mentioned above are effectively dealt with by several +L+ proposed schemes as described in the previous section. +L+ We consider a packet-dropping node as not merely a faulty +L+ node, but also an arbitrarily malicious node. Some previous +L+ work [3, 29, 36] on reliable delivery uses an acknowledge- +L+ ment (ACK) that its packets were correctly received at the +L+ next-hop node, in order to find out unreliable links. How- +L+ ever, in the presence of maliciously packet-dropping nodes, +L+ simply receiving ACK from a next-hop node does not guar- +L+ antee that the packet will be really forwarded by the next- +L+ hop node. For example, node u forwards a packet to com- +L+ promised node v, and node u waits for ACK from node v. +L+ Node v sends back ACK to node u, and then node v silently +L+ drops the packet. This simple example shows that receiving +L+ ACK is not enough for reliable delivery in face of maliciously +L+ packet-dropping nodes. +L+ </SectLabel_bodyText> <SectLabel_page> 60 +L+ </SectLabel_page> <SectLabel_bodyText> For more reliability, we should check whether the next- +L+ hop node really forwards the relaying packet to its proper +L+ next-hop node. Fortunately, due to the broadcast nature of +L+ communication pattern in sensor networks, we can overhear +L+ neighbors’ communication for free (for now per-link encryp- +L+ tion is ignored). After forwarding a packet to next-hop node +L+ v and buffering recently-sent packets, by listening in on node +L+ v’s traffic, we can tell whether node v really transmits the +L+ packet. Watchdog [28] mechanism (extension to DSR [22]), +L+ implicit ACK in M2 RC [29], and local monitoring in DI- +L+ CAS [25] detect misbehaving nodes in this way. However, +L+ this kind of simple overhearing schemes does not guarantee +L+ reliable delivery, either. +L+ With arbitrarily malicious nodes, we should be assured +L+ that the node, to which the next-hop node forwards the +L+ relaying packet, is really a neighbor of the next-hop node. +L+ For example, node u forwards a packet to compromised node +L+ v, and node u listens in on node v’s traffic to compare each +L+ overheard packet with the packet in the buffer. Node v +L+ transmits the relaying packet whose intended next-hop id +L+ marked with any id in the network such as x that is not a +L+ neighbor of v. Then node u overhears this packet from node +L+ v, and considers it forwarded correctly despite the fact that +L+ none actually receives the packet. The packet is eventually +L+ dropped without being detected. We refer to this attack as +L+ blind letter attack. +L+ We consider packet-dropping attacks to be addressed in +L+ this paper as ones ranging from the naive case (e.g. a faulty +L+ node) to the most malicious one (e.g. a node launching +L+ blind letter attack). We focus on developing a solution to +L+ such attacks. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.3 Notation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We use the following notation throughout the paper: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	u, v are principals, such as communicating nodes. +L+ •	R.. is a random number generated by u. +L+ •	fK is a family of pseudo-random function [12]. +L+ •MAC(K, M1 |M2) denotes the message authentication +L+ code (MAC) of message - concatenation of M1 and M2, +L+ with MAC key K. +L+ </SectLabel_listItem> <SectLabel_subsectionHeader> 2.4 Key-Establishment Scheme in LEAP +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> LEAP supports the establishment of four types of keys for +L+ each sensor node - an individual key shared with the base +L+ station, a pairwise key shared with its neighbor, a cluster +L+ key shared with its surrounding neighbors, and a group key +L+ shared by all the nodes in the networks. +L+ It assumes that the time interval Test for a newly deployed +L+ sensor node to complete the neighbor discovery phase (e.g. +L+ tens of seconds) is smaller than the time interval T.i. that is +L+ necessary for the attacker to compromise a legitimate node +L+ (i.e. T�i. > Test). Some existing work [1, 39] has made +L+ similar assumptions, which are believed to be reasonable. +L+ The four steps for a newly added node u to establish a +L+ pairwise key with each of its neighbors are as follows: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. KEY PRE-dIStRIbUtIOn. Each node u is loaded with +L+ a common initial key KI, and derives its master key +L+ K.. = fKI (u). +L+ 2. NEIghbOR DISCOVERY. Once deployed, node u sets +L+ up a timer to fire after time T�i., broadcasts its id, +L+ and waits for each neighbor v’s ACK. The ACK from +L+ v is authenticated using the master key K, of node v. +L+ Since node u knows KI, it can derive K, = fKI (v). +L+ u−→∗: u,R... +L+ v−→u: v, MAC(K,, R.. |v). +L+ 3. PAIRWISE KEY EStAblIShmEnt. Node u computes its +L+ pairwise key with v, K..,, as K.., = fKv (u). Node v +L+ also computes K.., in the same way. K.., serves as +L+ their pairwise key. +L+ 4. KEY ERASURE. When its timer expires, node u erases +L+ KI and all the master keys of its neighbors. Every +L+ node, however, keeps its own master key, in order to +L+ establish pairwise keys with later-deployed nodes. +L+ </SectLabel_listItem> <SectLabel_bodyText> Once erasing KI, a node will not be able to establish a +L+ pairwise key with any other nodes that have also erased KI. +L+ Without KI, a cloned node (by an attacker compromising a +L+ legitimate node after T.i.) fails to establish pairwise keys +L+ with any nodes except the neighbors of the compromised +L+ node. In such a way, LEAP localizes the security impact of +L+ a node compromise. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. A RESILIENT PACKET-FORWARDING +L+ SCHEME USING NEIGHBOR WATCH SYS- +L+ TEM +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we present our resilient packet-forwarding +L+ scheme using Neighbor Watch System (NWS). NWS works +L+ with the information provided by Neighbor List Verification +L+ (NLV) to be described in Section 3.2. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Neighbor Watch System +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our scheme seeks to achieve hop-by-hop reliable delivery +L+ in face of maliciously packet-dropping nodes, basically em- +L+ ploying single-path forwarding. To the best of our knowl- +L+ edge, proposed works so far rely on multipath forwarding +L+ or diffusion-based forwarding, exploiting a large number of +L+ nodes in order to deliver a single packet. ACK-based tech- +L+ nique is not a proper solution at all as explained in the +L+ previous section. +L+ With NWS, we can check whether the next-hop node re- +L+ ally forwards the relaying packet to the actual neighbor of +L+ the next-hop node. The basic idea of our scheme is as fol- +L+ lows: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Neighbor List Verification. After deployment, dur- +L+ ing neighbor discovery phase, every node u gets to +L+ know of not only its immediate neighbors, but also the +L+ neighbors’ respective neighbor lists (i.e. u’s neighbors’ +L+ neighbor lists). The lists are verified using Neighbor +L+ List Verification to be described in Section 3.2. Every +L+ node stores its neighbors’ neighbor lists in the neighbor +L+ table. +L+ 2. Packet Forwarding to Next-hop. If node u has +L+ a packet to be relayed, it buffers the packet and for- +L+ wards the packet (encrypted with cluster key of node +L+ u so that neighbors of node u can overhear it) to its +L+ next-hop node v. As in LEAP, a cluster key is a key +L+ shared by a node and all its neighbors, for passive par- +L+ ticipation. +L+ </SectLabel_listItem> <SectLabel_page> 61 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: Neighbor Watch System. Sub-watch +L+ nodes w and y, as well as primary-watch node u lis- +L+ ten in on v’s traffic. +L+ </SectLabel_figureCaption> <SectLabel_listItem> 3. Designation of Watch Nodes. Overhearing the +L+ packet from node u to node v, among neighbors of +L+ node u, the nodes that are also neighbors of node v (in +L+ Figure 1, nodes w and y) are designated as sub-watch +L+ nodes and store the packet in the buffer. Other nodes +L+ (that are not neighbors of node v) discard the packet. +L+ Node u itself is a primary-watch node. A primary- +L+ watch node knows which nodes are sub-watch nodes, +L+ since every node has the knowledge of not only its +L+ neighbors but also their respective neighbor lists. +L+ 4. Neighbor Watch by Sub-Watch Node. Sub-watch +L+ nodes w and y listen in on node v’s traffic to compare +L+ each overheard packet with the packet in the buffer. +L+ To defend against blind letter attack, each of them +L+ also checks whether the packet’s intended next-hop is +L+ a verified neighbor of node v, by looking up the neigh- +L+ bor table. If all correct, the packet in the buffer is +L+ removed and the role of the sub-watch node is over. +L+ If the packet has remained in the buffer for longer +L+ than a certain timeout, sub-watch nodes w and y for- +L+ ward the packet (encrypted with their respective clus- +L+ ter keys) to their respective next-hop nodes other than +L+ node v. Then the role of a sub-watch node is over (each +L+ of them is now designated as a primary-watch node for +L+ the packet it has forwarded). +L+ 5. Neighbor Watch by Primary-Watch Node. Primary- +L+ watch node u does the same job as sub-watch nodes. +L+ </SectLabel_listItem> <SectLabel_bodyText> The only difference, however, is that it listens in on +L+ not only node v’s traffic, but also sub-watch nodes w’s +L+ and y’s. If the packet is correctly forwarded on by at +L+ least one of them (nodes v, w, or y), primary-watch +L+ node u removes the packet in the buffer and the role +L+ of the primary-watch node is over. +L+ Otherwise, after a certain timeout, primary-watch node +L+ u forwards the packet (encrypted with its cluster key) +L+ to its next-hop other than node v. +L+ As the packet is forwarded on, this procedure (except for +L+ Neighbor List Verification) of NWS is performed at each +L+ hop so that hop-by-hop reliable delivery can be achieved +L+ with mainly depending on single-path forwarding. On the +L+ other hand, in the previous approaches [29, 39, 42], when +L+ forwarding a packet, a node broadcasts the packet with no +L+ designated next-hop, and all neighbors with smaller costs' +L+ 'The cost at a node is the minimum energy overhead to +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: An example of our packet-forwarding +L+ scheme. Only the nodes that relay the packet are +L+ presented. With the help of sub-watch nodes (grey +L+ ones), our scheme bypasses two packet-dropping +L+ nodes en-route to the base station. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> or within a specific geographic region continue forwarding +L+ the packet anyway. For example, in Figure 1, if nodes v, +L+ w, and y have smaller costs than node u in the previous +L+ approaches, they all forward2 the packet from node u. In +L+ our scheme, however, sub-watch nodes w and y are just on +L+ watch in designated next-hop node v, instead of uncondi- +L+ tionally forwarding the packet. If no packet-dropping occurs +L+ en-route to the base station, the packet may be forwarded +L+ along single-path all the way through. +L+ However, a packet-dropping triggers the multipath for- +L+ warding for the dropped packet. If the designated next-hop +L+ node v in Figure 1 has not forwarded the relaying packet to +L+ its certified neighbor by a certain timeout, sub-watch nodes +L+ w and y forward the packet to their respective next-hop. +L+ At the point, the packet is sent over multiple paths. Since +L+ the location where the packet-dropping occurs is likely in +L+ an unreliable region, this prompt reaction of the conver- +L+ sion to multipath forwarding augments the robustness in our +L+ scheme. The degree of multipath depends on the number of +L+ the sub-watch nodes. Figure 2 shows an example of our +L+ packet-forwarding scheme, bypassing two packet-dropping +L+ nodes en-route to the base station. If a node utilizes a cache +L+ [16, 21] for recently-received packets, it can suppress the +L+ same copy of previously-received one within a certain time- +L+ out, as nodes u and v in Figure 2. +L+ Our scheme requires that a relaying packet should be en- +L+ crypted with a cluster key of a forwarding node, in order +L+ that all its neighbors can decrypt and overhear it. In fact, +L+ per-link encryption provides better robustness to a node +L+ compromise, since a compromised node can decrypt only +L+ the packets addressed to it. Thus, there exists a tradeoff +L+ between resiliency against packet-dropping and robustness +L+ to a node compromise. However, encryption with a cluster +L+ key provides an intermediate level of robustness to a node +L+ compromise [24] (a compromised node can overhear only +L+ its immediate neighborhood), and also supports local broad- +L+ cast (i.e. resiliency against packet-dropping), so that we can +L+ achieve graceful degradation in face of compromised nodes. +L+ forward a packet from this node to the base station. +L+ </SectLabel_bodyText> <SectLabel_footnote> 2It is the broadcast transmission with no designated next- +L+ hop, and, if needed, the packet should be encrypted with a +L+ cluster key in order for all neighbors to overhear it. +L+ </SectLabel_footnote> <SectLabel_figure> u +L+ y +L+ v +L+ w +L+ Base +L+ Station +L+ v +L+ u +L+ </SectLabel_figure> <SectLabel_page> 62 +L+ </SectLabel_page> <SectLabel_bodyText> To make our scheme work (against blind letter attack), we +L+ must address the problem of how a node proves that it re- +L+ ally has the claimed neighbors. It is the identical problem of +L+ how a node verifies the existence of its neighbors’ neighbors. +L+ Apparently, a node has the knowledge of its direct neigh- +L+ bors by neighbor discovery and pairwise key establishment +L+ phases. However, in the case of two-hop away neighbors, +L+ as in Figure 1, malicious node v can inform its neighbor u +L+ that it also has neighbor node x (any possible id in the net- +L+ work) which in fact is not a neighbor of node v. Node u has +L+ to believe it, since node x is not a direct neighbor of node +L+ u, and only the node v itself knows its actual surrounding +L+ neighbors. Then, how do we verify the neighbors’ neigh- +L+ bors? The answer to this critical question is described in +L+ the next subsection. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Neighbor List Verification +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To verify neighbors’ neighbors, we present Neighbor List +L+ Verification (NLV) which extends the pairwise key estab- +L+ lishment in LEAP. During neighbor discovery in LEAP, two +L+ messages are exchanged between neighbors to identify each +L+ other. On the other hand, NLV adopts three-way handshak- +L+ ing neighbor discovery, in order to identify not only com- +L+ municating parties but also their respective neighbors. +L+ NLV has two cases of neighbor discovery. One is that +L+ neighbor discovery between two nodes that are both still +L+ within the initial Tmin3 (referred as pure nodes). The other +L+ is that neighbor discovery between a newly-deployed node +L+ within the initial Tmin and an existing node over the initial +L+ Tmin (referred as an adult node). +L+ Neighbor Discovery between Pure Nodes. Neighbor +L+ list verification process between pure nodes is quite simple. +L+ If a pure node broadcasts its neighbor list before the elapse of +L+ its initial Tmin, we can accept the list as verifiable. Thus, the +L+ key point here is to keep track of each other’s Tmin, and to +L+ make sure that both broadcast their respective neighbor lists +L+ before their respective Tmin. The following shows the three- +L+ way handshaking neighbor discovery between pure node u +L+ and v: +L+ </SectLabel_bodyText> <SectLabel_figure> u----+*: u,Ru. +L+ v ----+u : �v, Tv, �Rv ,  MAC(Kv, Ru J Ku JMv). +L+ M„ +L+ u----+v: u,Tu , MAC(Kuv, RvJMu). +L+ M. +L+ </SectLabel_figure> <SectLabel_bodyText> where Tv and Tu are the amount of time remaining until +L+ Tmin of v and Tmin of u, respectively. Once deployed, node +L+ u sets up a timer to fire after time Tmin. Then, it broadcasts +L+ its id, and waits for each neighbor v’s ACK. The ACK from +L+ every neighbor v is authenticated using the master key Kv of +L+ node v. Since node u knows KI4, it can derive Kv = fKI (v). +L+ The ACK from node v contains Tv, the amount of time +L+ remaining until Tmin of node v. If Tv is a non-zero value, +L+ node v claims to be a pure node. Ku in MAC proves node +L+ v to be a pure node, since pure node v should know KI +L+ and derive Ku = fKI (u). Node u records ˇTv (Tv added +L+ </SectLabel_bodyText> <SectLabel_footnote> 3Tmin is the time interval, necessary for the attacker to com- +L+ promise a legitimate node as in LEAP [46]. +L+ 4Each node u is loaded with a common initial key KI, and +L+ derives its master key Ku = fKI (u). After time Tmin, node +L+ u erases KI and all the master keys of its neighbors. +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 3: Neighbor Discovery between Pure node x +L+ and Adult node u. Grey and white nodes represent +L+ adult and pure nodes, respectively. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> to the current time of node u) in the entry for node v in +L+ the neighbor table. Node u computes its pairwise key with +L+ v, Kuv = fK„ (u).5 Node u also generates MAC(Kv, v Ju) +L+ (which means that v certifies u as an immediate neighbor), +L+ and stores it as a certificate. +L+ The ACK from node u also contains Tu, the amount of +L+ time remaining until Tmin of u. This ACK is authenticated +L+ using their pairwise key Kuv, which proves node u a pure +L+ node and u’s identity. Node v then records ˇTu (Tu added +L+ to the current time of v) in the entry for u in the neighbor +L+ table. It also generates MAC(Ku, uJv) and stores it as a +L+ certificate. Then, the three-way handshaking is done. +L+ Every pure node u broadcasts its neighbor list just prior +L+ to Tmin of u. Each receiving neighbor v checks whether the +L+ receiving time at v is prior to ˇTu in the neighbor table. If +L+ yes, the neighbor list of u is now certified by each neighbor v. +L+ Neighbor Discovery between A Pure Node and An +L+ Adult node. After most nodes have completed bootstrap- +L+ ping phase, new nodes can be added in the network. Con- +L+ sider Figure 3. The issue here is how adult node u can as- +L+ sure its existing neighbors (v and w) of the existence of its +L+ newly-added neighbor x. This is a different situation from +L+ the above neighbor list verification case between two pure +L+ nodes. Thus, the messages exchanged during the three-way +L+ handshaking are somewhat different in this case. The fol- +L+ lowing shows the three-way handshaking neighbor discovery +L+ between pure node x and adult node u: +L+ </SectLabel_bodyText> <SectLabel_equation> x----+ * :	x, Rx. +L+ , MAC(Kxu, Ru JMx). +L+ </SectLabel_equation> <SectLabel_bodyText> Newly-added node x sets up a timer to fire after time Tmin. +L+ Then, it broadcasts its id, and waits for each neighbor u’s +L+ </SectLabel_bodyText> <SectLabel_footnote> 5Node v also computes Kuv in the same way. Kuv serves as +L+ their pairwise key. +L+ </SectLabel_footnote> <SectLabel_equation> r +L+ t +L+ z +L+ v +L+ w +L+ u +L+ x +L+ q +L+ certificate ��	� +L+ certificate ��	� +L+ u----+ x : u, Tu, Ru, v,	MAC(Kv, vJu), w, MAC(Kw, wJu) +L+ 	�	��	� +L+ M. +L+ , MAC(Ku, Rx JMu). +L+ x----+u:	certificate ��	�	one—time cert. �	one—time cert. +L+ 			^ � +L+ MAC(Kx, xJu), v, MAC(Kv, xJu), w, MAC(Kw, xJu) +L+ �	��	� +L+ Ms +L+ x, Tx, +L+ </SectLabel_equation> <SectLabel_page> 63 +L+ </SectLabel_page> <SectLabel_bodyText> ACK. The ACK from every neighbor u is authenticated us- +L+ ing the master key Ku of node u. Since node x knows KI, +L+ it can derive Ku = fKI (u). The ACK from node u contains +L+ Tu, the amount of time remaining until Tmin of u. If Tu is +L+ zero, node u is an adult node that may already have mul- +L+ tiple neighbors as in Figure 3. Node u reports its certified +L+ neighbor list (v and w) to x by including their respective +L+ certificates in the ACK. Node x verifies u’s neighbor list by +L+ examining each certificate, since x can generate any certifi- +L+ cate with KI. If all correct, x computes its pairwise key with +L+ u, Kxu = fKu (x). Node x also generates MAC(Ku, ujx) and +L+ stores it as a certificate. +L+ The ACK from x also contains Tx, the amount of time +L+ remaining until Tmin of x. This ACK is authenticated using +L+ their pairwise key Kxu, which proves node x a pure node +L+ and x’s identity. Node u then records ˇTx (Tx added to the +L+ current time of u) in the entry for x in the neighbor table. +L+ Since adult node u cannot generate MAC(Kx, xju) by itself, +L+ pure node x provides the certificate for u in the ACK. Node +L+ x also provides one-time certificates6 for each of u’s certified +L+ neighbors (v and w). Then, the three-way handshaking is +L+ done. +L+ After that, adult node u broadcasts one-time certificates +L+ (from newly-discovered pure node x), in order to assure u’s +L+ existing neighbors (v and w) of the discovery of new neighbor +L+ x. The packet containing one-time certificates is as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> Mu +L+ , MAC(Kcu, Mu). +L+ </SectLabel_equation> <SectLabel_bodyText> where x is a new neighbor of u, KAu is a local broadcast au- +L+ thentication key in u’s one-way key chain, Kcu is the cluster +L+ key of u. Each receiving neighbor v of u verifies u’s new +L+ neighbor x by examining the one-time certificate designated +L+ for v, MAC(Kv, xju)6. If ok, node x is now certified by each +L+ neighbor v of u. Then, one-time certificates can be erased, +L+ since they are of no use any more. +L+ Broadcast authentication only with symmetric keys such +L+ as cluster key Kcu fails to prevent an impersonation attack, +L+ since every neighbor of u shares the cluster key of u. Thus, +L+ we employ the reverse disclosure of one-way key chain KAu +L+ as in LEAP. +L+ Just prior to Tmin of x, pure node x broadcasts its neigh- +L+ bor list. Each receiving neighbor u of x checks whether the +L+ receiving time at u is prior to ˇTx in the neighbor table. If +L+ yes, the neighbor list of x is now certified by each neighbor u. +L+ In summary, through the proposed three-way handshak- +L+ ing neighbor discovery process, pure node u identifies each +L+ immediate neighbor v and v’s certified neighbor list (if v is +L+ an adult node), and keeps track of Tmin of v. Just prior +L+ to Tmin of u, node u broadcasts its direct neighbor list so +L+ that every neighbor of u accepts the list as verifiable. Then, +L+ node u becomes an adult node. After that, if newly-added +L+ node x initiates neighbor discovery with adult node u, node +L+ u identifies pure node x, keeps track of Tmin of x, provides +L+ u’s certified neighbor list to x, and, in return, takes one-time +L+ certificates from x. Node u then broadcasts these one-time +L+ </SectLabel_bodyText> <SectLabel_footnote> 6One-time certificate, for instance MAC(Kv, xju), assures +L+ </SectLabel_footnote> <SectLabel_bodyText> v that x is an immediate neighbor of u. It is generated by +L+ pure node x with master key of v. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 1: An example of the Neighbor Table of u. +L+ </SectLabel_tableCaption> <SectLabel_table> Neighbor ID	Certificate	Verified Neighbor List +L+ v	MAC(Kv, v ju)	u, w, t +L+ w	MAC(Kw, wju)	u, v, z +L+ x	MAC(Kx, xju)	u, r, q +L+ </SectLabel_table> <SectLabel_bodyText> certificates, in order to assure u’s existing neighbors of the +L+ discovery of new neighbor x. Thus, every time adult node u +L+ discovers newly-added node x through three-way handshak- +L+ ing, node u informs (by broadcasting) its existing neighbors +L+ of the discovery of new neighbor x. Also, whenever receiv- +L+ ing neighbor list information from pure neighbor x, node u +L+ checks whether the receiving time at u is prior to ˇTx in the +L+ neighbor table. If yes, u now accepts the neighbor list of x +L+ as verifiable. +L+ Through the above neighbor list verification in the boot- +L+ strapping phase, every node gets the knowledge of its neigh- +L+ bors’ certified neighbors. Our Neighbor Watch System makes +L+ use of this information to prevent blind letter attack. With +L+ this knowledge, watch nodes are able to check whether the +L+ relaying packet’s intended next-hop is a verified neighbor of +L+ the forwarding node. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Neighbor Table Maintenance +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The information obtained through neighbor list verifica- +L+ tion (e.g. its direct neighbors, corresponding certificates, +L+ neighbors’ neighbor lists, etc) is stored in the neighbor table +L+ of each node. Table 1 shows an example of the neighbor +L+ table of node u. In densely-deployed sensor networks, the +L+ expected degree of a node is high. However, in this example, +L+ for simplicity, node u has only three neighbors v, w, and x +L+ as in Figure 3. +L+ The entries in the neighbor table are accessed and main- +L+ tained with immediate neighbor IDs. For example, if node +L+ u overhears the packet sent from w to v, node u begins to +L+ listen in on v’s traffic as a sub-watch node (since the neigh- +L+ bor table of u has both v’s and w’s entries in it). Unless v +L+ forwards the packet to a node of the Verified Neighbor List +L+ in v’s entry by a certain timeout, sub-watch node u will for- +L+ ward the packet to its next-hop other than v; many existing +L+ routing protocols [5, 18, 21, 27, 37, 43] enable each node to +L+ maintain multiple potential next-hop. Once forwarding the +L+ packet, sub-watch node u becomes a primary-watch node +L+ and begins to listen in on its next-hop’s traffic as described +L+ above. +L+ If newly-added node y initiates the three-way handshaking +L+ with u, node u provides its neighbor list to y by sending +L+ certificates in the neighbor table. Node u, in return from +L+ node y, takes the certificate for y and one-time certificates +L+ for u’s existing neighbors. Then, node u stores the certificate +L+ in the new entry for y. However, node u does not store the +L+ one-time certificates but broadcasts them to its neighbors. +L+ If new neighbor y broadcasts its neighbor list within Tmin, +L+ node u stores the list in the entry for y. +L+ If node u is compromised, not only cryptographic key +L+ information but also certificates in the neighbor table are +L+ exposed. However, the attacker cannot misuse these cer- +L+ tificates for other purposes. Since a certificate only attests +L+ neighborship between two specific nodes, it cannot be ap- +L+ plied to any other nodes. In fact, it can be made even public. +L+ However, colluding nodes can deceive a pure node anyway, +L+ one—time cert.	one—time cert. +L+ </SectLabel_bodyText> <SectLabel_equation> � �	� +L+ MAC(Kv, xju), w, MAC(Kw, xju), KAu +L+ �	��	� +L+ u____+ * :	u, x, v, +L+ � +L+ </SectLabel_equation> <SectLabel_page> 64 +L+ </SectLabel_page> <SectLabel_bodyText> by fabricating a bogus certificate. We will describe this lim- +L+ itation in Section 4.4. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we evaluate the communication and stor- +L+ age cost, and analyze the security of our resilient forwarding +L+ scheme (Neighbor Watch System) as well as Neighbor List +L+ Verification. We then present the simulation results of our +L+ forwarding scheme. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Communication Cost +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Unlike the previously proposed diffusion-based reliable- +L+ forwarding schemes [21, 29, 39, 42] that exploit a large num- +L+ ber of nodes to deliver a single packet, our scheme requires +L+ only the designated next-hop node to relay the packet, un- +L+ der the supervision of watch nodes. We note that, like over- +L+ hearing by watch nodes in our scheme, those diffusion-based +L+ schemes require each node to listen to all its neighbors, since +L+ they forward a packet by broadcasting with no designated +L+ next-hop. With a smaller number of relaying nodes, our +L+ scheme makes a report successfully reach the base station. +L+ Thus, the average communication cost of our forwarding +L+ scheme for delivery of a single packet is smaller than those +L+ of the previous schemes. +L+ Our neighbor list verification during the bootstrapping +L+ phase requires the three-way handshaking neighbor discov- +L+ ery. Unlike the neighbor discovery between two pure nodes, +L+ the size of the messages exchanged between a pure and an +L+ adult node varies with the degree of the adult node. A large +L+ number of certificates caused by the high degree can be over- +L+ burdensome to a single TinyOS packet which provides 29 +L+ bytes for data. Considering 8-byte certificates and a 4-byte7 +L+ message authentication code (MAC), the adult node is able +L+ to include at most two neighbors’ information in a single +L+ TinyOS packet. Thus, when the entire neighbor list cannot +L+ be accommodated within a single packet, the node should +L+ allot the list to several packets and send them serially. In a +L+ network of size N with the expected degree d of each node, +L+ the average number of packets invoked by a newly-added +L+ node per each node is nearly (d — 1)2/2(N — 1). +L+ Therefore, as node density d grows, the total number +L+ of packets transmitted from adult nodes to a newly-added +L+ node increases. However, neighbor discovery between a pure +L+ and an adult node occurs much less than between two pure +L+ nodes, since most neighbor discoveries throughout the net- +L+ work are between two pure nodes in the early stage of the +L+ network. Neighbor discovery between a pure and an adult +L+ node occurs generally when a new node is added to the net- +L+ work. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Storage Overhead +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In LEAP, each node keeps four types of keys and a man- +L+ ageable length of hash chain, which is found to be scalable. +L+ In our scheme, each node needs to additionally store its di- +L+ rect neighbors’ certificates and their respective neighbor lists +L+ as in Table 1. Thus, for a network of the expected degree +L+ d and the byte size l of node ID, the additional storage re- +L+ quirement for each node is d • (8 + ld) bytes. +L+ Although our storage requirement for these neighbor lists +L+ is O(d 2), for a reasonable degree d, memory overhead does +L+ </SectLabel_bodyText> <SectLabel_footnote> 74-byte MAC is found to be not detrimental in sensor net- +L+ works as in TinySec [24] which employs 4-byte MAC. +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 4: Examples of critical area C1 and C2. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> not exceed 1 KB (a Berkeley MICA2 Mote with 128 KB +L+ flash memory and 4 KB SRAM). For example, when d = 20 +L+ and l = 2, a node needs 960 bytes of memory to store such +L+ information. +L+ If node density of a network is so high that the required +L+ space for those neighbor lists significantly increases and the +L+ storage utilization becomes an issue, we can employ a storage- +L+ reduction technique such as Bloom filter [2]. For example, +L+ when d = 30 and l = 2, a node requires 2,040 bytes of addi- +L+ tional space mainly for the neighbor lists. Instead of storing +L+ neighbors’ neighbor lists, applying each of the neighbor lists +L+ (480 bits) to a Bloom filter (of 5 hash functions mapping to +L+ a 256 bit vector), a node needs the reduced space of 1,200 +L+ bytes for such information (with the false positive probabil- +L+ ity = 0.02). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Resilience to Packet-Dropping Attacks +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In face of maliciously packet-dropping nodes, the higher +L+ degree of multipath we provide, the more resiliency our +L+ scheme achieves against such attacks. The average degree +L+ of multipath depends on the number of sub-watch nodes +L+ around a packet-dropping node. Sub-watch nodes should +L+ be located in the region within the communication range of +L+ both forwarding node u and designated next-hop v. We re- +L+ fer to such a region as critical area. As in Figure 4, if nodes +L+ u and v are located farther away, the size of critical area C2 +L+ gets smaller than that of C1, and the probability (p.) that +L+ at least one sub-watch node exists in the critical area goes +L+ down. The probability (p.) is +L+ </SectLabel_bodyText> <SectLabel_equation> p. = 1 — (1 — c)d-1, +L+ </SectLabel_equation> <SectLabel_bodyText> where c is the ratio of the critical area size to the node’s com- +L+ munication range, and the expected degree d of the node. +L+ To determine the appropriate degree d, we set the smallest +L+ critical area C2 in Figure 4 as a lower bound case (c = 0.4). +L+ Figure 5 shows that, even in the lower bound critical area, +L+ with d = 6 and d = 10, probability p. is above 0.9 and above +L+ 0.99, respectively. +L+ Since, in a network of degree d, the probability that there +L+ exist m sub-watch nodes in the critical area of the ratio c is +L+ </SectLabel_bodyText> <SectLabel_equation> p(m) = �d — m J 1) cm(1 — c)d-m-1 +L+ </SectLabel_equation> <SectLabel_bodyText> the expected number of sub-watch nodes, m, in the critical +L+ area is given by +L+ </SectLabel_bodyText> <SectLabel_equation> E[m] = (d — 1)c. +L+ </SectLabel_equation> <SectLabel_bodyText> Thus, in the lower bound (c = 0.4) critical area, when d = +L+ 10, 15, 20, the number of sub-watch nodes (i.e. the degree +L+ of multipath) is 3.6, 5.6, 7.6 on average, respectively. This +L+ </SectLabel_bodyText> <SectLabel_figure> u	v	?	u	v	? +L+ C1	C2 +L+ , +L+ </SectLabel_figure> <SectLabel_page> 65 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 5: Probability (p.) that at least one sub- +L+ watch node exists in the lower bound (c = 0.4) criti- +L+ cal area. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> shows that the higher degree of each node has, our scheme +L+ has the higher degree of multipath and resiliency against +L+ packet-dropping nodes. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.4 The Security of Neighbor List Verification +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our Neighbor List Verification(NLV) keeps the nice prop- +L+ erties of LEAP. Adult nodes fail to establish pairwise keys +L+ with any adult nodes in arbitrary locations, so that the im- +L+ pact of a node compromise is localized. NLV performs the +L+ three-way handshaking neighbor discovery, instead of two- +L+ message exchange in LEAP. The three-way handshaking en- +L+ ables each node to verify not only its direct neighbors but +L+ also their respective neighbor lists. +L+ Moreover, this this three-way handshaking can be a po- +L+ tential solution to deal with irregularity of radio range [15, +L+ 37, 45]. In reality, due to the noise and some environmen- +L+ tal factors, radio range of each node is not exactly circu- +L+ lar. So, communication links among nodes are asymmetric; +L+ node u can hear node v which is unable to hear u. With +L+ two-message exchange, only the node initiating the neigh- +L+ bor discovery is assured of the link’s bidirectionality. By the +L+ three-way handshaking, both of neighbors can be assured of +L+ their symmetric connectivity. +L+ With NLV, only the verified lists are stored and utilized +L+ for our packet-forwarding scheme. NLV verifies the neighbor +L+ list of an adult node with certificates. These certificates +L+ merely attest neighborship between two specific nodes. Even +L+ if a node is compromised, the attacker fails to abuse the +L+ certificates of the captured node for other purpose. +L+ However, collusion among compromised nodes can fab- +L+ ricate bogus certificates in order to deceive a newly-added +L+ node. For example, consider two colluding nodes u and v at +L+ the different locations. When compromised node u discovers +L+ newly-added node x, node u provides x with u’s neighbor +L+ list (maliciously including v in it). Even though node v is +L+ not an actual neighbor of u, colluding node v can generate +L+ the bogus certificate for u, MAC(K,, v1u). Then, x falsely +L+ believes that v is a direct neighbor of u. This attack, how- +L+ ever, affects only the one newly-added node x. Thus, when +L+ compromised node u tries to launch the blind letter attack 8, +L+ </SectLabel_bodyText> <SectLabel_footnote> 8Compromised node u transmits the relaying packet with its +L+ </SectLabel_footnote> <SectLabel_bodyText> other surrounding adult neighbors of u can still detect it +L+ anyway. +L+ The more serious case is that colluding nodes exploit a +L+ newly-added node to generate bogus one-time certificates. +L+ For example, consider two colluding nodes u and v that +L+ share all their secret information as well as all their certifi- +L+ cates. When newly-added node x initiates the three-way +L+ handshaking with u, compromised node u pretends to be +L+ v and provides x with v’ neighbor list. Then, x in return +L+ provides u with one-time certificates for each neighbor of +L+ v; these one-time certificates falsely attest that v has new +L+ neighbor x. Node u sends this information to v over the +L+ covert channel. Then, v broadcasts these one-time certifi- +L+ cates, and neighbors of v falsely believe that x is a direct +L+ neighbor of v. +L+ Unfortunately, we do not provide a proper countermea- +L+ sure to defend against this type of man-in-the-middle at- +L+ tacks. However, we point out that this type of attacks has +L+ to be launched in the passive manner. The adversary has +L+ to get the chance of discovery of a newly-added node. In +L+ other words, compromised nodes wait for the initiation of +L+ the three-way handshaking from a newly-added node. Since +L+ the attacker does not know where the new nodes will be +L+ added, it has to compromise a sufficient number of legiti- +L+ mate nodes in order to increase the probability of discovery +L+ of newly-added nodes. +L+ As an active defense against such man-in-the-middle at- +L+ tacks, we can apply a node replication detection mechanism +L+ such as Randomized or Line-Selected Multicast [31], which +L+ revokes the same ID node at the different location claims. +L+ To successfully launch such man-in-the-middle attacks, two +L+ colluding nodes should pretend to be each other so that each +L+ of them claims to be at two different locations with the same +L+ ID. Location-binding key-assignment scheme by Yang et al. +L+ [39] with a little modification also can be a good solution +L+ to such attacks. Since it binds secret keys with nodes’ geo- +L+ graphic locations, the key bound to the particular location +L+ cannot be used at any arbitrary locations. Adopting this, +L+ NLV can check whether the claimed neighbors are really lo- +L+ cated within geographically two hops away. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.5 Simulations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To further evaluate the performance of our resilient for- +L+ warding scheme, we run simulations of our scheme in the +L+ presence of packet-dropping nodes on a network simulator, +L+ ns-2 [9]. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.5.1 Simulation Model +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In our simulations, we deploy N sensor nodes uniformly at +L+ random within 500 x 500m2 target field, with N = 300 and +L+ 600. Each sensor node has a constant transmission range of +L+ 30m, so that the degree of each node is approximately 10 +L+ (N = 300) and 20 (N = 600) on average. We position a base +L+ station and a source node in opposite corners of the field, at +L+ a fixed point (50, 50) and (450, 450), respectively. They are +L+ located approximately 18 hops away from each other. +L+ We distribute compromised nodes over an inner square +L+ area with 200m each side (from 150m to 350m of each side +L+ of the 500 x 500m2 target area). Thus, compromised nodes +L+ are strategically-placed in between the base station and the +L+ source node. In the simulations, those compromised nodes +L+ drop all the relaying packets. +L+ next-hop id as v, so that x considers it forwarded correctly. +L+ </SectLabel_bodyText> <SectLabel_figure> 1 +L+ 0.9 +L+ 0.8 +L+ 0.7 +L+ 0.6 +L+ 0.5 +L+ 0.4 +L+ 0.3 +L+ 0.2 +L+ 0.1 +L+ 0 +L+ 1	5	10	15	20 +L+ Degree of a node +L+ 66 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6: Simulation Results (averaged over 100 runs). +L+ </SectLabel_figureCaption> <SectLabel_figure> 0	5	10	15	20	25	30	35	40	45	50 +L+ Number of Packet-dropping Nodes +L+ (a) Success ratio (N = 300, x = 0 — 50) +L+ 0	10	20	30	40	50	60	70	80	90	100 +L+ Number of Packet-dropping Nodes +L+ (b) Success ratio (N = 600, x = 0 — 100) +L+ 100 +L+ 100 +L+ 10 +L+ 10 +L+ 0 +L+ 0 +L+ 90 +L+ 90 +L+ ( 300 nodes ) +L+ 80 +L+ 70 +L+ 60 +L+ 50 +L+ 40 +L+ 30 +L+ 20 +L+ 80 +L+ 70 +L+ 60 +L+ 50 +L+ 40 +L+ 30 +L+ 20 +L+ ( 600 nodes ) +L+ 0	5	10	15	20	25	30	35	40	45	50 +L+ Number of Packet-Dropping Nodes +L+ 0	10	20	30	40	50	60	70	80	90	100 +L+ Number of Packet-dropping Nodes +L+ (c) The number of relaying nodes with N = 300 +L+ (d) The number of relaying nodes with N = 600 +L+ ( 300 nodes ) +L+ 0.6 +L+ 0.5 +L+ 0.4 +L+ 1 +L+ 0.9 +L+ 0.8 +L+ 0.7 +L+ 0.3 +L+ 0.2 +L+ 0.1 +L+ 0 +L+ 1 +L+ 0.9 +L+ 0.8 +L+ 0.7 +L+ 0.6 +L+ 0.5 +L+ 0.4 +L+ 0.3 +L+ 0.2 +L+ 0.1 +L+ 0 +L+ ( 600 nodes ) +L+ Single Path Forwarding	with NWS +L+ Single Path Forwarding	with NWS +L+ Single Path Forwarding	with NWS +L+ Single Path Forwarding	with NWS +L+ </SectLabel_figure> <SectLabel_bodyText> We use the typical TinyOS beaconing [17] with a little +L+ modification as a base routing protocol in our simulations. +L+ We add a hop count value in a beacon message9. To have +L+ multiple potential next-hops, when receiving a beacon with +L+ the same or better hop count than the parent node’s, each +L+ node marks the node sending the beacon as a potential next- +L+ hop. +L+ Each simulation experiment is conducted using 100 differ- +L+ ent network topologies, and each result is averaged over 100 +L+ runs of different network topologies. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.5.2 Simulation Results +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In the presence of compromised node dropping all the re- +L+ laying packets, we measure the success ratio (i.e. the per- +L+ centage of the packets that successfully reach the base sta- +L+ tion from the source) and the number of relaying nodes by +L+ the primitive single-path forwarding and with NWS in a +L+ network of size N, with N = 300 and 600. +L+ </SectLabel_bodyText> <SectLabel_footnote> 9The base station initiates the beacon-broadcasting, which +L+ floods through the network, in order to set up a routing tree. +L+ </SectLabel_footnote> <SectLabel_bodyText> Figure 6(a) shows the success ratio in face of x packet- +L+ dropping nodes (varying x=0 to 50) in a 300-sensor-node +L+ network with the approximate degree d = 10. Although +L+ the success ratio gently decreases with x, it keeps up above +L+ 0.8 even with x = 30, with the help of NWS. This ten- +L+ dency of decreasing success ratio can be attributed to the +L+ degree d = 10 (3.6 sub-watch nodes on average) as well as +L+ an increasing number of packet-dropping nodes. Due to the +L+ strategically-placement of compromised nodes in our sim- +L+ ulations, as x increases on, it is likely that a forwarding +L+ node’s all potential sub-watch nodes themselves are packet- +L+ dropping nodes. Figure 6(c) shows the number of nodes +L+ that relay the packet from the source to the base station +L+ in the same experiments. Since the source is located about +L+ 18 hops away from the base station, the number of relaying +L+ nodes only with the single-path forwarding remains at 18. +L+ With NWS, the number of relaying nodes increases with x, +L+ in order to bypass an increasing number of packet-dropping +L+ nodes. In face of such nodes, our scheme converts single- +L+ path forwarding into multipath data forwarding, with the +L+ </SectLabel_bodyText> <SectLabel_page> 67 +L+ </SectLabel_page> <SectLabel_bodyText> help of sub-watch nodes around such packet-dropping nodes. +L+ Utilizing a cache for recently-received packets can suppress +L+ the same copy within a certain timeout, which reduces the +L+ number of relaying nodes. +L+ Figure 6(b) shows the success ratio in a 600-sensor-node +L+ network with the approximate degree d = 20 with x packet- +L+ dropping nodes (varying x=0 to 100). Unlike that with N = +L+ 300, the success ratio stays constantly at around 0.99 even +L+ with x = 100, with the help of NWS. This tendency of high +L+ success ratio can be mainly attributed to the degree d = 20 +L+ (7.6 sub-watch nodes on average in the lower bound case), +L+ which is found to be high enough to bypass a large number +L+ of packet-dropping nodes. Figure 6(d) shows the number +L+ of relaying nodes from the source to the base station in the +L+ same experiments. With NWS, the increase in the number +L+ of relaying nodes with x is more conspicuous than that with +L+ N = 300, since more than twice as many as sub-watch nodes +L+ help forward the packets so that it can bypass a large number +L+ of packet-dropping nodes anyway. +L+ In the simulation results, we note that our forwarding +L+ scheme dynamically adjusts its forwarding style, depending +L+ on the number of packet-dropping nodes en-route to the base +L+ station. As in Figures 6(c) and 6(d), while there exist none +L+ or a small number of packet-dropping nodes on the way, our +L+ scheme works almost like the single-path forwarding with +L+ the help of a few additional relaying nodes. On the other +L+ hand, when confronting a large number of packet-dropping +L+ nodes, our scheme makes full use of the help from additional +L+ relaying nodes, in order to successfully deliver the packet to +L+ the base station at any cost to the best efforts. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. CONCLUSIONS AND FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper we focus on defending against compromised +L+ nodes’ dropping of legitimate reports. We have presented +L+ a resilient packet-forwarding scheme using Neighbor Watch +L+ System (NWS) against maliciously packet-dropping nodes in +L+ sensor networks. In face of such nodes, NWS is specifically +L+ designed for hop-by-hop reliable delivery, and the prompt +L+ reaction of the conversion from single-path to multipath for- +L+ warding augments the robustness in our scheme so that the +L+ packet successfully reach the base station. +L+ In future work, we plan on further improving NLV to de- +L+ fend against the man-in-the-middle attacks, collusion among +L+ compromised nodes. Such attacks can be prevented by using +L+ a master key derived with not only a node ID but also its +L+ geographic information. We will also seek to address O(d 2) +L+ storage requirement for the neighbors’ neighbor lists. Fi- +L+ nally, we would like to perform an intensive experimental +L+ evaluation to compare our scheme with other reliable deliv- +L+ ery protocols [10, 29, 42]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This work was supported by grant No.R01-2006-000-10073- +L+ 0 from the Basic Research Program of the Korea Science and +L+ Engineering Foundation. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] R. Anderson, H. Chan, and A. Perrig, Key Infection: +L+ Smart Trust for Smart Dust, IEEE ICNP 2004 +L+ [2] Burton H. Bloom, Space/Time Trade-offs in Hash +L+ Coding with Allowable Errors, Communication of the +L+ ACM, vol. 13, 422-426, 1970 +L+ [3] B. Carbunar, I. Ioannidis, and C. Nita-Rotaru, +L+ JANUS: Towards Robust and Malicious Resilient +L+ Routing in Hybrid Wireless Networks, ACM workshop +L+ on Wireless security (WiSe’04), Oct. 2004 +L+ [4] H. Chan, A. Perrig, and D. Song, Random Key +L+ Predistribution Schemes for Sensor Networks, IEEE +L+ Symposium on Security and Privacy, pp. 197-213, May +L+ 2003. +L+ [5] B. Deb, S. Bhatnagar, and B. Nath, ReInForM: +L+ Reliable Information Forwarding Using Multiple Paths +L+ in Sensor Networks, IEEE Local Computer Networks +L+ (LCN 2003), pp. 406-415, Oct. 2003. +L+ [6] J. Deng, R. Han, and S. Mishra, A Performance +L+ Evaluation of Intrusion- Tolerant Routing in Wireless +L+ Sensor Networks, 2nd International Workshop on +L+ Information Processing in Sensor Networks (IPSN 03), +L+ pp. 349-364, Apr. 2003. +L+ [7] J. Deng, R. Han, and S. Mishra, Intrusion Tolerance +L+ and Anti-Traffic Analysis Strategies for Wireless +L+ Sensor Networks, IEEE International Conference on +L+ Dependable Systems and Networks (DSN), pp. +L+ 594-603, 2004. +L+ [8] J. Deng, R. Han, and S. Mishra, Defending against +L+ Path-based DoS Attacks in Wireless Sensor Networks, +L+ ACM Workshop on Security of Ad-Hoc and Sensor +L+ Networks (SASN’05) , Nov, 2005. +L+ [9] K. Fall and K. Varadhan (editors), NS notes and +L+ documentation, The VINT project, LBL, Feb 2000, +L+ http://www.isi.edu/nsnam/ns/ +L+ [10] D. Ganesan, R. Govindan, S. Shenker, and D. Estrin, +L+ Highly Resilient, Energy-Efficient Multipath Routing +L+ in Wireless Sensor Networks, Computing and +L+ Communications Review (MC2R) Vol 1., pp. 11-25, +L+ 2002. +L+ [11] V. D. Gligor, Security of Emergent Properties in +L+ Ad-Hoc Networks, International Workshop on Security +L+ Protocols, Apr. 2004. +L+ [12] O. Goldreich, S. Goldwasser, and S. Micali, How to +L+ Construct Random Functions, Journal of the ACM, +L+ Vol. 33, No. 4, 210-217, 1986 +L+ [13] L. Eschenauer and V. D. Gligor, A Key-Management +L+ Scheme for Distributed Sensor Networks, 9th ACM +L+ Conference on Computer and Communication +L+ Security (CCS), pp. 41-47, Nov. 2002. +L+ [14] C. Hartung, J. Balasalle, and R. Han, Node +L+ Compromise in Sensor Networks: The Need for Secure +L+ Systems, Technical Report CU-CS-990-05, +L+ Department of Computer Science University of +L+ Colorado at Boulder, Jan. 2005 +L+ [15] T. He, S. Krishnamurthy, J. A. Stankovic, T. F. +L+ Abdelzaher, L. Luo, R. Stoleru, T. Yan, L. Gu, J. Hui, +L+ and B. Krogh, An Energy-Efficient Surveillance +L+ System Using Wireless Sensor Networks, ACM +L+ MobiSys’04, June, 2004 +L+ [16] W.R. Heinzelman, J. Kulik, H. Balakrishnan, Adaptive +L+ Protocols for Information Dissemination in Wireless +L+ Sensor Networks, ACM MobiCom99, pp. 174.185, +L+ 1999. +L+ [17] J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. Culler, and +L+ K. Pister, System Architecture Directions for +L+ Networked Sensors, ACU ASPLOS IX, November +L+ 2000. +L+ </SectLabel_reference> <SectLabel_page> 68 +L+ </SectLabel_page> <SectLabel_reference> [18] X. Hong, M. Gerla, W. Hanbiao, and L. Clare, Load +L+ Balanced, Energy-Aware Communications for Mars +L+ Sensor Networks, IEEE Aerospace Conference, vol.3, +L+ 1109-1115, 2002. +L+ [19] Y.-C. Hu, D. B. Johnson, and A. Perrig, SEAD: +L+ Secure Efficient Distance Vector Routing for Mobile +L+ Wireless Ad Hoc Networks, IEEE Workshop on Mobile +L+ Computing Systems and Applications, pp. 3-13, Jun. +L+ 2002. +L+ [20] Y.-C. Hu, A. Perrig, and D. B. Johnson, Efficient +L+ Security Mechanisms for Routing Protocols, NDSS +L+ 2003, pp. 57-73, Feb. 2003. +L+ [21] C. Intanagonwiwat, R. Govindan and D. Estrin, +L+ Directed Diffusion: A Scalable and Robust +L+ Communication Paradigm for Sensor Networks, +L+ MobiCom’00, Aug. 2000. +L+ [22] D. Johnson, D.A. Maltz, and J. Broch, The Dynamic +L+ Source Routing Protocol for Mobile Ad Hoc Networks +L+ (Internet-Draft), Mobile Ad-hoc Network (MANET) +L+ Working Group, IETF, Oct. 1999. +L+ [23] C. Karlof and D. Wagner, Secure Routing in Wireless +L+ Sensor Networks: Attacks and Countermeasures, The +L+ First IEEE International Workshop on Sensor Network +L+ Protocols and Applications, pp. 113-127, May 2003 +L+ [24] C. Karlof, N. Sastry, and D. Wagner, TinySec: A Link +L+ Layer Security Architecture for Wireless Sensor +L+ Networks, ACM SensSys’04, pp. 162-175, Nov. 2004. +L+ [25] I. Khalil, S. Bagchi, and C. Nina-Rotaru, DICAS: +L+ Detection, Diagnosis and Isolation of Control Attacks +L+ in Sensor Networks, IEEE SecureComm 2005, pp. 89 - +L+ 100, Sep. 2005 +L+ [26] Y. Liu and W. K.G. Seah, A Priority-Based +L+ Multi-Path Routing Protocol for Sensor Networks, +L+ 15th IEEE International Symposium on Volume 1, 216 +L+ -220, 2004 +L+ [27] S.-B. Lee and Y.-H. Choi, A Secure Alternate Path +L+ Routing in Sensor Networks, Computer +L+ Communications (2006), +L+ doi:10.1016 /j . comcom.2006.08.006. +L+ [28] S. Marti, T.J. Giuli, K. Lai, and M. Baker, Mitigating +L+ Routing Misbehavior in Mobile Ad Hoc Networks, +L+ ACM/IEEE International Conference on Mobile +L+ Computing and Networking, pp. 255-265, 2000 +L+ [29] H. Morcos, I. Matta, and A. Bestavros, M2 RC: +L+ Multiplicative-Increase/Additive-Decrease Multipath +L+ Routing Control for Wireless Sensor Networks, ACM +L+ SIGBED Review, Vol. 2, Jan 2005. +L+ [30] J. Newsome, E. Shi, D. Song, and A. Perrig, The Sybil +L+ Attack in Sensor Networks: Analysis and Defenses, +L+ IEEE IPSN’04, pp. 259-268, Apr. 2004. +L+ [31] B. Parno, A. Perrig, and V. D. Gligor, Distributed +L+ Detection of Node Replication Attacks in Sensor +L+ Networks, the 2005 IEEE Symposium on Security and +L+ Privacy, pp. 49-63, May 2005. +L+ [32] A. Perrig, R. Szewczyk, V. Wen, D. Culler, and +L+ J. Tygar, SPINS: Security Protocols for Sensor +L+ Networks, ACM MobiCom’01, pp. 189-199, 2001. +L+ [33] A. Perrig, J. Stankovic, and D. Wagner, Security in +L+ Wireless Sensor Networks, Communications of the +L+ ACM, 47(6), Special Issue on Wireless sensor +L+ networks, pp.53- 57, Jun. 2004 +L+ [34] B. Przydatek, D. Song, and A. Perrig, SIA: Secure +L+ Information Aggregation in Sensor Networks, 1st +L+ International Conference on Embedded Networked +L+ Sensor Systems, 255-256, 2003 +L+ [35] E. Shi and A. Perrig, Designing Secure Sensor +L+ Networks, Wireless Communications, IEEE Volume +L+ 11, Issue 6, pp. 38-43, Dec. 2004. +L+ [36] D. Tian and N.D. Georganas, Energy Efficient +L+ Routing with Guaranteed Delivery in Wireless Sensor +L+ Networks, IEEE Wireless Communications and +L+ Networking (WCNC 2003), IEEE Volume 3, 1923 - +L+ 1929, March 2003 +L+ [37] A. Woo, T. Tong, and D. Culler, Taming the +L+ Underlying Challenges of Reliable Multhop Routing in +L+ Sensor Networks, ACM SenSys03, Nov, 2003 +L+ [38] A. Wood and J. Stankovic, Denial of Service in Sensor +L+ Networks, IEEE Computer, Vol.35, 54-62, Oct. 2002 +L+ [39] H.Yang, F. Ye, Y. Yuan, S. Lu and W. Arbough, +L+ Toward Resilient Security in Wireless Sensor +L+ Networks, ACM MobiHoc’05, 34-45, May 2005 +L+ [40] Y. Yang, X. Wang, S. Zhu, and G. Cao SDAP: A +L+ Secure Hop-by-Hop Data Aggregation Protocol for +L+ Sensor Networks, ACM MobiHoc’06 May 2006 +L+ [41] F. Ye, H. Luo, S. Lu and L. Zhang, Statictial En-route +L+ Filtering of Injected False Data in Sensor Networks, +L+ IEEE INFOCOM, 2004 +L+ [42] F. Ye, G. Zhong, S. Lu and L. Zhang, GRAdient +L+ Broadcast: A Robust Data Delivery Protocol for Large +L+ Scale Sensor Networks, ACM Wireless Networks +L+ (WINET), March 2005 +L+ [43] Y. Yu, R. Govindan, and D. Estrin, Geographical and +L+ Energy Aware Routing: a recursive data dissemination +L+ protocol for wireless sensor networks, UCLA +L+ Computer Science Department Technical Report +L+ UCLA/CSD-TR-01-0023, May 2001. +L+ [44] W. Zhang and G. Cao, Group Rekeying for Filtering +L+ False Data in Sensor Networks: A Predistribution and +L+ Local Collaboration-Based Approach, IEEE +L+ INFOCOM’05. Vol. 1, 503-514, March 2005 +L+ [45] G. Zhou, T. He, S. Krishnamurthy, and J. A. +L+ Stankovic, Impact of radio irregularity on wireless +L+ sensor networks, the 2nd International Conference on +L+ Mobile Systems, Applications, and Services +L+ (MobiSys04), June, 2004 +L+ [46] S. Zhu, S. Setia, and S. Jajodia, LEAP: Efficient +L+ Security Mechanisms for Large-Scale Distributed +L+ Sensor Networks, The 10th ACM Conference on +L+ Computer and Communications Security (CCS ’03), +L+ 62-72, 2003 +L+ [47] S.Zhu, S. Setia, S. Jajodia, and P. Ning, An +L+ Interleaved Hop-by-Hop Authentication Scheme for +L+ Filtering False Data in Sensor Networks, IEEE +L+ Symposium on Security and Privacy, 2004 +L+ </SectLabel_reference> <SectLabel_page> 69 +L+ </SectLabel_page>
<SectLabel_title> A Similarity Measure for Motion Stream +L+ Segmentation and Recognition* +L+ </SectLabel_title> <SectLabel_author> Chuanjun Li	B. Prabhakaran +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ The University of Texas at Dallas, Richardson, TX 75083 +L+ </SectLabel_affiliation> <SectLabel_email> {chuanjun, praba}@utdallas.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Recognition of motion streams such as data streams gener- +L+ ated by different sign languages or various captured human +L+ body motions requires a high performance similarity mea- +L+ sure. The motion streams have multiple attributes, and mo- +L+ tion patterns in the streams can have different lengths from +L+ those of isolated motion patterns and different attributes +L+ can have different temporal shifts and variations. To ad- +L+ dress these issues, this paper proposes a similarity measure +L+ based on singular value decomposition (SVD) of motion ma- +L+ trices. Eigenvector differences weighed by the corresponding +L+ eigenvalues are considered for the proposed similarity mea- +L+ sure. Experiments with general hand gestures and human +L+ motion streams show that the proposed similarity measure +L+ gives good performance for recognizing motion patterns in +L+ the motion streams in real time. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors: H.2.8 [Database +L+ Management]: Database Applications – Data Mining +L+ General Terms: Algorithm +L+ Keywords: Pattern recognition, gesture, data streams, seg- +L+ mentation, singular value decomposition. +L+ 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Motion streams can be generated by continuously per- +L+ formed sign language words [14] or captured human body +L+ motions such as various dances. Captured human motions +L+ can be applied to the movie and computer game industries +L+ by reconstructing various motions from video sequences [10] +L+ or images [15] or from motions captured by motion capture +L+ systems [4]. Recognizing motion patterns in the streams +L+ with unsupervised methods requires no training process, and +L+ is very convenient when new motions are expected to be +L+ added to the known pattern pools. A similarity measure +L+ with good performance is thus necessary for segmenting and +L+ recognizing the motion streams. Such a similarity measure +L+ needs to address some new challenges posed by real world +L+ </SectLabel_bodyText> <SectLabel_footnote> *Work supported partially by the National Science Founda- +L+ tion under Grant No. 0237954 for the project CAREER: +L+ Animation Databases. +L+ </SectLabel_footnote> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> motion streams: first, the motion patterns have dozens of at- +L+ tributes, and similar patterns can have different lengths due +L+ to different motion durations; second, different attributes of +L+ similar motions have different variations and different tem- +L+ poral shifts due to motion variations; and finally, motion +L+ streams are continuous, and there are no obvious ”pauses” +L+ between neighboring motions in a stream. A good similarity +L+ measure not only needs to capture the similarity of complete +L+ motion patterns, but also needs to capture the differences +L+ between complete motion patterns and incomplete motion +L+ patterns or sub-patterns in order to segment a stream for +L+ motion recognition. +L+ As the main contribution of this paper, we propose a sim- +L+ ilarity measure to address the above issues. The proposed +L+ similarity measure is defined based on singular value decom- +L+ position of the motion matrices. The first few eigenvectors +L+ are compared for capturing the similarity of two matrices, +L+ and the inner products of the eigenvectors are given differ- +L+ ent weights for their different contributions. We propose to +L+ use only the eigenvalues corresponding to the involved eigen- +L+ vectors of the two motion matrices as weights. This simple +L+ and intuitive weighing strategy gives the same importance to +L+ eigenvalues of the two matrices. We also show that the 95% +L+ variance rule for choosing the number of eigenvectors [13] is +L+ not sufficient for recognizing both isolated patterns and mo- +L+ tion streams. Our experiments demonstrate that at least the +L+ first 6 eigenvectors need to be considered for motion streams +L+ of either 22 attribute or 54 attributes, and the first 6 eigen- +L+ values accounts for more than 99.5% of the total variance in +L+ the motion matrices. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Multi-attribute pattern similarity search, especially in con- +L+ tinuous motion streams, has been widely studied for sign +L+ language recognition and for motion synthesis in computer +L+ animation. The recognition methods usually include tem- +L+ plate matching by distance measures and hidden Markov +L+ models (HMM). +L+ Template matching by using similarity/distance measures +L+ has been employed for multi-attribute pattern recognition. +L+ Joint angles are extracted in [11] as features to represent dif- +L+ ferent human body static poses for the Mahalanobis distance +L+ measure of two joint angle features. Similarly, momentum, +L+ kinetic energy and force are constructed in [2,5] as activ- +L+ ity measure and prediction of gesture boundaries for various +L+ segments of the human body, and the Mahalanobis distance +L+ function of two composite features are solved by dynamic +L+ programming. +L+ </SectLabel_bodyText> <SectLabel_page> 89 +L+ </SectLabel_page> <SectLabel_bodyText> Similarity measures are defined for multi-attribute data +L+ in [6,12,16] based on principal component analysis (PCA). +L+ Inner products or angular differences of principal compo- +L+ nents (PCs) are considered for similarity measure defini- +L+ tions, with different weighted strategies for different PCs. +L+ Equal weights are considered for different combinations of +L+ PCs in [6], giving different PCs equal contributions to the +L+ similarity measure. The similarity measure in [12] takes the +L+ minimum of two weighted sums of PC inner products, and +L+ the two sums are respectively weighted by different weights. +L+ A global weight vector is obtained by taking into account all +L+ available isolated motion patterns in [16], and this weight +L+ vector is used for specifying different contributions from dif- +L+ ferent PC inner products to the similarity measure Eros. +L+ The dominating first PC and a normalized eigenvalue vector +L+ are considered in [7,8] for pattern recognition. In contrast, +L+ this paper propose to consider the first few PCs, and the +L+ angular differences or inner products of different PCs are +L+ weighted by different weights which depends on the data +L+ variances along the corresponding PCs. +L+ The HMM technique has been widely used for sign lan- +L+ guage recognition, and different recognition rates have been +L+ reported for different sign languages and different feature se- +L+ lection approaches. Starner et al. [14] achieved 92% and 98% +L+ word accuracy respectively for two systems, the first of the +L+ systems used a camera mounted on a desk and the second +L+ one used a camera in a user’s cap for extracting features +L+ as the input of HMM. Similarly Liang and Ouhyoung [9] +L+ used HMM for postures, orientations and motion primitives +L+ as features extracted from continuous Taiwan sign language +L+ streams and an average 80.4% recognition rate was achieved. +L+ In contrast, the approach proposed in this paper is an un- +L+ supervised approach, and no training as required for HMM +L+ recognizers is needed. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. SIMILARITY MEASURE FOR MOTION +L+ STREAM RECOGNITION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The joint positional coordinates or joint angular values of +L+ a subject in motion can be represented by a matrix: the +L+ columns or attributes of the matrix are for different joints, +L+ and the rows or frames of the matrix are for different time +L+ instants. Similarity of two motions is the similarity of the +L+ resulting motion matrices, which have the same number of +L+ attributes or columns, and yet can have different number +L+ of rows due to different motion durations. To capture the +L+ similarity of two matrices of different lengths, we propose +L+ to apply singular value decomposition (SVD) to the motion +L+ matrices in order to capture the similarity of the matrix +L+ geometric structures. Hence we briefly present SVD and its +L+ associated properties below before proposing the similarity +L+ measure based on SVD in this section. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Singular Value Decomposition +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The geometric structure of a matrix can be revealed by +L+ the SVD of the matrix. As shown in [3], any real m x n +L+ matrix A can be decomposed into A = UEVT , where U = +L+ [u1, u2, ... , um] E Rm×m and V = [v1, v2, ... , vn] E RnXn +L+ are two orthogonal matrices, and E is a diagonal matrix with +L+ diagonal entries being the singular values of A: v1 > v2 > +L+ . . . > Qmin(m,n) > 0. Column vectors ui and vi are the ith +L+ left and right singular vectors of A, respectively. +L+ It can be shown that the right singular vectors of the sym- +L+ metric n x n matrix M = AT A are identical to the corre- +L+ sponding right singular vectors of A, referred to as eigenvec- +L+ tors of M. The singular values of M, or eigenvalues of M, +L+ are squares of the corresponding singular values of A. The +L+ eigenvector with the largest eigenvalue gives the first prin- +L+ cipal component. The eigenvector with the second largest +L+ eigenvalue is the second principal component and so on. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Similarity Measure +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Since SVD exposes the geometric structure of a matrix, it +L+ can be used for capturing the similarity of two matrices. We +L+ can compute the SVD of M = AT A instead of computing +L+ the SVD of A to save computational time. The reasons are +L+ that the eigenvectors of M are identical to the corresponding +L+ right singular vectors of A, the eigenvalues of M are the +L+ squares of the corresponding singular values of A, and SVD +L+ takes O(n 3) time for the n x n M and takes O(mn2) time +L+ with a large constant for the m x n A, and usually m > n. +L+ Ideally, if two motions are similar, their corresponding +L+ eigenvectors should be parallel to each other, and their cor- +L+ responding eigenvalues should also be proportional to each +L+ other. This is because the eigenvectors are the correspond- +L+ ing principal components, and the eigenvalues reflect the +L+ variances of the matrix data along the corresponding prin- +L+ cipal components. But due to motion variations, all corre- +L+ sponding eigenvectors cannot be parallel as shown in Fig- +L+ ure 1. The parallelness or angular differences of two eigen- +L+ vectors u and v can be described by the absolute value of +L+ their inner products: l cosOl = lu • vl/(lullvl) = lu • vl, where +L+ lul = lvl = 1. We consider the absolute value of the in- +L+ ner products because eigenvectors can have different signs +L+ as shown in [8]. +L+ Since eigenvalues are numerically related to the variances +L+ of the matrix data along the associated eigenvectors, the im- +L+ portance of the eigenvector parallelness can be described by +L+ the corresponding eigenvalues. Hence, eigenvalues are to be +L+ used to give different weights to different eigenvector pairs. +L+ Figure 2 shows that the first eigenvalues are the dominat- +L+ ing components of all the eigenvalues, and other eigenval- +L+ ues become smaller and smaller and approach zero. As the +L+ eigenvalues are close to zero, their corresponding eigenvec- +L+ tors can be very different even if two matrices are similar. +L+ Hence not all the eigenvectors need to be incorporated into +L+ the similarity measure. +L+ Since two matrices have two eigenvalues for the corre- +L+ sponding eigenvector pair, these two eigenvalues should have +L+ equal contributions or weights to the eigenvector parallel- +L+ ness. In addition, the similarity measure of two matrices +L+ should be independent to other matrices, hence only eigen- +L+ vectors and eigenvalues of the two matrices should be con- +L+ sidered. +L+ Based on the above discussions, we propose the following +L+ similarity measure for two matrices Q and P: +L+ </SectLabel_bodyText> <SectLabel_equation> k +L+ 1 +L+   (Q, P) = +L+ 2 i=1 +L+ </SectLabel_equation> <SectLabel_bodyText> where vi and Ai are the ith eigenvalues corresponding to the +L+ ith eigenvectors ui and vi of square matrices of Q and P, +L+ respectively, and 1 < k < n. Integer k determines how many +L+ eigenvectors are considered and it depends on the number +L+ of attributes n of motion matrices. Experiments with hand +L+ gesture motions (n = 22) and human body motions (n = +L+ </SectLabel_bodyText> <SectLabel_equation> n +L+ ((�i/ +L+ i=1 +L+ n +L+ Qi+Ai/ +L+ i=1 +L+ Ai)lui •vil) +L+ </SectLabel_equation> <SectLabel_page> 90 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: Eigenvectors of similar patterns. The first +L+ eigenvectors are similar to each other, while other +L+ eigenvectors, such as the second vectors shown in +L+ the bottom, can be quite different. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> 54) in Section 4 show that k = 6 is large enough without +L+ loss of pattern recognition accuracy in streams. We refer to +L+ this non-metric similarity measure as k Weighted Angular +L+ Similarity (kWAS) , which captures the angular similarities +L+ of the first k corresponding eigenvector pairs weighted by +L+ the corresponding eigenvalues. +L+ It can be easily verified that the value of kWAS ranges over +L+ [0,1]. When all corresponding eigenvectors are normal to +L+ each other, the similarity measure will be zero, and when two +L+ matrices are identical, the similarity measure approaches the +L+ maximum value one if k approaches n. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Stream Segmentation Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In order to recognize motion streams, we assume one mo- +L+ tion in a stream has a minimum length l and a maximum +L+ length L. The following steps can be applied to incremen- +L+ tally segment a stream for motion recognition: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. SVD is applied to all isolated motion patterns P to +L+ obtain their eigenvectors and eigenvalues. Let S be +L+ the incremented stream length for segmentation, and +L+ let L be the location for segmentation. Initially L = l. +L+ 2. Starting from the beginning of the stream or the end of +L+ the previously recognized motion, segment the stream +L+ at location L. Compute the eigenvectors and eigenval- +L+ ues of the motion segment Q. +L+ 3. Compute kWAS between Q and all motion patterns +L+ </SectLabel_listItem> <SectLabel_bodyText> P. Update T... to be the highest similarity after the +L+ previous motion’s recognition. +L+ </SectLabel_bodyText> <SectLabel_listItem> 4. If L+S < L, update L = L+S and go to step 2. Other- +L+ </SectLabel_listItem> <SectLabel_bodyText> wise, the segment corresponding to T,r,.. is recognized +L+ to be the motion pattern which gives the highest simi- +L+ larity T..., update L = l starting from the end of the +L+ last recognized motion pattern and go to step 2. +L+ </SectLabel_bodyText> <SectLabel_figure> 100 +L+ 99 +L+ 95 +L+ 93 +L+ 91 +L+ 89 +L+ 87 +L+ 1	2	3	4	5	6	7	8 +L+ Number of Eigenvalues +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2: Accumulated eigenvalue percentages in +L+ total eigenvalues for CyberGlove data and captured +L+ human body motion data. There are 22 eigenvalues +L+ for the CyberGlove data and 54 eigenvalues for the +L+ captured motion data. The sum of the first 2 eigen- +L+ values is more than 95% of the corresponding total +L+ eigenvalues, and the sum of the first 6 eigenvalues is +L+ almost 100% of the total eigenvalues. +L+ </SectLabel_figureCaption> <SectLabel_sectionHeader> 4. PERFORMANCE EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This section evaluates experimentally the performances +L+ of the similarity measure kWAS proposed in this paper. It +L+ has been shown in [16] that Eros [16] outperforms other +L+ similarity measures mentioned in Section 2 except MAS [8]. +L+ Hence in this section, we compare the performances of the +L+ proposed kWAS with Eros and MAS for recognizing similar +L+ isolated motion patterns and for segmenting and recognizing +L+ motion streams from hand gesture capturing CyberGlove +L+ and human body motion capture system. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Data Generation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A similarity measure should be able to be used not only +L+ for recognizing isolated patterns with high accuracy, but also +L+ for recognizing patterns in continuous motions or motion +L+ streams. Recognizing motion streams is more challenging +L+ than recognizing isolated patterns. This is because many +L+ very similar motion segments or sub-patterns needs to be +L+ compared in order to find appropriate segmentation loca- +L+ tions, and a similarity measure should capture the difference +L+ between a complete motion or pattern and its sub-patterns. +L+ Hence, both isolated motion patterns and motion streams +L+ were generated for evaluating the performance of kWAS. +L+ Two data sources are considered for data generation: a Cy- +L+ berGlove for capturing hand gestures and a Vicon motion +L+ capture system for capturing human body motions. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.1.1 CyberGlove Data +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> A CyberGlove is a fully instrumented data glove that pro- +L+ vides 22 sensors for measuring hand joint angular values to +L+ capture motions of a hand, such as American Sign Language +L+ (ASL) words for hearing impaired. The data for a hand ges- +L+ ture contain 22 angular values for each time instant/frame, +L+ one value for a joint of one degree of freedom. The mo- +L+ tion data are extracted at around 120 frames per second. +L+ Data matrices thus have 22 attributes for the CyberGlove +L+ motions. +L+ One hundred and ten different isolated motions were gen- +L+ erated as motion patterns, and each motion was repeated +L+ for three times, resulting in 330 isolated hand gesture mo- +L+ tions. Some motions have semantic meanings. For example, +L+ </SectLabel_bodyText> <SectLabel_figure> 2	4	6	8	10	12	14	16	18	20	22 +L+ Motion341 +L+ Motion342 +L+ Motion341 +L+ Motion342 +L+ 2	4	6	8	10	12	14	16	18	20	22 +L+ Component of Second Eigenvector +L+ 	0.6 0.4 0.2 0 −0.2 −0.4 +L+ 0.2 +L+ 0.1 +L+ 0 +L+ −0.1 +L+ −0.2 +L+ −0.3 +L+ −0.4 +L+ −0.5 +L+ −0.6 +L+ −0.7 +L+ Component of First Eigenvector +L+ 97 +L+ 85 +L+ CyberGlove Data +L+ MoCap Data +L+ </SectLabel_figure> <SectLabel_page> 91 +L+ </SectLabel_page> <SectLabel_bodyText> the motion for BUS as shown in Table 1 is for the ASL sign +L+ ”bus”. Yet for segmentation and recognition, we only re- +L+ quire that each individual motion be different from others, +L+ and thus some motions are general motions, and do not have +L+ any particular semantic meanings, such as the THUMBUP +L+ motion in Table 1. +L+ The following 18 motions shown in Table 1 were used to +L+ generate continuous motions or streams. Twenty four dif- +L+ ferent motion streams were generated for segmentation and +L+ recognition purpose. There are 5 to 10 motions in a stream +L+ and 150 motions in total in 24 streams, with 6.25 motions in +L+ a stream on average. It should be noted that variable-length +L+ transitional noises occur between successive motions in the +L+ generated streams. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 1: Individual motions used for streams +L+ </SectLabel_tableCaption> <SectLabel_table> 35 60 70 80 90 BUS GOODBYE +L+ HALF IDIOM JAR JUICE KENNEL KNEE +L+ MILK TV SCISSOR SPREAD THUMBUP +L+ </SectLabel_table> <SectLabel_subsubsectionHeader> 4.1.2 Motion Capture Data +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The motion capture data come from various motions cap- +L+ tured collectively by using 16 Vicon cameras and the Vicon +L+ iQ Workstation software. A dancer wears a suit of non- +L+ reflective material and 44 markers are attached to the body +L+ suit. After system calibration and subject calibration, global +L+ coordinates and rotation angles of 19 joints/segments can +L+ be obtained at about 120 frames per second for any mo- +L+ tion. Similarity of patterns with global 3D positional data +L+ can be disguised by different locations, orientations or differ- +L+ ent paths of motion execution as illustrated in Figure 3(a). +L+ Since two patterns are similar to each other because of sim- +L+ ilar relative positions of corresponding body segments at +L+ corresponding time, and the relative positions of different +L+ segments are independent of locations or orientations of the +L+ body, we can transform the global position data into local +L+ position data as follows. +L+ Let Xp, Yp, Zp be the global coordinates of one point on +L+ pelvis, the selected origin of the ”moving” local coordinate +L+ system, and a,,3, -y be the rotation angles of the pelvis seg- +L+ ment relative to the global coordinate system axes, respec- +L+ tively. The translation matrix is T as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> 1	0	0	0 +L+ 0	1	0	0 +L+ 0	0	1	0 +L+ —Xp	—Yp	—Zp	1 +L+ The rotation matrix R = R. x Ry x Rz, where +L+ 1	0	0	0 +L+ 0 cos a — sin a 0 +L+ 0 sin a cos a 0 +L+ 0 0 0 1 +L+ cos,3 0 sin,3 0 +L+ 0	1	0	0 +L+ —sin,3 0 cos,3 0 +L+ 0	0	0	1 +L+ </SectLabel_equation> <SectLabel_figure> Motion Capture Frames	Motion Capture Frames +L+ (a)	(b) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: 3D motion capture data for similar motions +L+ executed at different locations and in different orien- +L+ tations: (a) before transformation; (b) after transfor- +L+ mation. +L+ </SectLabel_figureCaption> <SectLabel_equation> cos-y —sin -y 0 0 +L+ sin -y cos -y 0 0 +L+ 0 0 1 0 +L+ 0 0 0 1 +L+ </SectLabel_equation> <SectLabel_bodyText> Let X, Y, Z be the global coordinates of one point on any +L+ segments, and x, y, z be the corresponding transformed local +L+ coordinates. x, y and z can be computed as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> [x y z 1]=[X Y Z 1] x T x R +L+ </SectLabel_equation> <SectLabel_bodyText> The transformed data are positions of different segments +L+ relative to a moving coordinate system with the origin at +L+ some fixed point of the body, for example the pelvis. The +L+ moving coordinate system is not necessarily aligned with +L+ the global system, and it can rotate with the body. So data +L+ transformation includes both translation and rotation, and +L+ the transformed data would be translation and rotation in- +L+ variant as shown in Figure 3(b). The coordinates of the +L+ origin pelvis are not included, thus the transformed matri- +L+ ces have 54 columns. +L+ Sixty two isolated motions including Taiqi, Indian dances, +L+ and western dances were performed for generating motion +L+ capture data, and each motion was repeated 5 times, yield- +L+ ing 310 isolated human motions. Every repeated motion has +L+ a different location and different durations, and can face +L+ different orientations. Twenty three motion streams were +L+ generated for segmentation. There are 3 to 5 motions in +L+ a stream, and 93 motions in total in 23 streams, with 4.0 +L+ motions in a stream on average. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Performance of kWAS for Capturing Sim- +L+ ilarities and Segmenting Streams +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We first apply kWAS to isolated motion patterns to show +L+ that the proposed similarity measure kWAS can capture the +L+ similarities of isolated motion patterns. Then kWAS is ap- +L+ plied to motion streams for segmenting streams and recog- +L+ nizing motion patterns in the streams. We experimented +L+ with different k values in order to find out the smallest k +L+ without loss of good performance. +L+ Figure 2 shows the accumulated eigenvalue percentages +L+ averaged on 330 hand gestures and 310 human motions, re- +L+ spectively. Although the first two eigenvalues account for +L+ </SectLabel_bodyText> <SectLabel_figure> 1500 +L+ 1000 +L+ 500 +L+ 0 +L+ −500 +L+ −1000 +L+ −1500 +L+ 0	50	100	150	200	250	300	350	400	450 +L+ −1000 +L+ 0	50	100	150	200	250	300	350	400	450 +L+ 1000 +L+ 500 +L+ 0 +L+ −500 +L+ 2000 +L+ 1500 +L+ 1000 +L+ 500 +L+ 0 +L+ 0	50	100	150	200	250	300	350	400	450 +L+ −1000 +L+ 0	50	100	150	200	250	300	350	400	450 +L+ 1000 +L+ 500 +L+ 0 +L+ −500 +L+ T= +L+ R, = +L+ Ry = +L+ Rz = +L+ 92 +L+ Number of Nearest Neghbors (Most Simlar Patterns) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: Recognition rate of similar CyberGlove +L+ motion patterns. When k is 3, kWAS can find the +L+ most similar motions for about 99.7% of 330 mo- +L+ tions, and can find the second most similar motions +L+ for 97.5% of the them. +L+ </SectLabel_figureCaption> <SectLabel_figure> Number of Nearest Neighbors (Most aimilar Patterns1 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: Recognition rate of similar captured mo- +L+ tion patterns. When k is 5, by using kWAS, the most +L+ similar motions of all 310 motions can be found, and +L+ the second most similar motions of 99.8% of the 310 +L+ motions can also be found. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> more than 95% of the respective sums of all eigenvalues, +L+ considering only the first two eigenvectors for kWAS is not +L+ sufficient as shown in Figure 4 and Figure 5. For Cyber- +L+ Glove data with 22 attributes, kWAS with k = 3 gives the +L+ same performance as kWAS with k = 22, and for motion +L+ capture data with 54 attributes, kWAS with k = 5 gives the +L+ same performance as kWAS with k = 54. Figure 4 and Fig- +L+ ure 5 illustrate that kWAS can be used for finding similar +L+ motion patterns and outperforms MAS and Eros for both +L+ hand gesture and human body motion data. +L+ The steps in Section 3.3 are used for segmenting streams +L+ and recognizing motions in streams. The recognition accu- +L+ racy as defined in [14] is used for motion stream recognition. +L+ The motion recognition accuracies are shown in Table 2. For +L+ both CyberGlove motion and captured motion data, k = 6 +L+ is used for kWAS, which gives the same accuracy as k = 22 +L+ for CyberGlove data and k = 54 for motion capture data, +L+ respectively. +L+ Figure 6 shows the time taken for updating the candi- +L+ date segment, including updating the matrix, computing the +L+ SVD of the updated matrix, and computing the similarities +L+ of the segment and all motion patterns. The code imple- +L+ mented in C++ was run on one 2.70 GHz Intel processor +L+ of a GenuineIntel Linux box. There are 22 attributes for +L+ the CyberGlove streams, and 54 attributes for the captured +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 6: Computation time for stream segment up- +L+ date and similarity computation. +L+ </SectLabel_figureCaption> <SectLabel_tableCaption> Table 2: Stream Pattern Recognition Accuracy (%) +L+ </SectLabel_tableCaption> <SectLabel_table> Similarity Measures	CyberGlove	Motion Capture +L+ 	Streams	Streams +L+ Eros	68.7	78.5 +L+ MAS	93.3	78.5 +L+ kWAS (k=6)	94.0	94.6 +L+ </SectLabel_table> <SectLabel_bodyText> motion streams. Hence updating captured motion segments +L+ takes longer than updating CyberGlove motion segments as +L+ shown in Figure 6. The time required by kWAS is close to +L+ the time required by MAS, and is less than half of the time +L+ taken by using Eros. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Discussions +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> kWAS captures the similarity of square matrices of two +L+ matrices P and Q, yet the temporal order of pattern execu- +L+ tion is not revealed in the square matrices. As shown in [7], +L+ two matrices with the identical row vectors in different or- +L+ ders have identical eigenvectors and identical eigenvalues. If +L+ different temporal orders of pattern execution yield patterns +L+ with different semantic meanings, we need to further con- +L+ sider the temporal execution order, which is not reflected in +L+ the eigenvectors and eigenvalues and has not been consid- +L+ ered previously in [6,12,16]. +L+ Since the first eigenvectors are close or parallel for similar +L+ patterns, we can project pattern A onto its first eigenvector +L+ ul by Aul. Then similar patterns would have similar projec- +L+ tions (called projection vectors hereafter), showing similar +L+ temporal execution orders while the projection variations +L+ for each pattern can be maximized. The pattern projection +L+ vectors can be compared by computing their dynamic time +L+ warping (DTW) distances, for DTW can align sequences +L+ of different lengths and can be solved easily by dynamic +L+ programming [1]. Incorporating temporal order information +L+ into the similarity measure can be done as for MAS in [7] +L+ if motion temporal execution orders cause motion pattern +L+ ambiguity to kWAS. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper has proposed a similarity measure kWAS for +L+ motion stream segmentation and motion pattern recogni- +L+ tion. kWAS considers the first few k eigenvectors and com- +L+ putes their angular similarities/differences, and weighs con- +L+ tributions of different eigenvector pairs by their correspond- +L+ </SectLabel_bodyText> <SectLabel_figure> 100 +L+ 99 +L+ 98 +L+ 97 +L+ 96 +L+ 95 +L+ 94 +L+ 93 +L+ 92 +L+ 91 +L+ 90 +L+ 1	2 +L+ kWAS (k = 22) +L+ kWAS (k = 5) +L+ kWAS (k = 3) +L+ kWAS (k = 2) +L+ MAS +L+ EROS +L+ 99.5 +L+ 98.5 +L+ 97.5 +L+ 96.5 +L+ 95.5 +L+ 100 +L+ 99 +L+ 98 +L+ 97 +L+ 96 +L+ 95 +L+ 123	4 +L+ kWna (k = 541 +L+ kWna (k = 51 +L+ kWna (k = 41 +L+ kWna (k = 31 +L+ Mna +L+ EROa +L+ 20 +L+ 18 +L+ 16 +L+ 14 +L+ 12 +L+ 10 +L+ 8 +L+ 6 +L+ 4 +L+ 2 +L+ 0 +L+ CyberGlove Streams	Motion Capture Streams +L+ MAS +L+ kWAS (k = 6) +L+ EROS +L+ </SectLabel_figure> <SectLabel_page> 93 +L+ </SectLabel_page> <SectLabel_bodyText> ing eigenvalues. Eigenvalues from two motion matrices are +L+ given equal importance to the weights. Experiments with +L+ CyberGlove hand gesture streams and captured human body +L+ motions such as Taiqi and dances show that kWAS can rec- +L+ ognize 100% most similar isolated patterns and can recog- +L+ nize 94% motion patterns in continuous motion streams. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] D. Berndt and J. Clifford. Using dynamic time +L+ warping to find patterns in time series. In AAAI-94 +L+ Workshop on Knowledge Discovery in Databases, +L+ pages 229–248, 1994. +L+ [2] V. M. Dyaberi, H. Sundaram, J. James, and G. Qian. +L+ Phrase structure detection in dance. In Proceedings of +L+ the ACM Multimedia Conference 2004, pages 332–335, +L+ Oct. 2004. +L+ [3] G. H. Golub and C. F. V. Loan. Matrix Computations. +L+ The Johns Hopkins University Press, +L+ Baltimore,Maryland, 1996. +L+ [4] L. Ikemoto and D. A. Forsyth. Enriching a motion +L+ collection by transplanting limbs. In Proceedings of the +L+ 2004 ACM SIGGRAPH/Eurographics symposium on +L+ Computer animation, pages 99 – 108, 2004. +L+ [5] K. Kahol, P. Tripathi, S. Panchanathan, and +L+ T. Rikakis. Gesture segmentation in complex motion +L+ sequences. In Proceedings of IEEE International +L+ Conference on Image Processing, pages II – 105–108, +L+ Sept. 2003. +L+ [6] W. Krzanowski. Between-groups comparison of +L+ principal components. J. Amer. Stat. Assoc., +L+ 74(367):703–707, 1979. +L+ [7] C. Li, B. Prabhakaran, and S. Zheng. Similarity +L+ measure for multi-attribute data. In Proceedings of the +L+ 2005 IEEE International Conference on Acoustics, +L+ Speach, and Signal Processing (ICASSP), Mar. 2005. +L+ [8] C. Li, P. Zhai, S.-Q. Zheng, and B. Prabhakaran. +L+ Segmentation and recognition of multi-attribute +L+ motion sequences. In Proceedings of the ACM +L+ Multimedia Conference 2004, pages 836–843, Oct. +L+ 2004. +L+ [9] R. H. Liang and M. Ouhyoung. A real-time continuous +L+ gesture recognition system for sign language. In +L+ Proceedings of the 3rd. International Conference on +L+ Face and Gesture Recognition, pages 558–565, 1998. +L+ [10] K. Pullen and C. Bregler. Motion capture assisted +L+ animation: texturing and synthesis. In SIGGRAPH, +L+ pages 501–508, 2002. +L+ [11] G. Qian, F. Guo, T. Ingalls, L. Olson, J. James, and +L+ T. Rikakis. A gesture-driven multimodal interactive +L+ dance system. In Proceedings of IEEE International +L+ Conference on Multimedia and Expo, June 2004. +L+ [12] C. Shahabi and D. Yan. Real-time pattern isolation +L+ and recognition over immersive sensor data streams. +L+ In Proceedings of the 9th International Conference on +L+ Multi-Media Modeling, pages 93–113, Jan 2003. +L+ [13] A. Singhal and D. E. Seborg. Clustering of +L+ multivariate time-series data. In Proceedings of the +L+ American Control Conference, pages 3931–3936, 2002. +L+ [14] T. Starner, J. Weaver, and A. Pentland. Real-time +L+ american sign language recognition using desk and +L+ wearable computer based video. IEEE Transactions +L+ on Pattern Analysis and Machine Intelligence, +L+ 20(12):1371–1375, 1998. +L+ [15] C. J. Taylor. Reconstruction of articulated objects +L+ from point correspondences in a single image. +L+ Computer Vision and Image Understanding, +L+ 80(3):349–363, 2000. +L+ [16] K. Yang and C. Shahabi. A PCA-based similarity +L+ measure for multivariate time series. In Proceedings of +L+ the Second ACM International Workshop on +L+ Multimedia Databases, pages 65–74, Nov. 2004. +L+ </SectLabel_reference> <SectLabel_page> 94 +L+ </SectLabel_page>
<SectLabel_title> A Taxonomy of Ambient Information Systems: +L+ Four Patterns of Design +L+ </SectLabel_title> <SectLabel_author> Zachary Pousman +L+ </SectLabel_author> <SectLabel_affiliation> College of Computing/GVU Center +L+ Georgia Institute of Technology +L+ </SectLabel_affiliation> <SectLabel_address> Atlanta, GA 30332 +L+ +1 (404) 385-2447 +L+ </SectLabel_address> <SectLabel_email> zach@cc.gatech.edu +L+ </SectLabel_email> <SectLabel_author> John Stasko +L+ </SectLabel_author> <SectLabel_affiliation> College of Computing/GVU Center +L+ Georgia Institute of Technology +L+ </SectLabel_affiliation> <SectLabel_address> Atlanta, GA 30332 +L+ + 1 (404) 894 5617 +L+ </SectLabel_address> <SectLabel_email> stasko@cc.gatech.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Researchers have explored the design of ambient information +L+ systems across a wide range of physical and screen-based media. +L+ This work has yielded rich examples of design approaches to the +L+ problem of presenting information about a user’s world in a way +L+ that is not distracting, but is aesthetically pleasing, and tangible to +L+ varying degrees. Despite these successes, accumulating theoretical +L+ and craft knowledge has been stymied by the lack of a unified +L+ vocabulary to describe these systems and a consequent lack of a +L+ framework for understanding their design attributes. We argue that +L+ this area would significantly benefit from consensus about the +L+ design space of ambient information systems and the design +L+ attributes that define and distinguish existing approaches. We +L+ present a definition of ambient information systems and a +L+ taxonomy across four design dimensions: Information Capacity, +L+ Notification Level, Representational Fidelity, and Aesthetic +L+ Emphasis. Our analysis has uncovered four patterns of system +L+ design and points to unexplored regions of the design space, +L+ which may motivate future work in the field. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> Visual Interface Design, Tangible Interfaces +L+ </SectLabel_category> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Ubiquitous Computing, Ambient Display, Peripheral Display, +L+ Notification System, Taxonomy, Design Guidelines +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> From the very first formulation of Ubiquitous Computing, the +L+ idea of a calmer and more environmentally integrated way of +L+ displaying information has held intuitive appeal. Weiser called this +L+ “calm computing” [35] and described the area through an elegant +L+ example: a small, tangible representation of information in the +L+ world, a dangling string that would wiggle based on network +L+ traffic. When information can be conveyed via calm changes in +L+ the environment, users are more able to focus on their primary +L+ work tasks while staying aware of non-critical information that +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> AVI '06, May 23-26, 2006, Venezia, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2006 ACM 1-59593-353-0/06/0005. $5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> affects them. Research in this sub-domain goes by various names +L+ including “ambient displays”, “peripheral displays”, and +L+ “notification systems”. The breadth of the systems in these broad +L+ categories is quite large. We seek to disentangle the terminology +L+ used to describe and categorize the wide array of systems in order +L+ to provide a common language for discussing research therein. +L+ An ambient display can represent many types of data, from +L+ stock prices, to weather forecasts, to the presence or absence of +L+ colleagues. Maintaining awareness of co-located and distant work +L+ and social groups has been a long-term research thread in the area +L+ of Computer Supported Cooperative Work (CSCW) [5, 8]. The +L+ Tangible Media Group at the MIT Media Lab, directed by Ishii, +L+ also helped shape the field of ambient computation. They coined +L+ the term “tangible media,” citing inspiration from Weiser’s vision +L+ [35] and from Pederson and Sokoler’s AROMA system [29] and +L+ developed AmbientROOM [17] and Ambient Fixtures [6, 18]. +L+ These systems use ambient displays to make people aware of both +L+ group activity and other information such as network traffic. +L+ Recent work in Ambient Intelligence has brought techniques from +L+ Artificial Intelligence to ambient systems, spearheaded by the +L+ Disappearing Computer initiative of the European Union [31]. +L+ This research thrust seeks to imbue ambient systems with +L+ contextual knowledge about the environment. The Roomware +L+ project has resulted in smart architectural spaces that support +L+ information conveyance (and group collaboration) [33]. +L+ Researchers have developed systems that use a multitude of +L+ everyday objects to display information. Examples include lights +L+ of various sorts [2, 17], sounds [25], shadows [8], artificial flowers +L+ [18], mobiles [24], and office-décor water fountains [12, 16]. +L+ Further research has sought to use framed photographs [26] and +L+ larger artistic pictures to represent information from the world in +L+ an art-like manner [14, 30, 32]. There are also peripheral display +L+ “modes” of a user’s main desktop, including screensavers like +L+ What’s Happening [36], information bars and menus such as those +L+ leveraged in Sideshow and Irwin [6, 22], and alternate panes, like +L+ Apple’s Dashboard [3]. As one can see, the design space is large. +L+ All these systems provide a rich history of system design +L+ principles, approaches, and decisions, but accumulating theoretical +L+ and craft knowledge has been stymied by the lack of a unified +L+ vocabulary to define and describe these systems. In this paper we +L+ propose a set of design choices that developers of ambient +L+ information systems must confront to build successful and +L+ compelling systems. First we set out a definition of an ambient +L+ information system that is a synthesis of the varied definitions +L+ given in published research. We hone the intuitive set of +L+ </SectLabel_bodyText> <SectLabel_page> 67 +L+ </SectLabel_page> <SectLabel_bodyText> characteristics that distinguish ambient systems from other +L+ ubiquitous computing research systems. Next, we propose a set of +L+ design dimensions for ambient information systems. The four +L+ dimensions of system design elucidate the main decisions one +L+ confronts when designing an effective ambient system. Finally, we +L+ explore the clusters across dimensions to uncover four coherent +L+ combinations of system designs, which work as design patterns for +L+ the field. The results also identify new ways of combining the +L+ design attributes to explore new possibilities for ambient +L+ information systems. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. AMBIENT INFORMATION SYSTEMS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Many different terms have been used to describe the types of +L+ systems we discuss in this paper. Three of the most commonly +L+ used terms are “ambient display,” “peripheral display,” and +L+ “notification system.” But how does one differentiate these +L+ terms? Based on general understandings, we claim that: +L+ </SectLabel_bodyText> <SectLabel_listItem> - all ambient displays are peripheral displays, +L+ - some notification systems are peripheral displays +L+ </SectLabel_listItem> <SectLabel_bodyText> (some notification systems are not peripheral but are +L+ instead the object of focused work and attention) +L+ The words of researchers themselves likely best explain their +L+ conceptions of the systems that they have built. Below, we present +L+ germane definitional quotes. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Ishii et al: “[In Ambient Displays] information is moved off +L+ the screen into the physical environment, manifesting itself as +L+ subtle changes in form, movement, sound, color, smell, +L+ temperature, or light. Ambient displays are well suited as a +L+ means to keep users aware of people or general states of large +L+ systems, like network traffic and weather.” [17] +L+ •	Matthews et al: Peripheral displays, then, are displays that +L+ </SectLabel_listItem> <SectLabel_bodyText> show information that a person is aware of, but not focused on. +L+ [24] +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Matthews et al: “Ambient displays might be defined as those +L+ </SectLabel_listItem> <SectLabel_bodyText> that are "minimally attended" (e.g. just salient enough for +L+ conscious perception) while alerting displays are "maximally +L+ divided" (e.g. slightly less salient than focal tasks). [24] +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Stasko et al: Ambient displays typically communicate just one, +L+ </SectLabel_listItem> <SectLabel_bodyText> or perhaps a few at the most, pieces of information and the +L+ aesthetics and visual appeal of the display is often paramount. +L+ Peripheral displays refer to systems that are out of a person’s +L+ primary focus of attention and may communicate one or more +L+ pieces of information.” [32] +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Mankoff et al: “Ambient displays are abstract and aesthetic +L+ </SectLabel_listItem> <SectLabel_bodyText> peripheral displays portraying non-critical information on the +L+ periphery of a user’s attention... They generally support +L+ monitoring of non-critical information.” “Ambient displays +L+ have the ambitious goal of presenting information without +L+ distracting or burdening the user.” [20] +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Rounding and Greenberg: “The [notification collage] is +L+ </SectLabel_listItem> <SectLabel_bodyText> designed to present info[rmation] as lightweight and peripheral +L+ objects. It does not demand the full attention of its users: rather +L+ it can be attended to in passing, where people collaborate should +L+ the need or desire arise.” [14] +L+ </SectLabel_bodyText> <SectLabel_listItem> •	McCrickard et al: “Often implemented as ubiquitous systems or +L+ </SectLabel_listItem> <SectLabel_bodyText> within a small portion of the traditional desktop, notification +L+ systems typically deliver information of interest in a parallel, +L+ multitasking approach, extraneous or supplemental to a user’s +L+ attention priority.” [21 ] +L+ </SectLabel_bodyText> <SectLabel_listItem> •	McCrickard et al: Notification systems are defined as +L+ </SectLabel_listItem> <SectLabel_bodyText> interfaces that are typically used in a divided-attention, +L+ multitasking situation, attempting to deliver current, valued +L+ information through a variety of platforms and modes in an +L+ efficient and effective manner [21 ]. +L+ The easiest way to explain the differences between systems is +L+ to look at the design motivations that informed them. Ambient +L+ displays are those that have pointed aesthetic goals and present a +L+ very small number of information elements. These systems are a +L+ proper subset of peripheral displays, which can appear either in the +L+ environment or on secondary or even primary computer displays. +L+ Notification systems’ design motivation results from divided +L+ attention situations. As such, they can be equal to a primary work +L+ task in their attentional needs or be secondary. When notification +L+ systems are designed to be secondary to a primary task, the +L+ systems are appropriately defined as peripheral. +L+ In this paper, we propose the term ambient information system +L+ as the unit of study and define the behavioral characteristics of +L+ such as systems as follows: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Display information that is important but not critical. +L+ •	Can move from the periphery to the focus of attention and +L+ back again. +L+ •	Focus on the tangible; representations in the environment. +L+ •	Provide subtle changes to reflect updates in information +L+ (should not be distracting). +L+ •	Are aesthetically pleasing and environmentally appropriate. +L+ 3. PREVIOUS TAXONOMIES +L+ </SectLabel_listItem> <SectLabel_bodyText> A small number of research papers that describe ambient +L+ information systems also include extended discussions of the +L+ design dimensions that motivate and contextualize their work. The +L+ authors provide dimensions to compare and contrast their systems +L+ to others in order to explain their design rationales. +L+ Matthews et al use the dimensions notification level, +L+ transition, and abstraction to characterize systems in this space +L+ [24]. They developed the Peripheral Display Toolkit [23] that +L+ helps people to develop ambient information displays more easily. +L+ Their concept of notification level means the relative importance +L+ of a particular data stream. Transitions are the programmatic +L+ changes to the display, based on the data. Transitions include +L+ fading, scrolling, or animation effects. They define abstraction as +L+ the mapping that takes a piece of numerical or ordinal data and +L+ turns it into something that the ambient display can use, something +L+ “more easily interpreted with less [user] attention.” +L+ Matthews et al segregate notification level into five levels: +L+ Ignore, Change Blind, Make Aware, Interrupt, and Demand +L+ Attention. The gradations run from low, a system ignoring the +L+ change in the data, to high, a system demanding attention in a way +L+ that must also be explicitly dismissed. They propose categories of +L+ transition: interrupt, make aware, and change blind. Finally, they +L+ bifurcate abstraction into feature abstraction or degradation. +L+ McCrickard et al introduce a different set of three dimensions +L+ to classify notification systems: interruption, reaction, and +L+ comprehension [21]. Interruption is defined psychologically, +L+ similar to Matthews’ notion, “as an event prompting transition and +L+ reallocation of attention focus from a [primary] task to the +L+ notification.” Reaction is defined as the rapid response to a given +L+ stimulus, while comprehension is the long-term notion of +L+ remembering and sense-making. +L+ </SectLabel_bodyText> <SectLabel_page> 68 +L+ </SectLabel_page> <SectLabel_bodyText> McCrickard et al then plot the design space as a 3-tuple of +L+ interaction, reaction, and comprehension (IRC). Each dimension is +L+ assigned a rating of high (1) or low (0), creating models like 0-1-0. +L+ They label these models with meaningful names like “Ambient +L+ Media, 0-0-1” “Indicator, 0-1-0” and “Critical Activity Monitor, +L+ 1-1-1.” Eight models serve as the corners of a design space. The +L+ resulting space, it should be noted, is larger than the design space +L+ of ambient information systems as we discuss in this paper +L+ because it contains games, secondary displays, and critical activity +L+ monitors (which by our definition, are notification systems that are +L+ not also peripheral systems). McCrickard also classifies a set of 14 +L+ extant systems in the design space on the three dimensions. +L+ Both of these taxonomies deal thoroughly with interruption +L+ and detail some of the criteria for categorizing systems along this +L+ design dimension. We extend this analysis to other dimensions of +L+ data representation, flexibility, and aesthetics. This more holistic +L+ view points out design trade-offs between aesthetic emphasis and +L+ and flexibility, and between a system’s information display style +L+ and display capacity. +L+ Mankoff et al proposed a set of heuristics for evaluating +L+ ambient systems [20], which may also assist system builders. The +L+ heuristics attempt to give guidance for the formative evaluation of +L+ ambient systems, but they also can be viewed as high-level design +L+ guidelines, such as “The display should be designed to give ‘just +L+ enough’ information. Too much information cramps the display, +L+ and too little makes the display less useful.” +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. DESIGN DIMENSIONS OF AMBIENT +L+ SYSTEMS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Designers of ambient information systems make decisions +L+ about how much information to display, what specific aspects to +L+ depict, and how exactly to display it, transparently or abstractly, +L+ on a monitor or via a decorative sculpture. We present four design +L+ dimensions that capture the space of ambient information systems. +L+ The dimensions can be thought of as design choices or design +L+ questions that system builders must answer. The dimensions are: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	information capacity +L+ •	notification level +L+ •	representational fidelity +L+ •	aesthetic emphasis +L+ </SectLabel_listItem> <SectLabel_bodyText> We rank 19 research systems and three consumer ambient +L+ information systems on each of the four axes. Each axis is divided +L+ into 5 bands, from low to high. We place systems into groups +L+ based on information from published conference and journal +L+ proceedings, including images and videos of systems in use if +L+ available. The 19 systems we chose are not intended to be an +L+ exhaustive list of all ambient information systems in the research +L+ literature. The 19 systems are representative of the breadth of the +L+ field and we feel that attempting an exhaustive list, while +L+ amplifying completeness, would not significantly alter the design +L+ dimensions. +L+ Research systems that we analyzed include: Bus Mobile [24], +L+ Dangling String [35], Digital Family Portrait [26], InfoCanvas +L+ [33], Informative Art [30], Information Percolator [16], Irwin [22], +L+ Kandinsky [11], Kiumra [19], Lumitouch [5], Notification Collage +L+ [14], Scope [34], Sideshow [7], Table Fountain [12], Water Lamp +L+ [8], and What’s Happening [36]. We include three consumer +L+ systems that fit our definition of ambient information systems, +L+ Ambient Devices Ambient Orb [2], the My Yahoo! web portal +L+ [27] and Apple’s Dashboard [3]. +L+ Figure 1 shows the four dimensions for our analysis, and +L+ each of the 19 systems placed into a group along each. Thin +L+ colored lines trace the rankings of systems on each axis, similar to +L+ a parallel coordinates plot. Each axis has values that range from +L+ low to high through five grades. The dimensions of notification +L+ level and representational fidelity have more descriptive axis +L+ labels that will be explained in detail below. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Information Capacity +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Ambient information systems are created to convey +L+ information to users—information that typically is important to a +L+ user’s sense of wellbeing and general awareness, but not critical to +L+ their work or personal life. Information capacity represents the +L+ number of discrete information sources that a system can +L+ represent. Some systems are capable of displaying a single piece +L+ of data such as the current price of a stock index. Others can +L+ display the value of 20 (or more) different information elements +L+ on one screen. We rank systems from “Low” to “High” on this +L+ design dimension. +L+ Information elements are discrete information “nuggets”. For +L+ example, if a system monitors campus shuttle buses, each bus is a +L+ single nugget. If the system can represent both the time to a +L+ location and a direction of travel, then there are two nuggets of +L+ information for each bus that is monitored. +L+ Information capacity makes visible the design trade-off +L+ between space and time. A designer can increase the information +L+ capacity of a display by increasing the space for information to be +L+ presented or by creating a display that transitions through a set of +L+ views over time. If a system is designed with multiple views or +L+ uses scrolling, we rank it in the top tier, since the number of pieces +L+ of information that it could display is arbitrarily large. +L+ A further caveat about information capacity is necessary. +L+ Some of the analyzed systems such as InfoCanvas, Sideshow, and +L+ Dashboard are user-configured and user-customizable. This means +L+ that these and other systems could potentially be made to display +L+ hundreds of elements. Instead of attempting to calculate a +L+ theoretical maximum throughput for the display in these cases, we +L+ use the system designer’s naturalistic portrayal in their published +L+ work to determine the “everyday maximum.” Each of these +L+ systems is also in the top tier of information capacity. +L+ The design dimension of information capacity has a barbell +L+ distribution. Five of the 19 systems display a single information +L+ element and are ranked “Low”. Conversely, there are eight +L+ systems that display from ten to 20 information elements, with +L+ some systems having the potential to display more and these are +L+ ranked “High.” Only a few systems take a middle-ground +L+ approach, attempting to display a small number (from two to ten) +L+ of information elements. +L+ The systems with low ratings on the attribute of information +L+ conveyance are those that are physical displays. Fountains, +L+ glowing lights, and office-decoration sculptures afford designers +L+ only so much flexibility for changes. +L+ </SectLabel_bodyText> <SectLabel_page> 69 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: Parallel Coordinate plot of 19 existing ambient information systems across four design dimensions. Colored lines trace +L+ each system’s ranking along the design dimensions. Different colors are used to denote groups of systems which are similar as +L+ explained more fully in Section 5. +L+ Since the number of changes possible is small, the total number +L+ of information nuggets that can be represented is +L+ correspondingly small. The systems with high information +L+ conveyance are those that are presented on LCD screens. The +L+ systems that run at full screen (instead of as a small section of a +L+ focused main monitor) are ranked the highest. +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 4.2 Notification Level +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Notification level is the degree to which system alerts are +L+ meant to interrupt a user. Notification level is a design attribute +L+ that is present in the two taxonomies of ambient and peripheral +L+ information systems we reviewed earlier. Matthews et al +L+ subdivides notification level into five categories: ignore, change +L+ blind, make aware, interrupt, and demand attention. For our +L+ analysis we adopt those categories but replace the lowest level +L+ of system alert function, ignore (a degenerate case) with user +L+ poll. Systems such as Apple Dashboard and My Yahoo! do not +L+ always appear in a user’s environment and must be explicitly +L+ called to the fore. +L+ Notification level can be thought of as the “ambience” of +L+ the systems in question. Some systems in the ambient space are +L+ quiet, and afford opportunistic glances to the information, while +L+ others provide more strident alerts by blinking, flashing, +L+ beeping, or even opening dialog windows. Systems that provide +L+ unobtrusive change blind or make aware notifications to the user +L+ are at the core of the ambient information system design space. +L+ Systems that interrupt users with alarms or that demand +L+ attention (by launching system dialog windows) are not subtle, +L+ so are further from the core concept of ambient information +L+ systems, though, as Matthews et al argues, the smooth transition +L+ from more subtle to more jarring is an interesting design +L+ direction for ambient system designers. +L+ Notification level is the designer-intended level of alert. +L+ We do not take pains to distinguish between systems that are +L+ proven to be “change blind” through user experimentation +L+ versus those that merely claim change blindness. We remain +L+ agnostic here about the techniques used for ensuring subtlety +L+ including slow animation, scrolling, and fading (these +L+ implementation details are at a lower level of design rationale). +L+ Once the decision has been made to produce a system with +L+ change blind transitions, the designer must then produce system +L+ transitions that meet the goal in the specifics of the system. Our +L+ analysis focuses on the high level decision on the part of the +L+ designer or design team. +L+ The distribution of systems here shows a good fit to our +L+ definition of ambient information systems. It is apparent that +L+ most ambient information systems adhere to the central notion +L+ of subtle visual or representational changes. The vast majority of +L+ ambient information systems fall into the change blind and make +L+ aware transition categories (somewhat low and medium). Few +L+ systems are designed to interrupt users or demand attention. +L+ </SectLabel_bodyText> <SectLabel_page> 70 +L+ </SectLabel_page> <SectLabel_bodyText> Two that do however are Scope and Sideshow. Note that most +L+ systems that are physical displays do not have make-aware or +L+ interruption-level alerts, much less demand attention alerts. The +L+ Bus Mobile does enable make-aware transitions, when, for +L+ example, the last bus of the day approaches. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Representational Fidelity +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Representational fidelity describes a system’s display +L+ components and how the data from the world is encoded into +L+ patterns, pictures, words, or sounds. Some systems reproduce +L+ the information being monitored in a very direct way, while +L+ others are much more abstract in their representation. Matthews +L+ et al’s taxonomy characterizes this design choice as abstraction, +L+ but only distinguishes two sub-types, feature degradation and +L+ feature abstraction. We consider this design dimension to be rich +L+ and complex, so we will try to tease apart the many different +L+ types of abstraction that appear in ambient information systems. +L+ Representational fidelity can be described in the language +L+ of Semiotics, the branch of Philosophy that deals with signs, sign +L+ systems (such as natural languages) and their meanings. As such +L+ it has an accepted vocabulary for the elements of a symbolic +L+ representation. Semiotics can help analyze the way that +L+ particular signifiers—words, pictures, sounds, and other +L+ things—stand for the things they represent. +L+ A semiotic sign is made up of three parts [28]. The object +L+ is called the signified; it is the physical thing or idea that the +L+ sign stands for. The signifier is the representation of the object, +L+ which could be a word, a picture, or a a sound. The sense is the +L+ understanding that an observer gets from seeing or experiencing +L+ either the signified or its signifier. The signifier and the signified +L+ need not have any direct relationship. However, both the +L+ signified and the signifier create the same sense in the head of an +L+ observer; seeing a log aflame and seeing the word “fire” create +L+ the same meaning for a person. +L+ Ambient information systems, in the vocabulary of +L+ semiotics, contain one or more signs. Each sign has its object, +L+ information in the world, and its representation, the lights, +L+ pictures, or sounds used to signify that information. Many +L+ ambient information systems contain multiple signs—each +L+ picture element standing for a different piece of information. +L+ The theory of Semiotics also helps to explain the notion +L+ that some signs are transparent, easily understood, while others +L+ are metaphorical and still others are abstract. Signs can be +L+ symbolic, iconic, or indexical. Symbolic signs are those that are +L+ completely arbitrary. For example languages are arbitrary, for +L+ the word “bachelor” has no more natural relation to an +L+ unmarried man than does the word “foobar. ” Symbolic signs +L+ are those signs for which a code, or rule-following convention, +L+ is required to understand. Language characters and numbers are +L+ all symbolic, as are abstract visual representations (the color red +L+ standing for “danger”). Iconic signs are those signs that have an +L+ intermediate degree of transparency to the signified object. +L+ Iconic signs include metaphors as well as doodles, drawings, +L+ and caricatures. Icons represent their objects by having some +L+ similarity or resemblance to the object or to an essential aspects +L+ of the object. Indexical signs are those that are directly +L+ connected to the signified. Examples include measuring +L+ instruments, maps, and photographs. +L+ We have subdivided the three main categories of +L+ representational fidelity to distinguish between ambient +L+ information systems. We propose five groups, ranked from +L+ indexical (high) to symbolic (low): +L+ </SectLabel_bodyText> <SectLabel_listItem> •	INDEXICAL: measuring instruments, maps, +L+ photographs +L+ •	ICONIC: drawings, doodles, caricatures +L+ •	ICONIC: Metaphors +L+ ■ SYMBOLIC: language symbols (letters and numbers) +L+ ■ SYMBOLIC: abstract symbols +L+ </SectLabel_listItem> <SectLabel_bodyText> Some ambient information systems have displays that do +L+ not afford representational flexibility, because of the constraints +L+ of the display. For example, the LiveWire system and the +L+ Ambient Orb cannot represent language symbols, nor can they +L+ convey indexical forms like photographs. However, some +L+ flexibility is present. The systems might map information in an +L+ arbitrary way, remaining fully abstract (representing stock +L+ increases with the color green and losses with the color red), or +L+ it could map information more metaphorically, as would be the +L+ case if LiveWire were connected to information from a +L+ seismograph or ocean tides. As one can see, the question +L+ concerning representational flexibility requires one to consider +L+ both the display and the information that is displayed. +L+ The InfoCanvas is a very flexible system when considering +L+ representational fidelity. The InfoCanvas uses all five types of +L+ representational fidelity. It uses abstract symbols, such as the +L+ color red standing for traffic being stopped, metaphors, like a +L+ cartoon drawing of a cloud representing cloudy conditions, and +L+ also photographs and words of news stories, which are fully +L+ indexical. We show this ability for a system to straddle multiple +L+ representational forms by duplicating the system in each +L+ category and noting them with an asterisk (see Figure 1). +L+ Systems which are designed to represent information at multiple +L+ levels of fidelity are: Apple’s Dashboard, InfoCanvas, +L+ Informative Art, Notification Collage, Sideshow, and What’s +L+ Happening. In these cases, we draw the parallel coordinate plot +L+ to the top-most tier of representational fidelity for each system. +L+ The majority of systems however, only afford a single level +L+ of representational fidelity. Many of the sculptural displays only +L+ afford symbolic, that is abstract, representations, while a smaller +L+ number afford text and photographic representations. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.4 Aesthetic Emphasis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The final dimension concerns the relative importance of the +L+ aesthetics of the display. Some system designers seek to build +L+ displays and artifacts with sculptural or artistic conventions. For +L+ these systems, being visually pleasing is a primary objective. +L+ Others however place relatively little focus on aesthetics and +L+ typically focus more on information communication ability. +L+ Since aesthetic judgment is at its core a subjective phenomenon, +L+ we do not judge systems on their relative artistic merits. Instead +L+ we attempt to rank ambient information systems by our +L+ perception of the importance given to aesthetics. There is often a +L+ tradeoff made between communication capacity, +L+ representational fidelity, and aesthetics, a relationship that we +L+ explore in this section. +L+ Ambient information systems are intended to be visible; +L+ positioned on a shelf, hung on the wall, or placed as a small +L+ sculpture on a desk, the systems are seen not just by a user, but +L+ also by co-workers, colleagues, or family members. There are a +L+ </SectLabel_bodyText> <SectLabel_page> 71 +L+ </SectLabel_page> <SectLabel_bodyText> multitude of approaches when it comes to building aesthetically +L+ pleasing devices. One approach is to build systems that mirror +L+ existing artworks by a particular artist, as is the case in +L+ Kandinsky and Informative Art. A second approach is to design +L+ a display that is representative of a particular style or art +L+ movement. InfoCanvas, through its use of themes, allows the +L+ display to take on characteristics of Asian water-color paintings, +L+ for example. +L+ We rank systems on the design dimension of aesthetic +L+ emphasis as low, somewhat low, medium, somewhat high and +L+ high. Note again that we are not assessing the degree to which +L+ the systems are successful as art. We are providing a subjective +L+ measure of how much the system designers focused on +L+ aesthetics and how much they emphasized aesthetic +L+ considerations in their research and design decisions. +L+ Most systems that we analyzed had medium or somewhat +L+ high degrees of aesthetic emphasis (12 of 19). The decisions of +L+ designers to strive for visually pleasing displays is most clear in +L+ the cases where the display is intended to leverage the work of +L+ existing artists. The physical ambient information displays are +L+ often sculptural in their design decisions. They attempt to set +L+ themselves off from the rest of the environment, often on +L+ pedestals or stands. Their capability to display much information +L+ (information capacity) is often limited by their design clarity and +L+ austerity. We consider this design trade-off in the next section. +L+ Systems that we ranked at the middle of the spectrum of +L+ aesthetic emphasis are those which are not intended by their +L+ designers to be art worthy of contemplation as art objects. But +L+ they are explicitly intended to be viewed as calm pleasing +L+ objects and displays. Apple’s Dashboard widgets have a clean +L+ design sense about them, as does Kimura, What’s Happening +L+ and the Information Percolator. The systems that are ranked low +L+ on aesthetic emphasis are Scope, Sideshow, Bus Mobile, Elvin, +L+ and My Yahoo!. These systems put information conveyance at a +L+ higher priority than being aesthetically pleasing. They are still +L+ calm and environmentally appropriate, but their designers did +L+ not emphasize their aesthetic qualities. Cleary, some systems +L+ that are early-stage prototypes like Bus Mobile, may not have +L+ the aesthetic polish of more finished systems. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. FOUR DESIGN PATTERNS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we introduce four design patterns for +L+ ambient information systems, after Alexander’s pattern language +L+ for architectural studies [1]. The design patterns illustrate four +L+ coherent combinations of the four design dimensions previously +L+ presented. We have already pointed out trends and clusters that +L+ are present in each particular design dimension. However, there +L+ are fruitful conclusions for system designers as we consider the +L+ interaction between the design dimensions to form design +L+ patterns. +L+ Considering the clusters of systems in each dimension and +L+ the correspondences that are visible in the parallel coordinate +L+ plot, we find four main archetypes in existing ambient +L+ information system design: Symbolic Sculptural Display, +L+ Multiple-Information Consolidators, Information Monitor +L+ Display, and High Throughput Textual Display. Figure 2 +L+ shows the pattern of each archetype across the dimensions. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: a-d System design archetypes shown in the context +L+ </SectLabel_figureCaption> <SectLabel_bodyText> of the design space. Heavy boxes indicate core design +L+ decisions, while light boxes show alternate choices. +L+ Symbolic Sculptural Displays are ambient information systems +L+ that display very few pieces of information, usually a single +L+ element. They represent information in an abstract sculptural +L+ way with light, water, or moving objects. They are intended to +L+ be decorative objects for a home or office setting and as such are +L+ highly aesthetic in their design (see Figure 2a). This design +L+ pattern is a core of ambient system design, and accounts for six +L+ of our analyzed systems: Ambient Orb, Dangling String, Digital +L+ Family Portrait, Information Percolator, Lumitouch, Table +L+ Fountain, and Water Lamp. The Digital Family Portrait +L+ combines multiple information sources and so truly represents +L+ more information than the other members of this type. +L+ </SectLabel_bodyText> <SectLabel_page> 72 +L+ </SectLabel_page> <SectLabel_bodyText> Multiple Information Consolidators are ambient systems that +L+ display many individual pieces of information in a consolidated +L+ manner. They are typically screen-based in order to convey +L+ much information and make users aware of changes to that +L+ information (usually by blinking the visual representation of a +L+ certain element). They are reasonably aesthetically motivated, +L+ but all clearly demonstrate the trade-off between aesthetics and +L+ customization and information capacity (see Figure 2b). Systems +L+ which illustrate this design pattern are: Kandinsky, Kimura, +L+ InfoCanvas, Notification Collage, and What’s Happening. +L+ Kandinsky departs from the other systems in that it is explicitly +L+ modeled on the fine art of Kandinsky, and as such is highly +L+ stylized and design-focused. It does so at the expense of +L+ flexibility, since it can only display photographs in its slots. +L+ Information Monitor Displays are displays that are a +L+ peripheral part of a user’s computer desktop. As such, they +L+ afford different interactions and design choices. They display +L+ multiple sources of information, and do so usually by visual +L+ metaphors. They are capable of notifying users in multiple ways +L+ about changes in the source data, including subtle awareness, +L+ interrupting, and even demanding user attention when necessary +L+ (i.e., requiring the user to switch focus to dismiss a notification). +L+ The systems achieve aesthetics, but their primary purpose is not +L+ good looks (see Figure 2c). Examples of this design archetype +L+ include: Scope, and Sideshow. +L+ High Throughput Textual Display systems are those that use +L+ text and very simple graphics (icons) to denote information. +L+ They are capable of representing voluminous information, but +L+ do not draw attention with interruption-level notifications. These +L+ systems are not primarily as concerned with aesthetics as they +L+ are with information conveyance (see Figure 2d). These systems +L+ are simple but efficient for certain types of tasks. Examples of +L+ this design archetype are: Elvin, and My Yahoo!. +L+ The four design archetypes cover nearly all of the analyzed +L+ systems, but do not cleanly categorize three systems. Apple’s +L+ Dashboard system is most similar to a Multiple Information +L+ Consolidator. It fails being a pure example of this archetype +L+ because of its inability to alert users to changes in information – +L+ it requires users poll the system by calling up the transparent +L+ pane via a hot key. The Bus Mobile is an early stage prototype, +L+ and as such is not concerned with aesthetics to a large degree. +L+ With a higher degree of aesthetic emphasis, it might be closer to +L+ a Information Monitor Display (albeit a physical instead of +L+ screen-based system). Informative Art is quite unlike the four +L+ design archetypes. Informative Art has high aesthetic emphasis, +L+ but low information capacity (e.g. 5 or 6 city’s weather forecast +L+ information). It is metaphorical and abstract in its information +L+ mapping fidelity. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. EXTENDING THE PATTERNS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The four patterns for system design can help designers to +L+ make appropriate choices as they develop new ambient +L+ information systems. The design patterns can be used as models +L+ so a designer can decide to build “an information monitor +L+ display for a home health awareness application”, or “a set of +L+ symbolic sculptural displays for work-group collaboration”. +L+ Further, the designer may be depart from the pattern, by building +L+ up a system’s range of possible notification levels, or by +L+ choosing to trade aesthetics for increased information capacity. +L+ However, our analysis also points at what has not yet been +L+ explored. The four design patterns show four coherent +L+ combinations, but they are not the only possibilities for building +L+ useful ambient systems. Combined with longer-term trends in +L+ the fields of Ambient Intelligence and Ubiquitous Computing, +L+ new archetypes for system design are emerging. We note +L+ possibilities here, which change both the dimensions and the +L+ four design patterns. +L+ We do not expect the information capacity for ambient +L+ systems to increase by dramatically. Though scrolling or time- +L+ divided ambient systems (What’s Happening, Elvin) can already +L+ display data elements numbering in the hundreds, simultaneous +L+ visual displays are usually limited to 25 or 30 elements by +L+ readability and user learnability. Ambient information systems +L+ will not turn into information visualization systems showing +L+ thousands of data points. However, contextual sets of +L+ information may be useful for ambient systems in specialized +L+ environments. Systems which display contextual sets of +L+ information like that of the Bus Mobile (all of the buses on a +L+ college campus) or Scope (email and calendar data) would +L+ increase the number of systems in the middle portion of this +L+ design dimension. +L+ We also expect to see changes to the design dimension of +L+ representational flexibility. Designers have begun to explore the +L+ affordances of abstract and symbolic mappings between +L+ information sources and their representations. We see this +L+ continuing, with new systems focusing on personally relevant +L+ symbolic representations, and utilizing metaphors from the +L+ natural and built worlds. Another shift that we foresee is the +L+ designers creating systems where multiple information sources +L+ and aspects interact to affect a single part of the representation. +L+ This is apparent already in Digital Family Portrait where the size +L+ of the butterflies represents “activity,” even though activity is +L+ not the reading from a single sensor, but it instead a reading +L+ from multiple sensors in a home. Informative Art also has +L+ aspects of this approach, changing both the color and +L+ dimensions of squares based on two different aspects of weather. +L+ As regards aesthetic emphasis, we foresee a more radical +L+ change. We predict further exploration of the space of truly +L+ artistically motivated ambient information systems. These +L+ generative artworks use information from the world to drive +L+ their behavior and ask (and answer) art questions as well as +L+ technology questions. Though most of these works are outside +L+ the academy (they are shown in galleries instead of computer +L+ science conferences), Bolen and Mateas’ Office Plant #1 [4] is a +L+ sculpture that characterizes the mood of a user’s email stream +L+ and conveys it via transformations of a robotic plant. These +L+ systems are going to create a new design space above the top tier +L+ that we depict in this work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this work we synthesize a definition that distinguishes +L+ research in ambient information systems from that of +L+ notification systems and peripheral displays. We propose four +L+ design dimensions, rank systems to show clusters, and uncover +L+ four design patterns on which system developers may model +L+ their system designs. Future work will expand the four +L+ dimensions to include aspects of the social interaction and +L+ impact that system have on the behavior of individuals and +L+ groups. +L+ In this work we point toward open areas in the design +L+ space, and we point to new design directions that may fill these +L+ gaps. Future work may also turn this taxonomy into an +L+ evaluation framework for ambient information systems. +L+ </SectLabel_bodyText> <SectLabel_page> 73 +L+ </SectLabel_page> <SectLabel_sectionHeader> 8. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Alexander, C., A Pattern Language: Towns, Buildings, +L+ Construction. Oxford University Press, 1977. +L+ 2. Ambient Orb. http://www.ambientdevices.com/ +L+ 3. Apple Mac OS X Dashboard. http://www.apple.com/ +L+ macosx/features/dashboard/index.htm +L+ 4. Bohlen, M., and Mateas, M. Office Plant #1. Leonardo 31:5. pp. +L+ 345-349. +L+ 5. Chang, A., Resner, B., Koerner B., Wang, X and Ishii, H., +L+ Lumitouch: An emotional communication device. Extended +L+ Abstracts of CHI 2001, pp. 371-372. +L+ 6. Cadiz, J., Fussell, S., Kraut, R., Lerch, J., and Scherlis, W. The +L+ Awareness Monitor: A Coordination Tool for +L+ Asynchronous, Distributed Work Teams. Unpublished +L+ manuscript. Demonstrated at CSCW 1998. +L+ 7. Cadiz, J., Venolia, G., Janke, G., ans Gupta, A. Designing +L+ and deploying an information awareness interface. +L+ Proceedings of CSCW 2002, pp. 314 - 323. +L+ 8. Dahley, A., Wisneski, C., and Ishii, H. Water lamp and +L+ pinwheels: Ambient projection of digital information into +L+ architectural space. CHI Conference Summary 1998, pp. +L+ 269–270. +L+ 9. Espinosa, A., Cadiz, J., Rico-Gutierrez L., Kraut, R., Sherlis, +L+ W., and Lautenbacher, G. Coming to the Wrong Decision +L+ Quickly: Why Awareness Tools Must be Matched with +L+ Appropriate Tasks. Proceedings of CHI 2000, pp. 392-399. +L+ 10. Fitzpatrick, G., Kaplan, S., Arnold, D., Phelps, T., and +L+ Segall, B. Augmenting the Workaday World with Elvin. +L+ Proceedings of ECSCW 1999, pp. 431-450. +L+ 11. Fogarty, J., Forlizzi, J., and Hudson, S. Aesthetic +L+ Information Collages: Generating Decorative Displays that +L+ Contain Information. Proceedings of the UIST 2001, +L+ pp. 141-150. +L+ 12. Gellersen, H.-W., Schmidt, A. and Beigl. M. Ambient Media +L+ for Peripheral Information Display. Personal Technologies +L+ 3, 4 : 199-208. 1999. +L+ 13. Greenberg, S., and Fitchett, C. Phidgets: Easy development +L+ of physical interfaces through physical widgets. Proceedings +L+ of UIST 2001. pp 209-218. +L+ 14. Greenberg, S., and Rounding, M. The Notification Collage: +L+ Posting Information to Public and Personal Displays. +L+ Proceedings of CHI 2001, pp. 515-521. +L+ 15. De Guzman, E., Yau M, Park, A., and Gagliano, A. +L+ Exploring the Design and Use of Peripheral Displays of +L+ Awareness Information. Extended Abstracts of CHI 2004, +L+ pp. 1247-1250. +L+ 16. Heiner, J. M., Hudson, S., and Kenichiro, T. The +L+ Information Percolator: Ambient information display in a +L+ decorative object. In Proc. of UIST 1999, pp. 141-148. +L+ 17. Ishii, H.,Wisenski, C., Brave, S., Dahley, A., Gorbet, M., +L+ Ullmer, B., and Yarin, P. AmbientROOM: Integrating +L+ Ambient Media with Architectural Space. Summary of CHI +L+ 1998, pp. 173-174. +L+ 18. Ishii, H., Ren, S., and Frei, P. Pinwheels: visualizing +L+ information flow in an architectural space. Extended +L+ Abstracts of CHI 2001, pp. 111-112. +L+ 19. MacIntyre, B., Mynatt, E., Voida, S., Hansen, K., Tullio, J., +L+ and Corso, G. Support For Multitasking and Background +L+ Awareness Using Interactive Peripheral Displays. +L+ Proceedings of UIST 2001, pp. 41-50. +L+ 20. Mankoff, J., Dey, A., Heish, G., Kientz, J., Lederer, S., and +L+ Ames, M. Heuristic evaluation of ambient displays. +L+ Proceedings of CHI 2003, pp. 169-176. +L+ 21. McCrickard, D. S., Chewar, C., Somervell, J., and +L+ Ndiwalana, A. A Model for Notification Systems +L+ Evaluation—Assessing User Goals for Multitasking +L+ Activity. ACM Transactions on CHI 10,4 : 312 – 338. 2002 +L+ 22. McCrickard, D.S., Catrambone, R., and Stasko, J. Evaluating +L+ animation in the periphery as a mechanism for maintaining +L+ awareness. Proceedings of INTERACT 2001, pp. 148-156. +L+ 23. Matthews, T., Dey, A.., Mankoff, J., Carter S., and +L+ Rattenbury, T. A Toolkit for Managing User Attention in +L+ Peripheral Displays. Proceedings of UIST 2004, pp. 247- +L+ 256. +L+ 24. Matthews ,T., Rattenbury, T., Carter, S., Dey, A., and +L+ Mankoff, J. A Peripheral Display Toolkit. Tech Report IRB- +L+ TR-03-018. Intel Research Berkeley. 2002. +L+ 25. Mynatt, E.D., Back, M., Want, R., and Ellis, J.B. Designing +L+ audio aura. Proceedings of CHI 1998, pp. 566-573. +L+ 26. Mynatt, E.D., Rowan, J., Jacobs, A., and Craighill, S. Digital +L+ Family Portraits: Supporting Peace of Mind for Extended +L+ Family Members. Proceedings of CHI 2001, pp. 333-340. +L+ 27. My Yahoo!. http://my.yahoo.com/index.html +L+ 28. Ogden, C., and Richards I. The Meaning of Meaning. +L+ Routledge & Kegan. London, England. 1923. +L+ 29. Pederson, E. R., and Sokoler, T. AROMA: Abstract +L+ Representation of Presence Supporting Mutual Awareness. +L+ Proceedings of CHI 1997, pp.51-58. +L+ 30. Redstrom, J., Skog, T., and Hallanas, L. Informative Art: +L+ Using Amplified Artworks as Information Displays. +L+ Proceedings of DARE 2000, pp. 103-114. +L+ 31. Russel, D., Streitz, N., and Winograd, T. Building +L+ Disappearing Computers. Communications of the ACM. +L+ 48(3):42-48. 2005. +L+ 32. Stasko, J., Miller, T., Pousman Z., Plaue, C., and Ullah, O. +L+ Personalized Peripheral Information Awareness through +L+ Information Art. Proceedings of UbiComp 2004, pp. 18-35. +L+ 33. Streitz, N., Tandler, P., Muller-Tomfelde, C., and Konomi, +L+ S. Roomware: Towards the Next Generation of Human- +L+ Computer Interaction based on an Integrated Design of Real +L+ and Virtual Worlds. In: J. Carroll (Ed.): Human-Computer +L+ Interaction in the New Millennium, Addison-Wesley. pp. +L+ 553-578. 2001. +L+ 34. Van Dantzich, M., Robbins, D., Horvitz, E., and Czerwinski, +L+ M. Scope: Providing Awareness of Multiple Notifications at +L+ a Glance. Proceedings of AVI 2002. pp. 157-166. +L+ 35. Weiser, M. and Brown, J.S. Designing Calm Technology. +L+ PowerGrid Journal, 1:1, 1996. +L+ 36. Zhao, A., and Stasko, J. What's Happening?: Promoting +L+ Community Awareness through Opportunistic, Peripheral +L+ Interfaces. Proceedings of AVI 2002, pp. 69-74. +L+ </SectLabel_reference> <SectLabel_page> 74 +L+ </SectLabel_page>
<SectLabel_title> A Two-Phase Sampling Technique for Information +L+ Extraction from Hidden Web Databases +L+ </SectLabel_title> <SectLabel_author> Y.L. Hedley, M. Younas, A. James +L+ </SectLabel_author> <SectLabel_affiliation> School of Mathematical and Information Sciences +L+ Coventry University, Coventry CV1 5FB, UK +L+ </SectLabel_affiliation> <SectLabel_email> {y.hedley, m.younas, a.james}@coventry.ac.uk +L+ ABSTRACT +L+ </SectLabel_email> <SectLabel_bodyText> Hidden Web databases maintain a collection of specialised +L+ documents, which are dynamically generated in response to users’ +L+ queries. However, the documents are generated by Web page +L+ templates, which contain information that is irrelevant to queries. +L+ This paper presents a Two-Phase Sampling (2PS) technique that +L+ detects templates and extracts query-related information from the +L+ sampled documents of a database. In the first phase, 2PS queries +L+ databases with terms contained in their search interface pages and +L+ the subsequently sampled documents. This process retrieves a +L+ required number of documents. In the second phase, 2PS detects +L+ Web page templates in the sampled documents in order to extract +L+ information relevant to queries. We test 2PS on a number of real- +L+ world Hidden Web databases. Experimental results demonstrate +L+ that 2PS effectively eliminates irrelevant information contained in +L+ Web page templates and generates terms and frequencies with +L+ improved accuracy. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.3.5 [Information Storage and Retrieval]: Online Information +L+ Services – Web-based services. +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Algorithms, Experimentation. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Hidden Web Databases, Document Sampling, Information +L+ Extraction. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> An increasing number of databases on the Web maintain a +L+ collection of documents such as archives, user manuals or news +L+ articles. These databases dynamically generate documents in +L+ response to users’ queries and are referred to as Hidden Web +L+ databases [5]. As the number of databases proliferates, it has +L+ become prohibitive for specialised search services (such as +L+ search.com) to evaluate databases individually in order to answer +L+ users’ queries. +L+ Current techniques such as database selection and categorisation +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> WIDM’04, November 12–13, 2004, Washington, DC, USA. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2004 ACM 1-58113-978-0/04/0011...$5.00. +L+ M. Sanderson +L+ </SectLabel_copyright> <SectLabel_affiliation> Department of Information Studies +L+ University of Sheffield, Sheffield, S1 4DP, UK +L+ </SectLabel_affiliation> <SectLabel_email> m.sanderson@sheffield.ac.uk +L+ </SectLabel_email> <SectLabel_bodyText> have been employed to enhance the effectiveness of information +L+ retrieval from databases [2, 5, 10, 11, 15]. In the domain of the +L+ Hidden Web, knowledge about the contents of databases is often +L+ unavailable. Existing approaches such as in [2, 10, 15] acquire +L+ knowledge through sampling documents from databases. For +L+ instance, query-based sampling [2] queries databases with terms +L+ that are randomly selected from those contained in the sampled +L+ documents. The techniques in [10, 15] sample databases with +L+ terms obtained from Web logs to retrieve additional topic terms. +L+ A major issue associated with existing techniques is that they also +L+ extract information irrelevant to queries. That is, information +L+ extracted is often found in Web page templates, which contain +L+ navigation panels, search interfaces and advertisements. +L+ Consequently, the accuracy of terms and frequencies generated +L+ from sampled documents has been reduced. +L+ In addition, approximate string matching techniques are adopted +L+ by [13] to extract information from Web pages, but this approach +L+ is limited to textual contents only. Alternatively, the approaches +L+ proposed in [3, 4] analyse Web pages in tree-like structures. +L+ However, such an approach requires Web pages with well- +L+ conformed HTML tag trees. Furthermore, [3] discovers +L+ dynamically generated objects from Web pages, which are +L+ clustered into groups of similar structured pages based on a set of +L+ pre-defined templates, such as exception page templates and +L+ result page templates. +L+ In this paper, we propose a sampling and extraction technique, +L+ which is referred to as Two-Phase Sampling (2PS). 2PS aims to +L+ extract information relevant to queries in order to acquire +L+ information contents of underlying databases. Our technique is +L+ applied in two phases. First, it randomly selects a term from those +L+ found in the search interface pages of a database to initiate the +L+ process of sampling documents. Subsequently, 2PS queries the +L+ database with terms randomly selected from those contained in +L+ the sampled documents. Second, 2PS detects Web page templates +L+ and extracts query-related information from which terms and +L+ frequencies are generated to summarise the database contents. +L+ Our approach utilises information contained in search interface +L+ pages of a database to initiate the sampling process. This differs +L+ from current sampling techniques such as query-based sampling, +L+ which performs an initial query with a frequently used term. +L+ Furthermore, 2PS extracts terms that are relevant to queries thus +L+ generating statistics (i.e., terms and frequencies) that represent +L+ database contents with improved accuracy. By contrast, the +L+ approaches in [2, 10, 15] extract all terms from sampled +L+ documents, including those contained in Web page templates. +L+ Consequently, information that is irrelevant to queries is also +L+ extracted. +L+ </SectLabel_bodyText> <SectLabel_page> 1 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1. The Two-Phase Sampling (2PS) technique. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> 2PS is implemented as a prototype system and tested on a number +L+ of real-world Hidden Web databases, which contain computer +L+ manuals, healthcare archives and news articles. Experimental +L+ results show that our technique effectively detects Web page +L+ templates and generates terms and frequencies (from sampled +L+ documents) that are relevant to the queries. +L+ The remainder of the paper is organised as follows. Section 2 +L+ introduces current approaches to the discovery of information +L+ contents of Hidden Web databases. Related work on the +L+ information extraction from Web pages or dynamically generated +L+ documents is also discussed. Section 3 describes the proposed +L+ 2PS technique. Section 4 presents experimental results. Section 5 +L+ concludes the paper. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A major area of current research into the information retrieval of +L+ Hidden Web databases focuses on the automatic discovery of +L+ information contents of databases, in order to facilitate their +L+ selection or categorisation. For instance, the technique proposed +L+ in [6] analyses the hyperlink structures of databases in order to +L+ facilitate the search for databases that are similar in content. The +L+ approach adopted by [10, 15] examines the textual contents of +L+ search interface pages maintained by data sources to gather +L+ information about database contents. +L+ A different approach is to retrieve actual documents to acquire +L+ such information. However, in the domain of Hidden Web +L+ databases, it is difficult to obtain all documents from a database. +L+ Therefore, a number of research studies [2, 10, 15] obtain +L+ information by retrieving a set of documents through sampling. +L+ For instance, query-based sampling [2] queries databases with +L+ terms that are randomly selected from those contained in the +L+ sampled documents. The techniques in [10, 15] sample databases +L+ with terms extracted from Web logs to obtain additional topic +L+ terms. These techniques generate terms and frequencies from +L+ sampled documents, which are referred to as Language Models +L+ [2], Textual Models [10, 15] or Centroids [11]. +L+ A key issue associated with the aforementioned sampling +L+ techniques is that they extract information that is often irrelevant +L+ to queries, since information contained in Web page templates +L+ such as navigation panels, search interfaces and advertisements is +L+ also extracted. For example, a language model generated from the +L+ sampled documents of the Combined Health Information +L+ Database (CHID) contains terms (such as ‘author’ and ‘format’) +L+ with high frequencies. These terms are not relevant to queries but +L+ are used for descriptive purposes. Consequently, the accuracy of +L+ terms and frequencies generated from sampled documents has +L+ been reduced. The use of additional stop-word lists has been +L+ considered in [2] to eliminate irrelevant terms - but it is +L+ maintained that such a technique can be difficult to apply in +L+ practice. +L+ Existing techniques in information extraction from Web pages are +L+ of varying degrees of complexity. For instance, approximate +L+ string matching techniques are adopted by [13] to extract texts +L+ that are different. This approach is limited to finding textual +L+ similarities and differences. The approaches proposed in [3, 4] +L+ analyse textual contents and tag structures in order to extract data +L+ from Web pages. However, such an approach requires Web pages +L+ that are produced with well-conformed HTML tag-trees. +L+ Computation is also needed to convert and analyse Web pages in +L+ a tree-like structure. Moreover, [3] identifies Web page templates +L+ based on a number of pre-defined templates, such as exception +L+ page templates and result page templates. +L+ Our technique examines Web documents based on textual +L+ contents and the neighbouring tag structures rather than analysing +L+ their contents in a tree-like structure. We also detect information +L+ contained in different templates through which documents are +L+ generated. Therefore, it is not restricted to a pre-defined set of +L+ page templates. +L+ Furthermore, we focus on databases that contain documents such +L+ as archives and new articles. A distinct characteristic of +L+ documents found in such a domain is that the content of a +L+ document is often accompanied by other information for +L+ supplementary or navigation purposes. The proposed 2PS +L+ technique detects and eliminates information contained in +L+ templates in order to extract the content of a document. This +L+ differs from the approaches in [1, 4], which attempt to extract a +L+ set of data from Web pages presented in a particular pattern. For +L+ example, the Web pages of a bookstore Web site contain +L+ information about authors followed by their associated list of +L+ publications. However, in the domain of document databases, +L+ information contained in dynamically generated Web pages is +L+ often presented in a structured fashion but irrelevant to queries. +L+ Other research studies [9, 8, 12] are specifically associated with +L+ the extraction of data from query forms in order to further the +L+ retrieval of information from the underlying databases. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. TWO-PHASE SAMPLING +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This section presents the proposed technique for extracting +L+ information from Hidden Web document databases in two phases, +L+ which we refer to as Two-Phase Sampling (2PS). Figure 1 depicts +L+ the process of sampling a database and extracting query-related +L+ </SectLabel_bodyText> <SectLabel_page> 2 +L+ </SectLabel_page> <SectLabel_bodyText> information from the sampled documents. In phase one, 2PS +L+ obtains randomly sampled documents. In phase two, it detects +L+ Web page templates. This extracts information relevant to the +L+ queries and then generates terms and frequencies to summarise +L+ the database content. The two phases are detailed in section 3.1 +L+ and 3.2. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Phase One: Document Sampling +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the first phase we initiate the process of sampling documents +L+ from a database with a randomly selected term from those +L+ contained in the search interface pages of the database. This +L+ retrieves top N documents where N represents the number of +L+ documents that are the most relevant to the query. A subsequent +L+ query term is then randomly selected from terms extracted from +L+ the sampled documents. This process is repeated until a required +L+ number of documents are sampled. The sampled documents are +L+ stored locally for further analysis. +L+ Figure 2 illustrates the algorithm that obtains a number of +L+ randomly sampled documents. tq denotes a term extracted from +L+ the search interface pages of a database, D. qtp represents a query +L+ term selected from a collection of terms, Q, qtp e Q, 1 <_ p <_ m; +L+ where m is the distinct number of terms extracted from the search +L+ interface pages and the documents that have been sampled. R +L+ represents the set of documents randomly sampled from D. tr is a +L+ term extracted from di. di represents a sampled document from D, +L+ di e D, 1 <_ i <_ n, where n is the number of document to sample. +L+ </SectLabel_bodyText> <SectLabel_figure> Algorithm SampleDocument +L+ Extract tq from search interface pages of D, Q = tq +L+ For i = 1 to n +L+ Randomly select qtp from Q +L+ If (qtp has not been selected previously) +L+ Execute the query with qtp on D +L+ j = 0 +L+ While j <= N +L+ If (di o R) +L+ Retrieve di from D +L+ Extract tr from di, +L+ R = di +L+ Q = tr +L+ Increase j by 1 +L+ End if +L+ End while +L+ End if +L+ End for +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2. The algorithm for sampling documents from a +L+ database. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> 2PS differs from query-based sampling in terms of selecting an +L+ initial query. The latter selects an initial term from a list of +L+ frequently used terms. 2PS initiates the sampling process with a +L+ term randomly selected from those contained in the search +L+ interface pages of the database. This utilises a source of +L+ information that is closely related to its content. Moreover, 2PS +L+ analyses the sampled documents in the second phase in order to +L+ extract query-related information. By contrast, query-based +L+ sampling does not analyse their contents to determine whether +L+ terms are relevant to queries. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Phase Two: Document Content Extraction +L+ and Summarisation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The documents sampled from the first phase are further analysed +L+ in order to extract information relevant to the queries. This is then +L+ followed by the generation of terms and frequencies to represent +L+ the content of the underlying database. This phase is carried out +L+ through the following processes. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 3.2.1 Generate Document Content Representations +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The content of each sampled document is converted into a list of +L+ text and tag segments. Tag segments include start tags, end tags +L+ and single tags specified in HyperText Markup Language +L+ (HTML). Text segments are text that resides between two tag +L+ segments. The document content is then represented by text +L+ segments and their neighbouring tag segments, which we refer to +L+ as Text with Neighbouring Adjacent Tag Segments (TNATS). The +L+ neighbouring adjacent tag segments of a text segment are defined +L+ as the list of tag segments that are located immediately before and +L+ after the text segment until another text segment is reached. The +L+ neighbouring tag segments of a text segment describe how the +L+ text segment is structured and its relation to the nearest text +L+ segments. Assume that a document contains n segments, a text +L+ segment, txs, is defined as: txs = (txi, tg-lstj, tg-lstk), where txi is +L+ the textual content of the ith text segment, 1 <_ i <_ n; tg-lstj +L+ represents p tag segments located before txi and tg-lstk represents +L+ q tag segments located after txi until another text segment is +L+ </SectLabel_bodyText> <SectLabel_equation> reached. tg-lstj = (tg1, ..., tgp), 1 <_ j <_ p and tg-lstk = (tg1, ..., tgq), +L+ 1 <_ k <_ q. +L+ </SectLabel_equation> <SectLabel_figureCaption> Figure 3. A template-generated document from CHID. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Figure 3 shows a template-generated document retrieved from the +L+ CHID database. The source code for this document is given in +L+ Figure 4. For example, text segment, ‘1. Equipos Mas Seguros: +L+ Si Te Inyectas Drogas.’, can be identified by the text (i.e., ‘1. +L+ Equipos Mas Seguros: Si Te Inyectas Drogas.’) and its +L+ neighbouring tag segments. These include the list of tags located +L+ before the text (i.e., </TITLE>, </HEAD>, <BODY>, <HR>, +L+ <H3>, <B> and <I>) and the neighbouring tags located after the +L+ text (i.e., </I>, </B>, </H3>, <I> and <B>). Thus, this segment is +L+ then represented as (‘1. Equipos Mas Seguros: Si Te Inyectas +L+ Drogas.’, (</TITLE>, </HEAD>, <BODY>, <HR>, <H3>, <B> +L+ ,<I>), (</I>, </B>, </H3>, <I>, <B>)). Figure 5 shows the content +L+ </SectLabel_bodyText> <SectLabel_page> 3 +L+ </SectLabel_page> <SectLabel_bodyText> representation of the CHID document (given in Figure 3) +L+ generated based on TNATS. Given a sampled document, d, with n +L+ text segments, the content of d is then represented as: Content(d) +L+ = {txs1, ..., txsn}, where txsi represents a text segment, 1 <_ i <_ n. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4. The source code for the CHID document. +L+ Figure 5. The content representation of the CHID document +L+ using TNATS. +L+ </SectLabel_figureCaption> <SectLabel_subsubsectionHeader> 3.2.2 Detect Templates +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In the domain of Hidden Web databases, documents are often +L+ presented to users through one or more templates. Templates are +L+ typically employed in order to describe document contents or to +L+ assist users in navigation. For example, information contained in +L+ the document (as shown in Figure 3) can be classified into the two +L+ following categories: +L+ </SectLabel_bodyText> <SectLabel_listItem> (i) Template-Generated Information. This includes information +L+ such as navigation panels, search interfaces and +L+ advertisements. In addition, information may be given to +L+ describe the content of a document. Such information is +L+ irrelevant to a user’s query. For example, navigation links +L+ (such as ‘Next Doc’ and ‘Last Doc’) and headings (such +L+ ‘Subfile’ and ‘Format’) are found in the document. +L+ (ii) Query-Related Information. This information is retrieved in +L+ response to a user’s query, i.e., ‘1. Equipos Mas Seguros: +L+ Si Te Inyectas Drogas. ...’. +L+ </SectLabel_listItem> <SectLabel_bodyText> The 2PS technique detects Web page templates employed by +L+ databases to generate documents in order to extract information +L+ that is relevant to queries. Figure 6 describes the algorithm that +L+ detects information contained in Web page templates from n +L+ sampled documents. di represents a sampled document from the +L+ database D, di, e D, 1 <_ i <_ n. Content(di) denotes the content +L+ representation of di. +L+ </SectLabel_bodyText> <SectLabel_figure> Algorithm DetectTemplate +L+ For i = 1 to n +L+ If T = 0 +L+ If S = 0 +L+ S = di +L+ Else if S ;t� 0 +L+ While l <= s AND T = 0 +L+ Compare (Content(di),Content(dl)) +L+ If Content(di) = Content(dl) +L+ wptk = Content(di) n Content(dl), +L+ Store wptk, T = wptk +L+ Delete (Content(di) n Content(dl)) from +L+ Content(di), Content(dl) +L+ Gk = di, Gk = dl +L+ Delete dl from S +L+ End if +L+ End while +L+ If T = 0 +L+ S = di +L+ End if +L+ End if +L+ Else if T ;t� 0 +L+ While k <= r AND di o Gk +L+ Compare (Content(wptk), Content(di)) +L+ If Content(wptk) = Content(di) +L+ Delete (Content(wptk) n Content(di)) from +L+ Content(di) +L+ Gk = di +L+ End if +L+ End while +L+ If S ;t� 0 AND di o Gk +L+ While l <= s AND di o Gk +L+ Compare (Content(di),Content(dl)) +L+ If Content(di) = Content(dl) +L+ wptk = Content(di) n Content(dl) +L+ Store wptk, T = wptk +L+ Delete (Content(di) n Content(dl)) from +L+ Content(di), Content(dl) +L+ Gk = di, Gk = dl +L+ Delete dl from S +L+ End if +L+ End while +L+ End if +L+ If di o Gk +L+ S = di +L+ End if +L+ End if +L+ End for +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6. The algorithm for detecting and eliminating the +L+ information contained in Web page templates. +L+ </SectLabel_figureCaption> <SectLabel_figure> ... +L+ <HTML><HEAD><TITLE>CHID Document +L+ </TITLE></HEAD> +L+ <BODY> +L+ <HR><H3><B><I> 1. Equipos Mas Seguros: Si Te Inyectas +L+ Drogas. +L+ </I></B></H3> +L+ <I><B>Subfile: </B></I> +L+ AIDS Education<BR> +L+ <I><B>Format (FM): </B></I> +L+ 08 - Brochure. +L+ <BR> +L+ ... +L+ ... +L+ ‘CHID Document’, (<HTML>, <HEAD>, <TITLE>), +L+ (</TITLE>, </HEAD>, <BODY>, <HR>, <H3>, <B>, +L+ <I>); +L+ ‘1. Equipos Mas Seguros: Si Te Inyectas Drogas.’, +L+ (</TITLE>, </HEAD>, <BODY>, <HR>, <H3>, <B>, +L+ <I>), (</I>, </B>, </H3>, <I>, <B>); +L+ ‘Subfile:’, (</I>, </B>, </H3>, <I>, <B>), (</B>, </I>); +L+ ‘AIDS Education’, (</B>, </I>), (<BR>, <I>, <B>); +L+ ‘Format (FM):’, (<BR>, <I>, <B>), (</B>, </I>); +L+ ... +L+ </SectLabel_figure> <SectLabel_page> 4 +L+ </SectLabel_page> <SectLabel_bodyText> Similar to the representation for the contents of sampled +L+ documents, the content of a Web page template, wpt, is +L+ represented as Content(wpt) = {txs1, ..., txsq}, where q is the +L+ number of text segments, txsj, 1 ≤ j ≤ q. T represents a set of +L+ templates detected. T = {wpt1, ..., wptr}, where r is the distinct +L+ number of templates, wptk, 1 ≤ k ≤ r. Gk represents a group of +L+ documents generated from wptk. Furthermore, S represents the +L+ sampled documents from which no templates have yet been +L+ detected. Thus, S = {d1, ..., ds}, where s is the number of +L+ temporarily stored document, dl, 1 ≤ l ≤ s. +L+ The process of detecting templates is executed until all sampled +L+ documents are analysed. This results in the identification of one +L+ or more templates. For each template, two or more documents are +L+ assigned to a group associated with the template from which the +L+ documents are generated. Each document contains text segments +L+ that are not found in their respective template. These text +L+ segments are partially related to their queries. In addition to a set +L+ of templates, the content representations of zero or more +L+ documents in which no matched patterns are found are stored. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 3.2.3 Extract Query-Related Information +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> This process analyses a group of documents associated with each +L+ template from which documents are generated. It further identifies +L+ any repeated patterns from the remaining text segments of the +L+ documents in order to extract query-related information. +L+ We compute cosine similarity [14] given in (1) to determine the +L+ similarities between the text segments of different documents that +L+ are associated the template where the documents are generated. +L+ The textual content of each text segment is represented as a vector +L+ of terms with weights. The weight of a term is obtained by its +L+ occurrence in the segment. +L+ from the document content (given in Figure 4) as a result of +L+ eliminating information contained in the Web page template. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 3.2.4 Generate Content Summary +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Frequencies are computed for the terms extracted from randomly +L+ sampled documents. These summarise the information content of +L+ a database, which we refer to as Content Summary. +L+ </SectLabel_bodyText> <SectLabel_figure> Algorithm ExtractQueryInfo +L+ For each (da ∈ Gk) +L+ For each (db ∈ Gk), da ≠ db +L+ Compare (Content(da),Content(db)) +L+ If Content(da) = Content(db) +L+ Delete (Content(da) ∩ Content(db)) from +L+ Content(da), Content(db) +L+ End if +L+ End for +L+ End for +L+ For each (di ∈ Gk) +L+ Extract txm of txsm from Content(di) +L+ End for +L+ For each (dl ∈ S) +L+ Extract txn of txsn from Content(dl) +L+ End for +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 7. The algorithm for extracting query-related +L+ information from template-generated documents. +L+ </SectLabel_figureCaption> <SectLabel_figure> 1. Equipos Mas Seguros: Si Te Inyectas Drogas. +L+ AIDS Education +L+ ... +L+ </SectLabel_figure> <SectLabel_equation> t +L+ (	,	)	(	)	(	) 2	(	) +L+ txs txs	tw tw	tw	tw +L+ i	j	ik	jk	ik +L+ =	∗	∗ +L+ ∑	∑	∑ jk +L+ k=1	k=1 +L+ </SectLabel_equation> <SectLabel_figureCaption> (1)	Figure 8. The query-related information extracted from the +L+ CHID document. +L+ </SectLabel_figureCaption> <SectLabel_equation> COSINE +L+ t +L+ t +L+ 1 +L+ = +L+ k +L+ 2 +L+ . +L+ </SectLabel_equation> <SectLabel_bodyText> where txsi and txsj represent two text segments in a document; twik +L+ is the weight of term k in txsi, and twjk is the weight of term k in +L+ txsj . This is only applied to text segments with identical adjacent +L+ tag segments. Two segments are considered to be similar if their +L+ similarity exceeds a threshold value. The threshold value is +L+ determined experimentally. +L+ The algorithm that extracts information relevant to queries is +L+ illustrated in Figure 7. da and db represent the sampled documents +L+ from the database, D, da, db ∈ Gk, where Gk denotes a group of +L+ documents associated with the template, wptk, from which the +L+ documents are generated. txm represents the textual content of a +L+ text segment, txsm, contained in di, di ∈ Gk. txn represents the +L+ textual content of a text segment, txsn, contained in dl, dl ∈ S. S +L+ represents the sampled documents from which no templates are +L+ detected. +L+ The results of the above algorithm extract text segments with +L+ different tag structures. It also extracts text segments that have +L+ identical adjacent tag structures but are significantly different in +L+ their textual contents. Figure 8 shows the information extracted +L+ Previous experiments in [2] demonstrate that a number of +L+ randomly sampled documents (i.e., 300 documents) sufficiently +L+ represent the information content of a database. +L+ In the domain of Hidden Web databases, the inverse document +L+ frequency (idf), used in traditional information retrieval, is not +L+ applicable, since the total number of documents in a database is +L+ often unknown. Therefore, document frequency (df), collection +L+ term frequency (ctf) and average term frequency (avg_tf) initially +L+ used in [2] are applied in this paper. We consider the following +L+ frequencies to compute the content summary of a Hidden Web +L+ database. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Document frequency (df): the number of documents in the +L+ </SectLabel_listItem> <SectLabel_bodyText> collection of documents sampled that contain term t, +L+ where d is the document and f is the frequency +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Collection term frequency (ctf): the occurrence of a term +L+ </SectLabel_listItem> <SectLabel_bodyText> in the collection of documents sampled, where c is the +L+ collection, t is the term and f is the frequency +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Average term frequency (avg_tf): the average frequency +L+ </SectLabel_listItem> <SectLabel_bodyText> of a term obtained from dividing collection term +L+ frequency by document frequency (i.e., avg_tf = ctf / df) +L+ </SectLabel_bodyText> <SectLabel_page> 5 +L+ </SectLabel_page> <SectLabel_tableCaption> Table 1. 3 Hidden Web databases used in the experiments +L+ </SectLabel_tableCaption> <SectLabel_table> Database	URL	Subject	Content	Template +L+ Help Site	www.help-site.com	Computer manuals	Homogeneous	Multiple templates +L+ CHID	www.chid.nih.gov	Healthcare articles	Homogeneous	Single template +L+ Wired News	www.wired.com	General news articles	Heterogeneous	Single template +L+ </SectLabel_table> <SectLabel_bodyText> The content summary of a document database is defined as +L+ follows. Assume that a Hidden Web database, D, is sampled with +L+ N documents. Each sampled document, d, is represented as a +L+ vector of terms and their associated weights [14]. Thus d = (w1, +L+ ..., wm), where wi is the weight of term ti, and m is the number of +L+ distinct terms in d e D, 1 <_ i ≤ m. Each wi is computed using term +L+ frequency metric, avg_tf (i. e., wi = ctfi/dfi). The content summary +L+ is then denoted as CS(D), which is generated from the vectors of +L+ sampled documents. Assume that n is the number of distinct terms +L+ in all sampled documents. CS(D) is, therefore, expressed as a +L+ vector of terms: CS(D)= {w1, ..., wn}, where wi is computed by +L+ adding the weights of ti in the documents sampled from D and +L+ dividing the sum by the number of sampled documents that +L+ contain ti, 1 <_ i ≤ n. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. EXPERIMENTAL RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This section reports on a number of experiments conducted to +L+ assess the effectiveness of the 2PS technique in terms of: (i) +L+ detecting Web page templates, and (ii) extracting relevant +L+ information from the documents of a Hidden Web databases +L+ through sampling. The experimental results are compared with +L+ those from query-based sampling (abbreviated as QS). We +L+ compare 2PS with QS as it is a well-established technique and has +L+ also been widely adopted by other relevant studies [5, 10, 11, 15]. +L+ Experiments are carried out on three real-world Hidden Web +L+ document databases including Help Site, CHID and Wired News, +L+ which provide information about user manuals, healthcare +L+ archives and news articles, respectively. Table 1 summarises +L+ these databases in terms of their subjects, contents and templates +L+ employed. For instance, Help Site and CHID contain documents +L+ relating to subjects on computing and healthcare, respectively. +L+ Their information contents are homogeneous in nature. By +L+ contrast, Wired News contains articles that relate to different +L+ subjects of interest. +L+ Where the number of templates is concerned, CHID and Wired +L+ News generate documents from one Web page template. Help +L+ Site maintains a collection of documents produced by other +L+ information sources. Subsequently, different Web page templates +L+ are found in Help Site sampled documents. +L+ The experiment conducted using QS initiates the first query to a +L+ database with a frequently used term to obtain a set of sampled +L+ documents. Subsequent query terms are randomly selected from +L+ those contained in the sampled documents. It extracts terms +L+ (including terms contained in Web page templates) and updates +L+ the frequencies after each document is sampled. By contrast, 2PS +L+ initiates the sampling process with a term contained in the search +L+ interface pages of a database. In addition, 2PS analyses the +L+ sampled documents in the second phase in order to extract query- +L+ related information, from which terms and frequencies are +L+ generated. +L+ Experimental results in [2] conclude that QS obtains +L+ approximately 80% of terms from a database, when 300 +L+ documents are sampled and top 4 documents are retrieved for +L+ each query. These two parameters are used to obtain results for +L+ our experiments in which terms and frequencies are generated for +L+ QS and 2PS after 300 documents have been sampled. The results +L+ generated from QS provide the baseline for the experiments. +L+ Three sets of samples are obtained for each database and 300 +L+ documents are retrieved for each sample. First, we manually +L+ examine each set of sampled documents to obtain the number of +L+ Web page templates used to generate the documents. This is then +L+ compared with the number of templates detected by 2PS. The +L+ detection of Web page templates from the sampled documents is +L+ important as this determines whether irrelevant information is +L+ effectively eliminated. +L+ Next, we compare the number of relevant terms (from top 50 +L+ terms) retrieved using 2PS with the number obtained by QS. +L+ Terms are ranked according to their ctf frequencies to determine +L+ their relevancy to the queries. This frequency represents the +L+ occurrences of a term contained in the sampled documents. Ctf +L+ frequencies are used to demonstrate the effectiveness of +L+ extracting query-related information from sampled documents +L+ since the terms extracted from Web page templates are often +L+ ranked with high ctf frequencies. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 2. The number of templates employed by databases and +L+ </SectLabel_tableCaption> <SectLabel_table> the number detected by 2PS +L+ Databases		Number of templates +L+ 		Employed	Detected +L+ Help Site	Sample 1	17	15 +L+ 	Sample 2	17	16 +L+ 	Sample 3	19	17 +L+ CHID	Sample 1	1	1 +L+ 	Sample 2	1	1 +L+ 	Sample 3	1	1 +L+ Wired News	Sample 1	1	1 +L+ 	Sample 2	1	1 +L+ 	Sample 3	1	1 +L+ </SectLabel_table> <SectLabel_bodyText> Experimental results for QS and 2PS are summarised as follows. +L+ Firstly, Table 2 gives the number of Web page templates +L+ employed by the databases and the number detected by 2PS. It +L+ shows that 2PS effectively identifies the number of templates +L+ found in the sampled documents. However, a small number of +L+ templates are not detected from Help Site. For instance, 2PS does +L+ not detect two of the templates from the first set of sampled +L+ documents, since the two templates are very similar in terms of +L+ content and structure. +L+ </SectLabel_bodyText> <SectLabel_page> 6 +L+ </SectLabel_page> <SectLabel_bodyText> Table 3 summarises the number of relevant terms (from top 50 +L+ terms ranked according to their ctf frequencies) obtained for the +L+ three databases. These terms are retrieved using 2PS and QS. We +L+ determine the relevancy of a term by examining whether the term +L+ is found in Web page templates. Table 3 gives the number of +L+ retrieved terms that do not appear in Web page templates. The +L+ results show that 2PS obtains more relevant terms. For instance, +L+ in the first set of documents sampled from CHID using 2PS, the +L+ number of relevant terms retrieved is 47. By comparison, the +L+ number of terms obtained for QS is 20. +L+ The results generated from CHID and Wired News demonstrate +L+ that 2PS retrieves more relevant terms, as a large number of terms +L+ contained in the templates have been successfully eliminated from +L+ the top 50 terms. However, the elimination of template terms is +L+ less noticeable for Help Site. Our observation is that template +L+ terms attain high frequencies since the CHID and Wired News +L+ databases generate documents using a single Web page template. +L+ By comparison, a larger number of Web page templates are found +L+ in the documents sampled from Help Site. As a result, terms +L+ contained in the templates do not attain high frequencies as those +L+ found in the templates employed by CHID and Wired News. +L+ Table 4 and 5 show the results of the top 50 terms ranked +L+ according to their ctf frequencies retrieved from the first set of +L+ sampled documents of the CHID database. Table 4 shows the top +L+ 50 terms retrieved for QS whereby terms contained in Web page +L+ templates are not excluded. As a result, a number of terms (such +L+ as ‘author’, ‘language’ and ‘format’) have attained much higher +L+ frequencies. By contrast, Table 5 lists the top 50 terms retrieved +L+ using 2PS. Our technique eliminates terms (such as ‘author’ and +L+ ‘format’) and obtains terms (such as ‘treatment’, ‘disease’ and +L+ ‘immunodeficiency’) in the higher rank. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 3. The number of relevant terms retrieved (from top 50 +L+ terms) according to ctf frequencies +L+ </SectLabel_tableCaption> <SectLabel_table> Databases		Number of relevant terms +L+ 		QS	2PS +L+ Help Site	Sample 1	46	48 +L+ 	Sample 2	47	48 +L+ 	Sample 3	46	48 +L+ CHID	Sample 1	20	47 +L+ 	Sample 2	19	47 +L+ 	Sample 3	20	47 +L+ Wired News	Sample 1	14	42 +L+ 	Sample 2	10	43 +L+ 	Sample 3	11	39 +L+ </SectLabel_table> <SectLabel_sectionHeader> 5. CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper presents a sampling and extraction technique, 2PS, +L+ which utilises information that is contained in the search interface +L+ pages and documents of a database in the sampling process. This +L+ technique extracts information relevant to queries from the +L+ sampled documents in order to generate terms and frequencies +L+ with improved accuracy. Experimental results demonstrate that +L+ our technique effectively eliminates information contained in +L+ Web page templates, thus attaining terms and frequencies that are +L+ of a higher degree of relevancy. This can also enhance the +L+ effectiveness of categorisation in which such statistics are used to +L+ represent the information contents of underlying databases. +L+ We obtain promising results by applying 2PS in the experiments +L+ on three databases that differ in nature. However, experiments on +L+ a larger number of Hidden Web databases are required in order to +L+ further assess the effectiveness of the proposed technique. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 4. Top 50 terms and frequencies ranked according to ctf generated from CHID when QS is applied +L+ </SectLabel_tableCaption> <SectLabel_table> Rank	Term	Rank	Term	Rank	Term +L+ 1	hiv	18	document	35	lg +L+ 2	aids	19	disease	36	ve +L+ 3	information	20	published	37	yr +L+ 4	health	21	physical	38	ac +L+ 5	prevention	22	subfile	39	corporate +L+ 6	education	23	audience	40	mj +L+ 7	tb	24	update	41	description +L+ 8	accession	25	verification	42	www +L+ 9	number	26	major	43	cn +L+ 10	author	27	pamphlet	44	pd +L+ 11	persons	28	chid	45	english +L+ 12	language	29	human	46	national +L+ 13	sheet	30	date	47	public +L+ 14	format	31	abstract	48	immunodeficiency +L+ 15	treatment	32	code	49	virus +L+ 16	descriptors	33	ab	50	org +L+ 17	availability	34	fm +L+ </SectLabel_table> <SectLabel_page> 7 +L+ </SectLabel_page> <SectLabel_tableCaption> Table 5. Top 50 terms and frequencies ranked according to ctf generated from CHID when 2PS is applied +L+ </SectLabel_tableCaption> <SectLabel_table> Rank	Term	Rank	Term	Rank	Term +L+ 1	hiv	18	education	35	testing +L+ 2	aids	19	virus	36	programs +L+ 3	information	20	org	37	services +L+ 4	health	21	notes	38	clinical +L+ 5	prevention	22	nt	39	people +L+ 6	tb	23	cdc	40	hepatitis +L+ 7	persons	24	service	41	community +L+ 8	sheet	25	box	42	world +L+ 9	treatment	26	research	43	listed +L+ 10	disease	27	department	44	professionals +L+ 11	human	28	positive	45	training +L+ 12	pamphlet	29	tuberculosis	46	diseases +L+ 13	www	30	control	47	accession +L+ 14	http	31	drug	48	network +L+ 15	national	32	discusses	49	general +L+ 16	public	33	ill	50	std +L+ 17	immunodeficiency	34	organizations +L+ </SectLabel_table> <SectLabel_sectionHeader> 6. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] Arasu, A. and Garcia-Molina, H. Extracting Structured Data +L+ from Web Pages. In Proceedings of the 2003 ACM SIGMOD +L+ International Conference on Management, 2003, 337-348. +L+ [2] Callan, J. and Connell, M. Query-Based Sampling of Text +L+ Databases. ACM Transactions on Information Systems +L+ (TOIS), Vol. 19, No. 2, 2001, 97-130. +L+ [3] Caverlee, J., Buttler, D. and Liu, L. Discovering Objects in +L+ Dynamically-Generated Web Pages. Technical report, +L+ Georgia Institute of Technology, 2003. +L+ [4] Crescenzi, V., Mecca, G. and Merialdo, P. ROADRUNNER: +L+ Towards Automatic Data Extraction from Large Web Sites, +L+ In Proceedings of the 27th International Conference on Very +L+ Large Data Bases (VLDB), 2001, 109-118. +L+ [5] Gravano, L., Ipeirotis, P. G. and Sahami, M. QProber: A +L+ System for Automatic Classification of Hidden-Web +L+ Databases. ACM Transactions on Information Systems +L+ (TOIS), Vol. 21, No. 1, 2003. +L+ [6] Heß, M. and Drobnik, O. Clustering Specialised Web- +L+ databases by Exploiting Hyperlinks. In Proceedings of the +L+ Second Asian Digital Library Conference, 1999. +L+ [7] Hedley, Y.L., Younas, M., James, A. and Sanderson M. +L+ Query-Related Data Extraction of Hidden Web Documents. +L+ In Proceedings of the 27th Annual International ACM SIGIR +L+ Conference, 2004, 558-559. +L+ [8] Lage, J. P., da Silva, A. S., Golgher, P. B. and Laender, A. +L+ H. F. Automatic Generation of Agents for Collecting Hidden +L+ Web Pages for Data Extraction. Data & Knowledge +L+ Engineering, Vol. 49, No. 2, 2004, 177-196. +L+ [9] Liddle, S.W., Yau, S.H. and Embley, D. W. On the +L+ Automatic Extraction of Data from the Hidden Web. In +L+ Proceedings of the 20th International Conference on +L+ Conceptual Modeling, (ER) Workshops, 2001, 212-226. +L+ [ 10] Lin, K.I. and Chen, H. Automatic Information Discovery +L+ from the Invisible Web. International Conference on +L+ Information Technology: Coding and Computing (ITCC), +L+ 2002, 332-337. +L+ [ 11 ] Meng, W., Wang, W., Sun, H. and Yu, C. Concept +L+ Hierarchy Based Text Database Categorization. +L+ International Journal on Knowledge and Information +L+ Systems, Vol. 4, No. 2, 2002, 132-150. +L+ [ 12] Raghavan, S. and Garcia-Molina, H. Crawling the Hidden +L+ Web. In Proceedings of the 27th International Conference on +L+ Very Large Databases (VLDB), 2001, 129-138. +L+ [ 13] Rahardjo, B. and Yap, R. Automatic Information Extraction +L+ from Web Pages, In Proceedings of the 24th Annual +L+ International ACM SIGIR Conference, 2001, 430-431. +L+ [ 14] Salton, G. and McGill, M. Introduction to Modern +L+ Information Retrieval. New York, McCraw-Hill, 1983. +L+ [ 15] Sugiura, A. and Etzioni, O. Query Routing for Web Search +L+ Engines: Architecture and Experiments. In Proceedings of +L+ the 9th International World Wide Web Conference: The +L+ Web: The Next Generation, 2000, 417-430. +L+ </SectLabel_reference> <SectLabel_page> 8 +L+ </SectLabel_page>
<SectLabel_title> Accelerated Focused Crawling through +L+ Online Relevance Feedback* +L+ </SectLabel_title> <SectLabel_author> Soumen Chakrabartit	Kunal Punera	Mallela Subramanyam +L+ </SectLabel_author> <SectLabel_affiliation> IIT Bombay	IIT Bombay	University of Texas, Austin +L+ </SectLabel_affiliation> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The organization of HTML into a tag tree structure, which +L+ is rendered by browsers as roughly rectangular regions with +L+ embedded text and HREF links, greatly helps surfers locate +L+ and click on links that best satisfy their information need. +L+ Can an automatic program emulate this human behavior +L+ and thereby learn to predict the relevance of an unseen +L+ HREF target page w.r.t. an information need, based on +L+ information limited to the HREF source page? Such a +L+ capability would be of great interest in focused crawling and +L+ resource discovery, because it can fine-tune the priority of +L+ unvisited URLs in the crawl frontier, and reduce the number +L+ of irrelevant pages which are fetched and discarded. +L+ We show that there is indeed a great deal of usable +L+ information on a HREF source page about the relevance +L+ of the target page. This information, encoded suitably, can +L+ be exploited by a supervised apprentice which takes online +L+ lessons from a traditional focused crawler by observing +L+ a carefully designed set of features and events associated +L+ with the crawler. Once the apprentice gets a sufficient +L+ number of examples, the crawler starts consulting it to +L+ better prioritize URLs in the crawl frontier. Experiments on +L+ a dozen topics using a 482-topic taxonomy from the Open +L+ Directory (Dmoz) show that online relevance feedback can +L+ reduce false positives by 30% to 90%. +L+ </SectLabel_bodyText> <SectLabel_category> Categories and subject descriptors: +L+ H.5.4 [Information interfaces and presentation]: +L+ Hypertext/hypermedia; I.5.4 [Pattern recognition]: +L+ Applications, Text processing; I.2.6 [Artificial +L+ intelligence]: Learning; I.2.8 [Artificial intelligence]: +L+ Problem Solving, Control Methods, and Search. +L+ </SectLabel_category> <SectLabel_keyword> General terms: Algorithms, performance, +L+ measurements, experimentation. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords: Focused crawling, Document object model, +L+ Reinforcement learning. +L+ 1 Introduction +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Keyword search and clicking on links are the dominant +L+ modes of accessing hypertext on the Web. Support for +L+ keyword search through crawlers and search engines is very +L+ mature, but the surfing paradigm is not modeled or assisted +L+ </SectLabel_bodyText> <SectLabel_footnote> *(Note: The HTML version of this paper is best viewed using +L+ Microsoft Internet Explorer. To view the HTML version using +L+ Netscape, add the following line to your ~/.Xdefaults or +L+ ~/.Xresources file: +L+ Netscape*documentFonts.charset*adobe-fontspecific: iso-8859-1 +L+ For printing use the PDF version, as browsers may not print the +L+ mathematics properly.) +L+ tContact author, email soumen@cse.iitb.ac.in +L+ </SectLabel_footnote> <SectLabel_copyright> Copyright is held by the author/owner(s). +L+ </SectLabel_copyright> <SectLabel_note> WWW2002, May 7–11, 2002, Honolulu, Hawaii, USA. +L+ ACM 1-58113-449-5/02/0005 +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 1: A basic focused crawler controlled by one topic +L+ classifier/learner. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> as well. Support for surfing is limited to the basic interface +L+ provided by Web browsers, except for a few notable research +L+ prototypes. +L+ While surfing, the user typically has a topic-specific +L+ information need, and explores out from a few known +L+ relevant starting points in the Web graph (which may be +L+ query responses) to seek new pages relevant to the chosen +L+ topic/s. While deciding for or against clicking on a specific +L+ link (u, v), humans use a variety of clues on the source +L+ page u to estimate the worth of the (unseen) target page +L+ v, including the tag tree structure of u, text embedded in +L+ various regions of that tag tree, and whether the link is +L+ relative or remote. “Every click on a link is a leap of faith” +L+ [19], but humans are very good at discriminating between +L+ links based on these clues. +L+ Making an educated guess about the worth of clicking +L+ on a link (u, v) without knowledge of the target v is +L+ central to the surfing activity. Automatic programs which +L+ can learn this capability would be valuable for a number +L+ of applications which can be broadly characterized as +L+ personalized, topic-specific information foragers. +L+ Large-scale, topic-specific information gatherers are +L+ called focused crawlers [1, 9, 14, 28, 30]. In contrast to giant, +L+ all-purpose crawlers which must process large portions of +L+ the Web in a centralized manner, a distributed federation of +L+ focused crawlers can cover specialized topics in more depth +L+ and keep the crawl more fresh, because there is less to cover +L+ for each crawler. +L+ In its simplest form, a focused crawler consists of a +L+ supervised topic classifier (also called a ‘learner’) controlling +L+ the priority of the unvisited frontier of a crawler (see +L+ Figure 1). The classifier is trained a priori on document +L+ samples embedded in a topic taxonomy such as Yahoo! +L+ or Dmoz. It thereby learns to label new documents as +L+ belonging to topics in the given taxonomy [2, 5, 21]. The +L+ goal of the focused crawler is to start from nodes relevant +L+ to a focus topic c* in the Web graph and explore links to +L+ selectively collect pages about c*, while avoiding fetching +L+ pages not about c*. +L+ Suppose the crawler has collected a page u and +L+ </SectLabel_bodyText> <SectLabel_figure> If Pr(c*|u) is large enough +L+ then enqueue all outlinks v of u +L+ with priority Pr(c*|u) +L+ Dmoz +L+ topic +L+ taxonomy +L+ Class models +L+ consisting of +L+ term stats +L+ Frontier URLS +L+ priority queue +L+ Pick +L+ best +L+ Crawler +L+ Seed +L+ URLs +L+ Baseline learner +L+ Submit page for classification +L+ Newly fetched +L+ page u +L+ Crawl +L+ database +L+ </SectLabel_figure> <SectLabel_page> 148 +L+ </SectLabel_page> <SectLabel_bodyText> encountered in u an unvisited link to v. A simple crawler +L+ (which we call the baseline) will use the relevance of u +L+ to topic c* (which, in a Bayesian setting, we can denote +L+ Pr(c*lu)) as the estimated relevance of the unvisited page +L+ v. This reflects our belief that pages across a hyperlink +L+ are more similar than two randomly chosen pages on the +L+ Web, or, in other words, topics appear clustered in the +L+ Web graph [11, 23]. Node v will be added to the crawler’s +L+ priority queue with priority Pr(c*lu). This is essentially a +L+ “best-first” crawling strategy. When v comes to the head +L+ of the queue and is actually fetched, we can verify if the +L+ gamble paid off, by evaluating Pr(c* lv). The fraction of +L+ relevant pages collected is called the harvest rate. If V +L+ is the set of nodes collected, the harvest rate is defined +L+ as (1/lVl) E vEVPr(c*lv). Alternatively, we can measure +L+ the loss rate, which is one minus the harvest rate, i.e., the +L+ (expected) fraction of fetched pages that must be thrown +L+ away. Since the effort on relevant pages is well-spent, +L+ reduction in loss rate is the primary goal and the most +L+ appropriate figure of merit. +L+ For focused crawling applications to succeed, the “leap +L+ of faith” from u to v must pay off frequently. In other words, +L+ if Pr(c*lv) is often much less than the preliminary estimate +L+ Pr(c*lu), a great deal of network traffic and CPU cycles +L+ are being wasted eliminating bad pages. Experience with +L+ random walks on the Web show that as one walks away +L+ from a fixed page u0 relevant to topic c0, the relevance of +L+ successive nodes u1, u2,... to c0 drops dramatically within +L+ a few hops [9, 23]. This means that only a fraction of out- +L+ links from a page is typically worth following. The average +L+ out-degree of the Web graph is about 7 [29]. Therefore, a +L+ large number of page fetches may result in disappointment, +L+ especially if we wish to push the utility of focused crawling +L+ to topic communities which are not very densely linked. +L+ Even w.r.t. topics that are not very narrow, the +L+ number of distracting outlinks emerging from even fairly +L+ relevant pages has grown substantially since the early +L+ days of Web authoring [4]. Template-based authoring, +L+ dynamic page generation from semi-structured databases, +L+ ad links, navigation panels, and Web rings contribute many +L+ irrelevant links which reduce the harvest rate of focused +L+ crawlers. Topic-based link discrimination will also reduce +L+ these problems. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 1.1 Our contribution: Leaping with more faith +L+ In this paper we address the following questions: +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> How much information about the topic of the HREF +L+ target is available and/or latent in the HREF source page, +L+ its tag-tree structure, and its text? Can these sources be +L+ exploited for accelerating a focused crawler? +L+ Our basic idea is to use two classifiers. Earlier, the regular +L+ baseline classifier was used to assign priorities to unvisited +L+ frontier nodes. This no longer remains its function. The role +L+ of assigning priorities to unvisited URLs in the crawl frontier +L+ is now assigned to a new learner called the apprentice, and +L+ the priority of v is specific to the features associated with +L+ the (u, v) link which leads to it1. The features used by the +L+ apprentice are derived from the Document Object Model or +L+ </SectLabel_bodyText> <SectLabel_footnote> 'If many u’s link to a single v, it is easiest to freeze the priority of +L+ </SectLabel_footnote> <SectLabel_bodyText> v when the first-visited u linking to v is assessed, but combinations +L+ of scores are also possible. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: The apprentice is continually presented with +L+ training cases (u, v) with suitable features. The apprentice +L+ is interposed where new outlinks (u, v) are registered with +L+ the priority queue, and helps assign the unvisited node v a +L+ better estimate of its relevance. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> DOM (http://www.w3.org/DOM/) of u. Meanwhile, the role +L+ of the baseline classifier becomes one of generating training +L+ instances for the apprentice, as shown in Figure 2. We may +L+ therefore regard the baseline learner as a critic or a trainer, +L+ which provides feedback to the apprentice so that it can +L+ improve “on the job.” +L+ The critic-apprentice paradigm is related to reinforce- +L+ ment learning and AI programs that learn to play games +L+ [26, §1.2]. We argue that this division of labor is natural +L+ and effective. The baseline learner can be regarded as +L+ a user specification for what kind of content is desired. +L+ Although we limit ourselves to a generative statistical model +L+ for this specification, this can be an arbitrary black-box +L+ predicate. For rich and meaningful distinction between +L+ Web communities and topics, the baseline learner needs +L+ to be fairly sophisticated, perhaps leveraging off human +L+ annotations on the Web (such as topic directories). In +L+ contrast, the apprentice specializes in how to locate pages +L+ to satisfy the baseline learner. Its feature space is more +L+ limited, so that it can train fast and adapt nimbly to +L+ changing fortunes at following links during a crawl. In +L+ Mitchell’s words [27], the baseline learner recognizes “global +L+ regularity” while the apprentice helps the crawler adapt +L+ to “local regularity.” This marked asymmetry between +L+ the classifiers distinguishes our approach from Blum and +L+ Mitchell’s co-training technique [3], in which two learners +L+ train each other by selecting unlabeled instances. +L+ Using a dozen topics from a topic taxonomy derived +L+ from the Open Directory, we compare our enhanced crawler +L+ with the baseline crawler. The number of pages that are +L+ thrown away (because they are irrelevant), called the loss +L+ rate, is cut down by 30–90%. We also demonstrate that +L+ the fine-grained tag-tree model, together with our synthesis +L+ and encoding of features for the apprentice, are superior to +L+ simpler alternatives. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 1.2 Related work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Optimizing the priority of unvisited URLs on the crawl +L+ frontier for specific crawling goals is not new. FISHSEARCH +L+ by De Bra et al. [12, 13] and SHARKSEARCH by Hersovici +L+ et al. [16] were some of the earliest systems for localized +L+ searches in the Web graph for pages with specified keywords. +L+ </SectLabel_bodyText> <SectLabel_figure> ... submit (u,v) +L+ to the apprentice +L+ If Pr(c*|u)is +L+ large enough... +L+ Dmoz +L+ topic +L+ taxonomy +L+ Submit page for classification +L+ Baseline learner (Critic) +L+ + - +L+ Apprentice +L+ assigns more +L+ accurate priority +L+ to node v	Frontier URLS +L+ priority queue +L+ Class models +L+ consisting of +L+ term stats +L+ Apprentice learner +L+ Class +L+ models +L+ Newly fetched +L+ page u +L+ Crawler +L+ Pick +L+ best +L+ Online +L+ training +L+ An instance (u,v) +L+ for the apprentice +L+ Pr(c*|v) +L+ u +L+ Crawl +L+ database +L+ Pr(c|u) for +L+ all classes c +L+ v +L+ </SectLabel_figure> <SectLabel_page> 149 +L+ </SectLabel_page> <SectLabel_bodyText> In another early paper, Cho et al. [10] experimented with a +L+ variety of strategies for prioritizing how to fetch unvisited +L+ URLs. They used the anchor text as a bag of words to +L+ guide link expansion to crawl for pages matching a specified +L+ keyword query, which led to some extent of differentiation +L+ among out-links, but no trainer-apprentice combination was +L+ involved. No notion of supervised topics had emerged at +L+ that point, and simple properties like the in-degree or the +L+ presence of specified keywords in pages were used to guide +L+ the crawler. +L+ Topical locality on the Web has been studied for a few +L+ years. Davison made early measurements on a 100000- +L+ node Web subgraph [11] collected by the DISCOWEB system. +L+ Using the standard notion of vector space TFIDF similarity +L+ [31], he found that the endpoints of a hyperlink are much +L+ more similar to each other than two random pages, and that +L+ HREFs close together on a page link to documents which are +L+ more similar than targets which are far apart. Menczer has +L+ made similar observations [23]. The HYPERCLASS hypertext +L+ classifier also uses such locality patterns for better semi- +L+ supervised learning of topics [7], as does IBM’s Automatic +L+ Resource Compilation (ARC) and Clever topic distillation +L+ systems [6, 8]. +L+ Two important advances have been made beyond the +L+ baseline best-first focused crawler: the use of context graphs +L+ by Diligenti et al. [14] and the use of reinforcement learning +L+ by Rennie and McCallum [30]. Both techniques trained +L+ a learner with features collected from paths leading up to +L+ relevant nodes rather than relevant nodes alone. Such paths +L+ may be collected by following backlinks. +L+ Diligenti et al. used a classifier (learner) that regressed +L+ from the text of u to the estimated link distance from u to +L+ some relevant page w, rather than the relevance of u or an +L+ outlink (u, v), as was the case with the baseline crawler. +L+ This lets their system continue expanding u even if the +L+ reward for following a link is not immediate, but several +L+ links away. However, they do favor links whose payoffs +L+ are closest. Our work is specifically useful in conjunction +L+ with the use of context graphs: when the context graph +L+ learner predicts that a goal is several links away, it is crucial +L+ to offer additional guidance to the crawler based on local +L+ structure in pages, because the fan-out at that radius could +L+ be enormous. +L+ Rennie and McCallum [30] also collected paths leading +L+ to relevant nodes, but they trained a slightly different +L+ classifier, for which: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 An instance was a single HREF link like (u, v). +L+ 9 The features were terms from the title and headers +L+ (<h1> ... </h1> etc.) of u, together with the text +L+ in and ‘near’ the anchor (u, v). Directories and +L+ pathnames were also used. (We do not know the +L+ precise definition of ‘near’, or how these features were +L+ encoded and combined.) +L+ 9 The prediction was a discretized estimate of the +L+ number of relevant nodes reachable by following (u, v), +L+ where the reward from goals distant from v was +L+ geometrically discounted by some factor γ < 1/2 per +L+ hop. +L+ </SectLabel_listItem> <SectLabel_bodyText> Rennie and McCallum obtained impressive harvests of +L+ research papers from four Computer Science department +L+ sites, and of pages about officers and directors from 26 +L+ company Websites. +L+ Lexical proximity and contextual features have been +L+ used extensively in natural language processing for disam- +L+ biguating word sense [15]. Compared to plain text, DOM +L+ trees and hyperlinks give us a richer set of potential features. +L+ Aggarwal et al. have proposed an “intelligent crawling” +L+ framework [1] in which only one classifier is used, but similar +L+ to our system, that classifier trains as the crawl progresses. +L+ They do not use our apprentice-critic approach, and do not +L+ exploit features derived from tag-trees to guide the crawler. +L+ The “intelligent agents” literature has brought forth +L+ several systems for resource discovery and assistance to +L+ browsing [19]. They range between client- and site-level +L+ tools. Letizia [18], Powerscout, and WebWatcher [17] are +L+ such systems. Menczer and Belew proposed InfoSpiders +L+ [24], a collection of autonomous goal-driven crawlers without +L+ global control or state, in the style of genetic algorithms. A +L+ recent extensive study [25] comparing several topic-driven +L+ crawlers including the best-first crawler and InfoSpiders +L+ found the best-first approach to show the highest harvest +L+ rate (which our new system outperforms). +L+ In all the systems mentioned above, improving the +L+ chances of a successful “leap of faith” will clearly reduce +L+ the overheads of fetching, filtering, and analyzing pages. +L+ Furthermore, whereas we use an automatic first-generation +L+ focused crawler to generate the input to train the apprentice, +L+ one can envisage specially instrumented browsers being used +L+ to monitor users as they seek out information. +L+ We distinguish our work from prior art in the following +L+ important ways: +L+ Two classifiers: We use two classifiers. The first one is +L+ used to obtain ‘enriched’ training data for the second one. +L+ (A breadth-first or random crawl would have a negligible +L+ fraction of positive instances.) The apprentice is a simplified +L+ reinforcement learner. It improves the harvest rate, thereby +L+ ‘enriching’ the data collected and labeled by the first learner +L+ in turn. +L+ No manual path collection: Our two-classifier frame- +L+ work essentially eliminates the manual effort needed to +L+ create reinforcement paths or context graphs. The input +L+ needed to start off a focused crawl is just a pre-trained topic +L+ taxonomy (easily available from the Web) and a few focus +L+ topics. +L+ Online training: Our apprentice trains continually, ac- +L+ quiring ever-larger vocabularies and improving its accuracy +L+ as the crawl progresses. This property holds also for the +L+ “intelligent crawler” proposed by Aggarwal et al., but they +L+ have a single learner, whose drift is controlled by precise +L+ relevance predicates provided by the user. +L+ No manual feature tuning: Rather than tune ad-hoc +L+ notions of proximity between text and hyperlinks, we encode +L+ the features of link (u, v) using the DOM-tree of u, and +L+ automatically learn a robust definition of ‘nearness’ of a +L+ textual feature to (u, v). In contrast, Aggarwal et al +L+ use many tuned constants combining the strength of text- +L+ and link-based predictors, and Rennie et al. use domain +L+ knowledge to select the paths to goal nodes and the word +L+ bags that are submitted to their learner. +L+ </SectLabel_bodyText> <SectLabel_page> 150 +L+ </SectLabel_page> <SectLabel_sectionHeader> 2 Methodology and algorithms +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We first review the baseline focused crawler and then +L+ describe how the enhanced crawler is set up using the +L+ apprentice-critic mechanism. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.1 The baseline focused crawler +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The baseline focused crawler has been described in detail +L+ elsewhere [9, 14], and has been sketched in Figure 1. Here +L+ we review its design and operation briefly. +L+ There are two inputs to the baseline crawler. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	A topic taxonomy or hierarchy with example URLs +L+ for each topic. +L+ •	One or a few topics in the taxonomy marked as the +L+ topic(s) of focus. +L+ </SectLabel_listItem> <SectLabel_bodyText> Although we will generally use the terms ‘taxonomy’ and +L+ ‘hierarchy’, a topic tree is not essential; all we really need is +L+ a two-way classifier where the classes have the connotations +L+ of being ‘relevant’ or ‘irrelevant’ to the topic(s) of focus. +L+ A topic hierarchy is proposed purely to reduce the tedium +L+ of defining new focused crawls. With a two-class classifier, +L+ the crawl administrator has to seed positive and negative +L+ examples for each crawl. Using a taxonomy, she composes +L+ the ‘irrelevant’ class as the union of all classes that are not +L+ relevant. Thanks to extensive hierarchies like Dmoz in the +L+ public domain, it should be quite easy to seed topic-based +L+ crawls in this way. +L+ The baseline crawler maintains a priority queue on the +L+ estimated relevance of nodes v which have not been visited, +L+ and keeps removing the highest priority node and visiting it, +L+ expanding its outlinks and checking them into the priority +L+ queue with the relevance score of v in turn. Despite its +L+ extreme simplicity, the best-first crawler has been found to +L+ have very high harvest rates in extensive evaluations [25]. +L+ Why do we need negative examples and negative classes +L+ at all? Instead of using class probabilities, we could maintain +L+ a priority queue on, say, the TFIDF cosine similarity +L+ between u and the centroid of the seed pages (acting as an +L+ estimate for the corresponding similarity between v and the +L+ centroid, until v has been fetched). Experience has shown +L+ [32] that characterizing a negative class is quite important to +L+ prevent the centroid of the crawled documents from drifting +L+ away indefinitely from the desired topic profile. +L+ In this paper, the baseline crawler also has the implicit +L+ job of gathering instances of successful and unsuccessful +L+ “leaps of faith” to submit to the apprentice, discussed next. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 The basic structure of the apprentice +L+ learner +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In estimating the worth of traversing the HREF (u, v), we +L+ will limit our attention to u alone. The page u is modeled +L+ as a tag tree (also called the Document Object Model or +L+ DOM). In principle, any feature from u, even font color and +L+ site membership may be perfect predictors of the relevance +L+ of v. The total number of potentially predictive features will +L+ be quite staggering, so we need to simplify the feature space +L+ and massage it into a form suited to conventional learning +L+ algorithms. Also note that we specifically study properties +L+ of u and not larger contexts such as paths leading to u, +L+ meaning that our method may become even more robust and +L+ useful in conjunction with context graphs or reinforcement +L+ along paths. +L+ Initially, the apprentice has no training data, and passes +L+ judgment on (u, v) links according to some fixed prior +L+ obtained from a baseline crawl run ahead of time (e.g., see +L+ the statistics in §3.3). Ideally, we would like to train the +L+ apprentice continuously, but to reduce overheads, we declare +L+ a batch size between a few hundred and a few thousand +L+ pages. After every batch of pages is collected, we check if any +L+ page u fetched before the current batch links to some page +L+ v in the batch. If such a (u, v) is found, we extract suitable +L+ features for (u, v) as described later in this section, and add +L+ ((u, v), Pr(c* |v)� as another instance of the training data for +L+ the apprentice. Many apprentices, certainly the simple naive +L+ Bayes and linear perceptrons that we have studied, need not +L+ start learning from scratch; they can accept the additional +L+ training data with a small additional computational cost. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 2.2.1 Preprocessing the DOM tree +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> First, we parse u and form the DOM tree for u. Sadly, +L+ much of the HTML available on the Web violates any +L+ HTML standards that permit context-free parsing, but +L+ a variety of repair heuristics (see, e.g., HTML Tidy, +L+ available at http://www.w3.org/People/Raggett/tidy/) +L+ let us generate reasonable DOM trees from bad HTML. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 3: Numbering of DOM leaves used to derive offset +L+ attributes for textual tokens. ‘@’ means “is at offset”. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Second, we number all leaf nodes consecutively from left +L+ to right. For uniformity, we assign numbers even to those +L+ DOM leaves which have no text associated with them. The +L+ specific <a href ... > which links to v is actually an internal +L+ node a,, which is the root of the subtree containing the +L+ anchor text of the link (u, v). There may be other element +L+ tags such as <em> or <b> in the subtree rooted at a,. Let +L+ the leaf or leaves in this subtree be numbered f(a,) through +L+ r(a,) ≥ f(a,). We regard the textual tokens available from +L+ any of these leaves as being at DOM offset zero w.r.t. the +L+ (u, v) link. Text tokens from a leaf numbered p, to the left of +L+ f(a,), are at negative DOM offset p — f(a,). Likewise, text +L+ from a leaf numbered p to the right of r(a,) are at positive +L+ DOM offset p — r(a,). See Figure 3 for an example. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 2.2.2 Features derived from the DOM and text +L+ tokens +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Many related projects mentioned in §1.2 use a linear notion +L+ of proximity between a HREF and textual tokens. In the +L+ ARC system, there is a crude cut-off distance measured +L+ </SectLabel_bodyText> <SectLabel_figure> @-2		@-1		@0		@0		@1		@2		@3 +L+ TEXT +L+ tt +L+ li +L+ TEXT +L+ TEXT +L+ ul +L+ li +L+ a +L+ HREF +L+ font +L+ TEXT +L+ TEXT +L+ li +L+ TEXT em +L+ TEXT +L+ li +L+ </SectLabel_figure> <SectLabel_page> 151 +L+ </SectLabel_page> <SectLabel_bodyText> in bytes to the left and right of the anchor. In the +L+ Clever system, distance is measured in tokens, and the +L+ importance attached to a token decays with the distance. +L+ In reinforcement learning and intelligent predicate-based +L+ crawling, the exact specification of neighborhood text is not +L+ known to us. In all cases, some ad-hoc tuning appears to be +L+ involved. +L+ We claim (and show in §3.4) that the relation between +L+ the relevance of the target v of a HREF (u, v) and the +L+ proximity of terms to (u, v) can be learnt automatically. The +L+ results are better than ad-hoc tuning of cut-off distances, +L+ provided the DOM offset information is encoded as features +L+ suitable for the apprentice. +L+ One obvious idea is to extend the Clever model: a page +L+ is a linear sequence of tokens. If a token t is distant x from +L+ the HREF (u, v) in question, we encode it as a feature (t, x). +L+ Such features will not be useful because there are too many +L+ possible values of x, making the (t, x) space too sparse to +L+ learn well. (How many HREFS will be exactly five tokens +L+ from the term ‘basketball’?) +L+ Clearly, we need to bucket x into a small number of +L+ ranges. Rather than tune arbitrary bucket boundaries by +L+ hand, we argue that DOM offsets are a natural bucketing +L+ scheme provided by the page author. Using the node +L+ numbering scheme described above, each token t on page u +L+ can be annotated w.r.t. the link (u, v) (for simplicity assume +L+ there is only one such link) as (t, d), where d is the DOM +L+ offset calculated above. This is the main set of features +L+ used by the apprentice. We shall see that the apprentice +L+ can learn to limit IdI to less than dmax = 5 in most cases, +L+ which reduces its vocabulary and saves time. +L+ A variety of other feature encodings suggest themselves. +L+ We are experimenting with some in ongoing work (§4), +L+ but decided against some others. For example, we do not +L+ expect gains from encoding specific HTML tag names owing +L+ to the diversity of authoring styles. Authors use <div>, +L+ <span>, <layer> and nested tables for layout control in +L+ non-standard ways; these are best deflated to a nameless +L+ DOM node representation. Similar comments apply to +L+ HREF collections embedded in <ul>, <ol>, <td> and +L+ <dd>. Font and lower/upper case information is useful +L+ for search engines, but would make features even sparser +L+ for the apprentice. Our representation also flattens two- +L+ dimensional tables to their “row-major” representation. +L+ The features we ignore are definitely crucial for other +L+ applications, such as information extraction. We did not +L+ see any cases where this sloppiness led to a large loss rate. +L+ We would be surprised to see tables where relevant links +L+ occurred in the third column and irrelevant links in the fifth, +L+ or pages where they are rendered systematically in different +L+ fonts and colors, but are not otherwise demarcated by the +L+ DOM structure. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 2.2.3 Non-textual features +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Limiting d may lead us to miss features of u that may be +L+ useful at the whole-page level. One approach would be to use +L+ “d = oo” for all d larger in magnitude than some threshold. +L+ But this would make our apprentice as bulky and slow to +L+ train as the baseline learner. +L+ Instead, we use the baseline learner to abstract u for +L+ the apprentice. Specifically, we use a naive Bayes baseline +L+ learner to classify u, and use the vector of class probabilities +L+ returned as features for the apprentice. These features can +L+ help the apprentice discover patterns such as +L+ “Pages about /Recreation/Boating/Sailing often +L+ link to pages about /Sports/Canoe_and_Kayaking.” +L+ This also covers for the baseline classifier confusing between +L+ classes with related vocabulary, achieving an effect similar +L+ to context graphs. +L+ Another kind of feature can be derived from co-citation. +L+ If v1 has been fetched and found to be relevant and HREFS +L+ (u, v1) and (u, v2) are close to each other, v2 is likely to +L+ be relevant. Just like textual tokens were encoded as (t, d) +L+ pairs, we can represent co-citation features as (p, d), where +L+ p is a suitable representation of relevance. +L+ Many other features can be derived from the DOM tree +L+ and added to our feature pool. We discuss some options +L+ in §4. In our experience so far, we have found the (t, d) +L+ features to be most useful. For simplicity, we will limit our +L+ subsequent discussion to (t, d) features only. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.3 Choices of learning algorithms for the +L+ apprentice +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our feature set is thus an interesting mix of categorical, +L+ ordered and continuous features: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 Term tokens (t, d) have a categorical component t and +L+ a discrete ordered component d (which we may like to +L+ smooth somewhat). Term counts are discrete but can +L+ be normalized to constant document length, resulting +L+ in continuous attribute values. +L+ 9 Class names are discrete and may be regarded as +L+ </SectLabel_listItem> <SectLabel_bodyText> synthetic terms. The probabilities are continuous. +L+ The output we desire is an estimate of Pr(c* Iv), given all the +L+ observations about u and the neighborhood of (u, v) that +L+ we have discussed. Neural networks are a natural choice +L+ to accommodate these requirements. We first experimented +L+ with a simple linear perceptron, training it with the delta +L+ rule (gradient descent) [26]. Even for a linear perceptron, +L+ convergence was surprisingly slow, and after convergence, +L+ the error rate was rather high. It is likely that local +L+ optima were responsible, because stability was generally +L+ poor, and got worse if we tried to add hidden layers or +L+ sigmoids. In any case, convergence was too slow for use +L+ as an online learner. All this was unfortunate, because the +L+ direct regression output from a neural network would be +L+ convenient, and we were hoping to implement a Kohonen +L+ layer for smoothing d. +L+ In contrast, a naive Bayes (NB) classifier worked very +L+ well. A NB learner is given a set of training documents, +L+ each labeled with one of a finite set of classes/topic. A +L+ document or Web page u is modeled as a multiset or bag +L+ of words, {(T,n(u,T))} where T is a feature which occurs +L+ n(u, T) times in u. In ordinary text classification (such as +L+ our baseline learner) the features T are usually single words. +L+ For our apprentice learner, a feature T is a (t, d) pair. +L+ NB classifiers can predict from a discrete set of classes, +L+ but our prediction is a continuous (probability) score. To +L+ bridge this gap, We used a simple two-bucket (low/high +L+ relevance) special case of Torgo and Gama’s technique of +L+ using classifiers for discrete labels for continuous regression +L+ [33], using “equally probable intervals” as far as possible. +L+ </SectLabel_bodyText> <SectLabel_page> 152 +L+ </SectLabel_page> <SectLabel_bodyText> Torgo and Gama recommend using a measure of centrality, +L+ such as the median, of each interval as the predicted value of +L+ that class. Rennie and McCallum [30] corroborate that 2–3 +L+ bins are adequate. As will be clear from our experiments, the +L+ medians of our ‘low’ and ‘high’ classes are very close to zero +L+ and one respectively (see Figure 5). Therefore, we simply +L+ take the probability of the ‘high’ class as the prediction from +L+ our naive Bayes apprentice. +L+ The prior probability of class c, denoted Pr(c) is the +L+ fraction of training documents labeled with class c. The NB +L+ model is parameterized by a set of numbers 0c,T which is +L+ roughly the rate of occurrence of feature τ in class c, more +L+ exactly, +L+ </SectLabel_bodyText> <SectLabel_equation> ITI + Pu T n(u τ,), (1) +L+ </SectLabel_equation> <SectLabel_bodyText> where Vc is the set of Web pages labeled with c and T is the +L+ entire vocabulary. The NB learner assumes independence +L+ between features, and estimates +L+ </SectLabel_bodyText> <SectLabel_equation> Pr(cIu) a Pr(c) Pr(uIc) ≈ Pr(c) 11 0n (u,T) +L+ c,T. (2) +L+ TEu +L+ </SectLabel_equation> <SectLabel_bodyText> Nigam et al. provide further details [22]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 Experimental study +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our experiments were guided by the following requirements. +L+ We wanted to cover a broad variety of topics, some ‘easy’ and +L+ some ‘di cult’, in terms of the harvest rate of the baseline +L+ crawler. Here is a quick preview of our results. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	The apprentice classifier achieves high accuracy in +L+ predicting the relevance of unseen pages given (t, d) +L+ features. It can determine the best value of dmax to +L+ use, typically, 4–6. +L+ •	Encoding DOM offsets in features improves the +L+ accuracy of the apprentice substantially, compared +L+ to a bag of ordinary words collected from within the +L+ same DOM offset window. +L+ •	Compared to a baseline crawler, a crawler that is +L+ guided by an apprentice (trained offiine) has a 30% +L+ to 90% lower loss rate. It finds crawl paths never +L+ expanded by the baseline crawler. +L+ •	Even if the apprentice-guided crawler is forced to +L+ stay within the (inferior) Web graph collected by the +L+ baseline crawler, it collects the best pages early on. +L+ •	The apprentice is easy to train online. As soon as it +L+ </SectLabel_listItem> <SectLabel_bodyText> starts guiding the crawl, loss rates fall dramatically. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Compared to (t, d) features, topic- or cocitation-based +L+ features have negligible effect on the apprentice. +L+ </SectLabel_listItem> <SectLabel_bodyText> To run so many experiments, we needed three highly +L+ optimized and robust modules: a crawler, a HTML-to-DOM +L+ converter, and a classifier. +L+ We started with the w3c-libwww crawling library from +L+ http://www.w3c.org/Library/, but replaced it with our +L+ own crawler because we could effectively overlap DNS +L+ lookup, HTTP access, and disk access using a select over +L+ all socket/file descriptors, and prevent memory leaks visible +L+ in w3c-libwww. With three caching DNS servers, we could +L+ achieve over 90% utilization of a 2Mbps dedicated ISP +L+ connection. +L+ We used the HTML parser libxml2 library to extract +L+ the DOM from HTML, but this library has memory leaks, +L+ and does not always handle poorly written HTML well. We +L+ had some stability problems with HTML Tidy (http: //www. +L+ w3.org/People/Raggett/tidy/), the well-known HTML +L+ cleaner which is very robust to bad HTML. At present we +L+ are using libxml2 and are rolling our own HTML parser and +L+ cleaner for future work. +L+ We intend to make our crawler and HTML parser code +L+ available in the public domain for research use. +L+ For both the baseline and apprentice classifier we used +L+ the public domain BOW toolkit and the Rainbow naive +L+ Bayes classifier created by McCallum and others [20]. Bow +L+ and Rainbow are very fast C implementations which let us +L+ classify pages in real time as they were being crawled. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Design of the topic taxonomy +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We downloaded from the Open Directory (http://dmoz. +L+ org/) an RDF file with over 271954 topics arranged in a +L+ tree hierarchy with depth at least 6, containing a total of +L+ about 1697266 sample URLs. The distribution of samples +L+ over topics was quite non-uniform. Interpreting the tree as +L+ an is-a hierarchy meant that internal nodes inherited all +L+ examples from descendants, but they also had their own +L+ examples. Since the set of topics was very large and many +L+ topics had scarce training data, we pruned the Dmoz tree +L+ to a manageable frontier by following these steps: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Initially we placed example URLs in both internal and +L+ leaf nodes, as given by Dmoz. +L+ 2.We fixed a minimum per-class training set size of k = +L+ 300 documents. +L+ 3.We iteratively performed the following step as long +L+ as possible: we found a leaf node with less than k +L+ example URLs, moved all its examples to its parent, +L+ and deleted the leaf. +L+ 4.To each internal node c, we attached a leaf +L+ subdirectory called Other. Examples associated +L+ directly with c were moved to this Other subdirectory. +L+ 5. Some topics were populated out of proportion, either +L+ at the beginning or through the above process. We +L+ made the class priors more balanced by sampling +L+ down the large classes so that each class had at most +L+ 300 examples. +L+ </SectLabel_listItem> <SectLabel_bodyText> The resulting taxonomy had 482 leaf nodes and a total +L+ of 144859 sample URLs. Out of these we could successfully +L+ fetch about 120000 URLs. At this point we discarded the +L+ tree structure and considered only the leaf topics. Training +L+ time for the baseline classifier was about about two hours +L+ on a 729MHz Pentium III with 256kB cache and 512MB +L+ RAM. This was very fast, given that 1.4GB of HTML text +L+ had to be processed through Rainbow. The complete listing +L+ of topics can be obtained from the authors. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Choice of topics +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Depending on the focus topic and prioritization strategy, +L+ focused crawlers may achieve diverse harvest rates. Our +L+ </SectLabel_bodyText> <SectLabel_equation> 0c,T = +L+ 1 + Pu∈VC n(u, τ) +L+ </SectLabel_equation> <SectLabel_page> 153 +L+ </SectLabel_page> <SectLabel_bodyText> early prototype [9] yielded harvest rates typically between +L+ 0.25 and 0.6. Rennie and McCallum [30] reported recall +L+ and not harvest rates. Diligenti et al. [14] focused on very +L+ specific topics where the harvest rate was very low, 4–6%. +L+ Obviously, the maximum gains shown by a new idea in +L+ focused crawling can be sensitive to the baseline harvest +L+ rate. +L+ To avoid showing our new system in an unduly positive +L+ or negative light, we picked a set of topics which were fairly +L+ diverse, and appeared to be neither too broad to be useful +L+ (e.g., /Arts, /Science) nor too narrow for the baseline +L+ crawler to be a reasonable adversary. We list our topics +L+ in Figure 4. We chose the topics without prior estimates of +L+ how well our new system would work, and froze the list +L+ of topics. All topics that we experimented with showed +L+ visible improvements, and none of them showed deteriorated +L+ performance. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Baseline crawl results +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We will skip the results of breadth-first or random crawling +L+ in our commentary, because it is known from earlier work +L+ on focused crawling that our baseline crawls are already +L+ far better than breadth-first or random crawls. Figure 5 +L+ shows, for most of the topics listed above, the distribution +L+ of page relevance after running the baseline crawler to +L+ collect roughly 15000 to 25000 pages per topic. The E +L+ baseline crawler used a standard naive Bayes classifier on +L+ the ordinary term space of whole pages. We see that the +L+ relevance distribution is bimodal, with most pages being +L+ very relevant or not at all. This is partly, but only partly, a +L+ result of using a multinomial naive Bayes model. The naive +L+ Bayes classifier assumes term independence and multiplies +L+ together many (small) term probabilities, with the result +L+ that the winning class usually beats all others by a large +L+ margin in probability. But it is also true that many outlinks +L+ lead to pages with completely irrelevant topics. Figure 5 +L+ gives a clear indication of how much improvement we can +L+ expect for each topic from our new algorithm. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.4 DOM window size and feature selection +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A key concern for us was how to limit the maximum window +L+ width so that the total number of synthesized (t, d) features +L+ remains much smaller than the training data for the baseline +L+ classifier, enabling the apprentice to be trained or upgraded +L+ in a very short time. At the same time, we did not want +L+ to lose out on medium- to long-range dependencies between +L+ significant tokens on a page and the topic of HREF targets +L+ in the vicinity. We eventually settled for a maximum DOM +L+ window size of 5. We made this choice through the following +L+ experiments. +L+ The easiest initial approach was an end-to-end cross- +L+ validation of the apprentice for various topics while +L+ increasing dm x. We observed an initial increase in the +L+ validation accuracy when the DOM window size was +L+ increased beyond 0. However, the early increase leveled +L+ off or even reversed after the DOM window size was +L+ increased beyond 5. The graphs in Figure 6 display these +L+ results. We see that in the Chess category, though the +L+ validation accuracy increases monotonically, the gains are +L+ less pronounced after dm x exceeds 5. For the AI category, +L+ accuracy fell beyond dm x = 4. +L+ </SectLabel_bodyText> <SectLabel_figure> Topic	#Good	#Bad +L+ /Arts/Music/Styles/Classical/Composers	24000	13000 +L+ /Arts/Performing-Arts/Dance/Folk-Dancing	7410	8300 +L+ /Business/Industries.../Livestock/Horses...	17000	7600 +L+ /Computers/Artificial-Intelligence	7701	14309 +L+ /Computers/Software/Operating-Systems/Linux	17500	9300 +L+ /Games/Board-Games/C/Chess	17000	4600 +L+ /Health/Conditions-and-Diseases/Cancer	14700	5300 +L+ /Home/Recipes/Soups-and-Stews	20000	3600 +L+ /Recreation/Outdoors/Fishing/Fly-Fishing	12000	13300 +L+ /Recreation/Outdoors/Speleology	6717	14890 +L+ /Science/Astronomy	14961	5332 +L+ /Science/Earth-Sciences/Meteorology	19205	8705 +L+ /Sports/Basketball	26700	2588 +L+ /Sports/Canoe-and-Kayaking	12000	12700 +L+ /Sports/Hockey/Ice-Hockey	17500	17900 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: We chose a variety of topics which were neither +L+ too broad nor too narrow, so that the baseline crawler +L+ was a reasonable adversary. #Good (#Bad) show the +L+ approximate number of pages collected by the baseline +L+ crawler which have relevance above (below) 0.5, which +L+ indicates the relative difficulty of the crawling task. +L+ Figure 5: All of the baseline classifiers have harvest rates +L+ between 0.25 and 0.6, and all show strongly bimodal +L+ relevance score distribution: most of the pages fetched are +L+ very relevant or not at all. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> It is important to notice that the improvement in +L+ accuracy is almost entirely because with increasing number +L+ of available features, the apprentice can reject negative +L+ (low relevance) instances more accurately, although the +L+ accuracy for positive instances decreases slightly. Rejecting +L+ unpromising outlinks is critical to the success of the +L+ enhanced crawler. Therefore we would rather lose a little +L+ accuracy for positive instances rather than do poorly on the +L+ negative instances. We therefore chose dm x to be either 4 +L+ or 5 for all the experiments. +L+ We verified that adding offset information to text tokens +L+ was better than simply using plain text near the link [8]. +L+ One sample result is shown in Figure 7. The apprentice +L+ accuracy decreases with dm x if only text is used, whereas +L+ it increases if offset information is provided. This highlights +L+ </SectLabel_bodyText> <SectLabel_figure> d #pages +L+ 100000 +L+ 10000 +L+ 1000 +L+ 100 +L+ 10 +L+ Relevance probability +L+ AI +L+ Astronomy +L+ Basketball +L+ Cancer +L+ Chess +L+ Composers +L+ FlyFishing +L+ FolkDance +L+ Horses +L+ IceHockey +L+ Kayaking +L+ Linux +L+ Meteorology +L+ Soups +L+ Tobacco +L+ </SectLabel_figure> <SectLabel_page> 154 +L+ </SectLabel_page> <SectLabel_figure> Chess +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6: There is visible improvement in the accuracy +L+ of the apprentice if dmax is made larger, up to about 5– +L+ 7 depending on topic. The effect is more pronounced on +L+ the the ability to correctly reject negative (low relevance) +L+ outlink instances. ‘Average’ is the microaverage over all +L+ test instances for the apprentice, not the arithmetic mean +L+ of ‘Positive’ and ‘Negative’. +L+ Figure 7: Encoding DOM offset information with textual +L+ features boosts the accuracy of the apprentice substantially. +L+ the importance of designing proper features. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> To corroborate the useful ranges of dmax above, we +L+ compared the value of average mutual information gain for +L+ terms found at various distances from the target HREF. +L+ The experiments revealed that the information gain of terms +L+ found further away from the target HREF was generally +L+ lower than those that were found closer, but this reduction +L+ was not monotonic. For instance, the average information +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 8: Information gain variation plotted against +L+ distance from the target HREF for various DOM window +L+ sizes. We observe that the information gain is insensitive to +L+ dmax. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> gain at d = —2 was higher than that at d = —1; see Figure 8. +L+ For each DOM window size, we observe that the information +L+ gain varies in a sawtooth fashion; this intriguing observation +L+ is explained shortly. The average information gain settled +L+ to an almost constant value after distance of 5 from the +L+ target URL. We were initially concerned that to keep the +L+ computation cost manageable, we would need some cap on +L+ dmax even while measuring information gain, but luckily, +L+ the variation of information gain is insensitive to dmax, as +L+ Figure 8 shows. These observations made our final choice of +L+ dmax easy. +L+ In a bid to explain the occurrence of the unexpected +L+ saw-tooth form in Figure 8 we measured the rate B(t,d) at +L+ which term t occurred at offset d, relative to the total count +L+ of all terms occurring at offset d. (They are roughly the +L+ multinomial naive Bayes term probability parameters.) For +L+ fixed values of d, we calculated the sum of B values of terms +L+ found at those offsets from the target HREF. Figure 9(a) +L+ shows the plot of these sums to the distance(d) for various +L+ categories. The B values showed a general decrease as the +L+ distances from the target HREF increased, but this decrease, +L+ like that of information gain, was not monotonic. The B +L+ values of the terms at odd numbered distances from the +L+ target HREF were found to be lower than those of the +L+ terms present at the even positions. For instance, the sum +L+ of B values of terms occurring at distance —2 were higher +L+ than that of terms at position —1. This observation was +L+ explained by observing the HTML tags that are present +L+ at various distances from the target HREF. We observed +L+ that tags located at odd d are mostly non-text tags, thanks +L+ to authoring idioms such as <li><a ... ><li><a ... > and +L+ <a...><br><a ... ><br> etc. A plot of the frequency of +L+ HTML tags against the distance from the HREF at which +L+ </SectLabel_bodyText> <SectLabel_figure> 0	2	4	6	8 +L+ d_max +L+ 90 +L+ 85 +L+ AI +L+ 80 +L+ 75 +L+ 70 +L+ 65 +L+ Negative +L+ Positive +L+ Average +L+ 0 2 dm6 8 +L+ ax +L+ 100 +L+ 95 +L+ 90 +L+ 85 +L+ 80 +L+ 75 +L+ 70 +L+ 65 +L+ Negative +L+ Positive +L+ Average +L+ 0	1	2	3	4	5	6	7	8 +L+ d_max +L+ Text +L+ Offset +L+ 86 +L+ 84 +L+ 82 +L+ 80 +L+ 78 +L+ 76 +L+ AI +L+ Chess +L+ d_max=8 +L+ d_max=5 +L+ d_max=4 +L+ d_max=3 +L+ -8	-6	-4	-2	0	2	4	6	8 +L+ d +L+ 0.0002 +L+ 0.00018 +L+ 0.00016 +L+ 0.00014 +L+ 0.00012 +L+ 0.0001 +L+ 0.00008 +L+ 0.00006 +L+ 0.00004 +L+ 0.00002 +L+ AI +L+ -8	-6	-4	-2	0	2	4	6 +L+ d +L+ 9.00E-05 +L+ 8.00E-05 +L+ 6.00E-05 +L+ 5.00E-05 +L+ 4.00E-05 +L+ 7.00E-05 +L+ 1.00E-04 +L+ d_max=8 +L+ d_max=5 +L+ d_max=4 +L+ d_max=3 +L+ </SectLabel_figure> <SectLabel_page> 155 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 9: Variation of (a) relative term frequencies and +L+ (b) frequencies of HTML tags plotted against d. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> they were found is shown in Figure 9(b). (The <a...> tag +L+ obviously has the highest frequency and has been removed +L+ for clarity.) +L+ These were important DOM idioms, spanning many +L+ diverse Web sites and authoring styles, that we did not +L+ anticipate ahead of time. Learning to recognize these +L+ idioms was valuable for boosting the harvest of the enhanced +L+ crawler. Yet, it would be unreasonable for the user-supplied +L+ baseline black-box predicate or learner to capture crawling +L+ strategies at such a low level. This is the ideal job of +L+ the apprentice. The apprentice took only 3–10 minutes +L+ to train on its (u, v) instances from scratch, despite a +L+ simple implementation that wrote a small file to disk for +L+ each instance of the apprentice. Contrast this with several +L+ hours taken by the baseline learner to learn general term +L+ distribution for topics. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.5 Crawling with the apprentice trained +L+ off-line +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this section we subject the apprentice to a “field test” as +L+ part of the crawler, as shown in Figure 2. To do this we +L+ follow these steps: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Fix a topic and start the baseline crawler from all +L+ example URLs available from the given topic. +L+ 2. Run the baseline crawler until roughly 20000–25000 +L+ pages have been fetched. +L+ 3. For all pages (u, v) such that both u and v have +L+ been fetched by the baseline crawler, prepare an +L+ instance from (u, v) and add to the training set of +L+ the apprentice. +L+ 4. Train the apprentice. Set a suitable value for dmax. +L+ Folk Dancing +L+ </SectLabel_listItem> <SectLabel_figure> 0	2000	4000	6000	8000	10000 +L+ #Pages fetched +L+ Ice Hockey +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 10: Guidance from the apprentice significantly +L+ reduces the loss rate of the focused crawler. +L+ </SectLabel_figureCaption> <SectLabel_listItem> 5. Start the enhanced crawler from the same set of pages +L+ that the baseline crawler had started from. +L+ 6. Run the enhanced crawler to fetch about the same +L+ number of pages as the baseline crawler. +L+ 7. Compare the loss rates of the two crawlers. +L+ </SectLabel_listItem> <SectLabel_bodyText> Unlike with the reinforcement learner studied by Rennie +L+ and McCallum, we have no predetermined universe of URLs +L+ which constitute the relevant set; our crawler must go +L+ forth into the open Web and collect relevant pages from +L+ an unspecified number of sites. Therefore, measuring recall +L+ w.r.t. the baseline is not very meaningful (although we do +L+ report such numbers, for completeness, in §3.6). Instead, we +L+ measure the loss (the number of pages fetched which had to +L+ be thrown away owing to poor relevance) at various epochs +L+ in the crawl, where time is measured as the number of pages +L+ fetched (to elide fluctuating network delay and bandwidth). +L+ At epoch n, if the pages fetched are v1, ... , vn, then the total +L+ expected loss is (1/n) Pi (1− Pr(c*|vi)). +L+ Figure 10 shows the loss plotted against the number of +L+ pages crawled for two topics: Folk dancing and Ice hockey. +L+ The behavior for Folk dancing is typical; Ice hockey is +L+ one of the best examples. In both cases, the loss goes up +L+ substantially faster with each crawled page for the baseline +L+ crawler than for the enhanced crawler. The reduction of loss +L+ for these topics are 40% and 90% respectively; typically, this +L+ number is between 30% and 60%. In other words, for most +L+ </SectLabel_bodyText> <SectLabel_figure> -5 -4 -3 -2 -1	0	1	2	3	4	5 +L+ AI +L+ Chess +L+ Horses +L+ Cancer +L+ IceHockey +L+ Linux +L+ Bball+ +L+ Bball- +L+ 9000	Tags at various DOM offsets +L+ 8000 +L+ 7000 +L+ 6000 +L+ 5000 +L+ 4000 +L+ 3000 +L+ font +L+ td +L+ img +L+ b +L+ br +L+ p +L+ tr +L+ li +L+ comment +L+ div +L+ table +L+ center +L+ i +L+ span +L+ hr +L+ 2000 +L+ 1000 +L+ 0 +L+ -5 -4 -3 -2 -1 0	1	2 3	4 5 d +L+ 0.2 +L+ 0.18 +L+ 0.16 +L+ 0.14 +L+ 0.12 +L+ 0.1 +L+ 0.08 +L+ 0.06 +L+ 0.04 +L+ 0.02 +L+ d +L+ Baseline +L+ Apprentice +L+ Baseline +L+ Apprentice +L+ 8000 +L+ 4000 +L+ 0 +L+ 0	4000 8000 12000 16000 20000 +L+ #Pages fetched +L+ 	6000 +L+ 	4000 +L+ 	2000 +L+ 	0 +L+ </SectLabel_figure> <SectLabel_page> 156 +L+ </SectLabel_page> <SectLabel_bodyText> topics, the apprentice reduces the number of useless pages +L+ fetched by one-third to two-thirds. +L+ In a sense, comparing loss rates is the most meaningful +L+ evaluation in our setting, because the network cost of +L+ fetching relevant pages has to be paid anyway, and can be +L+ regarded as a fixed cost. Diligenti et al. show significant +L+ improvements in harvest rate, but for their topics, the loss +L+ rate for both the baseline crawler as well as the context- +L+ focused crawler were much higher than ours. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.6 URL overlap and recall +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The reader may feel that the apprentice crawler has an +L+ unfair advantage because it is first trained on DOM-derived +L+ features from the same set of pages that it has to crawl +L+ again. We claim that the set of pages visited by the baseline +L+ crawler and thea(off-linentrained) enhanced crawler have +L+ small overlap, and the superior results for the crawler guided 4011 8168 2199 +L+ by the apprentice are in large part because of generalizable +L+ learning. Thisgcan be seen from the examples in Figure 11. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 11: The apprentice-guided crawler follows paths +L+ which are quite different from the baseline crawler because +L+ of its superior priority estimation technique. As a result +L+ there is little overlap between the URLs harvested by these +L+ two crawlers. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Given that the overlap between the baseline and the +L+ enhanced crawlers is small, which is ‘better’? As per the +L+ verdict of the baseline classifier, clearly the enhanced crawler +L+ is better. Even so, we report the loss rate of a different +L+ version of the enhanced crawler which is restricted to visiting +L+ only those pages which were visited by the baseline learner. +L+ We call this crawler the recall crawler. This means that in +L+ the end, both crawlers have collected exactly the same set +L+ of pages, and therefore have the same total loss. The test +L+ then is how long can the enhanced learner prevent the loss +L+ from approaching the baseline loss. These experiments are a +L+ rough analog of the ‘recall’ experiments done by Rennie and +L+ McCallum. We note that for these recall experiments, the +L+ apprentice does get the benefit of not having to generalize, +L+ so the gap between baseline loss and recall loss could be +L+ optimistic. Figure 12 compares the expected total loss of +L+ the baseline crawler, the recall crawler, and the apprentice- +L+ guided crawler (which is free to wander outside the baseline +L+ collection) plotted against the number of pages fetched, for a +L+ few topics. As expected, the recall crawler has loss generally +L+ </SectLabel_bodyText> <SectLabel_figure> Ice Hockey +L+ 0	1000 2000 3000 4000 5000 6000 +L+ #Pages fetched +L+ Kayaking +L+ 0	5000	10000	15000	20000 +L+ #Pages fetched +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 12: Recall for a crawler using the apprentice but +L+ limited to the set of pages crawled earlier by the baseline +L+ crawler. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> somewhere between the loss of the baseline and the enhanced +L+ crawler. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.7 Effect of training the apprentice online +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Next we observe the effect of a mid-flight correction when +L+ the apprentice is trained some way into a baseline and +L+ switched into the circuit. The precise steps were: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Run the baseline crawler for the first n page fetches, +L+ then stop it. +L+ 2. Prepare instances and train the apprentice. +L+ 3. Re-evaluate the priorities of all unvisited pages v in +L+ the frontier table using the apprentice. +L+ 4. Switch in the apprentice and resume an enhanced +L+ crawl. +L+ </SectLabel_listItem> <SectLabel_bodyText> We report our experience with “Folk Dancing.” The baseline +L+ crawl was stopped after 5200 pages were fetched. Re- +L+ evaluating the priority of frontier nodes led to radical +L+ changes in their individual ranks as well as the priority +L+ distributions. As shown in Figure 13(a), the baseline learner +L+ is overly optimistic about the yield it expects from the +L+ frontier, whereas the apprentice already abandons a large +L+ fraction of frontier outlinks, and is less optimistic about +L+ </SectLabel_bodyText> <SectLabel_figure> 35% +L+ Baseline +L+ Apprentice +L+ Intersect +L+ Baseline +L+ Apprentice +L+ Intersect +L+ Basketball +L+ 4% +L+ FolkDance +L+ 9% +L+ 47% +L+ Baseline +L+ Apprentice +L+ Intersect +L+ 3% +L+ 39% +L+ 34% +L+ Baseline +L+ Apprentice +L+ Intersect +L+ 17% +L+ 57% +L+ FlyFishing +L+ 48% +L+ 49% +L+ 58% +L+ IceHockey +L+ Baseline +L+ Recall +L+ Apprentice +L+ 1000 +L+ 0 +L+ Baseline +L+ Recall +L+ Apprentice +L+ 	10000 +L+ 	5000 +L+ 	0 +L+ 157 +L+ Folk Dancing +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 13: The effect of online training of the apprentice. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> The apprentice makes sweeping changes in the +L+ estimated promise of unvisited nodes in the crawl frontier. +L+ (b) Resuming the crawl under the guidance of the +L+ apprentice immediately shows significant reduction in the +L+ loss accumulation rate. +L+ the others, which appears more accurate from the Bayesian +L+ perspective. +L+ Figure 13(b) shows the effect of resuming an enhanced +L+ crawl guided by the trained apprentice. The new (u, v) +L+ instances are all guaranteed to be unknown to the apprentice +L+ now. It is clear that the apprentice’s prioritization +L+ immediately starts reducing the loss rate. Figure 14 shows +L+ an even more impressive example. There are additional mild +L+ gains from retraining the apprentice at later points. It may +L+ be possible to show a more gradual online learning effect +L+ by retraining the classifier at a finer interval, e.g., every +L+ 100 page fetches, similar to Aggarwal et al. In our context, +L+ however, losing a thousand pages at the outset because of +L+ the baseline crawler’s limitation is not a disaster, so we need +L+ not bother. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.8 Effect of other features +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We experimented with two other kinds of feature, which we +L+ call topic and cocitation features. +L+ Our limiting dmax to 5 may deprive the apprentice of +L+ important features in the source page u which are far from +L+ the link (u, v). One indirect way to reveal such features +L+ to the apprentice is to classify u, and to add the names +L+ of some of the top-scoring classes for u to the instance +L+ (u, v). §2.2.3 explains why this may help. This modification +L+ resulted in a 1% increase in the accuracy of the apprentice. +L+ A further increase of 1% was observed if we added all +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 14: Another example of training the apprentice +L+ online followed by starting to use it for crawl guidance. +L+ Before guidance, loss accumulation rate is over 30%, after, +L+ it drops to only 6%. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> prefixes of the class name. For example, the full name +L+ for the Linux category is /Computers/Software/Operating- +L+ Systems/Linux. We added all of the following to the +L+ feature set of the source page: /, /Computers, /Computers/ +L+ Software, /Computers/Software/Operating-Systems and +L+ /Computers/Software/Operating-Systems/Linux. We also +L+ noted that various class names and some of their prefixes +L+ appeared amongst the best discriminants of the positive and +L+ negative classes. +L+ Cocitation features for the link (u, v) are constructed by +L+ looking for other links (u, w) within a DOM distance of dmax +L+ such that w has already been fetched, so that Pr(c*Iw) is +L+ known. We discretize Pr(c*Iw) to two values HiGH and Low +L+ as in §2.3, and encode the feature as (Low, d) or (HiGH, d). +L+ The use of cocitation features did not improve the accuracy +L+ of the apprentice to any appreciable extent. +L+ For both kinds of features, we estimated that random +L+ variations in crawling behavior (because of fluctuating +L+ network load and tie-breaking frontier scores) may prevent +L+ us from measuring an actual benefit to crawling under +L+ realistic operating conditions. We note that these ideas may +L+ be useful in other settings. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4 Conclusion +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have presented a simple enhancement to a focused +L+ crawler that helps assign better priorities to the unvisited +L+ URLs in the crawl frontier. This leads to a higher rate of +L+ fetching pages relevant to the focus topic and fewer false +L+ positives which must be discarded after spending network, +L+ CPU and storage resources processing them. There is no +L+ need to manually train the system with paths leading to +L+ relevant pages. The key idea is an apprentice learner which +L+ can accurately predict the worth of fetching a page using +L+ DOM features on pages that link to it. We show that the +L+ DOM features we use are superior to simpler alternatives. +L+ Using topics from Dmoz, we show that our new system can +L+ cut down the fraction of false positives by 30–90%. +L+ We are exploring several directions in ongoing work. +L+ We wish to revisit continuous regression techniques for the +L+ apprentice, as well as more extensive features derived from +L+ the DOM. For example, we can associate with a token t the +L+ length $ of the DOM path from the text node containing t to +L+ </SectLabel_bodyText> <SectLabel_figure> 0	0-.2	.2-.4.4-.6	.6-.8.8-1 +L+ Estimated relevance of outlinks +L+ Baseline +L+ Apprentice +L+ (b)	4500	#Pages crawled	5500 +L+ Collect instances +L+ for apprentice +L+ Train +L+ apprentice +L+ Apprentice +L+ guides crawl +L+ Folk Dancing +L+ 2700 +L+ 2600 +L+ 2500 +L+ 2400 +L+ 2300 +L+ 2200 +L+ 2100 +L+ 12000 +L+ 10000 +L+ 8000 +L+ 6000 +L+ 4000 +L+ 2000 +L+ 0 +L+ 1800 +L+ 1600 +L+ 1400 +L+ 1200 +L+ 1000 +L+ 800 +L+ 600 +L+ 2000 3000 4000 5000 6000 7000 8000 +L+ #Pages fetched +L+ Collect +L+ instances for +L+ apprentice +L+ Classical Composers +L+ Train +L+ apprentice +L+ Apprentice +L+ guides crawl +L+ </SectLabel_figure> <SectLabel_page> 158 +L+ </SectLabel_page> <SectLabel_bodyText> the HREF to v, or the depth of their least common ancestor +L+ in the DOM tree. We cannot use these in lieu of DOM offset, +L+ because regions which are far apart lexically may be close +L+ to each other along a DOM path. (t, f, d) features will be +L+ more numerous and sparser than (t, d) features, and could +L+ be harder to learn. The introduction of large numbers of +L+ strongly dependent features may even reduce the accuracy +L+ of the apprentice. Finally, we wish to implement some form +L+ of active learning where only those instances (u, v) with the +L+ largest I Pr(c* Iu) - Pr(c* Iv) I are chosen as training instances +L+ for the apprentice. +L+ Acknowledgments: Thanks to the referees for suggest- +L+ ing that we present Figure 7. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] C. C. Aggarwal, F. Al-Garawi, and P. S. Yu. Intelligent +L+ crawling on the World Wide Web with arbitrary predicates. In +L+ WWW2001, Hong Kong, May 2001. ACM. Online at http: +L+ //www10.org/cdrom/papers/110/. +L+ [2] C. Apte, F. Damerau, and S. M. Weiss. Automated learning +L+ of decision rules for text categorization. ACM Transactions on +L+ Information Systems, 1994. IBM Research Report RC18879. +L+ [3] A. Blum and T. M. Mitchell. Combining labeled and unlabeled +L+ data with co-training. In Computational Learning Theory, +L+ pages 92–100,1998. +L+ [4] S. Chakrabarti. Integrating the document object model with +L+ hyperlinks for enhanced topic distillation and information +L+ extraction. In WWW 10, Hong Kong, May 2001. Online at +L+ http://www10.org/cdrom/papers/489. +L+ [5] S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan. +L+ Scalable feature selection, classification and signature generation +L+ for organizing large text databases into hierarchical topic +L+ taxonomies. VLDB Journal, Aug. 1998. Online at http: +L+ //www.cs.berkeley.edu/~soumen/VLDB54_3.PDF. +L+ [6] S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. Raghavan, +L+ and S. Rajagopalan.	Automatic resource compilation by +L+ analyzing hyperlink structure and associated text. In 7th World- +L+ wide web conference (WWW7), 1998. Online at http://www7. +L+ scu.edu.au/programme/fullpapers/1898/com1898.html. +L+ [7] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced hypertext +L+ categorization using hyperlinks. In SIGMOD Conference. ACM, +L+ 1998. Online at http://www.cs.berkeley.edu/~soumen/sigmod98. +L+ ps. +L+ [8] S. Chakrabarti, B. E. Dom, D. A. Gibson, R. Kumar, +L+ P. Raghavan, S. Rajagopalan, and A. Tomkins. Topic distillation +L+ and spectral filtering. Artificial Intelligence Review, 13(5– +L+ 6):409–435, 1999. +L+ [9] S. Chakrabarti, M. van den Berg, and B. Dom. Focused +L+ crawling: a new approach to topic-specific web resource +L+ discovery. Computer Networks, 31:1623–1640, 1999. First +L+ appeared in the 8th International World Wide Web Conference, +L+ Toronto, May 1999. Available online at http://www8.org/ +L+ w8-papers/5a-search-query/crawling/index.html. +L+ [10] J. Cho, H. Garcia-Molina, and L. Page. Efficient crawling +L+ through URL ordering. In 7th World Wide Web Conference, +L+ Brisbane, Australia, Apr. 1998. Online at http://www7.scu.edu. +L+ au/programme/fullpapers/1919/com1919.htm. +L+ [11] B. D. Davison. Topical locality in the Web. In Proceedings +L+ of the 23rd Annual International Conference on Research and +L+ Development in Information Retrieval (SIGIR 2000), pages +L+ 272–279, Athens, Greece, July 2000. ACM. Online at http:// +L+ www.cs.rutgers.edu/~davison/pubs/2000/sigir/. +L+ [12] P. M. E. De Bra and R. D. J. Post. Information retrieval +L+ in the world-wide web: Making client-based searching feasible. +L+ In Proceedings of the First International World Wide Web +L+ Conference, Geneva, Switzerland, 1994. Online at http://www1. +L+ cern.ch/PapersWWW94/reinpost.ps. +L+ [13] P. M. E. De Bra and R. D. J. Post. Searching for arbitrary +L+ information in the WWW: The fish search for Mosaic. In Second +L+ World Wide Web Conference ’9¢: Mosaic and the Web, +L+ Chicago, Oct. 1994. Online at http://archive.ncsa.uiuc.edu/ +L+ SDG/IT94/Proceedings/Searching/debra/article.html and http: +L+ //citeseer.nj.nec.com/172936.html. +L+ [14] M. Diligenti, F. Coetzee, S. Lawrence, C. L. Giles, and M. Gori. +L+ Focused crawling using context graphs. In A. E. Abbadi, M. L. +L+ Brodie, S. Chakravarthy, U. Dayal, N. Kamel, G. Schlageter, +L+ and K.-Y. Whang, editors, VLDB 2000, Proceedings of +L+ 26th International Conference on Very Large Data Bases, +L+ September 10-1¢, 2000, Cairo, Egypt, pages 527–534. Morgan +L+ Kaufmann, 2000. Online at http://www.neci.nec.com/~lawrence/ +L+ papers/focus-vldb00/focus-vldb00.pdf. +L+ [15] W. A. Gale, K. W. Church, and D. Yarowsky. A method for +L+ disambiguating word senses in a large corpus. Computer and +L+ the Humanities, 26:415–439, 1993. +L+ [16] M. Hersovici, M. Jacovi, Y. S. Maarek, D. Pelleg, M. Shtalhaim, +L+ and S. Ur. The shark-search algorithm—an application: Tailored +L+ Web site mapping. In WWW7, 1998. Online at http://www7.scu. +L+ edu.au/programme/fullpapers/1849/com1849.htm. +L+ [17] T. Joachims, D. Freitag, and T. Mitchell. Web Watcher: A tour +L+ guide for the web. In IJCAI, Aug. 1997. Online at http://www. +L+ cs.cmu.edu/~webwatcher/ijcai97.ps. +L+ [18] H. Leiberman. Letizia: An agent that assists Web browsing. In +L+ International Joint Conference on Artificial Intelligence (IJ- +L+ CAI), Montreal, Aug. 1995. See Website at http://lieber.www. +L+ media.mit.edu/people/lieber/Lieberary/Letizia/Letizia.html. +L+ [19] H. Leiberman, C. Fry, and L. Weitzman. Exploring the Web +L+ with reconnaissance agents. CACM, 44(8):69–75, Aug. 2001. +L+ http://www.acm.org/cacm. +L+ [20] A. McCallum. Bow: A toolkit for statistical language modeling, +L+ text retrieval, classification and clustering. Software available +L+ from http://www.cs.cmu.edu/~mccallum/bow/,1998. +L+ [21] A. McCallum and K. Nigam. A comparison of event models for +L+ naive Bayes text classification. In AAAI/ICML-98 Workshop +L+ on Learning for Teat Categorization, pages 41–48. AAAI Press, +L+ 1998. Online at http://www.cs.cmu.edu/~knigam/. +L+ [22] A. McCallum and K. Nigam. A comparison of event models for +L+ naive Bayes text classification. In AAAI/ICML-98 Workshop +L+ on Learning for Teat Categorization, pages 41–48. AAAI Press, +L+ 1998. Also technical report WS-98-05, CMU; online at http: +L+ //www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf. +L+ [23] F. Menczer. Links tell us about lexical and semantic +L+ Web content. Technical Report Computer Science Abstract +L+ CS.IR/0108004, arXiv.org, Aug. 2001. Online at http://arxiv. +L+ org/abs/cs.IR/0108004. +L+ [24] F. Menczer and R. K. Belew. Adaptive retrieval agents: +L+ Internalizing local context and scaling up to the Web. Machine +L+ Learning, 39(2/3):203–242, 2000. Longer version available as +L+ Technical Report CS98-579, http://dollar.biz.uiowa.edu/~fil/ +L+ Papers/MLJ.ps, University of California, San Diego. +L+ [25] F. Menczer, G. Pant, M. Ruiz, and P. Srinivasan. Evaluating +L+ topic-driven Web crawlers. In SIGIR, New Orleans, Sept. 2001. +L+ ACM.	Online at http://dollar.biz.uiowa.edu/~fil/Papers/ +L+ sigir-01.pdf. +L+ [26] T. Mitchell. Machine Learning. McGraw Hill, 1997. +L+ [27] T. Mitchell. Mining the Web. In SIGIR 2001, Sept. 2001. Invited +L+ talk. +L+ [28] S. Mukherjea. WTMS: a system for collecting and analyzing +L+ topic-specific Web information. WWW9/Computer Networks, +L+ 33(1–6):457–471, 2000. Online at http://www9.org/w9cdrom/293/ +L+ 293.html. +L+ [29] S. RaviKumar, P. Raghavan, S. Rajagopalan, D. Sivakumar, +L+ A. Tomkins, and E. Upfal. Stochastic models for the Web graph. +L+ In FOCS, volume 41, pages 57–65. IEEE, nov 2000. Online at +L+ http://www.cs.brown.edu/people/eli/papers/focs00.ps. +L+ [30] J. Rennie and A. McCallum. Using reinforcement learning to +L+ spider the web efficiently. In ICML, 1999. Online at http:// +L+ www.cs.cmu.edu/~mccallum/papers/rlspider-icml99s.ps.gz. +L+ [31] G. Salton and M. J. McGill. Introduction to Modern +L+ Information Retrieval. McGraw-Hill, 1983. +L+ [32] M. Subramanyam, G. V. R. Phanindra, M. Tiwari, and M. Jain. +L+ Focused crawling using TFIDF centroid. Hypertext Retrieval +L+ and Mining (CS610) class project, Apr. 2001. Details available +L+ from manyam@cs.utexas.edu. +L+ [33] L. Torgo and J. Gama. Regression by classification. In D. Borges +L+ and C. Kaestner, editors, Brasilian AI Symposium, volume 1159 +L+ of Lecture Notes in Artificial Intelligence, Curitiba, Brazil, +L+ 1996. Springer-Verlag. Online at http://www.ncc.up.pt/~ltorgo/ +L+ Papers/list_pub.html. +L+ </SectLabel_reference> <SectLabel_page> 159 +L+ </SectLabel_page>
<SectLabel_title> A Computational Approach to Reflective Meta-Reasoning about +L+ Languages with Bindings * +L+ </SectLabel_title> <SectLabel_author> Aleksey Nogin Alexei Kopylov Xin Yu Jason Hickey +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ California Institute of Technology +L+ </SectLabel_affiliation> <SectLabel_address> M/C 256-80, Pasadena, CA 91125 +L+ </SectLabel_address> <SectLabel_email> {nogin,kopylov,xiny,jyh}@cs.caltech.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We present a foundation for a computational meta-theory of lan- +L+ guages with bindings implemented in a computer-aided formal rea- +L+ soning environment. Our theory provides the ability to reason ab- +L+ stractly about operators, languages, open-ended languages, classes +L+ of languages, etc. The theory is based on the ideas of higher-order +L+ abstract syntax, with an appropriate induction principle parameter- +L+ ized over the language (i. e. a set of operators) being used. In our ap- +L+ proach, both the bound and free variables are treated uniformly and +L+ this uniform treatment extends naturally to variable-length bind- +L+ ings. The implementation is reflective, namely there is a natural +L+ mapping between the meta-language of the theorem-prover and the +L+ object language of our theory. The object language substitution op- +L+ eration is mapped to the meta-language substitution and does not +L+ need to be defined recursively. Our approach does not require de- +L+ signing a custom type theory; in this paper we describe the im- +L+ plementation of this foundational theory within a general-purpose +L+ type theory. This work is fully implemented in the MetaPRL the- +L+ orem prover, using the pre-existing NuPRL-like Martin-L¨of-style +L+ computational type theory. Based on this implementation, we lay +L+ out an outline for a framework for programming language experi- +L+ mentation and exploration as well as a general reflective reasoning +L+ framework. This paper also includes a short survey of the existing +L+ approaches to syntactic reflection. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors D.3.1 [Programming Lan- +L+ guages]: Formal Definitions and Theory—Syntax; F.4.3 [Math- +L+ ematical Logic and Formal Languages]: Formal Languages— +L+ Operations on languages +L+ General Terms Languages, Theory, Verification +L+ Keywords Higher-Order Abstract Syntax, Reflection, Type The- +L+ ory, Meta PRL, N uPRL, Programming Language Experimentation, +L+ Languages with Bindings. +L+ </SectLabel_sectionHeader> <SectLabel_footnote> * An extended version of this paper is available as Caltech Technical Report +L+ CaltechCSTR:2005.003 [NKYH05] +L+ </SectLabel_footnote> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for personal or +L+ classroom use is granted without fee provided that copies are not made or distributed +L+ for profit or commercial advantage and that copies bear this notice and the full citation +L+ on the first page. To copy otherwise, to republish, to post on servers or to redistribute +L+ to lists, requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> MERLIN’05 September 30, 2005, Tallinn, Estonia. +L+ </SectLabel_note> <SectLabel_copyright> Copyright �c 2005 ACM 1-59593-072-8/05/0009...$5.00. +L+ </SectLabel_copyright> <SectLabel_sectionHeader> 1. Introduction +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 1.1 Reflection +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Very generally, reflection is the ability of a system to be “self- +L+ aware” in some way. More specifically, by reflection we mean the +L+ property of a computational or formal system to be able to access +L+ and internalize some of its own properties. +L+ There are many areas of computer science where reflection +L+ plays or should play a major role. When exploring properties of +L+ programming languages (and other languages) one often realizes +L+ that languages have at least two kinds of properties — semantic +L+ properties that have to do with the meaning of what the language’s +L+ constructs express and syntactic properties of the language itself. +L+ Suppose for example that we are exploring some language that +L+ contains arithmetic operations. And in particular, in this language +L+ one can write polynomials like x2 +2x + 1. In this case the number +L+ of roots of a polynomial is a semantic property since it has to do +L+ with the valuation of the polynomial. On the other hand, the degree +L+ of a polynomial could be considered an example of a syntactic +L+ property since the most natural way to define it is as a property of +L+ the expression that represents that polynomial. Of course, syntactic +L+ properties often have semantic consequences, which is what makes +L+ them especially important. In this example, the number of roots of +L+ a polynomial is bounded by its degree. +L+ Another area where reflection plays an important role is run- +L+ time code generation — in most cases, a language that supports +L+ run-time code generation is essentially reflective, as it is capable +L+ of manipulating its own syntax. In order to reason about run-time +L+ code generation and to express its semantics and properties, it is +L+ natural to use a reasoning system that is reflective as well. +L+ There are many different flavors of reflection. The syntactic +L+ reflection we have seen in the examples above, which is the ability +L+ of a system to internalize its own syntax, is just one of these +L+ many flavors. Another very important kind of reflection is logical +L+ reflection, which is the ability of a reasoning system or logic to +L+ internalize and reason about its own logical properties. A good +L+ example of a logical reflection is reasoning about knowledge — +L+ since the result of reasoning about knowledge is knowledge itself, +L+ the logic of knowledge is naturally reflective [Art04]. +L+ In most cases it is natural for reflection to be iterated. In the +L+ case of syntactic reflection we might care not only about the syntax +L+ of our language, but also about the syntax used for expressing the +L+ syntax, the syntax for expressing the syntax for expressing the +L+ syntax and so forth. In the case of the logic of knowledge it is +L+ natural to have iterations of the form “I know that he knows that +L+ I know ...”. +L+ When a formal system is used to reason about properties of pro- +L+ gramming languages, iterated reflection magnifies the power of the +L+ </SectLabel_bodyText> <SectLabel_page> 2 +L+ </SectLabel_page> <SectLabel_bodyText> system, making it more natural to reason not just about individual +L+ languages, but also about classes of languages, language schemas, +L+ and so on. More generally, reflection adds a lot of additional power +L+ to a formal reasoning system [GS89, Art99]. In particular, it is +L+ well-known [G¨od36, Mos52, EM71, Par71] that reflection allows +L+ a super-exponential reduction in the size of certain proofs. In addi- +L+ tion, reflection could be a very useful mechanism for implement- +L+ ing proof search algorithms [ACU93, GWZ00, CFW04]. See also +L+ [Har95] for a survey of reflection in theorem proving. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 1.2 Uniform Reflection Framework +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> For each of the examples in the previous section there are many +L+ ad-hoc ways of achieving the specific benefits of a specific fla- +L+ vor of reflection. This work aims at creating a unifying reflective +L+ framework that would allow achieving most of these benefits in a +L+ uniform manner, without having to reinvent and re-implement the +L+ basic reflective methodology every time. We believe that such a +L+ framework will increase the power of the formal reasoning tools, +L+ and it may also become an invaluable tool for exploring the proper- +L+ ties of novel programming languages, for analyzing run-time code +L+ generation, and for formalizing logics of knowledge. +L+ This paper establishes a foundation for the development of this +L+ framework — a new approach to reflective meta-reasoning about +L+ languages with bindings. We present a theory of syntax that: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	in a natural way provides both a higher-order abstract syntax +L+ (HOAS) approach to bindings and a de Bruijn-style approach +L+ to bindings, with easy and natural translation between the two; +L+ •	provides a uniform HOAS-style approach to both bound and +L+ free variables that extends naturally to variable-length “vectors” +L+ of binders; +L+ •	permits meta-reasoning about languages — in particular, the +L+ operators, languages, open-ended languages, classes of lan- +L+ guages etc. are all first-class objects that can be reasoned about +L+ both abstractly and concretely; +L+ •	comes with a natural induction principle for syntax that can be +L+ parameterized by the language being used; +L+ •	provides a natural mapping between the object syntax and meta- +L+ syntax that is free of exotic terms, and allows mapping the +L+ object-level substitution operation directly to the meta-level one +L+ (i.e. P-reduction); +L+ •	is fully derived in a pre-existing type theory in a theorem +L+ prover; +L+ •	is designed to serve as a foundation for a general reflective +L+ reasoning framework in a theorem prover; +L+ •	is designed to serve as a foundation for a programming lan- +L+ guage experimentation framework. +L+ </SectLabel_listItem> <SectLabel_bodyText> The paper is structured as follows. Our work inherits a large +L+ number of ideas from previous efforts and we start in Section 2 +L+ with a brief survey of existing techniques for formal reasoning +L+ about syntax. Next in Section 3 we outline our approach to rea- +L+ soning about syntax and in Section 4 we present a formal account +L+ of our theory based on a Martin-L¨of style computational type the- +L+ ory [CAB+86, HAB+] and the implementation of that account in +L+ the MetaPRL theorem prover [Hic97, Hic99, Hic01, HNC+03, +L+ HNK+, HAB+]. Then in Section 5 we outline our plan for building +L+ a uniform reflection framework based on the syntactic reflection. +L+ Finally, in Section 6 we resume the discussion of related work that +L+ was started in Section 2. +L+ 1.3 Notation and Terminology +L+ We believe that our approach to reasoning about syntax is fairly +L+ general and does not rely on any special features of the theo- +L+ rem prover we use. However, since we implement this theory in +L+ MetaPRL, we introduce some basic knowledge about MetaPRL +L+ terms. +L+ A MetaPRL term consists of: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. An operator name (like “sum”), which is a unique name indi- +L+ cating the logic and component of a term; +L+ 2. A list of parameters representing constant values; and +L+ 3. A set of subterms with possible variable bindings. +L+ </SectLabel_listItem> <SectLabel_bodyText> We use the following syntax to describe terms, based on the N u P R L +L+ definition [ACHA90]: +L+ In addition, MetaPRL has a meta-syntax somewhat similar to +L+ the higher-order abstract syntax presented in Pfenning and Elliott +L+ [PE88]. MetaPRL uses the second-order variables in the style of +L+ Huet and Lang [HL78] to describe term schemas. For example, +L+ Xx.V [x], where V is a second-order variable of arity 1, is a schema +L+ that stands for an arbitrary term whose top-level operator is X. +L+ This meta-syntax requires that every time a binding occurrence +L+ is explicitly specified in a schema, all corresponding bound occur- +L+ rences have to be specified as well. This requirement makes it very +L+ easy to specify free variable restrictions — for example, Xx.V, +L+ where V is a second-order meta-variable of arity 0, is a schema +L+ that stands for an arbitrary term whose top-level operator is X and +L+ whose body does not have any free occurrences of the variable +L+ bound by that X. In particular, the schema Xx. V matches the term +L+ Xy.1, but not the term Xx.x. +L+ In addition, this meta-language allows specifying certain term +L+ transformations, including implicit substitution specifications. For +L+ example, a beta reduction transformation may be specified using +L+ the following schema: +L+ </SectLabel_bodyText> <SectLabel_equation> (Xx.V1 [x]) V2 H V1 [V2] +L+ </SectLabel_equation> <SectLabel_bodyText> Here the substitution of V2 for x in V1 is specified implicitly. +L+ Throughout this paper we will use this second-order notation to +L+ denote arbitrary terms — namely, unless stated otherwise, when we +L+ write “Xx.t[x]” we mean an arbitrary term of this form, not a term +L+ containing a concrete second-order variable named “t”. +L+ As in LF [HHP93] we assume that object level variables (i.e. +L+ the variables of the language whose syntax we are expressing) +L+ are directly mapped to meta-theory variables (i.e. the variable of +L+ the language that we use to express the syntax). Similarly, we +L+ assume that the object-level binding structure is mapped to the +L+ meta-level binding structure. In other words, the object-level notion +L+ of the “binding/bound occurrence” is a subset of that in the meta- +L+ language. We also consider a-equal terms — both on the object +L+ level and on the meta-level — to be identical and we assume that +L+ substitution avoids capture by renaming. +L+ The sequent schema language we use [NH02] contains a num- +L+ ber of more advanced features in addition to those outlined here. +L+ However, for the purposes of this presentation, the basic features +L+ outlined above are sufficient. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. Previous Models of Reflection +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In 1931 G¨odel used reflection to prove his famous incompleteness +L+ theorem [G¨od31]. To express arithmetic in arithmetic itself, he +L+ assigned a unique number (a G¨odel number) to each arithmetic +L+ </SectLabel_bodyText> <SectLabel_equation> opname +L+ �- J +L+ operator name +L+ [p1; .. .; pn] f�v1.t1; . . . ; �vm.tm} +L+ Y	Y	J +L+ parameters	subterms +L+ </SectLabel_equation> <SectLabel_page> 3 +L+ </SectLabel_page> <SectLabel_bodyText> formula. A G¨odel number of a formula is essentially a numeric +L+ code of a string of symbols used to represent that formula. +L+ A modern version of the G¨odel’s approach was used by Aitken +L+ et al. [ACHA90, AC92, ACU93, Con94] to implement reflection +L+ in the NuPRL theorem prover [CAB+86, ACE+00]. A large part +L+ of this effort was essentially a reimplementation of the core of the +L+ NuPRL prover inside NuPRL’s logical theory. +L+ In G¨odel’s approach and its variations (including Aitken’s one), +L+ a general mechanism that could be used for formalizing one logical +L+ theory in another is applied to formalizing a logical theory in itself. +L+ This can be very convenient for reasoning about reflection, but for +L+ our purposes it turns out to be extremely impractical. First, when +L+ formalizing a theory in itself using generic means, the identity +L+ between the theory being formalized and the one in which the +L+ formalization happens becomes very obfuscated, which makes it +L+ almost impossible to relate the reflected theory back to the original +L+ one. Second, when one has a theorem proving system that already +L+ implements the logical theory in question, creating a completely +L+ new implementation of this logical theory inside itself is a very +L+ tedious redundant effort. Another practical disadvantage of the +L+ G¨odel numbers approach is that it tends to blow up the size of +L+ the formulas; and iterated reflection would cause the blow-up to +L+ be iterated as well, making it exponential or worse. +L+ A much more practical approach is being used in some pro- +L+ gramming languages, such as Lisp and Scheme. There, the com- +L+ mon solution is for the implementation to expose its internal syntax +L+ representation to user-level code by the quote constructor (where +L+ quote (t) prevents the evaluation of the expression t). The prob- +L+ lems outlined above are solved instantly by this approach: there is +L+ no blow-up, there is no repetition of structure definitions, there is +L+ even no need for verifying that the reflected part is equivalent to the +L+ original implementation since they are identical. Most Scheme im- +L+ plementations take this even further: the eval function is the inter- +L+ nal function for evaluating a Scheme expression, which is exposed +L+ to the user-level; Smith [Smi84] showed how this approach can +L+ achieve an infinite tower of processors. A similar language with the +L+ quotation and antiquotation operators was introduced in [GMO03]. +L+ This approach, however, violates the congruence property with +L+ respect to computation: if two terms are computationally equal then +L+ one can be substituted for the other in any context. For instance, +L+ although 2 * 2 is equal to 4, the expressions “2*2” and “4” are +L+ syntactically different, thus we can not substitute 2*2 by 4 in +L+ the expression quote(2*2). The congruence property is essential +L+ in many logical reasoning systems, including the NuPRL system +L+ mentioned above and the MetaPRL system [HNC+03, HNK+, +L+ HAB+] that our group uses. +L+ A possible way to expose the internal syntax without violat- +L+ ing the congruence property is to use the so-called “quoted” or +L+ “shifted” operators [AA99, Bar01, Bar05] rather than quoting the +L+ whole expression at once. For any operator op in the original lan- +L+ guage, we add the quoted operator (denoted as op) to represent a +L+ term built with the operator op. For example, if the original lan- +L+ guage contains the constant “0” (which, presumably, represents the +L+ number 0), then in the reflected language, 0 would stand for the +L+ term that denotes the expression “0”. Generally, the quoted opera- +L+ tor has the same arity as the original operator, but it is defined on +L+ syntactic terms rather than on semantic objects. For instance, while +L+ * is a binary operator on numbers, * is a binary operator on terms. +L+ Namely, if t1 and t2 are syntactic terms that stand for expressions +L+ e1 and e2 respectively, then t1 *t2 is a new syntactic term that stands +L+ for the expression e1 *e2. Thus, the quotation of the expression 1 *2 +L+ would be 1 * 2. +L+ In general, the well-formedness (typing) rule for a quoted oper- +L+ ator is the following: +L+ </SectLabel_bodyText> <SectLabel_equation> t1 E Term	...	tn E Term +L+ op{t1; ... ; tn} E Term +L+ where Term is a type of terms. +L+ </SectLabel_equation> <SectLabel_bodyText> Note that quotations can be iterated arbitrarily many times, +L+ allowing us to quote quoted terms. For instance, 1 stands for the +L+ term that denotes the term that denotes the numeral 1. +L+ Problems arise when quoting expressions that contain binding +L+ variables. For example, what is the quotation of Xx.x? There are +L+ several possible ways of answering this question. A commonly +L+ used approach [PE88, DH94, DFH95, ACM02, ACM03] in logical +L+ frameworks such as Elf [Pfe89], LF [HHP93], and Isabelle [PN90, +L+ Pau94] is to construct an object logic with a concrete X operator +L+ that has a type like +L+ </SectLabel_bodyText> <SectLabel_equation> (Term -+ Term) -+ Term or (Var -+ Term) -+ Term. +L+ </SectLabel_equation> <SectLabel_bodyText> In this approach, the quoted Xx.x might look like X(Xx.x) and the +L+ quoted Xx.1 might look like X(Xx.1). Note that in these examples +L+ the quoted terms have to make use of both the syntactic (i. e. quoted) +L+ operator X and the semantic operator X. +L+ Exotic Terms. Naive implementations of the above approach +L+ suffer from the well-known problem of exotic terms [DH95, +L+ DFH95]. The issue is that in general we can not allow applying +L+ the X operator to an arbitrary function that maps terms to terms (or +L+ variables to terms) and expect the result of such an application to +L+ be a “proper” reflected term. +L+ Consider for example the following term: +L+ </SectLabel_bodyText> <SectLabel_equation> X(Xx. if x = 1 then 1 else 2) +L+ </SectLabel_equation> <SectLabel_bodyText> It is relatively easy to see that it is not a real syntactic term and +L+ can not be obtained by quoting an actual term. (For comparison, +L+ consider X(Xx. if x = 1 then 1 else 2), which is a quotation of +L+ Xx. if x = 1 then 1 else 2). +L+ How can one ensure that Xe denotes a “real” term and not an +L+ “exotic” one? That is, is it equal to a result of quoting an actual +L+ term of the object language? One possibility is to require e to be +L+ a substitution function; in other words it has to be equal to an +L+ expression of the form Xx.t[x] where t is composed entirely of term +L+ constructors (i.e. quoted operators) and x, while using destructors +L+ (such as case analysis, the if operator used in the example above, +L+ etc) is prohibited. +L+ There are a number of approaches to enforcing the above restric- +L+ tion. One of them is the usage of logical frameworks with restricted +L+ function spaces [PE88, HHP93], where X-terms may only con- +L+ tain constructors. Another is to first formalize the larger type that +L+ does include exotic terms and then to define recursively a predicate +L+ describing the “validity” or “well-formedness” of a term [DH94, +L+ DFH95] thus removing the exotic terms from consideration. Yet +L+ another approach is to create a specialized type theory that com- +L+ bines the idea of restricted function spaces with a modal type oper- +L+ ator [DPS97, DL99, DL01]. There the case analysis is disallowed +L+ on objects of “pure” type T, but is allowed on objects of a special +L+ type ❑T. This allows expressing both the restricted function space +L+ “T1 -+ T2” and the unrestricted one “(�T1) -+ T2” within a single +L+ type theory. +L+ Another way of regarding the problem of exotic terms is that it +L+ is caused by the attempt to give a semantic definition to a primarily +L+ syntactic property. A more syntax-oriented approach was used by +L+ Barzilay et al. [BA02, BAC03, Bar05]. In Barzilay’s approach, the +L+ quoted version of an operator that introduces a binding has the +L+ same shape (i.e. the number of subterms and the binding structure) +L+ as the original one and the variables (both the binding and the +L+ </SectLabel_bodyText> <SectLabel_none> (1) +L+ </SectLabel_none> <SectLabel_page> 4 +L+ </SectLabel_page> <SectLabel_bodyText> bound occurrences) are unaffected by the quotation. For instance, +L+ the quotation of Xx.x is just Xx.x. +L+ The advantages of this approach include: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	This approach is simple and clear. +L+ •	Quoted terms have the same structure as original ones, inherit- +L+ ing a lot of properties of the object syntax. +L+ •	In all the above approaches, the a-equivalence relation for +L+ quoted terms is inherited “for free”. For example, Xx.x and +L+ Xy.y are automatically considered to be the same term. +L+ •	Substitution is also easy: we do not need to re-implement the +L+ substitution that renames binding variables to avoid the capture +L+ of free variables; we can use the substitution of the original +L+ language instead. +L+ </SectLabel_listItem> <SectLabel_bodyText> To prune exotic terms, Barzilay says that Xx.t[x] is a valid term +L+ when Xx.t[x] is a substitution function. He demonstrates that it is +L+ possible to formalize this notion in a purely syntactical fashion. In +L+ this setting, the general well-formedness rule for quoted terms with +L+ bindings is the following: +L+ </SectLabel_bodyText> <SectLabel_equation> is substk {x1, · · · , xk.t["x]}	· · ·	is substl {z 1, · · · , zl.s["z]} +L+ op{x1, · · · , xk.t["x]; · · · ; z1, · · · , zl.s["z]} E Term +L+ </SectLabel_equation> <SectLabel_bodyText> (2) +L+ where is substn {x1 , · · · , xn.t["x]} is the proposition that t is a sub- +L+ stitution function over variables x1 , · · · , xn (in other words, it is a +L+ syntactic version of the Valid predicate of [DH94, DFH95]). This +L+ proposition is defined syntactically by the following two rules: +L+ </SectLabel_bodyText> <SectLabel_equation> is substn {x1, · · · , xn . xi} +L+ and +L+ is substn+k{x1, · · · , xn,y1, · · · , yk.t["x;"y]} +L+ ... +L+ is substn+l {x1, · · · , xn, z1, ··· , zl.s["x; "z]}} +L+ is substn {x1 · · ·xn.op{y1 · · ·yk.t["x; "y]; · · · ; z1 · · ·zl.s["x; "z]}} +L+ </SectLabel_equation> <SectLabel_bodyText> In this approach the is substn {} and X operators are essentially +L+ untyped (in NuPRL type theory, the computational properties of +L+ untyped terms are at the core of the semantics; types are added on +L+ top of the untyped computational system). +L+ Recursive Definition and Structural Induction Principle. A +L+ difficulty shared by both the straightforward implementations of +L+ the (Term -+ Term) -+ Term approach and by the Barzilay’s one +L+ is the problem of recursively defining the Term type. We want to +L+ define the Term type as the smallest set satisfying rules (1) and (2). +L+ Note, however, that unlike rule (1), rule (2) is not monotonic in the +L+ sense that is substk {x1, · · · , xk.t["x]} depends non-monotonically +L+ on the Term type. For example, to say whether Xx.t[x] is a term, we +L+ should check whether t is a substitution function over x. It means at +L+ least thatfor every x in Term, t[x] should be in Term as well. Thus +L+ we need to define the whole type Term before using (2), which +L+ produces a logical circle. Moreover, since X has type (Term -+ +L+ Term) -+ Term, it is hard to formulate the structural induction +L+ principle for terms built with the X term constructor. +L+ Variable-Length Lists of Binders. In Barzilay’s approach, for +L+ each number n, is substn {} is considered to be a separate operator +L+ — there is no way to quantify over n, and there is no way to +L+ express variable-length lists of binders. This issue of expressing the +L+ unbounded-length lists of binders is common to some of the other +L+ approaches as well. +L+ Meta-Reasoning. Another difficulty that is especially apparent +L+ in Barzilay’s approach is that it only allows reasoning about con- +L+ crete operators in concrete languages. This approach does not pro- +L+ vide the ability to reason about operators abstractly; in particular, +L+ there is no way to state and prove meta-theorems that quantify over +L+ operators or languages, much less classes of languages. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. Higher-Order Abstract Syntax +L+ with Inductive Definitions +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Although it is possible to solve the problems outlined in the previ- +L+ ous Section (and we will return to the discussion of some of those +L+ solutions in Section 6), our desire is to avoid these difficulties from +L+ the start. We propose a natural model of reflection that manages to +L+ work around those difficulties. We will show how to give a sim- +L+ ple recursive definition of terms with binding variables, which does +L+ not allow the construction of exotic terms and does allow structural +L+ induction on terms. +L+ In this Section we provide a conceptual overview of our ap- +L+ proach; details are given in Section 4. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Bound Terms +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> One of the key ideas of our approach is how we deal with terms +L+ containing free variables. We extend to free variables the principle +L+ that variable names do not really matter. In fact, we model free +L+ variables as bindings that can be arbitrarily a-renamed. Namely, +L+ we will write bterm{x1, · · · , xn.t["x]} for a term t over variables +L+ x1, · · ·, xn. For example, instead of term x*y we will use the +L+ term bterm{x, y.x*y} when it is considered over variables x and +L+ y and bterm{x, y, z.x*y} when it is considered over variables x, +L+ y and z. Free occurrences of xi in t["x] are considered bound +L+ in bterm{x1, · · · , xn.t["x]} and two a-equal bterm{} expressions +L+ (“bterms”) are considered to be identical. +L+ Not every bterm is necessarily well-formed. We will define the +L+ type of terms in such a way as to eliminate exotic terms. Consider +L+ for example a definition of lambda-terms. +L+ </SectLabel_bodyText> <SectLabel_construct> EXAMPLE 1. We can define a set of reflected lambda-terms as the +L+ smallest set such that +L+ </SectLabel_construct> <SectLabel_listItem> •	bterm{x1, · · · , xn.xi}, where 1 < i < n, is a lambda-term (a +L+ variable); +L+ •	ifbterm{x1, · · · , xn, xn+1.t["x] ) is a lambda-term, then +L+ bterm{x1 , · · · , xn .Xxn+1 .t["x]) +L+ is also a lambda-term (an abstraction); +L+ •	if bterm{x1, · · · , xn.t1 ["x]} and bterm{x1, · · · , xn.t2["x]} are +L+ lambda-terms, then +L+ bterm{x1; · · · ; xn.apply{t1 ["x]; t2["x]}} +L+ is also a lambda-term (an application). +L+ </SectLabel_listItem> <SectLabel_bodyText> In a way, bterms could be understood as an explicit coding for +L+ Barzilay’s substitution functions. And indeed, some of the basic +L+ definitions are quite similar. The notion of bterms is also very +L+ similar to that of local variable contexts [FPT99]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Terminology +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Before we proceed further, we need to define some terminology. +L+ </SectLabel_bodyText> <SectLabel_construct> DEFINITION 1. We change the notion of subterm so that the sub- +L+ terms of a bterm are also bterms. For example, the immediate sub- +L+ terms of bterm{x, y.x*y} are bterm{x, y.x} and bterm{x, y.y}; the +L+ immediate subterm ofbterm{x.Xy.x} is bterm{x, y.x}. +L+ DEFINITION 2. We call the number of outer binders in a bterm +L+ expression its binding depth. Namely, the binding depth of the +L+ bterm bterm{x1, · · · , xn.t["x]} is n. +L+ DEFINITION 3. Throughout the rest of the paper we use the notion +L+ of operator shape. The shape ofan operator is a list ofnatural num- +L+ bers each stating how many new binders the operator introduces on +L+ </SectLabel_construct> <SectLabel_page> 5 +L+ </SectLabel_page> <SectLabel_bodyText> the corresponding subterm. The length of the shape list is therefore +L+ the arity of the operator. For example, the shape of the + operator +L+ is [0; 0] and the shape of the X operator is [1]. +L+ The mapping from operators to shapes is also sometimes called +L+ a binding signature of a language [FPT99, Plo90]. +L+ </SectLabel_bodyText> <SectLabel_construct> DEFINITION 4. Let op be an operator with shape [d1; · · · ; dN], +L+ and let btl be a list of bterms [b1; · · · ; bM]. We say that btl is +L+ compatible with op at depth n when, +L+ </SectLabel_construct> <SectLabel_listItem> 1. N=M; +L+ 2. the binding depth of bterm bj is n + dj for each 1 < j < N. +L+ </SectLabel_listItem> <SectLabel_subsectionHeader> 3.3 Abstract Operators +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Expressions of the form bterm{"x.op{· · · }} can only be used to ex- +L+ press syntax with concrete operators. In other words, each expres- +L+ sion of this form contains a specific constant operator op. However, +L+ we would like to reason about operators abstractly; in particular, +L+ we want to make it possible to have variables of the type “Op” that +L+ can be quantified over and used in the same manner as operator +L+ constants. In order to address this we use explicit term constructors +L+ in addition to bterm{"x.op{· · · }} constants. +L+ The expression mk bterm{n; “op”; btl}, where “op” is some en- +L+ coding of the quoted operator op, stands for a bterm with binding +L+ depth n, operator op and subterms btl. Namely, +L+ </SectLabel_bodyText> <SectLabel_equation> mk bterm{n; op; bterm{x1 , · · · , xn, "y1 .t1 ["x; "y1]} :: · · · :: +L+ bterm{x1, · · · ,xn,"yk.tk["x; "yk]} :: nil} +L+ is bterm{x1, · · · , xn.op {"y1 .t1 ["x; "y1]; · · · ; "yk.tk["x; "yk]}}. Here, +L+ nil is the empty list and :: is the list cons operator and there- +L+ fore the expression b1 :: · · · :: bn :: nil represents the concrete list +L+ [b1; ··· ; bn]. +L+ </SectLabel_equation> <SectLabel_bodyText> Note that if we know the shape of the operator op and we know +L+ that the mk bterm expression is well-formed (or, more specifically, +L+ if we know that btl is compatible with op at depth n), then it +L+ would normally be possible to deduce the value of n (since n is +L+ the difference between the binding depth of any element of the list +L+ btl and the corresponding element of the shape(op) list). There are +L+ two reasons, however, for supplying n explicitly: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	When btl is empty (in other words, when the arity of op is 0), +L+ the value of n can not be deduced this way and still needs to be +L+ supplied somehow. One could consider 0-arity operators to be a +L+ special case, but this results in a significant loss of uniformity. +L+ •	When we do not know whether an mk bterm expression is +L+ necessarily well-formed (and as we will see it is often useful +L+ to allow this to happen), then a lot of definitions and proofs +L+ are greatly simplified when the binding depth of mk bterm +L+ expressions is explicitly specified. +L+ </SectLabel_listItem> <SectLabel_bodyText> Using the mk bterm constructor and a few other similar con- +L+ structors that will be introduced later, it becomes easy to reason ab- +L+ stractly about operators. Indeed, the second argument to mk bterm +L+ can now be an arbitrary expression, not just a constant. This has a +L+ cost of making certain definitions slightly more complicated. For +L+ example, the notion of “compatible with op at depth n” now be- +L+ comes an important part of the theory and will need to be explicitly +L+ formalized. However, this is a small price to pay for the ability to +L+ reason abstractly about operators, which easily extends to reason- +L+ ing abstractly about languages, classes of languages and so forth. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.4 Inductively Defining the Type of Well-Formed Bterms +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> There are two equivalent approaches to inductively defining the +L+ general type (set) of all well-formed bterms. The first one follows +L+ the same idea as in Example 1: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	bterm{x1 , · · · , xn.xi } is a well-formed bterm for 1 < i < n; +L+ •	mk bterm{n; op; btl} is a well-formed bterm when op is a well- +L+ formed quoted operator and btl is a list of well-formed bterms +L+ that is compatible with op at some depth n. +L+ </SectLabel_listItem> <SectLabel_bodyText> If we denote bterm{x1, · · · , xl, y, z1, · · · , zr.y} as var{l; r}, +L+ we can restate the base case of the above definition as “var{l; r}, +L+ where l and r are arbitrary natural numbers, is a well-formed +L+ bterm”. Once we do this it becomes apparent that the above def- +L+ inition has a lot of similarities with de Bruijn-style indexing of +L+ variables [dB72]. Indeed, one might call the numbers l and r the +L+ left and right indices of the variable var{l; r}. +L+ It is possible to provide an alternate definition that is closer to +L+ pure HOAS: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	bnd{x.t[x]}, where t is a well-formed substitution function, is +L+ a well-formed bterm (the bnd operation increases the binding +L+ depth of t by one by adding x to the beginning of the list of t’s +L+ outer binders). +L+ •	mk term{op; btl}, where op is a well-formed quoted operator, +L+ and btl is a list of well-formed bterms that is compatible with +L+ op at depth 0, is a well-formed bterm (of binding depth 0). +L+ </SectLabel_listItem> <SectLabel_bodyText> Other than better capturing the idea of HOAS, the latter defini- +L+ tion also makes it easier to express the reflective correspondence +L+ between the meta-syntax (the syntax used to express the theory of +L+ syntax, namely the one that includes the operators mk bterm, bnd, +L+ etc.) and the meta-meta-syntax (the syntax that is used to express +L+ the theory of syntax and the underlying theory, in other words, the +L+ syntax that includes the second-order notations.) Namely, provided +L+ that we define the subst{bt; t} operation to compute the result of +L+ substituting a closed term t for the first outer binder of the bterm +L+ bt, we can state that +L+ </SectLabel_bodyText> <SectLabel_equation> subst{bnd{x.t1 [x]} ; t2} ≡ t1 [t2] (3) +L+ (where t1 and t2 are literal second-order variables). In other words, +L+ we can state that the substitution operator subst and the implicit +L+ second-order substitution in the “meta-meta-” language are equiv- +L+ alent. +L+ </SectLabel_equation> <SectLabel_bodyText> The downside of the alternate definition is that it requires defin- +L+ ing the notion of “being a substitution function”. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.5 Our Approach +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In our work we try to combine the advantages of both approaches +L+ outlined above. In the next Section we present a theory that includes +L+ both the HOAS-style operations (bnd, mk term) and the de Bruijn- +L+ style ones (var, mk bterm). Our theory also allows deriving the +L+ equivalence (3). In our theory the definition of the basic syntactic +L+ operations is based on the HOAS-style operators; however, the +L+ recursive definition of the type of well-formed syntax is based on +L+ the de Bruijn-style operations. Our theory includes also support for +L+ variable-length lists of binders. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. Formal Implementation in a Theorem Prover +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this Section we describe how the foundations of our theory are +L+ formally defined and derived in the NuPRL-style Computational +L+ Type Theory in the MetaPRL Theorem Prover. For brevity, we +L+ will present a slightly simplified version of our implementation; +L+ full details are available in the extended version of this paper +L+ [NKYH05, Appendix]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Computations and Types +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In our work we make heavy usage of the fact that our type theory +L+ allows us to define computations without stating upfront (or even +L+ knowing) what the relevant types are. In NuPRL-style type theo- +L+ </SectLabel_bodyText> <SectLabel_page> 6 +L+ </SectLabel_page> <SectLabel_bodyText> ries (which some even dubbed “untyped type theory”), one may de- +L+ fine arbitrary recursive functions (even potentially nonterminating +L+ ones). Only when proving that such function belongs to a particular +L+ type, one may have to prove termination. See [All87a, All87b] for +L+ a semantics that justifies this approach. +L+ The formal definition of the syntax of terms consists of two +L+ parts: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	The definition of untyped term constructors and term oper- +L+ ations, which includes both HOAS-style operations and de +L+ Bruijn-style operations. As it turns out, we can establish most +L+ of the reduction properties without explicitly giving types to all +L+ the operations. +L+ •	The definition of the type of terms. We will define the type of +L+ terms as the type that contains all terms that can be legitimately +L+ constructed by the term constructors. +L+ </SectLabel_listItem> <SectLabel_subsectionHeader> 4.2 HOAS Constructors +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> At the core of our term syntax definition are two basic HOAS-style +L+ constructors: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	bnd{x.t[x]} is meant to represent a term with a free variable x. +L+ The intended semantics (which will not become explicit until +L+ later) is that bnd{x.t[x]} will only be considered well-formed +L+ when t is a substitution function. +L+ </SectLabel_listItem> <SectLabel_bodyText> Internally, bnd{x.t[x]} is implemented simply as the pair +L+ (0, Xx.t[x]). This definition is truly internal and is used only +L+ to prove the properties of the two destructors presented below; +L+ it is never used outside of this Section (Section 4.2). +L+ </SectLabel_bodyText> <SectLabel_listItem> •	mk term{op; ts} pairs op with ts. The intended usage of this +L+ operation (which, again, will only become explicit later) is that +L+ it represents a closed term (i.e. a bterm of binding depth 0) with +L+ operator op and subterms ts. It will be considered well-formed +L+ when op is an operator and ts is a list of terms that is compatible +L+ </SectLabel_listItem> <SectLabel_bodyText> with op at depth 0. For example, mk term{X; bnd{x.x}} is Xx.x. +L+ Internally, mk term{op; ts} is implemented as the nested pair +L+ (1, (op, ts)). Again, this definition is never used outside of this +L+ Section. +L+ We also implement two destructors: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	subst{bt; t} is meant to represent the result of substituting term +L+ t for the first variable of the bterm bt. Internally, subst{bt; t} +L+ is defined simply as an application (bt.2) t (where bt.2 is the +L+ second element of the pair bt). +L+ </SectLabel_listItem> <SectLabel_bodyText> We derive the following property of this substitution operation: +L+ </SectLabel_bodyText> <SectLabel_equation> subst{bnd{x.t1 [x]} ; t2} ≡ t1 [t2] +L+ where “≡” is the computational equality relation1 and t1 and +L+ t2 may be absolutely arbitrary, even ill-typed. This derivation +L+ is the only place where the internal definition of subst{bt; t} is +L+ used. +L+ </SectLabel_equation> <SectLabel_bodyText> Note that the above equality is exactly the “reflective property +L+ of substitution” (3) that was one of the design goals for our +L+ theory. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	weak dest {bt; bcase; op, ts.mkt case[op; ts]} is designed to +L+ provide a way to find out whether bt is a bnd{} or a mk term{op; ts} +L+ </SectLabel_listItem> <SectLabel_footnote> 1 In NuPRL-style type theories the computational equality relation (which +L+ is also sometimes called “squiggle equality” and is sometimes denoted +L+ </SectLabel_footnote> <SectLabel_bodyText> as“∼” or “←-+”) is the finest-grained equality relation in the theory. +L+ When a ≡ b is true, a may be replaced with b in an arbitrary context. +L+ Examples of computational equality include beta-reduction Xx.a[x]b ≡ +L+ a[b], arithmetical equalities (1 + 2 ≡ 3), and definitional equality (an +L+ abstraction is considered to be computationally equal to its definition). +L+ and to “extract” the op and ts in the latter case. In the rest of +L+ this paper we will use the “pretty-printed” form for weak dest +L+ — “match bt with bnd{ } -+ bcase I mk term{op; ts} -+ +L+ mkt case[op; ts]”. Internally, it is defined as +L+ if bt.1 = 0 then bcase else mkt case[bt.2.1; bt.2.2]. +L+ From this internal definition we derive the following properties +L+ of weak dest: +L+ </SectLabel_bodyText> <SectLabel_equation> ⎛	⎞ +L+ matchbnd{x.t[x]} with +L+ ⎝bnd{ } -+ bcase	⎠ +L+ bcase +L+ mk term{op; ts} -+ mkt case[op; ts]≡ +L+ ⎛ +L+ matchmk term{op; ts} with +L+ ⎝bnd{ } -+ bcase +L+ Imk term{o; t} -+ mkt case[o; t] +L+ 4.3 Vector HOAS Operations +L+ </SectLabel_equation> <SectLabel_bodyText> As we have mentioned at the end of Section 2, some approaches to +L+ reasoning about syntax make it hard or even impossible to express +L+ arbitrary-length lists of binders. In our approach, we address this +L+ challenge by allowing operators where a single binding in the meta- +L+ language stands for a list of object-level bindings. In particular, we +L+ allow representing bnd{x1.bnd{x2. · · · bnd{xn.t[x1; ... ; xn]} · · ·}} +L+ as +L+ vbnd{n; x.t[nth{1; x}; . . . ; nth{n; x}]}, where “nth{i; l}” is the “i- +L+ th element of the list l” function. +L+ We define the following vector-style operations: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	vbnd{n; x.t[x]} represents a “telescope” of nested bnd opera- +L+ tions. It is defined by induction2 on the natural number n as +L+ follows: +L+ </SectLabel_listItem> <SectLabel_equation> vbnd{0; x.t[x]}:= t[nil] +L+ vbnd{n + 1; x.t[x]}:= bnd{v.vbnd{n; x.t[v :: x]}} +L+ </SectLabel_equation> <SectLabel_bodyText> We also introduce vbnd{n; t} as a simplified notation for +L+ vbnd{n; x.t} when t does not have free occurrences of x. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	vsubst{bt; ts} is a “vector” substitution operation that is meant +L+ to represent the result of simultaneous substitution of the terms +L+ in the ts list for the first ItsI variables of the bterm bt (here IlI is +L+ the length of the list l). vsubst{bt; ts} is defined by induction on +L+ the list ts as follows: +L+ </SectLabel_listItem> <SectLabel_equation> vsubst{bt; nil}:= bt +L+ vsubst{bt; t :: ts}:= vsubst{subst{bt; t} ; ts} +L+ </SectLabel_equation> <SectLabel_bodyText> Below are some of the derived properties of these operations: +L+ </SectLabel_bodyText> <SectLabel_equation> 	bnd{v.t[v]} ≡ vbnd{1; hd(v)}	(4) +L+ Vm , n E N. +L+ �vbnd{m +n; x.t[x]} ≡ vbnd{m; y.vbnd{n; z.t[y@z]}}) (5) +L+ 	Vl E List. (vsubst{vbnd{Il I; v.t[v]} ;l} ≡ t[l])	(6) +L+ Vl E List.Vn E N. �(n ≥ IlI) ⇒	(7) +L+ (vsubst{vbnd{n; v.t[v]} ;l} ≡ vbnd{n − IlI; v.bt[l@v]})) +L+ Vn E N.	(8) +L+ vbnd{n; l.vsubst{vbnd{n; v.t[v]} ;l}} ≡ vbnd{n; l.t[l]}) +L+ </SectLabel_equation> <SectLabel_bodyText> where “hd” is the list “head” operation, “@” is the list append +L+ operation, “List” is the type of arbitrary lists (the elements of a list +L+ do not have to belong to any particular type), N is the type of natural +L+ numbers, and all the variables that are not explicitly constrained to +L+ a specific type stand for arbitrary expressions. +L+ </SectLabel_bodyText> <SectLabel_footnote> 2 Our presentation of the inductive definitions is slightly simplified by +L+ omitting some minor technical details. See [NKYH05, Appendix] for +L+ complete details. +L+ </SectLabel_footnote> <SectLabel_equation> ⎞⎠ ≡mkt case[op; ts] +L+ </SectLabel_equation> <SectLabel_page> 7 +L+ </SectLabel_page> <SectLabel_bodyText> Equivalence (5) allows the merging and splitting of vector bnd +L+ operations. Equivalence (6) is a vector variant of equivalence (3). +L+ Equivalence (8) is very similar to equivalence (6) applied in the +L+ vbnd{n; l. · · ·} context, except that (8) does not require l to be a +L+ member of any special type. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.4 De Bruijn-style Operations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Based on the HOAS constructors defined in the previous two sec- +L+ tions, we define two de Bruijn-style constructors. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	var{i; j} is defined as vbnd{i; bnd{v.vbnd{j; v}}}. It is easy to +L+ see that this definition indeed corresponds to the informal +L+ </SectLabel_listItem> <SectLabel_equation> bterm{x1,··· ,xl, y, z1,··· , zr .y} +L+ definition given in Section 3.4. +L+ •	mk bterm{n; op; ts} is meant to compute a bterm of binding +L+ depth n, with operator op, and with ts as its subterms. This op- +L+ eration is defined by induction on natural number n as follows: +L+ mk bterm{0; op; ts}:= mk term{op; ts} +L+ mk bterm{n + 1; op; ts}:= +L+ bnd{v.mk bterm{n; op; map Xt.subst{t; v} ts}} +L+ </SectLabel_equation> <SectLabel_bodyText> Note that, if ts is a list of bnd expressions (which is the intended +L+ usage of the mk bterm operation), then the +L+ </SectLabel_bodyText> <SectLabel_equation> bnd{v. · · · map Xt.subst{t; v} ts · · ·} +L+ </SectLabel_equation> <SectLabel_bodyText> has the effect of stripping the outer bnd from each of the mem- +L+ bers of the ts list and “moving” them into a single “merged” bnd +L+ on the outside. +L+ We also define a number of de Bruijn-style destructors, i.e., op- +L+ erations that compute various de Bruijn-style characteristics of a +L+ bterm. Since the var and mk bterm constructors are defined in terms +L+ of the HOAS constructors, the destructors have to be defined in +L+ terms of HOAS operations as well. Because of this, these defini- +L+ tions are often far from straightforward. +L+ It is important to emphasize that the tricky definitions that we +L+ use here are only needed to establish the basic properties of the +L+ operations we defined. Once the basic theory is complete, we can +L+ raise the level of abstraction and no usage of this theory will +L+ ever require using any of these definitions, being aware of these +L+ definitions, or performing similar tricks again. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	bdepth{t} computes the binding depth of term t. It is defined +L+ recursively using the Y combinator as +L+ </SectLabel_listItem> <SectLabel_equation> rXb.matchb with +L+ Y	bnd{ } -+ 1 + f (subst{b; mk term{0; 0}}) +L+ |mkterm{ ; }-+0 +L+ </SectLabel_equation> <SectLabel_bodyText> In effect, this recursive function strips the outer binders from a +L+ bterm one by one using substitution (note that here we can use +L+ an arbitrary mk bterm expression as a second argument for the +L+ substitution function; the arguments to mk bterm do not have +L+ to have the “correct” type) and counts the number of times it +L+ needs to do this before the outermost mk bterm is exposed. +L+ We derive the following properties of bdepth: +L+ </SectLabel_bodyText> <SectLabel_equation> Vl, r E ICY. (bdepth {var{l; r}} ≡ (l +r + 1)); +L+ Vn E ICY.(bdepth{mk bterm{n; op; ts}} ≡ n). +L+ </SectLabel_equation> <SectLabel_bodyText> Note that the latter equivalence only requires n to have the +L+ “correct” type, while op and ts may be arbitrary. Since the +L+ bdepth operator is needed for defining the type of Term of well- +L+ formed bterms, at this point we would not have been able to +L+ express what the “correct” type for ts would be. +L+ •	left{t} is designed to compute the “left index” of a var expres- +L+ sion. It is defined as +L+ </SectLabel_bodyText> <SectLabel_equation> 	�	� +L+ Xf.Xb.Xl. +L+ match b with	� +L+ 	Y	bnd{ } -+ +L+ �1 + f (subst{b; mk term {l; 0}})(l + 1) � � +L+ |mk term{lf; ) -+ lf +L+ </SectLabel_equation> <SectLabel_bodyText> In effect, this recursive function substitutes mk term{0; 0} +L+ for the first binding of t, mk term{1; 0} for the second one, +L+ mk term{2; 0} for the next one and so forth. Once all the binders +L+ are stripped and a mk term{l; 0} is exposed, l is the index +L+ we were looking for. Note that here we intentionally supply +L+ mk term with an argument of a “wrong” type (ICY instead of +L+ Op); we could have avoided this, but then the definition would +L+ have been significantly more complicated. +L+ As expected, we derive that +L+ </SectLabel_bodyText> <SectLabel_equation> Vl, r E ICY.(left{var{l; r}} ≡ l). +L+ </SectLabel_equation> <SectLabel_listItem> •	right{t} computes the “right index” of a var expression. It +L+ is trivial to define in terms of the previous two operators: +L+ right{t}:= bdepth{t} − left{t} − 1. +L+ •	get op{t; op} is an operation such that +L+ </SectLabel_listItem> <SectLabel_equation> Vn E ICY.(get op{mk bterm{n; op; ts} ; opf) ≡ op), +L+ Vl, r E ICY. ((get op{var{i; j} ; op} ≡ op). +L+ </SectLabel_equation> <SectLabel_bodyText> Its definition is similar to that of left{}. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	subterms{t} is designed to recover the last argument of a +L+ mk bterm expression. The definition is rather technical and +L+ complicated, so we omit it; see [NKYH05, Appendix C] for +L+ details. The main property of the subterms operation that we +L+ derive is +L+ </SectLabel_listItem> <SectLabel_bodyText> Vn E ICY.Vbtl E List.(subterms{mk bterm{n; op; btl}} ≡ +L+ map Xb.vbnd{n; v.vsubst{b; v}} btl) +L+ The right-hand side of this equivalence is not quite the plain +L+ “btl” that one might have hoped to see here. However, when +L+ btl is a list of bterms with binding depths at least n, which is +L+ necessarily the case for any well-formed mk bterm{n; op; btl}, +L+ equivalence (8) would allow simplifying this right-hand side to +L+ the desired btl. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.5 Operators +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> For this basic theory the exact representation details for operators +L+ are not essential and we define the type of operators Op abstractly. +L+ We only require that operators have decidable equality and that +L+ there exist a function of the type Op -+ ICY List that computes +L+ operators’ shapes. +L+ Using this shape function and the bdepth function from Sec- +L+ tion 4.4, it is trivial to formalize the “ts is compatible with op at +L+ depth n” predicate of Definition 4. We denote this predicate as +L+ shape compat{n; op; ts} and define it as +L+ </SectLabel_bodyText> <SectLabel_equation> |shape{op}| = |btl|A +L+ Vi E 1..|btl|.bdepth{nth{btl; i}} = n +nth{shape{op}; i} +L+ </SectLabel_equation> <SectLabel_subsectionHeader> 4.6 The Type of Terms +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In this section we will define the type of terms (i.e. well-formed +L+ bterms), Term, as the type of all terms that can be constructed by +L+ the de Bruijn constructors from Section 4.4. That is, the Term type +L+ contains all expressions of the forms: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	var{i; j} for all natural numbers i, j; or +L+ </SectLabel_listItem> <SectLabel_equation> )t +L+ t0 +L+ </SectLabel_equation> <SectLabel_page> 8 +L+ </SectLabel_page> <SectLabel_listItem> • mk bterm{n; op; ts} for any natural number n, operator op, and +L+ list of terms ts that is compatible with op at depth n. +L+ </SectLabel_listItem> <SectLabel_bodyText> The Term type is defined as a fixpoint of the following function +L+ from types to types: +L+ </SectLabel_bodyText> <SectLabel_equation> Iter(X) := Image(dom(X); x.mk(x)), +L+ </SectLabel_equation> <SectLabel_bodyText> where +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Image is a type constructor such that Image(T; x. f [xl) is the +L+ type of all the f [tl for t e T (for it to be well-formed, T must +L+ be a well-formed type and f must not have any free variables +L+ except for x); +L+ •	dom(X) is a type defined as +L+ </SectLabel_listItem> <SectLabel_equation> (N×N)+(n:N× op:Op× {ts:X List I shape compat{n; op; ts}}); +L+ </SectLabel_equation> <SectLabel_listItem> •	and mk(x) (where x is presumably a member of the type +L+ dom(X)) is defined as +L+ </SectLabel_listItem> <SectLabel_equation> matchx with +L+ inl (i, j) -+ var{i ; j} +L+ Iinr (n, op, ts) -+ mk bterm{n; op; ts} . +L+ </SectLabel_equation> <SectLabel_bodyText> The fixpoint of Iter is reached by defining +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Term0 := Void (an empty type) +L+ •	Termn+1 := Iter(Termn) +L+ •	Term := U Termn +L+ neN +L+ </SectLabel_listItem> <SectLabel_bodyText> We derive the intended introduction rules for the Term type: +L+ </SectLabel_bodyText> <SectLabel_equation> i eN	j eN +L+ var{i ; j} e Term +L+ and +L+ n e N op e Op ts e TermList shape compat{n; op; ts} +L+ . +L+ mk bterm{n; op; ts} e Term +L+ </SectLabel_equation> <SectLabel_bodyText> Also, the structural induction principle is derived for the Term +L+ type. Namely, we show that to prove that some property P[tl holds +L+ for any term t, it is sufficient to prove +L+ </SectLabel_bodyText> <SectLabel_listItem> •	(Base case) P holds for all variables, that is, P[var{i ; j}l holds +L+ for all natural numbers i and j; +L+ •	(Induction step) P[mk bterm{n; op; ts}l is true for any natural +L+ number n, any operator op, and any list of terms ts that is +L+ compatible with op at depth n, provided P[tl is true for any +L+ element t of the list ts. +L+ </SectLabel_listItem> <SectLabel_bodyText> Note that the type of “terms over n variables” (where n = 0 cor- +L+ responds to closed terms) may be trivially defined using the Term +L+ type and the “subset” type constructor — {t : Term II bdepth{t} = +L+ n}. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. Conclusions and Future Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In Sections 3 and 4 we have presented a basic theory of syntax +L+ that is fully implemented in a theorem prover. As we mentioned in +L+ the introduction, the approach is both natural and expressive, and +L+ provides a foundation for reflective reasoning about classes of lan- +L+ guages and logics. However, we consider this theory to be only +L+ the first step towards building a user-accessible uniform reflection +L+ framework and a user-accessible uniform framework for program- +L+ ming language reasoning and experimentation, where tasks similar +L+ to the ones presented in the POPLMARK challenge [ABF+05] can +L+ be performed easily and naturally. In this section we provide an out- +L+ line of our plans for building such frameworks on top of the basic +L+ syntactic theory. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.1 Higher-Level User Interface +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> One obvious shortcoming of the theory presented in Sections 3 +L+ and 4 is that it provides only the basic low-level operations such +L+ as bnd, var, subterms, etc. It presents a very low-level account of +L+ syntax in a way that would often fail to abstract away the details +L+ irrelevant to the user. +L+ To address this problem we are planning to provide user in- +L+ terface functionality capable of mapping the high-level concepts +L+ to the low-level ones. In particular, we are going to provide an +L+ interface that would allow instantiating general theorems to spe- +L+ cific collections of operators and specific languages. Thus, the user +L+ will be able to write something like “reflect language [Xx..; +L+ apply{.; .}] ” and the system will create all the components outlined +L+ in Example 1: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	It will create a definition for the type +L+ </SectLabel_listItem> <SectLabel_equation> Language[Xx..; apply{.; .}l +L+ </SectLabel_equation> <SectLabel_bodyText> of reflected lambda-terms (where Language[ll is a general def- +L+ inition of a language over a list of operators l); +L+ </SectLabel_bodyText> <SectLabel_listItem> •	It will state and derive the introduction rules for this type; +L+ •	It will state and derive the elimination rule for this type (the +L+ induction principle). +L+ </SectLabel_listItem> <SectLabel_bodyText> Moreover, we are planning to support even more complicated lan- +L+ guage declarations, such as +L+ </SectLabel_bodyText> <SectLabel_equation> t := int I t -+ t; e := v I Xx : t.e[xl I apply{e; e} +L+ </SectLabel_equation> <SectLabel_bodyText> that would cause the system to create mutually recursive type +L+ definitions and appropriate rules. +L+ Finally, we are also planning to support “pattern bindings” that +L+ are needed for a natural encoding of ML-like pattern matching +L+ (such as the one sketched in the POPLMARK challenge [ABF+05]). +L+ As far as the underlying theory goes, we believe that the mecha- +L+ nisms very similar to the “vector bindings” presented in Section 4.3 +L+ will be sufficient here. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 “Dereferencing” Quoted Terms +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As in Barzilay’s work, the quoted operator approach makes it easy +L+ to define the “unquoting” (or “dereferencing”) operator [lunq. If t +L+ is a syntactic term, then [tllunq is the value represented by t. By +L+ definition, +L+ </SectLabel_bodyText> <SectLabel_equation> [op{t1; ... ; tn}lunq = op{[t1lunq; ... ; [tnllunq}. +L+ For instance, [2 * 3lunq is 2 * 3 (i.e. 6). +L+ </SectLabel_equation> <SectLabel_bodyText> In order to define unquoting on terms with bindings, we need to +L+ introduce the “guard” operation hp pi such that [bt)llunq is t for an +L+ arbitrary expression t. Then [lunq can be defined as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> [op{x1, ..., xk.t[x1; ... ; xkl; ... ;z1, ..., zl.s[z1; ... ; zll}lunq = +L+ op{x1, . . . ,xk.[[t[(ix1 ; ... ; Ixk�llunq; +L+ . . .	; +L+ z1, . . . , zl.[s[(z1�� ; ... ; (1zl�llunq}. +L+ For example, [[Xx.2*xlunq = Xx.[2*��x�llunq = Xx.[2lunq * +L+ [bx)lunq =Xx.2 * x. +L+ </SectLabel_equation> <SectLabel_bodyText> The unquote operation establishes the identity between the orig- +L+ inal syntax and the reflected syntax, making it a “true” reflection. +L+ Note that the type theory (which ensures, in particular, that +L+ only terminating functions may be shown to belong to a function +L+ type) would keep the [ llunq operation from introducing logical +L+ paradoxes.3 +L+ </SectLabel_bodyText> <SectLabel_footnote> 3 This is, obviously, not a proper argument. While a proper argument can be +L+ made here, it is outside of the scope of this particular paper. +L+ </SectLabel_footnote> <SectLabel_page> 9 +L+ </SectLabel_page> <SectLabel_bodyText> Also, since the notion of the quoted operators is fully open- +L+ ended, each new language added to the system will automatically +L+ get to use the Q lunq operation for all its newly introduced opera- +L+ tors. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.3 Logical Reflection +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> After defining syntactic reflection, it is easy to define logical reflec- +L+ tion. If we consider the proof system open-ended, then the logical +L+ reflection is trivial — when P is a quotation of a proposition, we +L+ can regard “QPlunq” as meaning “P is true”. The normal modal +L+ rules for the Qlunq modality are trivially derivable. For example +L+ modus ponens +L+ </SectLabel_bodyText> <SectLabel_equation> QP  =:�  Qlunq =:� QPlunq =:� QQQlunq +L+ </SectLabel_equation> <SectLabel_bodyText> is trivially true because if we evaluate the first Qllunq (remember, +L+ </SectLabel_bodyText> <SectLabel_equation> QP =:� Qlunq = (QPlunq =:� QQlunq) +L+ </SectLabel_equation> <SectLabel_bodyText> by definition of Qlunq), we get an obvious tautology +L+ (QPlunq =:� QQQlunq) =:� QPlunq =:� QQlunq. +L+ In order to consider a closed proof system (in other words, if +L+ we want to be able to do induction over derivations), we would +L+ need to define a provability predicate for that system. We are +L+ planning to provide user interface functionality that would allow +L+ users to describe a set of proof rules and the system would generate +L+ appropriate proof predicate definitions and derive appropriate rules +L+ (in a style similar to the one outlined in Section 5.1 for the case of +L+ language descriptions). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. Related Work +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In Section 2 we have already discussed a number of approaches +L+ that we consider ourselves inheriting from. Here we would like to +L+ revisit some of them and mention a few other related efforts. +L+ Our work has a lot in common with the HOAS implemented in +L+ Coq by Despeyroux and Hirschowitz [DH94]. In both cases, the +L+ more general space of terms (that include the exotic ones) is later +L+ restricted in a recursive manner. In both cases, the higher-order +L+ analogs of first-order de Bruijn operators are defined and used as a +L+ part of the “well-formedness” specification for the terms. Despey- +L+ roux and Hirschowitz use functions over infinite lists of variables +L+ to define open terms, which is similar to our vector bindings. +L+ There are a number of significant differences as well. Our ap- +L+ proach is sufficiently syntactical, which allows eliminating all ex- +L+ otic terms, even those that are extensionally equal to the well- +L+ formed ones, while the more semantic approach of [DH94, +L+ DFH95] has to accept such exotic terms (their solution to this prob- +L+ lem is to consider an object term to be represented by the whole +L+ equivalence class of extensionally equal terms); more generally +L+ while [DH94] states that “this problem of extensionality is recur- +L+ rent all over our work”, most of our lemmas establish identity and +L+ not just equality, thus avoiding most of the issues of extensional +L+ equality. In our implementation, the substitution on object terms is +L+ mapped directly to P-reduction, while Despeyroux et al. [DFH95] +L+ have to define it recursively. In addition, we provide a uniform ap- +L+ proach to both free and bound variables that naturally extends to +L+ variable-length “vector” bindings. +L+ While our approach is quite different from the modal X-calculus +L+ one [DPS97, DL99, DL01], there are some similarities in the in- +L+ tuition behind it. Despeyroux et al. [DPS97] says “Intuitively, we +L+ interpret ❑B as the type of closed objects of type B. We can iter- +L+ ate or distinguish cases over closed objects, since all constructors +L+ are statically known and can be provided for.” The intuition be- +L+ hind our approach is in part based on the canonical model of the +L+ NuPRL type theory [All87a, All87b], where each type is mapped +L+ to an equivalence relations over the closed terms of that type. +L+ Gordon and Melham [GM96] define the type of X-terms as a +L+ quotient of the type of terms with concrete binding variables over +L+ a-equivalence. Michael Norrish [Nor04] builds upon this work by +L+ replacing certain variable “freshness” requirements with variable +L+ “swapping”. This approach has a number of attractive properties; +L+ however, we believe that the level of abstraction provided by the +L+ HOAS-style approaches makes the HOAS style more convenient +L+ and accessible. +L+ Ambler, Crole, and Momigliano [ACM02] have combined the +L+ HOAS with the induction principle using an approach which in +L+ some sense is opposite to ours. Namely, they define the HOAS +L+ operators on top of the de Bruijn definition of terms using higher +L+ order pattern matching. In a later work [ACM03] they have de- +L+ scribed the notion of “terms-in-infinite-context” which is quite sim- +L+ ilar to our approach to vector binding. While our vector bindings +L+ presented in Section 4.3 are finite length, the exact same approach +L+ would work for the infinite-length “vectors” as well. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgments +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The authors are grateful to Eli Barzilay whose ideas were an in- +L+ spiration for some of the work that lead to this paper. We are also +L+ grateful for his comments on an early draft of this paper. +L+ We are grateful to the anonymous reviewers for their very thor- +L+ ough and fair feedback and many helpful suggestions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> [AA99] Eric Aaron and Stuart Allen. Justifying calculational logic +L+ by a conventional metalinguistic semantics. Technical Report +L+ TR99-1771, Cornell University, Ithaca, New York, September +L+ 1999. +L+ [ABF+05] Brian E. Aydemir, Aaron Bohannon, Matthew Fairbairn, +L+ J. Nathan Foster, Benjamin C. Pierce, Peter Sewell, Dimitrios +L+ Vytiniotis, Geoffrey Washburn, Stephanie Weirich, and Steve +L+ Zdancewic. Mechanized metatheory for the masses: The +L+ POPLmark challenge. Available fromhttp://www.cis. +L+ upenn.edu/group/proj/plclub/mmm/,2005. +L+ [AC92] William Aitken and Robert L. Constable. Reflecting on +L+ NuPRL : Lessons 1–4. Technical report, Cornell University, +L+ Computer Science Department, Ithaca, NY, 1992. +L+ [ACE+00] Stuart Allen, Robert Constable, Richard Eaton, Christoph +L+ Kreitz, and Lori Lorigo. The NuPRL open logical envi- +L+ ronment. In David McAllester, editor, Proceedings of the +L+ 17th International Conference on Automated Deduction, vol- +L+ ume 1831 of Lecture Notes in Artificial Intelligence, pages +L+ 170–176. Springer Verlag, 2000. +L+ [ACHA90] Stuart F. Allen, Robert L. Constable, Douglas J. Howe, +L+ and William Aitken. The semantics of reflected proof. In +L+ Proceedings of the 5th Symposium on Logic in Computer +L+ Science, pages 95–197. IEEE Computer Society Press, June +L+ 1990. +L+ [ACM02] Simon Ambler, Roy L. Crole, and Alberto Momigliano. +L+ Combining higher order abstract syntax with tactical theorem +L+ proving and (co)induction. In TPHOLs ’02: Proceedings +L+ of the 15th International Conference on Theorem Proving +L+ in Higher Order Logics, pages 13–30, London, UK, 2002. +L+ Springer-Verlag. +L+ [ACM03] S. J. Ambler, R. L. Crole, and Alberto Momigliano. A +L+ definitional approach to primitive recursion over higher +L+ order abstract syntax. In Proceedings of the 2003 workshop +L+ on Mechanized reasoning about languages with variable +L+ binding, pages 1–11. ACM Press, 2003. +L+ [ACU93] William Aitken, Robert L. Constable, and Judith Underwood. +L+ Metalogical Frameworks II: Using reflected decision pro- +L+ cedures.Journal of Automated Reasoning, 22(2):171–221, +L+ 1993. +L+ </SectLabel_reference> <SectLabel_page> 10 +L+ </SectLabel_page> <SectLabel_reference> [All87a]	Stuart F. Allen. A Non-type-theoretic Definition of Martin- +L+ L¨of’s Types. In D. Gries, editor, Proceedings ofthe 2nd IEEE +L+ Symposium on Logic in Computer Science, pages 215–224. +L+ IEEE Computer Society Press, June 1987. +L+ [All87b]	Stuart F. Allen. A Non-Type-Theoretic Semantics for Type- +L+ Theoretic Language. PhD thesis, Cornell University, 1987. +L+ [Art99]	Sergei Artemov. On explicit reflection in theorem proving +L+ and formal verification. In Ganzinger [Gan99], pages 267– +L+ 281. +L+ [Art04]	Sergei Artemov. Evidence-based common knowledge. +L+ Technical Report TR-2004018, CUNY Ph.D. Program in +L+ Computer Science Technical Reports, November 2004. +L+ [BA02]	Eli Barzilay and Stuart Allen. Reflecting higher-order abstract +L+ syntax in NuPRL. In Victor A. Carre˜no, C´ezar A. Mu˜noz, +L+ and Sophi`ene Tahar, editors, Theorem Proving in Higher +L+ Order Logics; Track B Proceedings of the 15th International +L+ Conference on Theorem Proving in Higher Order Logics +L+ (TPHOLs 2002), Hampton, VA, August 2002, pages 23–32. +L+ National Aeronautics and Space Administration, 2002. +L+ [BAC03]	Eli Barzilay, Stuart Allen, and Robert Constable. Practical +L+ reflection in NuPRL. Short paper presented at 18th Annual +L+ IEEE Symposium on Logic in Computer Science, June 22– +L+ 25, Ottawa, Canada, 2003. +L+ [Bar01]	Eli Barzilay. Quotation and reflection in NuPRL and Scheme. +L+ Technical Report TR2001-1832, Cornell University, Ithaca, +L+ New York, January 2001. +L+ [Bar05]	Eli Barzilay. Implementing Reflection in NuPRL. PhD thesis, +L+ Cornell University, 2005. In preparation. +L+ [CAB+86] Robert L. Constable, Stuart F. Allen, H. M. Bromley, W. R. +L+ Cleaveland, J. F. Cremer, R. W. Harper, Douglas J. Howe, +L+ T. B. Knoblock, N. P. Mendler, P. Panangaden, James T. +L+ Sasaki, and Scott F. Smith. Implementing Mathematics with +L+ the NuPRL ProofDevelopment System. Prentice-Hall, NJ, +L+ 1986. +L+ [CFW04]	Luis Crus-Filipe and Freek Weidijk. Hierarchical reflection. +L+ In Slind et al. [SBG04], pages 66–81. +L+ [Con94]	Robert L. Constable. Using reflection to explain and enhance +L+ type theory. In Helmut Schwichtenberg, editor, Proof and +L+ Computation, volume 139 of NATO Advanced Study Insti- +L+ tute, International Summer School held in Marktoberdorf, +L+ Germany, July 20-August 1, NATO Series F, pages 65–100. +L+ Springer, Berlin, 1994. +L+ [dB72]	N. G. de Bruijn. Lambda calculus notation with nameless +L+ dummies, a tool for automatic formula manipulation, with +L+ application to the Church-Rosser theorem. Indagaciones +L+ Mathematische, 34:381–392, 1972. This also appeared in the +L+ Proceedings of the Koninklijke Nederlandse Akademie van +L+ Wetenschappen, Amsterdam, series A, 75, No. 5. +L+ [DFH95]	Jo¨elle Despeyroux, Amy Felty, and Andr´e Hirschowitz. +L+ Higher-order abstract syntax in Coq. In M. Dezani- +L+ Ciancaglini and G. Plotkin, editors, Proceedings of the +L+ International Conference on Typed Lambda Calculus and +L+ its Applications, volume 902 of Lecture Notes in Computer +L+ Science, pages 124–138. Springer-Verlag, April 1995. Also +L+ appears as INRIA research report RR-2556. +L+ [DH94]	Jo¨elle Despeyroux and Andr´e Hirschowitz. Higher-order +L+ abstract syntax with induction in Coq. In LPAR ’94: +L+ Proceedings of the 5th International Conference on Logic +L+ Programming and Automated Reasoning, volume 822 +L+ of Lecture Notes in Computer Science, pages 159–173. +L+ Springer-Verlag, 1994. Also appears as INRIA research +L+ report RR-2292. +L+ [DH95]	James Davis and Daniel Huttenlocher. Shared annotations for +L+ cooperative learning. In Proceedings of the ACM Conference +L+ on Computer Supported Cooperative Learning, September +L+ 1995. +L+ [DL99]	Jo¨elle Despeyroux and Pierre Leleu. A modal lambda +L+ calculus with iteration and case constructs. In T. Altenkirch, +L+ W. Naraschewski, and B. Reus, editors, Types for Proofs +L+ and Programs: International Workshop, TYPES ’98, Kloster +L+ Irsee, Germany, March 1998, volume 1657 of Lecture Notes +L+ in Computer Science, pages 47–61, 1999. +L+ [DL01]	Jo¨elle Despeyroux and Pierre Leleu. Recursion over objects +L+ of functional type. Mathematical Structures in Computer +L+ Science, 11(4):555–572, 2001. +L+ [DPS97]	Jo¨elle Despeyroux, Frank Pfenning, and Carsten Sch¨urmann. +L+ Primitive recursion for higher–order abstract syntax. In +L+ R. Hindley, editor, Proceedings of the Third International +L+ Conference on Typed Lambda Calculus and Applications +L+ (TLCA’97), volume 1210 of Lecture Notes in Computer +L+ Science, pages 147–163. Springer-Verlag, April 1997. An +L+ extended version is available as Technical Report CMU-CS- +L+ 96-172, Carnegie Mellon University. +L+ [EM71]	Andrzej Ehrenfeucht and Jan Mycielski. Abbreviating +L+ proofs by adding new axioms. Bulletin of the American +L+ Mathematical Society, 77:366–367, 1971. +L+ [F+86]	Solomon Feferman et al., editors. Kurt G¨odel Collected +L+ Works, volume 1. Oxford University Press, Oxford, +L+ Clarendon Press, New York, 1986. +L+ [FPT99]	Marcelo Fiore, Gordon Plotkin, and Daniele Turi. Abstract +L+ syntax and variable binding. In Proceedings of 14th IEEE +L+ Symposium on Logic in Computer Science, pages 193+. IEEE +L+ Computer Society Press, 1999. +L+ [Gan99]	Harald Ganzinger, editor. Proceedings of the 16th Interna- +L+ tional Conference on Automated Deduction, volume 1632 +L+ of Lecture Notes in Artificial Intelligence, Berlin, July 7–10 +L+ 1999. Trento, Italy. +L+ [GM96]	A. D. Gordon and T. Melham. Five axioms of alpha- +L+ conversion. In J. von Wright, J. Grundy, and J. Harrison, +L+ editors, Theorem Proving in Higher Order Logics: 9th +L+ International Conference, Turku, Finland, August 1996: +L+ Proceedings, volume 1125 of Lecture Notes in Computer +L+ Science, pages 173–190. Springer-Verlag, 1996. +L+ [GMO03] Jim Grundy, Tom Melham, and John O’Leary. A reflective +L+ functional language for hardware design and theorem +L+ proving. Technical Report PRG-RR-03-16, Oxford Univerity, +L+ Computing Laboratory, 2003. +L+ [G¨od31 ]	Kurt G¨odel. ¨Uber formal unentscheidbare s¨atze der principia +L+ mathematica und verwandter systeme I. Monatshefte f¨ur +L+ Mathematik und Physik, 38:173–198, 1931. English version +L+ in [vH67]. +L+ [G¨od36]	K. G¨odel. ¨Uber die L¨ange von beweisen. Ergebnisse +L+ eines mathematischen Kolloquiums, 7:23–24, 1936. English +L+ translation in [F+86], pages 397–399. +L+ [GS89]	F. Giunchiglia and A. Smaill. Reflection in constructive +L+ and non-constructive automated reasoning. In H. Abramson +L+ and M. H. Rogers, editors, Meta-Programming in Logic +L+ Programming, pages 123–140. MIT Press, Cambridge, +L+ Mass., 1989. +L+ [GWZ00] H. Geuvers, F. Wiedijk, and J. Zwanenburg. Equational rea- +L+ soning via partial reflection. In J. Harrison and M. Aagaard, +L+ editors, Theorem Proving in Higher Order Logics: 13th Inter- +L+ national Conference, TPHOLs 2000, volume 1869 of Lecture +L+ Notes in Computer Science, pages 162–178. Springer-Verlag, +L+ 2000. +L+ [HAB+]	Jason J. Hickey, Brian Aydemir, Yegor Bryukhov, Alexei +L+ Kopylov, Aleksey Nogin, and Xin Yu. A listing of Meta PRL +L+ theories. http://metaprl.org/theories.pdf. +L+ [Har95]	J. Harrison. Metatheory and reflection in theorem proving: +L+ A survey and critique. Technical Report CRC-53, SRI +L+ International, Cambridge Computer Science Research +L+ Centre, Millers Yard, Cambridge, UK, February 1995. +L+ </SectLabel_reference> <SectLabel_page> 11 +L+ </SectLabel_page> <SectLabel_reference> [HHP93]	Robert Harper, Furio Honsell, and Gordon Plotkin. A +L+ framework for defining logics. Journal of the Association +L+ for Computing Machinery, 40(1):143–184, January 1993. A +L+ revised and expanded verion of ’87 paper. +L+ [Hic97]	Jason J. Hickey. NuPRL-Light: An implementation +L+ framework for higher-order logics. In William McCune, +L+ editor, Proceedings of the 14th International Conference +L+ on Automated Deduction, volume 1249 of Lecture Notes in +L+ Artificial Intelligence, pages 395–399. Springer, July 13–17 +L+ 1997. An extended version of the paper can be found at +L+ http://www.cs.caltech.edu/~jyh/papers/cade14_ +L+ nl/default.html. +L+ [Hic99]	Jason J. Hickey. Fault-tolerant distributed theorem proving. +L+ In Ganzinger [Gan99], pages 227–231. +L+ [Hic01]	Jason J. Hickey. The MetaPRL Logical Programming +L+ Environment. PhD thesis, Cornell University, Ithaca, NY, +L+ January 2001. +L+ [HL78]	G´erard P. Huet and Bernard Lang. Proving and applying +L+ program transformations expressed with second-order +L+ patterns. Acta Informatica, 11:31–55,1978. +L+ [HNC+03] Jason Hickey, Aleksey Nogin, Robert L. Constable, +L+ Brian E. Aydemir, Eli Barzilay, Yegor Bryukhov, Richard +L+ Eaton, Adam Granicz, Alexei Kopylov, Christoph Kreitz, +L+ Vladimir N. Krupski, Lori Lorigo, Stephan Schmitt, Carl +L+ Witty, and Xin Yu. MetaPRL — A modular logical en- +L+ vironment. In David Basin and Burkhart Wolff, editors, +L+ Proceedings of the 16th International Conference on Theo- +L+ rem Proving in Higher OrderLogics (TPHOLs 2003), volume +L+ 2758 of Lecture Notes in Computer Science, pages 287–303. +L+ Springer-Verlag, 2003. +L+ [HNK+]	Jason J. Hickey, Aleksey Nogin, Alexei Kopylov, et al. +L+ MetaPRL home page. http://metaprl.org/. +L+ [Mos52]	Andrzej Mostowski. Sentences undecidable in formalized +L+ arithmetic: an exposition of the theory of Kurt G¨odel. +L+ Amsterdam: North-Holland, 1952. +L+ [NH02]	Aleksey Nogin and Jason Hickey. Sequent schema for +L+ derived rules. In Victor A. Carre˜no, C´ezar A. Mu˜noz, +L+ and Sophi`ene Tahar, editors, Proceedings of the 15th +L+ International Conference on Theorem Proving in Higher +L+ Order Logics (TPHOLs 2002), volume 2410 of Lecture Notes +L+ in Computer Science, pages 281–297. Springer-Verlag, 2002. +L+ [NKYH05] Aleksey Nogin, Alexei Kopylov, Xin Yu, and Jason Hickey. +L+ A computational approach to reflective meta-reasoning +L+ about languages with bindings. Technical Report Cal- +L+ techCSTR:2005.003, California Institure of Technology, +L+ 2005. Available at http://resolver.caltech.edu/ +L+ CaltechCSTR:2005.003. +L+ [Nor04]	Michael Norrish. Recursive function definition for types with +L+ binders. In Slind et al. [SBG04], pages 241–256. +L+ [Par71]	R. Parikh. Existence and feasibility in arithmetic. The Journal +L+ ofSymbolic Logic, 36:494–508,1971. +L+ [Pau94]	Lawrence C. Paulson. Isabelle: A Generic Theorem Prover, +L+ volume 828 of Lecture Notes in Computer Science. Springer- +L+ Verlag, New York, 1994. +L+ [PE88]	Frank Pfenning and Conal Elliott. Higher-order abstract +L+ syntax. In Proceedings oftheACMSIGPLAN’88 Conference +L+ on Programming Language Design and Implementation +L+ (PLDI), volume 23(7) of SIGPLANNotices, pages 199–208, +L+ Atlanta, Georgia, June 1988. ACM Press. +L+ [Pfe89]	Frank Pfenning. Elf: a language for logic definition and +L+ verified metaprogramming. In Proceedings of the 4th IEEE +L+ Symposium on Logic in Computer Science, pages 313–322, +L+ Asilomar Conference Center, Pacific Grove, California, June +L+ 1989. IEEE Computer Society Press. +L+ [Plo90]	Gordon Plotkin. An illative theory of relations. In R. Cooper, +L+ K. Mukai, and J. Perry, editors, Situation Theory and Its +L+ Applications, Volume 1, number 22 in CSLI Lecture Notes, +L+ pages 133–146. Centre for the Study of Language and +L+ Information, 1990. +L+ [PN90]	L. Paulson and T. Nipkow. Isabelle tutorial and user’s man- +L+ ual. Technical report, University of Cambridge Computing +L+ Laboratory, 1990. +L+ [SBG04]	Konrad Slind, Annette Bunker, and Ganesh Gopalakrishnan, +L+ editors. Proceedings of the 17th International Conference +L+ on Theorem Proving in Higher Order Logics (TPHOLs +L+ 2004), volume 3223 of Lecture Notes in Computer Science. +L+ Springer-Verlag, 2004. +L+ [Sch01]	Carsten Sch¨urmann. Recursion for higher-order encodings. +L+ In L. Fribourg, editor, Computer Science Logic, Proceedings +L+ of the 10th Annual Conference of the EACSL, volume 2142 +L+ of Lecture Notes in Computer Science, pages 585–599. +L+ Springer-Verlag, 2001. +L+ [Smi84]	B.C. Smith. Reflection and semantics in Lisp. Principles of +L+ Programming Languages, pages 23–35, 1984. +L+ [vH67]	J. van Heijenoort, editor. From Frege to G¨odel: A Source +L+ Book in Mathematical Logic, 1879–1931. Harvard University +L+ Press, Cambridge, MA, 1967. +L+ </SectLabel_reference> <SectLabel_page> 12 +L+ </SectLabel_page>
<SectLabel_title> An expressive aspect language for system applications +L+ with Arachne +L+ </SectLabel_title> <SectLabel_author> R´emi Douence, Thomas Fritz, Nicolas Loriant, +L+ Jean-Marc Menaud, Marc S´egura-Devillechaise, Mario S¨udholt +L+ </SectLabel_author> <SectLabel_affiliation> OBASCO project +L+ ´Ecole des Mines de Nantes/INRIA +L+ </SectLabel_affiliation> <SectLabel_address> 4 rue Alfred Kastler +L+ 44307 Nantes Cedex 3, France +L+ </SectLabel_address> <SectLabel_email> {douence,tfritz,nloriant,jmenaud,msegura,sudholt}@emn.fr +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> C applications, in particular those using operating system +L+ level services, frequently comprise multiple crosscutting con- +L+ cerns: network protocols and security are typical examples +L+ of such concerns. While these concerns can partially be ad- +L+ dressed during design and implementation of an application, +L+ they frequently become an issue at runtime, e.g., to avoid +L+ server downtime. A deployed network protocol might not be +L+ efficient enough and may thus need to be replaced. Buffer +L+ overflows might be discovered that imply critical breaches in +L+ the security model of an application. A prefetching strategy +L+ may be required to enhance performance. +L+ While aspect-oriented programming seems attractive in +L+ this context, none of the current aspect systems is expres- +L+ sive and efficient enough to address such concerns. This +L+ paper presents a new aspect system to provide a solution to +L+ this problem. While efficiency considerations have played +L+ an important part in the design of the aspect language, the +L+ language allows aspects to be expressed more concisely than +L+ previous approaches. In particular, it allows aspect pro- +L+ grammers to quantify over sequences of execution points as +L+ well as over accesses through variable aliases. We show how +L+ the former can be used to modularize the replacement of net- +L+ work protocols and the latter to prevent buffer overflows. +L+ We also present an implementation of the language as an +L+ extension of Arachne, a dynamic weaver for C applications. +L+ Finally, we present performance evaluations supporting that +L+ Arachne is fast enough to extend high performance applica- +L+ tions, such as the Squid web cache. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> aspect language, sequence pointcut, dynamic weaving, sys- +L+ tem applications +L+ </SectLabel_keyword> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> AOSD 05 Chicago Illinois USA +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2005 ACM 1-59593-042-6/05/03 ...$ 5.00. +L+ </SectLabel_copyright> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Real-world applications typically comprise multiple cross- +L+ cutting concerns. This applies, in particular, to C applica- +L+ tions using operating system level services. We have exam- +L+ ined three concerns which are typical for this domain in the +L+ context of a large application, the open source web cache +L+ Squid [36]. More concretely, we have considered translation +L+ of network protocols (which may be necessary for efficiency +L+ reasons), insertion of checks for buffer overflows (which are +L+ at the heart of many of today’s security issues), and in- +L+ troduction of prefetching strategies within the cache (which +L+ can be used to enhance efficiency of the web cache). We +L+ have found that all these concerns are scattered over large +L+ portions of the code of Squid. +L+ Hence, the three concerns are crosscutting in the sense +L+ of Aspect-Oriented Programming (AOP) [24] and aspects +L+ should therefore be a means of choice for their modular- +L+ ization. The concerns have three important characteristics. +L+ First, they must frequently be applied at runtime, e.g., in +L+ order to rapidly fix a buffer overflow and thus prevent secu- +L+ rity breaches without incurring server downtime. A dynamic +L+ aspect weaver is therefore needed. Second, they expose in- +L+ tricate relationships between execution points, e.g., network +L+ protocols are most concisely expressed in terms of sequences +L+ of execution points, not individual ones. The aspect system +L+ must therefore support expressive means for the definition of +L+ aspects, in particular pointcuts. Third, efficiency is crucial +L+ in the application domain we consider. +L+ To our knowledge, none of the current aspect systems for +L+ C meet these three requirements and is suitable for the mod- +L+ ularization of such concerns. Moreover, requirements for +L+ dynamic weaving and efficiency often trade off with expres- +L+ sivity. Squid should be as efficient as possible and therefore +L+ exploit any suitable operating system and hardware partic- +L+ ularity. Its code base is therefore difficult to understand and +L+ manipulate, thus hindering in particular modularization ef- +L+ forts. It is therefore highly questionable that the considered +L+ modularization problems can be solved without aspects. +L+ In this paper we propose a solution to the aspectization of +L+ such concerns of C applications. More concretely, we provide +L+ three main contributions. First, we provide a new expressive +L+ aspect language featuring a construct for quantification over +L+ sequences of execution points as well as over accesses to lo- +L+ cal aliases of global variables. We show how this aspect lan- +L+ </SectLabel_bodyText> <SectLabel_page> 27 +L+ </SectLabel_page> <SectLabel_bodyText> guage permits concise expression of the considered concerns +L+ as aspects. Second, we present how the aspect language can +L+ be implemented efficiently through runtime weaving into bi- +L+ nary code. Concretely, this is done by integrating the aspect +L+ language into our tool Arachne, a dynamic weaver for C ap- +L+ plications. Furthermore, we present how Arachne improves +L+ on our previous work µDyner [32]. Finally, we give evidence +L+ that our approach meets strong efficiency requirements by +L+ showing performance evaluations in the context of Squid. +L+ The paper is structured as follows. Section 2 presents the +L+ motivating concerns we identified within Squid. Section 3 +L+ shows how to modularize these concerns as aspects and de- +L+ fines our aspect language. Section 4 describes its implemen- +L+ tation within Arachne. Section 5 assesses the performance +L+ of our implementation. Section 6 describes related work. +L+ Section 7 concludes and suggests futures work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. MOTIVATIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Legacy C applications involve multiple crosscutting con- +L+ cerns. Many of them remain challenging, both in terms +L+ of expressiveness required to handle them properly in an +L+ aspect-oriented language and in terms of constraints posed +L+ on the weaver. This section describes three such concerns +L+ in C applications: switching the network protocol, buffer +L+ overflows and prefetching. The network protocol concern is +L+ typically scattered through the entire application. It is an +L+ issue when administrators discover at runtime that the re- +L+ tained protocol is not efficient enough. Likewise the security +L+ threats posed by buffer overflows is a real concrete problem +L+ for administrators. While guarding all buffers against over- +L+ flows might decrease performance considerably, administra- +L+ tors are left with no other option than accepting the trade- +L+ off between security and performance chosen at application’s +L+ design time. Prefetching is another well-known crosscutting +L+ concern [12]. Since prefetching aims at increasing perfor- +L+ mance, prefetching aspects make only sense with an efficient +L+ weaver. Yet, it is still difficult to modularize these three con- +L+ cerns in today’s aspect-oriented language. In this section, +L+ we first describe the context in which the concerns arise be- +L+ fore showing their crosscutting nature and finally explaining +L+ the lack in current aspect-oriented languages to handle them +L+ properly. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.1 TCP to UDP protocol +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> HTTP was essentially designed as a file transfer proto- +L+ col running on top of TCP, a connection-oriented protocol +L+ ensuring communication reliability. While the average Web +L+ page size does not exceed 8 KB [4], the cost of retrieving +L+ a Web page is often dominated by data exchanged for con- +L+ trol purposes of TCP rather than by the page content itself. +L+ This is not a new problem, many researches have already +L+ pointed out that TCP is not suitable for short-lived connec- +L+ tions. While HTTP 1.1 has introduced persistent connec- +L+ tions allowing a client to retrieve multiple pages from the +L+ same server through the same TCP connection, the number +L+ of simultaneous TCP connections is limited by operating +L+ systems. Servers have a strong incentive to close HTTP +L+ connections as soon as possible. Hence, despite the per- +L+ sistent connection mechanism, many studies conclude that +L+ TCP should be replaced by UDP to retrieve short pages [10, +L+ 29, 7]. In spite of its performance improvements, the number +L+ of legacy Web applications has prevented a wide adoption +L+ of this solution. Typical legacy Web applications have to be +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1: Typical usage of the TCP and UDP APIs. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> stopped to switch the protocol. The traditional approach +L+ to avoid depriving a subnetwork from Internet connectivity +L+ while stopping the cache is to swap the application between +L+ different machines. This approach is not only expensive in +L+ terms of hardware, it complicates the administrative task of +L+ the Web cache administrator and poses the problem of con- +L+ sistently transferring the runtime state of the application +L+ before restarting it. Stopping an e-commerce Web server +L+ means a loss of money and many small companies can not +L+ afford the cost of redundant servers. For a wide acceptance, +L+ a HTTP dialect using UDP as transport protocol should +L+ thus be deployable on demand at runtime. +L+ In addition, replacing TCP by UDP in an application is +L+ relatively difficult. The choice of a transport protocol is +L+ usually based on standards believed to be ever-lasting and +L+ made at an early design stage. Hence no particular effort is +L+ made to localize this design decision in a single piece of code. +L+ For example, despite a modularization effort, the TCP API +L+ provided by the operating system is used directly in 7 of the +L+ 104 ” . c” source files of the Squid Web cache. +L+ As shown in Fig. 1, the TCP API is built around a set of +L+ C functions to be invoked sequentially by the application. In +L+ a properly written program, TCP functions are first used to +L+ establish the connection (typically with socket, connect, +L+ bind and listen), exchange data through the connection +L+ (typically with read and write) and then close it (typically +L+ close). UDP uses similar but less functions. UDP applica- +L+ tions first direct the operating system to dedicate the appro- +L+ priate resources to exchange data (typically with socket and +L+ bind), then exchange data through these resources (typically +L+ with sendto and recvfrom) before releasing them (typically +L+ with close). Hence, the problem is not only difficult be- +L+ cause TCP-related function invocations are scattered but +L+ because the relative order of each invocation is important in +L+ order to map it onto the appropriate UDP function. +L+ This example is typical of protocol based APIs. When +L+ such an API is used in an undisciplined way, it becomes +L+ quickly impossible to replace it by another one. Today, +L+ aspect-oriented systems lack an appropriate sequencing con- +L+ struct in their language. Moreover, many do not provide the +L+ ability to weave aspects dynamically. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Buffer overflows +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In C, the size of an array is fixed at allocation time. Ac- +L+ cording to ISO and ANSI standards [2], an invalid array +L+ access does not result in an immediate error but leads to +L+ an implementation-dependent behavior. Such behavior is +L+ increasingly exploited by hackers to circumvent security re- +L+ </SectLabel_bodyText> <SectLabel_figure> TCP Protocol	Time	UDP Protocol +L+ Server Network Client	Server Network Client +L+ bind +L+ listen +L+ accept +L+ read +L+ write +L+ close +L+ socket +L+ connect +L+ write +L+ read +L+ close +L+ socket +L+ socket +L+ bind +L+ recvfrom +L+ sendto +L+ close +L+ socket +L+ recvfrom +L+ close +L+ sendto +L+ </SectLabel_figure> <SectLabel_page> 28 +L+ </SectLabel_page> <SectLabel_bodyText> strictions [37]. It is therefore crucial for C programmers to +L+ ensure every access to an array to be valid. On the other +L+ hand, bound checking code is error prone: it is easy to for- +L+ get to check an access and even when the access is checked, +L+ it is easy to compare the index locating the access with an +L+ inappropriate bound. Therefore, researchers have proposed +L+ to make compilers responsible for enforcing proper array ac- +L+ cess [22, 31]. The problem is that even the most efficient +L+ system (CRED [31]) slows down an application up to 130%. +L+ Moreover, most frequently used compilers like gcc do not +L+ support bound checking. +L+ Today, administrators discovering a buffer overflow in pro- +L+ duction software are left with no other option than stopping +L+ the application and restarting a bug free version. This was +L+ the solution chosen when a buffer overflow was discovered +L+ in Squid in [6]. While widely used, this solution suffers from +L+ three major drawbacks. First, it does not enforce continuous +L+ servicing since the service delivered by the application is not +L+ available during the update. Second, this solution entails an +L+ important information loss: an administrator has no means +L+ to learn whether the buffer overflow has been exploited by +L+ a hacker or not. Third, it misunderstands the performance +L+ trade-off, i.e. it is not necessary to check every array access, +L+ it is only necessary to perform enough checking to discour- +L+ age hackers. Therefore, bound checking code should only +L+ run when an environment becomes hostile [23]. +L+ Bound checking code tends to crosscut the entire applica- +L+ tion. For example, properly written C functions accepting +L+ an array argument commonly take a second argument hold- +L+ ing the array size: the first one allows the function to access +L+ the array while the second is used to ensure correctness of +L+ accesses. In Squid, bound checking code can be found in +L+ any of the 104 ” . c” files of its source code. On the 57635 +L+ lines composing these ” . c” files, at least 485 check bounds. +L+ This problem fails to be handled properly in current as- +L+ pect languages as they lack the ability to trigger advices +L+ upon access made through the alias of a variable. Again, +L+ many aspect-oriented systems offer only static weaving ca- +L+ pabilities preventing the administrator to choose the trade- +L+ off security/performance suiting his needs. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.3 From fetching to prefetching +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Operations like retrieving a file on a local disk or over the +L+ Web can be sped up if the underlying software anticipates +L+ user requests and start to fetch documents beforehand. Such +L+ prefetching schemes distinguish themselves from each other +L+ in the way they predict future user requests. These ”ora- +L+ cles” actually prevent a clean encapsulation of prefetching +L+ in a single module communicating with the rest of the appli- +L+ cation through well-defined interfaces since predictions are +L+ based on information meant to be private to other modules. +L+ In addition, it is very likely that there is no universal per- +L+ fect oracle [19]. A statically linked prefetching module is +L+ therefore inappropriate, but prefetching modules along with +L+ the necessary oracles should be loaded and unloaded on the +L+ fly. Due to their crosscutting nature, prefetching modules +L+ including such oracles are better written with aspects [32]. +L+ Coady et al. have already pointed out the crosscutting +L+ nature of prefetching in the FreeBSD OS [12]. In our pre- +L+ vious work considering the Squid Web cache, we reached a +L+ similar conclusion [32]. We have previously shown that this +L+ concern can be addressed with cflow-like constructs. +L+ Despite potential performance improvements, prefetching +L+ also increases resource consumption (e.g. network prefetch- +L+ ing consumes local storage and bandwidth). When the pres- +L+ sure on resources is too high, prefetching computation com- +L+ petes for them against regular user requests, and slows down +L+ their treatment instead of speeding it up. In such cases, +L+ prefetching should therefore be, temporarily, disabled. Squid +L+ essentially manages file descriptors, a resource only available +L+ in a limited quantity. A file descriptor is used between the +L+ underlying operating system and applications to describe a +L+ network connection or a file on the disk. Squid’s file descrip- +L+ tor management is based on a global variable that tracks the +L+ number of file descriptors currently in use. By comparing +L+ its value with the maximum number of file descriptors al- +L+ lowed by the operating system, it is possible to estimate that +L+ prefetching should be disabled or resumed. +L+ For this problem of file descriptor consumption, the cur- +L+ rent practice of checking if prefetching should be disabled or +L+ not within the advice, is a bad practice that impedes both +L+ readability and maintainability. A mechanism is needed +L+ within the aspect language to restraint the advice execu- +L+ tion at times where the pressure on resources is too high. +L+ This problem were not addressed in our previous work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. AN EXPRESSIVE ASPECT LANGUAGE +L+ FOR SYSTEM PROGRAMMING IN C +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> While AOP seems to be the obvious choice to tackle the +L+ crosscutting concerns introduced above, none of the existing +L+ AO systems provides explicit support for some of their es- +L+ sential elements, in particular, join point sequences for pro- +L+ tocols, and references to aliases which are local to a function. +L+ In this section we introduce a new aspect language for +L+ system programming in C that allows such crosscutting con- +L+ cerns to be expressed concisely. In order to make this point, +L+ we first revisit the examples by concisely aspectizing them +L+ using our language. (Note that our aspect language is ex- +L+ pressive in the sense of enabling the concise definition of cer- +L+ tain types of aspects, especially compared to other tools for +L+ system-level manipulations, but not necessarily more expres- +L+ sive than existing approaches in a language-theoretic sense.) +L+ We then define the join point model underlying our language +L+ precisely, followed by the definition of its syntax and infor- +L+ mal semantics. Finally, we illustrate how its semantics can +L+ be formally defined in terms of a small-step operational se- +L+ mantics using the framework introduced in [14]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Example crosscutting concerns revisited +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We now revisit the concerns discussed in section 2 in order +L+ to show our language in action and give evidence that it +L+ allows such concerns to be concisely modularized. +L+ The aspect shown in Fig. 2 translates transport protocols +L+ from TCP to UDP. A protocol defines a sequence of func- +L+ tion calls, so the top-level operator of this aspect is seq. +L+ The sequence aspect syntactically consists of a list of pairs +L+ of pointcut and advice (separated by then). In the exam- +L+ ple, the TCP protocol starts with a call to socket() with +L+ three constant arguments: AF INET, SOCK STREAM and +L+ 0. When such a call is matched, the second parameter is +L+ replaced by SOCK DGRAM as required by the UDP proto- +L+ col. The result of this transformed call, the file descriptor, +L+ is bound to fd by return(fd). Then the next call to con- +L+ nect() with the same file descriptor fd as its first parameter +L+ is matched. In this case the values of the other parameters +L+ </SectLabel_bodyText> <SectLabel_page> 29 +L+ </SectLabel_page> <SectLabel_figure> seq( call(int socket(int, int, int)) && args(AF INET, SOCK STREAM, 0) && return(fd) +L+ then socket(AF INET, SOCK DGRAM, 0); +L+ call(int connect(int, struct socketaddr*, socklen t)) && args(fd, address, length) +L+ then returnZero(); // where int returnZero() { return 0; } +L+ ( call(size t read(int, void*, size t)) && args(fd, readBuffer, readLength) +L+ then recvfrom(fd, readBuffer, readLength, 0, address, length); +L+ 11call(size t write(int, void*, size t)) && args(fd, writeBuffer, writeLength) +L+ then sendto(fd, writeBuffer, writeLength, 0, address, length);) * +L+ call(int close(int)) && args(fd) ; ) +L+ Figure 2: An Aspect for Switching Transport Protocols, from TCP to UDP +L+ seq( call(void * malloc(size t)) +L+ && args(allocatedSize) && return(buffer) ; +L+ write(buffer) && size(writtenSize) +L+ && if(writtenSize > allocatedSize) +L+ then reportOverflow(); * +L+ call(void free(void*)) ) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: An Aspect for Detecting Buffer Overflow +L+ </SectLabel_figureCaption> <SectLabel_bodyText> are bound to arguments address and length, and the original +L+ call is replaced by returnZero(). Indeed, there is no connect +L+ step in the UDP protocol. After that, calls to read() and +L+ write() (using the ‘or’ on aspects: 11) on the same file de- +L+ scriptor fd are translated to UDP recvfrom() and sendto(), +L+ respectively. Note that sequences of such access are poten- +L+ tially translated (due to use of the repetition operator *). +L+ Finally, a call to close() on fd terminates the TCP protocol +L+ as well as the UDP protocol and thus is not modified (i.e., +L+ there is no then clause). This last step is required to free +L+ the variables used in the sequence (here, fd, address and +L+ length). Indeed, this aspect can use numerous (instances of +L+ these) variables when it deals with interleaved sequences, as +L+ each call to socket() creates a new instance of the sequence. +L+ The aspect shown in Fig. 3 detects buffer overflows. The +L+ corresponding sequence starts when the function malloc() +L+ returns the buffer address which is then bound to buffer. +L+ Then, each time this address is accessed (through a global +L+ variable or a local alias) the size of the data to be written is +L+ compared with the size of the initially allocated memory. If +L+ the former exceeds the latter, an overflow is indicated. The +L+ sequence ends when the memory is deallocated using free(). +L+ The aspect in Fig. 4 introduces prefetching in a web cache. +L+ The first controlf low phrase initializes prefetching when +L+ an HTTP response is built (clientBuildReply()) within the +L+ control flow of a client request (clientSendMoreData()). The +L+ until clause stops prefetching when the number of connec- +L+ tion becomes too large, a situation where prefetching would +L+ effectively degrade performance. The second controlf low +L+ phrase analyzes hyperlinks in a page being transmitted (i.e., +L+ when comm write mbuf() is called within the control flow +L+ of clientSendMoreData()). Finally, the last call phrase pre- +L+ fetches hyperlinks analyzed by the second aspect. It does so +L+ by replacing the method call to clientWriteComplete() with +L+ retrieveHyperlinks(). Finally, note that the two require +L+ clauses at the top of the aspect declare the types of the +L+ global variables of the base program used in the aspects. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Join points +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A join point model defines the points in the execution +L+ of the base program to which pointcuts may refer. In our +L+ </SectLabel_bodyText> <SectLabel_figure> JP::= callJP(valfunId( −→val)) +L+ readGlobalJP(varId,val) +L+ readJP(@, val) +L+ writeGlobalJP(varId, val, size) +L+ writeJP(@, val, size) +L+ controlflowJP(−−−−→ +L+ funId, cfEnd) +L+ controlflowstarJP(−−−−→ +L+ funId, cfEnd) +L+ cfEnd::= callJP(val funId(−→val)) +L+ 1readGlobalJP(varId,val) +L+ 1writeGlobalJP(varId, val, size) +L+ val::= 011121...	//int +L+ 1@0 1 @1 1 @2 1 ... // int* +L+ 1... // values of other C types +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: Join point model +L+ </SectLabel_figureCaption> <SectLabel_bodyText> case, join points are defined by JP in the grammar shown +L+ in Fig. 5. A join point is either: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	A call of a function callJP(v1 funId(−→v2)) with function +L+ name funId, return value vi and a vector of arguments →−v2. +L+ •	A read access which comes in two variants: +L+ readGlobalJP(varId,v) denotes reading a global vari- +L+ able with name varId holding the value v; readJP(@, v) +L+ denotes reading a global variable or a local alias with +L+ address @ holding the value v. +L+ •	Write access which also comes in two variants: +L+ writeGlobalJP(varId, v, size) denotes assignment to a global +L+ variable with name varId of the value v of size size. +L+ writeJP(@, v, size) denotes assignment to a global variable +L+ or a local alias with address @ of the value v of size size. +L+ •	A cflow expression controlflowJP( f Inu d, c), where +L+ f Inu d = [funId1, .., funIda] is a stack of function names, and +L+ c (either a function call or an access to a global variable) oc- +L+ </SectLabel_listItem> <SectLabel_bodyText> curs within the body of function funId�. Such a join point +L+ requires a call to funId�+1 within the body of funId�. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	A cflow expression controlflowstarJP( f unId, c), where +L+ −−−−→ +L+ </SectLabel_listItem> <SectLabel_bodyText> f Inu d = [funId1, .., funIda] is a partial stack of function +L+ names, and c (either a function call or an access to a global +L+ variable) occurs within the control flow of function funId�. +L+ Such a join point requires a call to funId�+1 within the +L+ control flow of (i.e., not necessarily in the body of) funId�. +L+ Two features of this join point model may be surprising +L+ at first sight: distinction of accesses to aliases from those to +L+ global variables and explicit representation of control flow +L+ </SectLabel_bodyText> <SectLabel_page> 30 +L+ </SectLabel_page> <SectLabel_figure> require Number Of Fd as int*; +L+ require Squid MaxFd as int*; +L+ controlflow(call(void clientSendMoreData(void*, char*, size t)), +L+ call(HttpReply * clientBuildReply(clientHttpRequest*, char*, size t)) +L+ && args( request, buffer, buffer Size )) +L+ then startPrefetching(request, buffer, bufferSize); +L+ && until(writeGlobal(int * Number Of Fd) && if((*Number Of Fd) * 100/(*Squid MaxFd) ≥ 75) ; ) +L+ controlflow( call(void clientSendMoreData(void*, char*, size t)), +L+ call(void comm write mbuf(int, MemBuf, void*, void*)) +L+ && args(fd, mb, handler, handlerData) && if (! isPre f etch(handler)) ) +L+ then parseHyperlinks(fd, mb, handler, handlerData); +L+ call(void clientWriteComplete(int, char*, size t, int, void*)) +L+ && args(fd, buf, size, error, data) && if(! isPre f etch(handler)) +L+ then retrieveHyperlinks(fd, buf, size, error, data); +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: An Aspect for Prefetching +L+ </SectLabel_figureCaption> <SectLabel_bodyText> expressions. Both are motivated by our quest for efficiency +L+ and are grounded in strong implementation constraints in +L+ the context of dynamic weaving of binary C code: an access +L+ to a local alias is several magnitudes slower than that to a +L+ global variable and matching of control flow join points can +L+ be done using an atomic test on the implementation level. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Pointcuts +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We now present a pointcut language (see Fig. 6) that pro- +L+ vides constructs to match individual join points. +L+ Primitive pointcuts are defined by PPrim and comprise +L+ three basic pointcuts matching calls, global variable accesses, +L+ and control flow join points. Primitive pointcuts can also be +L+ combined using a logical “or” noted I I. +L+ A call pointcut PCall selects all function call join points +L+ −→ +L+ callJP(val funId(val)), i.e., all calls to a function matching +L+ the signature type funId(t pe), where the arguments of the +L+ function can be bound to pointcut variables using argument +L+ binder args( −−−−−→ +L+ pattern ) and the return value can be bound to +L+ a pointcut variable using a return clause return( pattern ). +L+ The two constructs args( −−−−−→ +L+ pattern ) and return( pattern ) +L+ can also provide pattern matching by using values (or al- +L+ ready bound pointcut variables) in pattern. Pointcuts can +L+ also depend on a boolean condition using the if-constructor. +L+ A global access pointcut PAccGlobal selects either all read +L+ join points readGlobalJP(varId, val) or all write join points +L+ writ eGlobalJP(varId, val, size) on the global base program +L+ variable varId. In these cases, the read or written value can +L+ be bound to a variable using value(pattern); in addition, the +L+ size of the written value can be bound with size(varName). +L+ Pattern matching can also be used for variable access. +L+ A control flow pointcut PCf of the form controlflow( +L+ PCallSig1,..., PCallSign, PCfEnd) matches all join points +L+ of the form controlflowJP(funId1, ..., funIdn, cfEnd), where +L+ the function identifier in PCallSigi is funIdi. Similarly, a +L+ control flow pointcut may match a global variable access +L+ for a given stack configuration. The pointcuts of the form +L+ controlflowstar(... ) select calls or global variable accesses +L+ in a stack context allowing for calls that are not directly +L+ nested within one another. +L+ Finally, PAcc, an access pointcut for a global variable or +L+ all of its local aliases, matches all join points of the form +L+ readJP or writeJP. +L+ </SectLabel_bodyText> <SectLabel_figure> Asp::= AspPrim [ && until( AspPrim) ] +L+ IAspSeq [ && until( AspPrim ) ] +L+ AspPrim::= PPrim Advice +L+ AspSeq::= seq( AspPrim +L+ AspSeqElts +L+ AspSeqElt ) +L+ AspSeqElts ::_ [AspSeqElts] AspSeqElt [ * ] +L+ AspSeqElt::= AspPrim +L+ IPAcc Advice +L+ I(AspSeqElt II AspSeqElt) +L+ Advice::= [ then funId(pat�) ] ; +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 7: Aspect language +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 3.4 Aspect Language +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The aspect language we propose is defined in Fig. 7. As- +L+ pects Asp are either primitive AspPrim, or sequences of +L+ primitive aspects AspSeq. +L+ A primitive aspect AspPrim combines a primitive point- +L+ cut with an advice that will be applied to all join points +L+ selected by the pointcut. If the primitive pointcut has the +L+ form p1 II p2, then all variables used in the advice have to +L+ be bound in both, p1 and p2. +L+ An advice (Advice) is a C function call that replaces a join +L+ point in the base program execution (similarly to around in +L+ AspectJ). It must have the same return type as the join +L+ point it replaces: the type of the global variable in case of a +L+ read access, void for a write access and the return type of +L+ the function for a call. When the advice is empty (no then +L+ clause), the original join point is executed. The original join +L+ point can be skipped by calling an empty C function. +L+ A sequence aspect is composed of a sequence of primitive +L+ aspects. A sequence starts when the first primitive aspect +L+ matches. Then the second primitive aspect becomes active +L+ instead of the first one. When it matches, the third aspect +L+ becomes active instead of the second one. And so on, until +L+ the last primitive aspect in the sequence. All but the first +L+ and last primitive aspects can be repeated zero or multiple +L+ times by using *: in this case, the primitive aspect is ac- +L+ </SectLabel_bodyText> <SectLabel_page> 31 +L+ </SectLabel_page> <SectLabel_figure> PPrim +L+ PCall +L+ PCallSig +L+ PIf +L+ PAccGlobal +L+ PCf	::= +L+ 1controlflowstar( PCallSigList, PCfEnd ) +L+ PCallSigList ::= PCallSig [ , PCallSigList] +L+ PCall 1 PAccGlobal +L+ PCall +L+ 1PAccGlobal +L+ 1PCf +L+ 1PPrim11 PPrim +L+ ::= +L+ PCallSig [ && args(−−−−−→ +L+ pattern) ] [ && return( pattern) ] [ && PIf ] +L+ call( type funId(t pe) ) +L+ if( expr ) [ && PIf ] +L+ readGlobal( type varId) [ && value( pattern) ] [ && PIf ] +L+ 1writeGlobal( type varId) [ && value( pattern) ] [ && size( pattern) ] [ && PIf ] +L+ ::= +L+ ::= +L+ ::= +L+ ::= +L+ controlflow( PCallSigList, PCfEnd ) +L+ PCf End	::= +L+ PAcc +L+ pattern	::= +L+ var 1 val +L+ read( var ) [ && value(pattern ) ] [ && PIf ] +L+ 1write( var ) [ && value( pattern ) ] [ && size( pattern) ] [ && PIf ] +L+ ::= +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6: Pointcut language +L+ </SectLabel_figureCaption> <SectLabel_figure> A::= A' +L+ 1A 11 A	; parallelism +L+ A'::= µa.A'	; recursive definition (a E Rec) +L+ 1C D I; A	; prefixing +L+ 1C D I; a	; end of sequence (a E Rec) +L+ 1C D I; STOP ; halting aspect +L+ 1A' ❑ A'	; choice +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 8: Tiny aspect language +L+ </SectLabel_figureCaption> <SectLabel_bodyText> tive as long as the following one in the sequence does not +L+ match. Branching, i.e., a logical ‘or’ between two primitive +L+ aspects, can be introduced in a sequence by the operator 11. +L+ An element of the sequence can also match a global vari- +L+ able of the base program and accesses to its local aliases, as +L+ soon as its address is known (i.e., a previous primitive point- +L+ cut has already bound its address to a pointcut variable). +L+ Hence, an aspect matching accesses cannot start a sequence. +L+ Every join point matching the first primitive pointcut of a +L+ sequence starts a new instance of the sequence. The different +L+ instances are matched in parallel. +L+ A primitive or a sequence aspect a can be used in combi- +L+ nation with an expression until (a1 ), to restrict its scope. In +L+ this case, once a join point has been matched by a, the execu- +L+ tion of a proceeds as previously described until a1 matches. +L+ To conclude the presentation of our language, note that it +L+ does not include some features, such as named pointcuts as +L+ arguments to controlf lows and conjunctive terms, which +L+ are not necessary for the examples we considered but which +L+ could easily be added. (As an aside, note that such exten- +L+ sions of the pointcut language may affect the computability +L+ of advanced algorithmic problems, such as whether a point- +L+ cut matches some part of any base program [25].) +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.5 Towards a formal semantics for expressive +L+ aspects +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the previous sections, we have given an informal se- +L+ mantics of our aspect language. We now illustrate how the +L+ aspect language could be formally defined by translating one +L+ of the example aspects into formal aspect language by ex- +L+ tension of that used in the formal framework of [14]. +L+ The original formal language must be extended in order to +L+ deal with halting aspects, an unbounded number of sequen- +L+ tial aspects and arbitrary join point predicates. The gram- +L+ mar of the extension, our tiny aspect language, is defined in +L+ Figure 8. In this language, aspect expressions A consists of +L+ parallel combinations of aspects, C is a join point predicate +L+ (similar to our pointcut language) expressed as a conjunc- +L+ tion of a term pattern and possibly an expression from the +L+ constraint logic programming language CLP(R) [20]. +L+ An aspect A' is either: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	A recursive definition. +L+ •	A sequence formed using the prefix operation C D I; X, +L+ where X is an aspect or a recursion variable and I a piece +L+ of code (i.e., an advice). +L+ •	A choice construction A1 ❑ A2 which chooses the first +L+ aspect that matches a join point (the other is thrown away). +L+ If both match the same join point, A1 is chosen. +L+ •	A parallel composition of two aspects A1 11 A2 that +L+ cannot occur in choice construction. +L+ •	A halting aspect STOP. +L+ </SectLabel_listItem> <SectLabel_bodyText> The semantics of the protocol translation aspect (from +L+ TCP to UDP) is given in Fig. 9. A sequence can have sev- +L+ eral instances. This is translated into the language A by the +L+ expression a1 11 ... which starts a new sequence a1 once +L+ the first join point has been matched and continue to match +L+ the rest of the sequence in progress. The repetition oper- +L+ ator ∗ is translated into recursion on variable the a2. The +L+ branching operator 11 is translated into the choice operator +L+ </SectLabel_bodyText> <SectLabel_page> 32 +L+ </SectLabel_page> <SectLabel_figure> µa1. callJP(fd socket(AF INET,  SOCK  STREAM, 0)) D socket(AF INET, SOCK DGRAM, 0); +L+ a1 ( callJP(a connect(fd, address, length)) D returnZero(); +L+ µa2. callJP(b close(fd)) D skip; STOP +L+ ❑	callJP(c read(fd, readBuffer, readLength)) D recvfrom(fd, readBuffer, readLength, 0, address, length); a2 +L+ ❑	callJP(d write(fd, writeBuffer, writeLength)) D recvfrom(fd, writeBuffer, writeLength, 0, address, length); a2 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 9: Definition of the protocol translation using the tiny aspect language +L+ </SectLabel_figureCaption> <SectLabel_bodyText> ❑. Finally, the last primitive aspect of the sequence occurs +L+ as the first aspect of a choice to get priority over the join +L+ points read and write because of the *. Note that we use +L+ pattern matching in A and that an overbar marks the first +L+ occurrence of a variable (i.e., its definition not a use). +L+ Note that formal definitions such as that of the protocol +L+ translation aspect precisely define several important issues, +L+ in particular, when new instances of the sequence aspect are +L+ created, and disambiguate of potentially non-deterministic +L+ situations, e.g., when two pointcuts of consecutive primitive +L+ aspects in the sequence match at the same time. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. DYNAMIC WEAVING WITH ARACHNE +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Arachne is built around two tools, an aspect compiler and +L+ a runtime weaver. The aspect compiler translates the aspect +L+ source code into a compiled library that, at weaving time, di- +L+ rects the weaver to place the hooks in the base program. The +L+ hooking mechanisms used in Arachne are based on improved +L+ techniques originally developed for µDyner [32]. These tech- +L+ niques allow to rewrite the binary code of executable files +L+ on the fly i.e.without pausing the base program, as long +L+ as these files conform to the mapping defined by the Unix +L+ standard [35] between the C language and x86 assembly lan- +L+ guage. Arachne’s implementation is structured as an open +L+ framework that allows to experiment with new kinds of join +L+ points and pointcut constructs. Another important differ- +L+ ence between Arachne and µDyner is, that µDyner requires +L+ a compile time preparation of the base program, whereas +L+ Arachne does not. Hence Arachne is totally transparent for +L+ the base program while µDyner is not. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 The Arachne Open Architecture +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The Arachne open architecture is structured around three +L+ main entities: the aspect compiler, the instrumentation ker- +L+ nel, and the different rewriting strategies. The aspect com- +L+ piler translates the aspect source code into C before com- +L+ piling it. Weaving is accomplished through a command line +L+ tool weave that acts as a front end for the instrumentation +L+ kernel. weave relays weaving requests to the instrumen- +L+ tation kernel loaded in the address space of the program +L+ through Unix sockets. Upon reception of a weaving request, +L+ the instrumentation kernel selects the appropriate rewriting +L+ strategies referred by the aspects to be woven and instru- +L+ ments the base program accordingly. The rewriting strat- +L+ egy consults the pointcut analysis performed by the aspect +L+ compiler to locate the places where the binary code of the +L+ base program needs to be rewritten. It finally modifies the +L+ binary code to actually tie the aspects to the base program. +L+ With this approach, the Arachne core is independent of +L+ a particular aspect, of the aspect language, of the particu- +L+ lar processor architecture, and of a particular base program. +L+ In fact, all dependencies to aspect language implementation +L+ are limited to the aspect compiler. All dependencies to the +L+ operating system are localized in the instrumentation ker- +L+ nel and finally all dependencies to the underlying hardware +L+ architecture are modularized in the rewriting strategies. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.1.1 The Arachne aspect compilation process +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The aspect compilation scheme is relatively straightfor- +L+ ward: it transforms advices into regular C functions. Point- +L+ cuts are rewritten as C code driving hook insertions into +L+ the base program at weaving time. There are however cases +L+ where the sole introduction of hooks is insufficient to deter- +L+ mine whether an advice should be executed. In this case, +L+ the aspect compiler generates functions that complement +L+ the hooks with dynamic tests on the state of the base pro- +L+ gram. These dynamic tests are called residues in AspectJ +L+ and the rewritten instructions within the base program the +L+ shadow [16]. Once the aspects have been translated into C, +L+ the Arachne compiler uses a legacy C compiler to generate a +L+ dynamically linked library (DLL) for the compiled aspects. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.1.2 The Arachne weaving process +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> From a user viewpoint, the Arachne weave and deweave +L+ command line programs the same syntax than µDyner’s ver- +L+ sion. They both take two arguments. The first identifies the +L+ process to weave aspects in or deweave aspects from, and +L+ the second indicates the aspect DLL. However, Arachne can +L+ target potentially any C application running on the machine +L+ while µDyner was limited to applications compiled with it +L+ running on the machine. When Arachne’s weave receives a +L+ request to weave an aspect in a process that does not con- +L+ tain the Arachne instrumentation kernel, it loads the kernel +L+ in the process address space using standard techniques [11]. +L+ The instrumentation kernel is transparent for the base +L+ program as the latter cannot access the resources (mem- +L+ ory and sockets essentially) used by the former. Once in- +L+ jected, the kernel creates a thread with the Linux system +L+ call: clone. This thread handles the different weaving re- +L+ quests. Compared to the POSIX pthread create function, +L+ the usage of clone allows the instrumentation thread to pre- +L+ vent the base program to access its sockets. The instrumen- +L+ tation kernel allocates memory by using side effect free allo- +L+ cation routines (through the Linux mmap API). Because the +L+ allocation routines are side effect free, Arachne’s memory is +L+ totally invisible to the base program. It is up to the aspect +L+ to use Arachne’s memory allocation routines or base pro- +L+ gram specific allocation functions. This transparency turns +L+ out to be crucial in our experiments. Legacy applications +L+ such as Squid use dedicated resource management routines +L+ and expect any piece of code they run to use these routines. +L+ Failures will result in an application crash. +L+ After loading an aspect, the instrumentation kernel rewrites +L+ the binary code of the base program. These rewriting strate- +L+ gies are not included in the kernel and must be fetched on +L+ demand by each loaded aspect. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Rewriting strategies +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Rewriting strategies are responsible for transforming the +L+ binary code of the base program to effectively tie aspects to +L+ </SectLabel_bodyText> <SectLabel_page> 33 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 10: Generic hook operations. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the base program at weaving time. These strategies localize +L+ Arachne’s main dependencies to the underlying hardware +L+ architecture. In general, rewriting strategies need to col- +L+ lect information about the base program. These information +L+ typically consist of the addresses of the different shadows, +L+ their size, the symbol (i.e.function or global variable name) +L+ they manipulate, their length etc. In order to keep compiled +L+ aspects independent from the base program, this informa- +L+ tion is gathered on demand at runtime. The mapping be- +L+ tween a symbol name in the base program source code and +L+ its address in memory is inferred from linking information +L+ contained in the base program executable. However because +L+ these information can be costly to retrieve, Arachne collects +L+ and stores it into meta-information DLLs. these DLLs be- +L+ have as a kind of cache and lessen the problem of collecting +L+ the information required to instrument the base program. +L+ To implement our aspect language, Arachne provides a set +L+ of eight rewriting strategies that might eventually use each +L+ other. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.1 Strategiesfor call, readGlobal and writeGlobal +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In Arachne, call, readGlobal and writeGlobal allow an +L+ advice to be triggered upon a function call, a read on a +L+ global variable or a write respectively. While the implemen- +L+ tation of readGlobal and writeGlobal in Arachne is close +L+ to the one in µDyner, Arachne implements the strategy for +L+ call by rewriting function invocations found in the base +L+ program. µDyner instead rewrites the function body of the +L+ callee. On the Intel architecture, function calls benefit from +L+ the direct mapping to the x86 call assembly instruction +L+ that is used by almost, if not all, compilers. Write and read +L+ accesses to global variables are translated into instructions +L+ using immediate, hard coded addresses within the binary +L+ code of the base program. By comparing these addresses +L+ with linking information contained in the base program ex- +L+ ecutable, Arachne can determine where the global variable +L+ is being accessed. Therefore those primitive pointcuts do +L+ not involve any dynamic tests. The sole rewriting of the +L+ binary base program code is enough to trigger advice and +L+ residue1 executions at all appropriate points. +L+ The size of the x86 call instruction and the size of an x86 +L+ jump (jmp) instruction are the same. Since the instruction +L+ performing an access to a global variable involves a hard +L+ coded address, x86 instructions that read or write a global +L+ </SectLabel_bodyText> <SectLabel_footnote> 1Residues (i.e. dynamic tests on the base program state) are +L+ required when these primitive pointcuts are combined with +L+ conditional pointcuts or when pattern matching is involved. +L+ </SectLabel_footnote> <SectLabel_bodyText> variable have at least the size of a x86 jmp instruction. Hence +L+ at weaving time, Arachne rewrites them as a jmp instruction +L+ to a hook. Hooks are generated on the fly on freshly allo- +L+ cated memory. As shown in figure 10, hooks contain a few +L+ assembly instructions that save and restore the appropriate +L+ registers before and after an advice (or shadow) execution. +L+ A generic approach is to have hooks save the whole set of +L+ registers, then execute the appropriate residue and/or ad- +L+ vice code before restoring the whole set of registers; finally +L+ the instructions found at the join point shadow are executed +L+ to perform the appropriate side effects on the processor reg- +L+ isters. This is accomplished by relocating the instructions +L+ found at the join point shadow. Relocating the instructions +L+ makes the rewriting strategies handling read and write ac- +L+ cess to global variable independent from the instruction gen- +L+ erated by the compiler to perform the access 2. The limited +L+ number of x86 instructions used to invoke a function allows +L+ Arachne’s rewriting strategy to exploit more efficient, relo- +L+ cation free, hooks. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.2 Strategiesfor controlf low and controlflowstar +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Every time a C function is called, the Linux runtime +L+ creates an activation record on the call stack [35]. Like +L+ µDyner, Arachne’s implementation of the rewriting strat- +L+ egy for controlf low uses the most deeply nested function +L+ call (or global read or write access) in the control flow point- +L+ cut as shadow. This shadow triggers a residue. This residue +L+ uses the activation record’s chaining to check whether the +L+ remaining function calls of the control flow, are on the call +L+ stack maintained by the Linux runtime. An appropriate +L+ usage of hashtables that store the linking information con- +L+ tained in the base program executables can thereby de- +L+ crease the cost of determining if a specific function is the +L+ caller of another to a pointer comparison. Therefore, the +L+ residue for a controlf low with n directly nested functions +L+ implies exactly n pointer comparisons. However, the residue +L+ worst case runtime for the indirect control flow operator +L+ controlflowstar that allows for not directly nested func- +L+ tions, is proportional to the base program stack depth. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.3 Strategiesfor read and write +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> read and write are new join points not included in µDyner +L+ that have been added to the latest version of Arachne. Their +L+ implementation relays on a page memory protection as al- +L+ lowed by the Linux operating system interface (i.e. mprotect) +L+ and the Intel processor specifications [18]. A read or write +L+ pointcut triggers a residue to relocate the bound variable +L+ into a memory page that the base program is not allowed +L+ to access and adds a dedicated signal handler. Any attempt +L+ made by the base program to access the bound variable iden- +L+ tified will then trigger the execution of the previously added +L+ signal handler. This handler will then inspect the binary +L+ instruction trying to access the protected page to determine +L+ whether it was a read or a write access before eventually +L+ executing the appropriate advice. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.4 Strategiesfor seq +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Like read and write, seq is a new language feature of +L+ Arachne. µDyner offers no equivalent construct. Arachne’s +L+ rewriting strategy of this operator associates a linked list to +L+ </SectLabel_bodyText> <SectLabel_footnote> 2About 250 x86 instruction mnemonics can directly manip- +L+ ulate a global variable. This corresponds to more than one +L+ thousand opcodes. +L+ </SectLabel_footnote> <SectLabel_figure> execution flow +L+ Legacy base program +L+ shadow: rewriting +L+ site replaced by a +L+ jump +L+ B���piled baof sethe +L+ program +L+ x86 instruction +L+ x86 instruction +L+ x86 instruction +L+ x86 instruction +L+ Relocated tailored +L+ ���tructio�� +L+ up���ng re���ters +L+ Hooks generated at weavingAspect DLL +L+ time	generated at aspect compile time +L+ Entry hook +L+ save registers +L+ Return hook +L+ Restore registers +L+ Residue (dynamic tests) +L+ and/or advices +L+ </SectLabel_figure> <SectLabel_page> 34 +L+ </SectLabel_page> <SectLabel_bodyText> every stage inside the sequence except the last one. Each +L+ stage in a sequence triggers a residue that updates these +L+ linked lists to reflect state transitions of currently match- +L+ ing execution flows. Upon matching of the first pointcut +L+ of the first primitive aspect in the seq, a node is allocated +L+ and added to the associated linked list. This node con- +L+ tains a structure holding variables shared among the dif- +L+ ferent pointcuts within the sequence. Once a join point +L+ matches a pointcut of an primitive aspect denoting a stage +L+ in the sequence, Arachne consults every node in the linked +L+ list associated with the previous stage and executes the cor- +L+ responding advice 3. Arachne eventually updates the node +L+ and in the absence of a * moves it to the list associated +L+ with the currently matched pointcut.If the matching point- +L+ cut corresponds to the end of the sequence, structures are +L+ not moved into another list but freed. Our aspect compiler +L+ includes an optimization where structures are allocated from +L+ a resizable pool and upon a sequence termination, structures +L+ are not freed but returned to the pool. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Arachne limitations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Aggressive optimizations of the base program might pre- +L+ vent Arachne to seamlessly weave aspects. Two optimiza- +L+ tions are not yet supported by Arachne. First if the compiler +L+ inlines a function in another one within the binary code of +L+ the base program, the Arachne weaver will fail to properly +L+ handle pointcuts referring to that function. Second, con- +L+ trol flow pointcuts are based on the chaining of activation +L+ records. On the x86 architecture, in leaf functions, opti- +L+ mizing compilers sometimes do not maintain this chaining +L+ to free one register for the rest of the computation. This +L+ however has not been a problem during our experiments +L+ as we used the open source C compiler gcc. Arachne sup- +L+ ports two of the three optimization levels proposed by gcc. +L+ Stripping that removes linking information and aggressive +L+ optimizations that break the interoperability between com- +L+ pilers and/or debuggers are incompatible with Arachne. In +L+ practice, Arachne can be used on applications compiled like +L+ squid with two of the three gcc optimization level. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. PERFORMANCE EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Aspect-oriented solutions will be used if the aspect sys- +L+ tem’s language is expressive enough and if the aspect system +L+ overhead is low enough, for the task at hand. The purpose +L+ of this section is to study Arachne’s performance. We first +L+ present the speed of each Arachne language construct and +L+ compare it to similar C language constructs. We then study +L+ the overhead of extending Squid with a prefetching policy. +L+ This case study shows that even if the cost of some Arachne +L+ aspect language constructs might be high compared to C +L+ language constructs, this overhead is largely amortized in +L+ real applications. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.1 Evaluation of the language constructs +L+ This performance evaluation focuses on studying the cost +L+ of each construct of our aspect language. To estimate the +L+ cost for each construct of our aspect language, we wrote an +L+ aspect using this construct that behaves as an interpreter of +L+ </SectLabel_subsectionHeader> <SectLabel_footnote> 3In case the previous stage pointcut was used with a star +L+ *, Arachne examines nodes from linked list associated with +L+ the last two previous stages, and so on, until a not starred +L+ primitive aspect in the sequence is reached. +L+ </SectLabel_footnote> <SectLabel_table> Execution times (cycles) +L+ call	Arachne	Native	Ratio +L+ 	28±2.3%	21±1.9%	1.3 +L+ seq	201±0.5%	63±1.7%	3.2 +L+ cflow	228±1.6%	42±1.8%	5.4 +L+ readGlobal	2762±4.3%	1±0.2%	2762 +L+ read	9729±4.9%	1±0.6%	9729 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Speed of each language construct used to +L+ interpret the base program compared to a native +L+ execution. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> the base program. For example, to study the performance +L+ of readGlobal, we wrote an aspect whose action returns the +L+ value of the global variable referred in the pointcut, i.e., we +L+ wrote aspects behaving like the base program. For each of +L+ these aspects, we compare the time required to perform the +L+ operation matching the pointcut, in case the operation is +L+ interpreted by the woven aspect with the time required to +L+ carry out the operation natively (without the woven aspect). +L+ For example, to study the performance of readGlobal, we +L+ first evaluate the time needed to retrieve the global variable +L+ value through the code generated by the C compiler gcc +L+ without any aspect woven and compare this value to the +L+ time needed to retrieve the global variable value through +L+ the aspect once it has been woven in the base program. +L+ We express our measurements as a ratio between these two +L+ durations to abstract from the experimentation platform. +L+ This approach requires the ability to measure short peri- +L+ ods of time. For instance, a global variable value is usually +L+ retrieved (readGlobal in our aspect language) in a single +L+ clock tick. Since standard time measurement APIs were +L+ not precise enough, our benchmarking infrastructure relies +L+ on the rdtsc assembly instruction [18]. This instruction re- +L+ turns the number of clock cycles elapsed since power up. The +L+ Pentium 4 processor has the ability to dynamically reorder +L+ the instructions it executes. To ensure the validity of our +L+ measurement, we thus insert mfence instructions in the gen- +L+ erated code whose execution speed is being measured. An +L+ mfence forces the preceding instructions to be fully executed +L+ before going on. The pipeline mechanism in the Pentium 4 +L+ processor entails that the speed of a piece of assembly code +L+ depends from the preceding instructions. To avoid such hid- +L+ den dependencies, we place the operation whose execution +L+ time is being measured in a loop. We use gcc to unroll the +L+ loop at compile time and we measure the time to execute +L+ the complete loop. This measure divided by the number of +L+ loop repetitions yields an estimation of the time required +L+ to execute the operation. The number of times the loop is +L+ executed is chosen after the relative variations of the mea- +L+ sures ,i.e., we increased the number of repetitions until ten +L+ runs yields an average relative variation not exceeding 5%. +L+ To check the correctness of our experimental protocol, we +L+ measured the time needed to execute a nop assembly in- +L+ struction, that requires one processor cycle according to the +L+ Intel specification. The measures of nop presented a relative +L+ variation of 1.6%. +L+ Table 1 summarizes our experimental results. Using the +L+ aspect language to replace a function that returns immedi- +L+ ately is only 1.3 times slower than a direct, aspect-less, call +L+ to that empty function. Since the aspect compiler packages +L+ advices as regular C functions, and because a call pointcut +L+ involves no residue, this good result is not surprising. When +L+ </SectLabel_bodyText> <SectLabel_page> 35 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 11: controlflow, seq, and read performances +L+ </SectLabel_figureCaption> <SectLabel_bodyText> an access to a global variable is replaced by an advice exe- +L+ cution, the hooks generated by the rewriting strategy need +L+ to prepare the processor to call the advice function. This +L+ increases the time spent in the hooks. In addition, while +L+ an access to a global variable is often performed by a sin- +L+ gle x86 instruction, an empty function is often composed +L+ of four instructions. Hence the relative cost of an aspect +L+ triggered upon a global variable access and a direct, aspect- +L+ less, access to a global variable is slightly higher than the +L+ corresponding ratio for functions. A seq of three invoca- +L+ tions of empty functions is only 3.2 time slower than the +L+ direct, aspect-less, three successive functions calls. Com- +L+ pared to the pointcuts used to delimit the different stages, +L+ the seq overhead is limited to a few pointer exchanges be- +L+ tween the linked lists holding the bound variable. On Intel +L+ x86, global variable accesses benefit from excellent hardware +L+ support. In the absence of aspects, a direct global variable +L+ read is usually carried out in a single unique cycle. To trig- +L+ ger the advice execution, the Arachne runtime has to save +L+ and restore the processor state to ensure the execution co- +L+ herency, as advices are packaged as regular C functions (see +L+ also 4.2.1). It is therefore not surprising that a global vari- +L+ able readGlobal appears as being 2762 times slower than +L+ a direct, aspect-less global variable read. read performance +L+ can be accounted in the same way: in the absence of aspect, +L+ local variables are accessed in a single unique cycle. The +L+ signal mechanism used in the read requires that the oper- +L+ ating system detects the base program attempt to read into +L+ a protected memory page before locating and triggering the +L+ signal handler set up by Arachne, as shown in 4.2.3. Such +L+ switches to and from kernel space remain slow. Using read +L+ to read a local variable is 9729 times slower than retrieving +L+ the local variable value directly, without aspects. +L+ seq and controlf low can refer to several points in the exe- +L+ cution of the base program (i.e. different stages for seq and +L+ different function invocations for the controlflow). The +L+ runtime of these pointcuts grows linearly with the number +L+ of execution points they refer to and with the number of +L+ matching instances. Figure 11 summarizes a few experimen- +L+ tal results for controlf low and seq proving these points. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Case Study on a real application +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Since, depending on the aspect construct used, interpret- +L+ ing the base program with aspects can slow it down by a fac- +L+ tor ranging between 1.3 and 9729, we studied Arachne’s per- +L+ formance on a real world application, the Web cache Squid. +L+ </SectLabel_bodyText> <SectLabel_table> 	Arachne	Manual	Diff +L+ 			(%) +L+ 	Top1	Top1 +L+ 	Top2	Top2 +L+ Throughput	5.59	5.59 +L+ (request/s)	5.58	5.59 +L+ Response Time (ms)	1131.42	1146.07	1.2–-1 +L+ 	1085.31	1074.55 +L+ Miss response time (ms)	2533.50	2539.52	0.2– 1.8 +L+ 	2528.35	2525.34 +L+ Hit response time (ms)	28.96	28.76	-0.6 – 3.8 +L+ 	30.62	31.84 +L+ Hit ratio	59.76	59.35	-0.6 – 0.7 +L+ 	61.77	62.22 +L+ Errors	0.51	0.50	-1.9–0 +L+ 	0.34	0.34 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Performances comparison between manual +L+ modification and Arachne, for prefechting policy in- +L+ tegration in Squid +L+ </SectLabel_tableCaption> <SectLabel_bodyText> We extended Squid with a prefetching policy [9]. As de- +L+ scribed in section 3.1, we implemented this policy as a set +L+ of aspects and made a second implementation of this policy +L+ by editing the Squid source code and recompiling it. This +L+ section compares the performance of these two implemen- +L+ tations using standard Web cache performance indicators: +L+ throughput, response time and hit ratio. +L+ Obtaining access traces adequate to study a Web cache +L+ performance is difficult. The trace must be long enough to +L+ fill the cache. Due to privacy issues, traces are usually not +L+ publicly available. Since traces do not include the content of +L+ the accessed pages, these pages must be downloaded again. +L+ In the meantime the page contents may have changed and +L+ even the URLs may have disappeared. +L+ Instead of traces, we based our evaluation on Web Poly- +L+ graph [30]. Polygraph is a benchmarking tool developed by +L+ the Squid team and featuring a realistic HTTP and SSL +L+ traffic generator and a flexible content simulator. +L+ We filled up the cache and simulated a one day workload +L+ with its two request rate peaks observed in real life environ- +L+ ments [30]. Table 2 shows results of our simulation. Mea- +L+ sures have been made during the two request peaks. The +L+ hit time and the miss time, time needed to deliver a docu- +L+ ment present, respectively not present, in the cache are very +L+ similar. It shows that differences are imperceptible between +L+ the version of Squid extended by Arachne and the one ex- +L+ tended manually (less than 1%). Hence, even if the cost +L+ of Arachne’s aspect language constructs might seem high, +L+ they are largely amortized in real applications. To give a +L+ typical example observed on our experimental platform: in +L+ case of a cache hit, a 3.8 MB page was retrieved in a single +L+ second, the time spent in prefetching advices amounted to +L+ 1801 µsec, and the time spent within Arachne to execute the +L+ hooks and dynamic tests to 0.45 µsec. In a miss case, on +L+ the average, a client retrieved the same page in 1.3 seconds, +L+ 16679 µsec were spent in the advices and 0.67 µsec within +L+ Arachne itself. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our work is directly related to other aspect weavers for +L+ C, approaches for expressive aspect languages, and dynamic +L+ weaving, in particular for C. In this section, we consider +L+ related work in each of these fields in turn. +L+ Apart from µDyner and Arachne, there are few aspect +L+ </SectLabel_bodyText> <SectLabel_figure> Sequence +L+ 1	2	3	4	5 +L+ 1	2	3	4	5 +L+ Number of imbricated calls	Number of matching instances +L+ Controlflow +L+ 10 +L+ 3000 +L+ 2000 +L+ 30 +L+ 20 +L+ 5 +L+ 1000 +L+ 10 +L+ </SectLabel_figure> <SectLabel_page> 36 +L+ </SectLabel_page> <SectLabel_bodyText> weavers for C (or even C like languages); some notewor- +L+ thy exceptions are AspectC [12] (no available implementa- +L+ tion), AspectC++ and [33]. All of these rely on source-code +L+ transformation and thus cannot apply aspects to running +L+ C applications as required by the applications we consider. +L+ Furthermore, none of these systems provides explicit sup- +L+ port for aspects over join point sequences. +L+ There is quite a large body of work now on the notion of +L+ expressive aspect languages where “more expressive” typi- +L+ cally compares to w.r.t. AspectJ’s pointcut and advice mod- +L+ els. Our work has been inspired by Event-based AOP [15], +L+ which aims at the definition of pointcuts in terms of arbi- +L+ trary relations between events. Nevertheless, many other +L+ approaches to expressive aspect languages exist: e.g., data- +L+ flow relations [26], logic programming [13], process algebras +L+ [3], graphs [5], and temporal logics [1], have all been pro- +L+ posed as a basis for the definition of expressive aspect lan- +L+ guages. However, few of these encompass dynamic weaving +L+ and only the latter has been applied to C code under effi- +L+ ciency considerations similar to our setting. +L+ Dynamic weaving is commonly realized in Java through +L+ preprocessing at load-time like [8] or through the JVM De- +L+ bugging Interface [28]. These tools rely on bytecode rewrit- +L+ ing techniques, have typically limited expressivity (some do +L+ not support field accesses) and incur a huge performance +L+ overhead. Dynamic weaving through modification at run- +L+ time is found infrequently for compiled languages. An ex- +L+ ception for Java is JasCo [21] whose most recent version (0.7) +L+ supports dynamic weaving through the new instrumentation +L+ API of Java 5. +L+ Many instrumentation techniques have been proposed to +L+ rewrite binary code on the fly. In these approaches, dif- +L+ ficulty issues range from the complexity to rewrite binary +L+ code to the lack of a well-defined relationship between source +L+ code and the compiler generated binary code. Hence many +L+ approaches work on an intermediate representation of the +L+ binary code and source language [34]. Producing this repre- +L+ sentation first and then regenerating the appropriate binary +L+ executable code has proven to be costly both in terms of +L+ memory consumption and in CPU time. +L+ A few other approaches have considered a direct rewrit- +L+ ing of the binary code at runtime. Dyninst [17] and dynamic +L+ probes [27] allow programmers to modify any binary instruc- +L+ tion belonging to an executable. Dyninst however relies on +L+ the Unix debugging API: ptrace. ptrace allows a third +L+ party process to read and write the base program memory. +L+ It is however highly inefficient: before using ptrace, the +L+ third party process has to suspend the execution of the base +L+ program and resume its execution afterwards. In compari- +L+ son, Arachne uses ptrace at most once, to inject its kernel +L+ DLL into the base program process. In addition, Dyninst +L+ does not free the programmer from dealing with low level +L+ details. For example, it seems difficult to trigger an advice +L+ execution upon a variable access with Dyninst: the transla- +L+ tion from the variable identifier to an effective address is left +L+ to the user. Worse, Dyninst does not grant that the manip- +L+ ulation of the binary instructions it performs will succeed. +L+ Dyninst uses an instrumentation strategy where several ad- +L+ jacent instructions are relocated. This is unsafe as one of +L+ the relocated instructions can be the target of branching +L+ instructions. In comparison, Arachne join point model has +L+ been carefully chosen to avoid these kind of issues; if an as- +L+ pect can be compiled with Arachne, it can always be woven. +L+ 7. CONCLUSION AND FUTURE WORK +L+ In this paper we have discussed three different crosscut- +L+ ting concerns which are typical for C applications using OS- +L+ level services and which frequently need to be applied at +L+ runtime. We have motivated that such concerns can be ex- +L+ pressed as aspects and have defined a suitable aspect lan- +L+ guage. This language is more expressive than those used in +L+ other aspect weavers for C in that it provides support for +L+ aspects defined over sequences of execution points as well as +L+ for variable aliases. We have presented an integration of this +L+ language into Arachne, a weaver for runtime weaving of as- +L+ pects in C applications. Finally, we have provided evidence +L+ that the integration is efficient enough to apply such aspects +L+ dynamically to high-performance applications, in particular +L+ the web cache “squid.” +L+ As future work, we intend to investigate the suitability of +L+ the proposed aspect language for other C-applications. We +L+ also intend to investigate Arachne extension to the C++ +L+ language. Indeed, object-oriented programming heavily uses +L+ protocol-based interfaces collaboration (hence sequence as- +L+ pects). Along with its open architecture, extending Arachne +L+ to support C++, will pave the way to a relatively language +L+ independent aspect and weaving infrastructure. Finally, +L+ Arachne’s toolbox should be extended with support for as- +L+ pect interactions (e.g., analyses and composition operators). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] R. A. ºAberg, J. L. Lawall, M. SÄudholt, G. Muller, and +L+ A.-F. L. Meur. On the automatic evolution of an os +L+ kernel using temporal logic and AOP. In Proceedings +L+ of Automated Software Engineering (ASE’03), pages +L+ 196{204. IEEE, 2003. +L+ [2] American National Standards Institute. +L+ ANSI/ISO/IEC 9899-1999: Programming Languages +L+ — C. American National Standards Institute, 1430 +L+ Broadway, New York, NY 10018, USA, 1999. +L+ [3] J. H. Andrews. Process-algebraic foundations of +L+ aspect-oriented programming. In Proceedings of the +L+ 3rd International Conference on Metalevel +L+ Architectures and Separation of Crosscutting +L+ Concerns, volume 2192 of LNCS. Springer Verlag, +L+ Sept. 2001. +L+ [4] M. Arlitt and T. Jin. A workload characterization +L+ study of the 1998 world cup web site. IEEE Network, +L+ 14(3):30{37, May 2000. +L+ [5] U. ABmann and A. Ludwig. Aspect weaving by graph +L+ rewriting. In U. W. Eisenecker and K. Czarnecki, +L+ editors, Generative Component-based Software +L+ Engineering (GCSE), Erfurt, Oct. 1999. +L+ [6] CERT - Carnegie Mellon University. Vulnerability +L+ note vu#613459, Feb. 2002. published on line: +L+ http://www.kb.cert.org/vuls/id/613459. +L+ [7] H. Chen and P. Mohapatra. Catp: A context-aware +L+ transportation protocol for http. In International +L+ Workshop on New Advances in Web Servers and +L+ Proxy Technologies Held with ICDCS, 2003. +L+ [8] S. Chiba and K. Nakagawa. Josh: An open +L+ AspectJ-like language. In Proceedings of the third +L+ </SectLabel_reference> <SectLabel_page> 37 +L+ </SectLabel_page> <SectLabel_reference> international conference on Aspect-oriented software +L+ development, pages 102–111. ACM Press, Mar. 2004. +L+ [9] K.-I. Chinen and S. Yamaguchi. An interactive +L+ prefetching proxy server for improvement of WWW +L+ latency. In Seventh Annual Conference of the Internet +L+ Society (INET’97), Kuala Lumpur, June 1997. +L+ [10] I. Cidon, A. Gupta, R. Rom, and C. Schuba. Hybrid +L+ tcp-udp transport for web traffic. In Proceedings of the +L+ 18th IEEE International Performance, Computing, +L+ and Communications Conference (IPCCC’99), pages +L+ 177–184, Feb. 1990. +L+ [11] S. Clowes. Injectso: Modifying and spying on running +L+ processes under linux. In Black hat briefings, 2001. +L+ [12] Y. Coady, G. Kiczales, M. Feeley, and G. Smolyn. +L+ Using AspectC to improve the modularity of +L+ Path-Specific customization in operating system code. +L+ In V. Gruhn, editor, Proc. of the Joint 8th European +L+ Software Engeneering Conference and 9th ACM +L+ SIGSOFT Symposium on the Foundation of Software +L+ Engeneering (ESEC/FSE-01), volume 26, 5 of +L+ SOFTWARE ENGINEERING NOTES, pages 88–98, +L+ New York, Sept. 10–14 2001. ACM Press. +L+ [13] K. de Volder. Aspect-oriented logic meta +L+ programming. In P. Cointe, editor, Meta-Level +L+ Architectures and Reflection, 2nd International +L+ Conference on Reflection, volume 1616 of LNCS, +L+ pages 250–272. Springer Verlag, 1999. +L+ [14] R. Douence, P. Fradet, and M. SÄudholt. A framework +L+ for the detection and resolution of aspect interactions. +L+ In Proceedings of the ACM SIGPLAN/SIGSOFT +L+ Conference on Generative Programming and +L+ Component Engineering (GPCE’02), volume 2487 of +L+ LLNCS, pages 173–188. Springer-Verlag, Oct. 2002. +L+ [15] R. Douence, O. Motelet, and M. SÄudholt. A formal +L+ definition of crosscuts. In Proceedings of the 3rd +L+ International Conference on Metalevel Architectures +L+ and Separation of Crosscutting Concerns, volume 2192 +L+ of LNCS, pages 170–186. Springer Verlag, Sept. 2001. +L+ [16] E. Hilsdale and J. Hugunin. Advice weaving in +L+ aspectj. In Proceedings of the 3rd international +L+ conference on Aspect-oriented software development, +L+ pages 26–35. ACM Press, 2004. +L+ [17] J. K. Hollingsworth, B. P. Miller, M. J. R. Goncalves, +L+ O. Naim, Z. Xu, and L. Zheng. MDL: A language and +L+ compiler for dynamic program instrumentation. In +L+ IEEE Conference on Parallel Architectures and +L+ Compilation Techniques (PACT), pages 201–213, Nov. +L+ 1997. +L+ [18] Intel Corportation. IA-32 Intel Architecture Software +L+ Developer’s Manual. Intel Corportation, 2001. +L+ [19] V. Issarny, M. Ban^atre, B. Charpiot, and J.-M. +L+ Menaud. Quality of service and electronic newspaper: +L+ The Etel solution. Lecture Notes in Computer Science, +L+ 1752:472–496, 2000. +L+ [20] J. Jaffar, S. Michaylov, P. J. Stuckey, and R. H. C. +L+ Yap. The clp( r ) language and system. ACM Trans. +L+ Program. Lang. Syst., 14(3):339–395, 1992. +L+ [21] JasCo home page. http://ssel.vub.ac.be/jasco/. +L+ [22] R. Jones and P. Kelly. Backwards-compatible bounds +L+ checking for arrays and pointers in c programs. In +L+ M. Kamkar, editor, Proceedings of the Third +L+ International Workshop on Automatic Debugging, +L+ volume 2, pages 13–26, May 1997. +L+ [23] A. D. Keromytis. ”Patch on Demand” Saves Even +L+ More Time? IEEE Computer, 37(8):94–96, 2004. +L+ [24] G. Kiczales, J. Lamping, A. Menhdhekar, C. Maeda, +L+ C. Lopes, J.-M. Loingtier, and J. Irwin. +L+ Aspect-oriented programming. In M. Ak»sit and +L+ S. Matsuoka, editors, Proceedings European +L+ Conference on Object-Oriented Programming, volume +L+ 1241, pages 220–242. JyvÄaskylÄa, Finland, June 1997. +L+ [25] K. J. Lieberherr, J. Palm, and R. Sundaram. +L+ Expressiveness and complexity of crosscut languages. +L+ Technical Report NU-CCIS-04-10, Northeastern +L+ University, Sept. 2004. +L+ [26] H. Masuhara and K. Kawauchi. Dataflow pointcut in +L+ aspect-oriented programming. In First Asian +L+ Symposium on Programming Languages and Systems +L+ (APLAS’03), 2003. +L+ [27] R. J. Moore. Dynamic probes and generalised kernel +L+ hooks interface for Linux. In USENIX, editor, +L+ Proceedings of the 4th Annual Linux Showcase and +L+ Conference, Atlanta, October 10–14, 2000, Atlanta, +L+ Georgia, USA, Berkeley, CA, USA, 2000. USENIX. +L+ [28] A. Popovici, G. Alonso, and T. Gross. Just-in-time +L+ aspects: efficient dynamic weaving for Java. In +L+ Proceedings of the 2nd international conference on +L+ Aspect-oriented software development, pages 100–109, +L+ Boston, Massachusetts, Mar. 2003. ACM Press. +L+ [29] M. Rabinovich and H. Wang. DHTTP: An efficient +L+ and cache-friendly transfer protocol for web traffic. In +L+ INFOCOM, pages 1597–1606, 2001. +L+ [30] A. Rousskov and D. Wessels. High-performance +L+ benchmarking with Web Polygraph. Software Practice +L+ and Experience, 34(2):187–211, Feb. 2004. +L+ [31] O. Ruwase and M. S. Lam. A practical dynamic buffer +L+ overflow detector. In Proceedings of the 11th Annual +L+ Network and Distributed System Security Symposium. +L+ Internet Society, Feb. 2004. +L+ [32] M. S¶egura-Devillechaise, J.-M. Menaud, G. Muller, +L+ and J. Lawall. Web cache prefetching as an aspect: +L+ Towards a dynamic-weaving based solution. In +L+ Proceedings of the 2nd international conference on +L+ Aspect-oriented software development, pages 110–119, +L+ Boston, MA, USA, Mar. 2003. ACM Press. +L+ [33] O. Spinczyk, A. Gal, and W. Schroeder-Preikschat. +L+ AspectC++: an aspect-oriented extension to the C++ +L+ programming language. In Proceedings of the Fortieth +L+ International Conference on Tools Pacific, pages +L+ 53–60. Australian Computer Society, Inc., 2002. +L+ [34] A. Srivastava and A. Edwards. Vulcan: Binary +L+ transformation in a distributed environment. Microsoft +L+ Research Tech. Rpt. MSR-TR-2001-50, 2001. +L+ [35] U. S. L. System Unix. System V Application Binary +L+ Interface Intel 386 Architecture Processor Supplement. +L+ Prentice Hall Trade, 1994. +L+ [36] D. Wessels. Squid: The Definitive Guide. O’Reilly and +L+ Associates, Jan. 2004. +L+ [37] J. Wilander and M. Kamkar. A comparison of publicly +L+ available tools for dynamic buffer overflow prevention. +L+ In Proceedings of the 10th Network and Distributed +L+ System Security Symposium, pages 149–162, San +L+ Diego, California, February 2003. +L+ </SectLabel_reference> <SectLabel_page> 38 +L+ </SectLabel_page>
<SectLabel_title> An Intensional Approach to the Specification of Test Cases +L+ for Database Applications +L+ </SectLabel_title> <SectLabel_author> David Willmor +L+ </SectLabel_author> <SectLabel_affiliation> School of Computer Science +L+ University of Manchester +L+ </SectLabel_affiliation> <SectLabel_address> Oxford Road, Manchester, UK +L+ </SectLabel_address> <SectLabel_email> d.willmor@cs.manchester.ac.uk +L+ ABSTRACT +L+ </SectLabel_email> <SectLabel_bodyText> When testing database applications, in addition to creating +L+ in-memory fixtures it is also necessary to create an initial +L+ database state that is appropriate for each test case. Cur- +L+ rent approaches either require exact database states to be +L+ specified in advance, or else generate a single initial state +L+ (under guidance from the user) that is intended to be suit- +L+ able for execution of all test cases. The first method allows +L+ large test suites to be executed in batch, but requires con- +L+ siderable programmer effort to create the test cases (and +L+ to maintain them). The second method requires less pro- +L+ grammer effort, but increases the likelihood that test cases +L+ will fail in non-fault situations, due to unexpected changes +L+ to the content of the database. In this paper, we propose a +L+ new approach in which the database states required for test- +L+ ing are specified intensionally, as constrained queries, that +L+ can be used to prepare the database for testing automati- +L+ cally. This technique overcomes the limitations of the other +L+ approaches, and does not appear to impose significant per- +L+ formance overheads. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> D.2.5 [Software Engineering]: Testing and Debugging— +L+ Testing tools +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Experimentation, Verification +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> databases, software testing, database testing +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Modern information systems are typically organised as +L+ collections of independent application programs that com- +L+ municate with one another by means of a central database. +L+ The database records the state of the organisation that the +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> ICSE’06, May 20–28, 2006, Shanghai, China. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2006 ACM 1-59593-085-X/06/0005 ...$5.00. +L+ </SectLabel_copyright> <SectLabel_author> Suzanne M. Embury +L+ </SectLabel_author> <SectLabel_affiliation> School of Computer Science +L+ University of Manchester +L+ </SectLabel_affiliation> <SectLabel_address> Oxford Road, Manchester, UK +L+ </SectLabel_address> <SectLabel_email> s.m.embury@cs.manchester.ac.uk +L+ </SectLabel_email> <SectLabel_bodyText> information system supports, while the application programs +L+ implement the business processes that manipulate the state. +L+ To take a simple but ubiquitous example, a database sys- +L+ tem might record details of customers, products and sales, +L+ while the application programs associated with it handle op- +L+ erations such as new product purchases and update of the +L+ product catalogue, as well as supporting decision making +L+ by generating reports regarding the most profitable product +L+ lines, names and addresses of loss-making customers, etc. +L+ In order to test such application programs, it is necessary +L+ to create test fixtures that simulate the presence of the rest +L+ of the information system. Fixtures for traditional test cases +L+ typically consist of in-memory objects and data structures +L+ that provide the inputs to the program being tested. This +L+ kind of fixture is also needed when testing database appli- +L+ cations (especially when performing unit testing); however, +L+ since it is unrealistic (and often incorrect) to execute test +L+ cases against an empty database, we need to create addi- +L+ tional fixture elements within the database itself. +L+ Current practice in the software industry is to maintain +L+ one or more test databases that can be used for testing in- +L+ dividual programs. These databases can be artificially gen- +L+ erated (e.g., using tools such as DBMonster1 and DataFac- +L+ tory2) or they may be subsets of the live database, taken +L+ as a snapshot at some recent point in time. Copies of the +L+ live data sets have the advantage that they are more likely +L+ to be representative of the patterns of data encountered in +L+ practice, while artificial data sets have the advantage that +L+ they can be made to embody specific characteristics (such +L+ as particular data skew patterns or volumes), which may be +L+ useful for load and stress testing. +L+ Both approaches, however, suffer from several disadvan- +L+ tages. The most significant problem occurs when none of +L+ the available test databases are suitable starting points for a +L+ particular test case. For example, suppose a particular test +L+ case executes a program which purges inactive customers, +L+ with the aim of verifying that the business rule forbidding +L+ deletion of customers with negative balances is correctly en- +L+ forced. If none of the test databases contains any inactive +L+ customers with negative balances, then the test case can- +L+ not be executed successfully. For a one-off test run, testing +L+ personnel can choose a database that is close to what is re- +L+ quired, and manually update it so that it is suitable for use +L+ with the test case. But if a complete test suite is to be exe- +L+ cuted (possibly including test cases which themselves make +L+ modifications to the database state) then in the worst case +L+ </SectLabel_bodyText> <SectLabel_footnote> 1http://DBMonster.kernelpanic.pl +L+ 2http://www.quest.com/datafactory +L+ </SectLabel_footnote> <SectLabel_page> 102 +L+ </SectLabel_page> <SectLabel_bodyText> this manual intervention will be required in between every +L+ test case execution. This is clearly undesirable if test suites +L+ are large or time-consuming to execute, or if the test suite +L+ is to be run in batch (as in the case of overnight regression +L+ testing, for example). +L+ Current research in testing for database systems proposes +L+ two approaches to this problem. One of these is to include +L+ within the test case description a full (extensional) specifica- +L+ tion of the database state against which it is to be run (and +L+ of the database state that should be produced if the test has +L+ executed successfully) [13, 14]. This solution is exemplified +L+ by DBUnit3, an extension of the JUnit testing framework4 +L+ that is designed for testing database applications written in +L+ Java. Each DBUnit test case is accompanied by an XML +L+ file describing the data set required for the test. Before each +L+ test run, DBUnit clears the database state and inserts the +L+ data described by the XML file. +L+ This approach has the advantage of simplicity, but it places +L+ a considerable burden on testing personnel, especially when +L+ complex database states are required. It is also inefficient, +L+ since the database must be continually destroyed and recre- +L+ ated between tests, even when significant parts of the database +L+ might have been reused by the succeeding tests. Moreover, +L+ maintenance of a large suite of such tests is extremely chal- +L+ lenging, since any small change to the database schema may +L+ require corresponding changes to many test cases. +L+ The second approach that has been explored in the liter- +L+ ature is more efficient in that it requires the creation of only +L+ one database state per test suite (rather than one per test +L+ case). It is exemplified by the AGENDA database testing +L+ toolkit [6, 7], which can automatically generate a database +L+ state given information about the schema, some data gen- +L+ eration functions for individual attributes and some user- +L+ selected heuristics describing the kind of database state re- +L+ quired. The AGENDA tool also generates test cases from a +L+ simple analysis of the program being verified. The user must +L+ then add preconditions to each test case that are checked +L+ just before it is executed and that will prevent a case from +L+ being executed against an inappropriate database state. This +L+ approach successfully relieves the user of the need to specify +L+ complete database states in full detail, but at a cost. The +L+ user must accept that some of the test cases may not be +L+ executed because the database state fails the precondition, +L+ even when it would require only a small change to bring the +L+ database into a suitable state for the test. Since only one +L+ database state is created per test suite, this problem of failed +L+ tests is likely to become more severe as the size of the test +L+ suite grows. There is also a potential inefficiency involved +L+ in generating test descriptions and inputs, and in creating +L+ the additional log tables and constraints/triggers needed by +L+ the AGENDA tool, for test cases that are not in fact going +L+ to be executed. +L+ Ideally, we would prefer to be able to combine the advan- +L+ tages of both these approaches, to give a form of database +L+ test case that is quick and natural to specify, and which +L+ maximises the number of cases within the suite that can be +L+ executed while minimising the number of full test databases +L+ that need to be maintained. Our thesis is that this can +L+ be achieved by allowing testing personnel to describe the +L+ database states involved in their test cases intensionally, in +L+ </SectLabel_bodyText> <SectLabel_footnote> 3http://www.dbunit.org +L+ 4http://www.junit.org +L+ </SectLabel_footnote> <SectLabel_bodyText> the form of declarative conditions that the input database +L+ must satisfy, and by providing a testing harness that can +L+ automatically adjust the input database so that the test +L+ conditions are satisfied [19]. +L+ In this paper, we present a language for specifying such +L+ intensional database tests, and describe its semantics and +L+ operational behaviour (Section 2). We present an algorithm +L+ for automatically modifying database states so that test pre- +L+ conditions are satisfied (Section 3), thus ensuring that all +L+ test cases can be executed without requiring any human +L+ intervention. We further describe how we have extended the +L+ JUnit testing framework to allow intensional database tests +L+ to be specified and executed in practice (Section 4). Finally, +L+ we present the results of an evaluation of the performance +L+ of the techniques (Section 5) and conclude (Section 6). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. SPECIFYING INTENSIONAL TESTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A conventional test case is typically modelled as a triple +L+ < p, i, o >, which denotes a test that executes program p +L+ with inputs (e.g., parameters) denoted by i. If no faults are +L+ encountered during the test execution, the output that will +L+ be produced is o. In the case of test cases for database ap- +L+ plications, we must add two further elements—the specifica- +L+ tion of the database state against which p is to be executed, +L+ and some statement of the database state that should result +L+ from the execution of p if it is operating correctly according +L+ to its specification. +L+ For example, consider the example program mentioned +L+ in Section 1 that prunes inactive customer details from the +L+ database. For this test case, we require a database state that +L+ contains at least one inactive customer. This could easily +L+ be stated as a predicate logic condition over the database, +L+ assuming the obvious mapping between stored relations and +L+ predicates, e.g.: +L+ </SectLabel_bodyText> <SectLabel_construct> (3custNo, lastOrderOn, a, b, c) +L+ customer(custNo, a, b, c, lastOrderOn) n +L+ lastOrderOn < today — 90 +L+ </SectLabel_construct> <SectLabel_bodyText> The program in question does not access any parts of the +L+ database other than the customer table. Therefore, we do +L+ not care what values the other tables contain and need not +L+ mention them in the intensional specification of the test. +L+ This approach works equally well for observing the results +L+ of the test. For example, when testing the customer pruning +L+ behaviour, we might require that no inactive customer with +L+ a non-negative balance should exist in the database after +L+ the test: +L+ </SectLabel_bodyText> <SectLabel_construct> -((3custNum, lastOrderDate, a, b, c) +L+ customer(custNum, a, bal, c, lastOrderDate) n +L+ lastOrderDate < today — 90 n bal > 0) +L+ </SectLabel_construct> <SectLabel_bodyText> Effectively, the test case describes a set of valid (i.e., fault- +L+ free) state transition for the database, as a classic pre/post- +L+ condition pair. +L+ This first-order-logic style of database specification does +L+ not work so well when we consider the testing problem in +L+ more depth, however. The problem is that we need to do +L+ more than test the input database for compliance with the +L+ requirements of the test case; we also need to extract in- +L+ formation from it to be used to instantiate other elements +L+ </SectLabel_bodyText> <SectLabel_page> 103 +L+ </SectLabel_page> <SectLabel_bodyText> of the test case. For example, suppose we wish to test a +L+ program that deletes details of individual customers. Such +L+ programs typically require some input from the user, identi- +L+ fying the specific customer record that is to be deleted (e.g., +L+ by supplying the relevant customer code as a parameter). +L+ This could be achieved by requiring the tester to embed the +L+ customer code into the test case elements, as literal values. +L+ Alternatively, we could search for a suitable customer that +L+ already exists in the database, using a standard database +L+ query, and use the values from that in specifying the inputs +L+ for the test case. This would minimise the amount of work +L+ required to prepare the database for test execution (since we +L+ would be using data already present in the database), and it +L+ would also mean that test cases can be written very quickly, +L+ since the user does not need to specify every last detail of +L+ the data to be used. +L+ Under this approach, the specification of the input database +L+ state now has a dual role: it must state the condition that +L+ determines whether the database state is suitable for execu- +L+ tion of the test case and it must also return bindings for the +L+ free variables that appear in the remaining components of +L+ the test case. For the latter purpose, we would prefer to use +L+ a straightforward query language, while for the former we +L+ require the ability to place conditions on the data. With a +L+ simple extension of a standard query language such as SQL, +L+ we can combine both these purposes in a single statement. +L+ For example, the following statement: +L+ </SectLabel_bodyText> <SectLabel_construct> ANY :cn GENERATED BY +L+ SELECT custNo FROM customer +L+ WHERE lastOrderDate < today() - 90 +L+ AND balance < 0 +L+ </SectLabel_construct> <SectLabel_bodyText> retrieves the customer code of some record that meets the +L+ given conditions (an inactive customer with negative bal- +L+ ance) from the database, and binds it to the variable : cn. +L+ It also places a cardinality constraint on the result of the +L+ query, that at least one such binding must exist (implied by +L+ the use of the keyword ANY). +L+ The variable : cn can then be used to specify other ele- +L+ ments of the test case. The obvious usage in this example is +L+ in specifying the inputs to the program being tested, but it +L+ can also be used in describing the expected outputs of the +L+ program. In this example test case, the correct behaviour +L+ of the DeleteCustomer program is to reject the deletion +L+ of : cn, since customers with a negative balance cannot be +L+ purged from the database. We might therefore give the fol- +L+ lowing specification of the desired output database state: +L+ </SectLabel_bodyText> <SectLabel_construct> AT LEAST 1 :cn2 GENERATED BY +L+ SELECT custNo FROM customer +L+ WHERE custNo = :cn +L+ </SectLabel_construct> <SectLabel_bodyText> Of course, not all test cases are best specified in terms of +L+ values retrieved from the database. For example, suppose +L+ that we wish to write test cases for a program that adds new +L+ customers to the database. The inputs to this program are +L+ the details of the new customer, and the precondition for one +L+ particular test case states that no customer should exist that +L+ has the same customer code as that of the customer being +L+ created. We cannot retrieve the customer details from the +L+ database in this case, as they have not yet been stored in it. +L+ Again, we could force the user to include the required values +L+ as literals in the test case, but ideally we would like to give +L+ </SectLabel_bodyText> <SectLabel_figure> <CONDITION>::= <TYPE> <BINDINGLIST> +L+ GENERATED BY <SELECT> +L+ <TYPE>::= ANY I NO I AT LEAST <i> I +L+ AT MOST <i> EXACTLY <i> | +L+ ALL I FIRST +L+ <i>::= {0-9} +L+ <BINDINGLIST> +L+ ::=<BINDING> { ‘,’ <BINDINGLIST> } +L+ <BINDING>::= {A-Z I a-z} +L+ <SELECT> ::= ... +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 1: Simplified BNF Grammar for SQL Exten- +L+ sions +L+ </SectLabel_figureCaption> <SectLabel_bodyText> more support to the process of test case generation. One +L+ way to achieve this is to allow user-defined data generator +L+ functions to be incorporated within queries as though they +L+ were relations. For example, the following expression states +L+ our requirements for this test case, while also binding the +L+ variables needed for input to the program: +L+ </SectLabel_bodyText> <SectLabel_construct> ANY :cn, :name, :addr, :bal GENERATED BY +L+ SELECT gc.custno, gc.name, gc.addr, 0 +L+ FROM genCustomerDetails() AS gc +L+ WHERE gc.custno NOT IN ( +L+ SELECT custno +L+ FROM customer +L+ WHERE balance > 0) +L+ </SectLabel_construct> <SectLabel_bodyText> Here, the data generator function getCustomerDetails ( ) +L+ is used as if it were a normal relation, whereas in fact the +L+ results it returns are computed on the fly. In fact, several +L+ of the main commercial database management systems al- +L+ ready allow user-defined functions to be embedded in queries +L+ in this way, so this does not require a further extension of +L+ SQL. Figure 1 shows the minimal extensions that are needed +L+ to support all the kinds of constrained query shown above +L+ using the SQL99 standard [17]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.1 Test Case Semantics +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Clearly, the semantics of these intensional database test +L+ cases is more complex than for traditional extensional tests. +L+ However, we can define their semantics formally in terms +L+ of a mapping from intensional tests to sets of equivalent +L+ extensional database test cases. We first present a formal +L+ definition of the structure of our intensional test cases: +L+ </SectLabel_bodyText> <SectLabel_construct> DefInItIOn 1. An intensional database test case is a quin- +L+ tuple < p, i, DBi, o, DBo >, where: +L+ </SectLabel_construct> <SectLabel_listItem> •	p is the program to be executed in the test, +L+ •	i is a tuple of n variables and literals that describes the +L+ inputs to be given to program p, where n is the number +L+ of parameters expected by p, +L+ •	DBi is a set of constrained queries that together specify +L+ the initial database state. +L+ •	o is a tuple of m variables and literal that describes the +L+ expected outputs from the program p. +L+ •	DBo is a set of constrained queries that together specify +L+ the conditions that must hold in the database state after +L+ execution of p if no fault has been encountered. +L+ </SectLabel_listItem> <SectLabel_page> 104 +L+ </SectLabel_page> <SectLabel_bodyText> A constrained query has the form < Q, min, max, vars >, +L+ where Q is a standard relational algebra query, min and +L+ max describe the constraints on the cardinality of the query +L+ result set, and vars is the list of variables bound by the +L+ query result. +L+ A database test case is well-formed for use with a partic- +L+ ular database schema Σ iff: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 for every variable v that occurs free in i, DBi, o and +L+ DBo, there exists a query in DBi that provides a bind- +L+ ing for v, +L+ 9 for every query < q, n, m, vs > in DBi U DBo, q is a +L+ well-formed query over Σ that returns k-tuples, where +L+ IvsI = k, and +L+ 9 there are no circular variable dependencies amongst +L+ the queries in DBi. +L+ </SectLabel_listItem> <SectLabel_bodyText> We can now define a semantics for the intensional database +L+ test cases as follows. Every intensional test case is equivalent +L+ to a set of extensional test cases. An extensional test case +L+ defines a specific test run, in terms of actual inputs and +L+ outputs, rather than expressions denoting sets of inputs and +L+ outputs. The set of all possible extensional test cases is +L+ given by: +L+ </SectLabel_bodyText> <SectLabel_equation> PxGnxDBxGxDB +L+ </SectLabel_equation> <SectLabel_bodyText> where P is the set of all programs, G is the set of all lit- +L+ erals, Gn is the set of all n-tuples formed from G and DB +L+ is the set of all database states (relative to all schemas)5. +L+ The components of each extensional test are the program +L+ to be tested, the input values, the initial database state, +L+ the expected output and the expected final database state, +L+ respectively. +L+ An intensional test case is effectively a shorthand expres- +L+ sion for a set of extensional test cases that are all derived +L+ from the same equivalence partition of the test case inputs. +L+ An intensional database test < p, i, DBi, o, DBo >, where +L+ DBi = {< qi, ni, mi, vi >} and DBo = {< qo, no, mo, vo >}, +L+ is equivalent to the following set of extensional tests: +L+ </SectLabel_bodyText> <SectLabel_equation> {< p, i[vi/v], dbi, o[vi/v], dbo > I +L+ dbi E DB n +L+ (ni <_ Iqi(dbi)I <_ mi) n +L+ v E qi(dbi) n +L+ dbo E DB n +L+ (no <_ I (qo [vi /v])(dbo)I <_ mo)} +L+ </SectLabel_equation> <SectLabel_bodyText> We use the notation exp[01/02] to express the substitution of +L+ the values in 01 by the corresponding values in 02 whereever +L+ they occur in exp. Therefore, this expression denotes the set +L+ of extensional tests where the input database satisfies the +L+ constraints imposed by the initial constrained query, and +L+ where the bindings from execution of that query (here ex- +L+ pressed as the tuple of variables v) are substituted into the +L+ </SectLabel_bodyText> <SectLabel_footnote> 5For simplicity of presentation, we assume that all programs +L+ require the same number of inputs (n). In practice, n can +L+ be the largest number of inputs required by any program, +L+ and the unused values can be filled with nulls. +L+ </SectLabel_footnote> <SectLabel_bodyText> expressions defining the inputs, expected output and ex- +L+ pected final database state before they too are evaluated 6. +L+ The idea underlying this notion of an intensional test is +L+ that when any of its corresponding extensional sets are ex- +L+ ecuted, the intensional test is itself deemed to have been +L+ executed. Thus, the use of intensional tests allows much +L+ greater freedom at test execution time, since we may choose +L+ any of the possible extensional tests, depending on which is +L+ closest to our starting environment. In the next section, we +L+ will consider the practical ramifications of this approach to +L+ testing, and describe how the semantics just described can +L+ be implemented in practice. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. DATABASE PREPARATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The execution of an intensional database test case con- +L+ sists of three distinct phases: 1) preparation of the environ- +L+ ment for test execution; 2) execution of the test with the +L+ prepared inputs; and 3) capture and storage of the results, +L+ for later analysis. Since all the work of finding bindings +L+ for the variables in the test case specification is done in the +L+ preparation phase, the final two phases are straightforward +L+ and differ little from standard testing procedures. When +L+ program execution is complete, the constrained query that +L+ determines whether the test has been successful or not is +L+ evaluated against the database, and the output from the +L+ program is checked against what is expected. In the case +L+ of test failure, the details of the actual extensional test that +L+ was executed are recorded, for diagnosis purposes. +L+ The first phase, however, is more complex. If we were +L+ content to execute only those test cases which happen to +L+ be suitable for use with the initial database state, then the +L+ preparation phase would simply be a matter of executing +L+ the input constrained queries against the database and, if +L+ they are all successful, using the bindings thus produced +L+ to instantiate the remaining components of the test case. +L+ However, thanks to the declarative nature of our test case +L+ specifications, the testing framework can be pro-active in +L+ cases where the given database is not suitable for use by +L+ the test case, and can automatically generate a sequence of +L+ updates that will cause the constrained queries to produce +L+ the required number of bindings. +L+ In fact, this problem is similar (though not identical) to +L+ one that has been studied by the database and artificial in- +L+ telligence communities for many years. It is known variously +L+ as the view update problem [9], the knowledge base update +L+ problem [12], and the transaction repair problem [10]. Many +L+ database systems have the capability to define views on top +L+ of the basic database. A view is a kind of virtual relation. +L+ To the user, it appears to be a normal relation, but it con- +L+ tains no stored data. Instead, the contents of the view are +L+ defined by a expression over other relations, and attempts +L+ to retrieve data from the view are converted into queries +L+ over these relations. To take a simple example for illustra- +L+ tion, we might create a view called Debtors which appears +L+ to be a relation of the same name containing all customers +L+ with a negative balance. Attempts to retrieve Debtors is +L+ </SectLabel_bodyText> <SectLabel_footnote> 6For simplicity of presentation, we assume here that there +L+ is only one query in each of DBi and DBo. In practice, +L+ it may be necessary to include several queries, each pro- +L+ ducing different bindings and imposing different cardinality +L+ constraints. In this case, the constraints must be conjoined, +L+ and the full set of bindings can be retrieved by performing +L+ a natural join of all the queries, with join condition true. +L+ </SectLabel_footnote> <SectLabel_page> 105 +L+ </SectLabel_page> <SectLabel_bodyText> converted into a query against the customer table with an +L+ added constraint on the balance. +L+ If views are truly to act as normal relations then it should +L+ be possible to update them as well query them. But what +L+ does it mean to update a virtual relation? In this case, the +L+ view update must be converted into a sequence of updates +L+ on the stored relations that will cause the desired change in +L+ the contents of the view itself. This is a non-trivial problem +L+ for realistic view languages, and becomes even more difficult +L+ when we move into the context of knowledge bases, where +L+ virtual relations can be defined using rules over other rela- +L+ tions, and when we add integrity constraints that must be +L+ maintained by all updates [1, 2, 3, 4, 5, 8, 11]. +L+ Only in very narrow circumstances does a view update +L+ have a single translation into real updates [15, 18]. Various +L+ heuristics for selecting from amongst the possible transla- +L+ tions have been proposed (of which the most common is to +L+ choose the update that results in the smallest change to the +L+ existing data set [2]), but in real applications user input is +L+ needed in order to identify the translation that corresponds +L+ most closely to the real world state that the database should +L+ reflect [10]. +L+ In the case of intensional database tests, we have a query +L+ (the constrained query that describes our requirements for +L+ the test) that does not produce the correct number of an- +L+ swers when executed against the test database. We need to +L+ find a sequence of updates to the base data that will cause +L+ our query to produce the number of answers we need. How- +L+ ever, in this case, there is no requirement to find the set of +L+ updates that matches the state of reality — any sensible up- +L+ date that satisfies the query conditions will be acceptable. +L+ This simplifies the problem considerably, removing the need +L+ for complex search procedures and for any user input. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 The Preparation Algorithm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> One of the advantages of using a query-based language +L+ for test specification (as opposed to a predicate calculus- +L+ based language) is that we can make use of a very common +L+ and easy-to-analyse internal form for (relational) database +L+ queries, called relational algebra. This form provides a small +L+ number of operations on relations that can be combined to +L+ form complex queries. For example, the three most basic +L+ (and useful) relational algebra operators are: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 The projection operator, πAttsR, which creates a re- +L+ lation from R by deleting all attributes not in Atts. +L+ For example, π[Country]Customer produces a relation +L+ that contains just the countries that appear in the +L+ Customer relation. +L+ 9 The selection operator, QcR, which creates a relation +L+ that contains all the rows from relation R that satisfy +L+ the condition c. For example, Qbal<0 Customer returns +L+ a relation containing details of all customers with neg- +L+ ative balances. +L+ 9 The join operator, R ✶c S, which creates a relation +L+ containing rows from the cross product of R and S that +L+ satisfy the join condition c. The query Debtor ✶dNo=WNo +L+ Inactive returns details of all debtors who are also in- +L+ active. +L+ </SectLabel_listItem> <SectLabel_bodyText> Since the result of each relational algebra operator is itself +L+ a relation, together they form a closed algebra. This means +L+ that we can form arbitrarily complex queries by applying +L+ operators to the results of other operators. For example, a +L+ query which retrieves the customer number of all customers +L+ with a negative balance would be written as: +L+ </SectLabel_bodyText> <SectLabel_equation> π[custNo] (Qbalance<0 Customer) +L+ </SectLabel_equation> <SectLabel_bodyText> A common way to visualise such expressions is as a tree of +L+ operators. The tree for the above query is shown in Figure 2. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: Relational Algebra Tree for Negative Bal- +L+ ance Query. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Our algorithm for preparing a database for testing is based +L+ around this notion of a relational algebra tree. We take the +L+ cardinality constraints from the test specification, and push +L+ them down through the nodes of the input database query +L+ tree, collecting up additional conditions as we go. When we +L+ reach a leaf node (i.e. a base relation), we make updates +L+ to the database so that the pushed-down constraints are +L+ satisfied for that relation. +L+ At each stage, we collect up the different kinds of con- +L+ straint and push them further down into the tree. These +L+ constraint types are: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 Min and Max, the upper and lower bounds on the de- +L+ sired cardinality of the result set. +L+ 9 SelC, the selection conditions on the relations that we +L+ are interested in. +L+ 9 UAtts, the collection of attributes that are used in the +L+ constrained query, and that must be populated in any +L+ new data that we insert. +L+ </SectLabel_listItem> <SectLabel_bodyText> We also build up a collection of queries that describe the +L+ data that has been prepared for testing so far, as we progress +L+ through the tree. We call these queries “bindings” (Bgs), +L+ since they give us values for the variables that occur within +L+ the selection and join conditions. At each stage, the bindings +L+ should contain one query for each leaf node that has so far +L+ been prepared. +L+ It is easiest to see how this works by considering a simple +L+ example, such as that shown in Figure 2. Let us assume we +L+ have a constrained query that requires at least one customer +L+ with negative balance to exist, and that our database does +L+ not currently contain any such customers. We begin at the +L+ root node of the tree, with only the cardinality constraints +L+ extracted from the test specification: +L+ </SectLabel_bodyText> <SectLabel_equation> Min = 1, Max = null, SelC = true, +L+ UAtts = 0, Bgs = 0 +L+ </SectLabel_equation> <SectLabel_bodyText> The top node is a projection operator. Projection does not +L+ affect the cardinality of the result set, nor impose any condi- +L+ tions, but it does tell us something about the attributes used +L+ </SectLabel_bodyText> <SectLabel_page> 106 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 3: Relational Algebra Tree Showing Multiple +L+ Joins +L+ </SectLabel_figureCaption> <SectLabel_bodyText> by the query. We therefore add the projection attributes to +L+ UAtts and push the constraints down to the next node: +L+ </SectLabel_bodyText> <SectLabel_equation> Min = 1, Max = null, SelC = true, +L+ UAtts = {custNo}, Bgs = 0 +L+ </SectLabel_equation> <SectLabel_bodyText> Next we must deal with the selection node. Selection nodes +L+ reduce the cardinality of their input, so we need to push +L+ down the selection conditions to ensure that any updates +L+ we may make affect the correct tuples. We also need to add +L+ any attributes appearing in the selection condition to UAtts: +L+ </SectLabel_bodyText> <SectLabel_equation> Min = 1, Max = null, SelC = balance < 0, +L+ UAtts = {custNo, balance}, Bgs = 0 +L+ </SectLabel_equation> <SectLabel_bodyText> The final node is the leaf node, representing the Customer +L+ relation. We construct a query from the conditions on that +L+ relation and execute it, to find out how many answers are +L+ currently in the database. In this case, there are none, so +L+ we need to insert a new Customer record with at least +L+ the custNo and balance attributes populated, and with +L+ a negative balance. If there are any integrity constraints +L+ on this relation, then we need to make sure they are also +L+ satisfied by the new data. +L+ We use the DBMonster data generator mentioned earlier +L+ to create the new data. It allows generation functions to +L+ be specified for attributes, and additional constraints to be +L+ placed on them. It will also maintain primary key, foreign +L+ key, non-null and domain constraints if configured appro- +L+ priately using the information present in the pushed-down +L+ constraints. +L+ Of course, this is a very simple example. In general, we +L+ can expect to have to deal with more complicated queries +L+ involving several joins, such as that shown in Figure 3. This +L+ relational algebra tree is equivalent to the following con- +L+ strained query: +L+ </SectLabel_bodyText> <SectLabel_construct> ANY :orderNo, :productNo GENERATED BY +L+ SELECT o.orderno, p.productno +L+ FROM Order o, Orderdetail d, Product p +L+ WHERE o.orderno = d.orderno AND +L+ d.productno = p.productno AND +L+ p.price > 50 +L+ </SectLabel_construct> <SectLabel_bodyText> which requires that at least one order must exist that in- +L+ volves the purchase of at least one product that costs more +L+ than £50. Joins complicate the process of preparing the +L+ database, because they introduce dependencies between the +L+ updates that take place at different leaf nodes. For example, +L+ imagine that we have processed the tree shown in Figure 3 as +L+ far as the leaf node representing the OrderDetail relation. +L+ Join operators further constrain the selection condition (by +L+ conjoining in their join condition), but add no other con- +L+ straints. So, by the time we reach this leaf node, SelC will +L+ have been set to: +L+ </SectLabel_bodyText> <SectLabel_equation> o.orderno = d.orderno A d.productno = p.productno +L+ </SectLabel_equation> <SectLabel_bodyText> We need to find out whether a suitable OrderDetail record +L+ exists within the database. However, in order to do this, +L+ we need to know something about what preparation actions +L+ were performed when the Product leaf node was processed. +L+ Maybe there were already plenty of £50-plus products in +L+ the catalogue, or maybe there were none and one had to +L+ be created. How is this information passed through to the +L+ OrderDetail node so that the correct tuple can be identi- +L+ fied or created? +L+ In the current version of our algorithm, we have chosen +L+ to use the database itself to communicate these values. If +L+ there are many suitable Product records, then we can find +L+ one by querying the database directly once again. If a new +L+ product had to be created, then it will now be present in +L+ the database, so we can still retrieve it by querying. The +L+ information needed to construct these queries is present in +L+ the selection conditions that have been considered during +L+ the processing of the relational algebra tree up to this point. +L+ For example, in order to search for an OrderDetail tuple +L+ that is connected to a suitable Product, we need to issue +L+ the following query: +L+ </SectLabel_bodyText> <SectLabel_construct> SELECT d.* FROM OrderDetail d, Product p +L+ WHERE d.productno = p.productno AND +L+ p.price > 50 +L+ </SectLabel_construct> <SectLabel_bodyText> This query cannot be constructed from only the constraints +L+ pushed-down from the parent nodes of the leaf node; instead, +L+ we need to collect up the constraints imposed by all nodes +L+ visited before the current node, so that they are available for +L+ query formation. This is done using the Bgs data structure +L+ mentioned earlier. +L+ Figure 4 presents the complete algorithm, showing the be- +L+ haviour required for each different type of operator. The al- +L+ gorithm is presented as a side-effecting function which takes +L+ the constrained query that is to be satisfied by the database, +L+ and a set of initial conditions that state the required cardi- +L+ nality bounds and initialise SelC to true, UAtts to 0 and Bgs +L+ to 0. The function returns a set of bindings, but these are +L+ discarded. The main task of the algorithm is carried out +L+ by the side-effecting updates that occur when leaf nodes are +L+ processed. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. DOT-UNIT TESTING FRAMEWORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The intensional database test language and accompanying +L+ preparation algorithm have been implemented within a test- +L+ ing tool, called DOT- Unit. This tool is part of a larger Data- +L+ Oriented Testing7 framework that is under development at +L+ the University of Manchester [20]. DOT-Unit has been im- +L+ plemented as an extension to the JUnit testing framework +L+ </SectLabel_bodyText> <SectLabel_footnote> 7http://www.cs.man.ac.uk/—willmord/dot/ +L+ </SectLabel_footnote> <SectLabel_page> 107 +L+ </SectLabel_page> <SectLabel_figure> Projection operator +L+ prepare(irattsQ, Min, Max, UAtts, SelC, Bgs) +L+ = prepare(Q, Min, Max, UAtts U Atts, SelC, Bgs) +L+ Selection operator +L+ prepare(acQ, Min, Max, UAtts, SelC, Bgs) +L+ = prepare(Q, Min, Max, UAtts, SelC n c, Bgs) +L+ Join operator +L+ prepare(Q1 ✶jc Q2, Min, Max, UAtts, SelC, Bgs) +L+ = prepare(Q2, Min, Max, UAtts, SelC n jc, +L+ prepare(Q1, Min, Max, UAtts, SelC, Bgs)) +L+ Relation (leaf node) +L+ prepare(Rasv, Min, Max, UAtts, SelC, Bgs) +L+ Q = bindingQuery(v, SelC, Bgs) +L+ Execute Q to produce result set RS +L+ if IRSI < Min then +L+ Invoke DBMonster to create (Min - IRSI) more +L+ instances of R that satisfy the conditions in Q +L+ else if IRSI > Max then +L+ Delete the first (IRSI - Max) tuples in RS +L+ else +L+ No preparation updates needed +L+ return (Bgs U binding(v, Q)) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 4: The Database Preparation Algorithm +L+ </SectLabel_figureCaption> <SectLabel_bodyText> for the unit testing of Java applications [16]. We have sub- +L+ classed the standard JUnit TestCase class, to create a ded- +L+ icated DatabaseTestCase class for specifying and man- +L+ aging intensional database tests. DatabaseTestCase pro- +L+ vides facilities for specifying pre-conditions on database state, +L+ generating and manipulating the bindings that are produced +L+ by such pre-conditions, and evaluating post-conditions on +L+ the database state after the test has been completed. The +L+ standard JUnit methods for determining the results of test +L+ execution on the in-memory fixture can also be used. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5 shows an example DatabaseTestCase that in- +L+ cludes two individual tests. The first verifies that when a +L+ customer with a non-negative balance is deleted, all cus- +L+ tomers with that customer number really do disappear from +L+ the database. The second uses a data generation function to +L+ propose attribute values for a new customer record (includ- +L+ ing a unique customer number), and checks that after the +L+ program has executed only one customer with the generated +L+ customer number exists. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> We use a prefixed colon to indicate variables that are +L+ shared amongst the test components — a notation that will +L+ be familiar to many database programmers, since it is com- +L+ monly used in various forms of embedded SQL. The shared +L+ variables acquire their values when the test harness evalu- +L+ ates the precondition (and performs any necessary database +L+ preparation steps). These values can then be accessed us- +L+ ing the binding method, and can be used in arbitrarily +L+ complex assert conditions, as well as in instantiating the +L+ post-condition query. +L+ One of the main advantages of using the JUnit framework +L+ as the basis for the implementation of DOT-Unit is that it +L+ allows us to integrate our tool seamlessly into existing de- +L+ velopment environments, such as Eclipse8. Thus, DOT-Unit +L+ tests are executed in exactly the same way as a standard JU- +L+ nit test case, and the results are displayed using the same +L+ interface components. This allows testing of database and +L+ non-database components to be interleaved in a convenient +L+ and natural manner. +L+ </SectLabel_bodyText> <SectLabel_footnote> 8http://www.eclipse.org +L+ </SectLabel_footnote> <SectLabel_sectionHeader> 5. EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The practicality of this intensional test case approach de- +L+ pends largely on the performance overhead imposed by the +L+ database preparation algorithm. If the time required to ex- +L+ ecute each individual test case is significantly higher using +L+ our approach than with DBUnit, say, then fewer tests will +L+ be able to be executed in the time available and the benefits +L+ of faster test development and fewer spurious test failures +L+ will be negated. +L+ To gain a handle on the degree of performance overhead +L+ to be expected from DOT-Unit, we made use of an exist- +L+ ing extensional DB test suite that we created for earlier +L+ work [20]. This suite was designed for mp3cd browser9, an +L+ open-source Java/JDBC program that stories information +L+ about mp3 files in a MySQL 5.0 database10. The schema +L+ of the database consists of 6 relations with 22 attributes, 7 +L+ primary key constraints and 6 foreign key constraints. We +L+ created an equivalent intensional test suite, consisting of 20 +L+ test cases, from the extensional suite by converting each test +L+ case into DOT-Unit pre- and post-conditions. We also re- +L+ placed each hard-coded test parameter in the original tests +L+ into constrained query bindings. +L+ We wanted to investigate two specific aspects of the per- +L+ formance of DOT-Unit. First, we wanted to compare its +L+ performance with that of DBUnit over the equivalent test +L+ cases as the database size grows. Second, we wanted to gain +L+ some idea of what aspects of DB preparation and testing +L+ were dominating the performance of DOT-Unit. The re- +L+ sults of the experiments we performed are presented below. +L+ All experiments were run on a Pentium-M 2.0GHz machine, +L+ with 1Gb RAM, running Ubuntu Linux. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.1 Comparison with DBUnit +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> At first sight, the extensional approach, as exemplified +L+ by DBUnit, would seem to be the more efficient method +L+ of the two, as the testing harness does not need to spend +L+ any time figuring out what updates need to be made prior +L+ to each test—it only needs to execute them. This does +L+ </SectLabel_bodyText> <SectLabel_footnote> 9http://mp3cdbrowser.sourceforge.net/mp3cd/ +L+ 10http://www.mysql.com +L+ </SectLabel_footnote> <SectLabel_page> 108 +L+ </SectLabel_page> <SectLabel_figure> public class ProgramTest extends DatabaseTestCase { +L+ public void testDeleteCustomer() { +L+ preCondition("ANY :cn GENERATED BY SELECT custNo FROM customer WHERE balance > 0;"); +L+ Program p = new Program(); +L+ p.deleteCustomer(binding(":cn")); +L+ postCondition("NO :cn2 GENERATED BY SELECT custno FROM customer WHERE custNo = :cn;"); +L+ } +L+ public void testNewCustomer() { +L+ preCondition("ANY :cn, :name, :addr GENERATED BY SELECT gc.custNo, gc.name, gc.addr FROM +L+ genCustomerDetails() AS gc WHERE gc.custNo NOT IN (SELECT custNo FROM customer);"); +L+ Program p = new Program(); +L+ boolean b = p.newCustomer(binding(":cn"), binding(":name"), binding(":addr")); +L+ assertTrue(b); +L+ postCondition("EXACTLY 1 :cn, :name, :addr GENERATED BY SELECT custno, name, addr +L+ FROM customer;"); +L+ } +L+ } +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: Example DOT-Unit Test Case +L+ </SectLabel_figureCaption> <SectLabel_bodyText> not happen by accident, but because a human programmer +L+ has spent time earlier, deciding exactly what the database +L+ should look like for each test case. However, when writing +L+ DBUnit tests, it is common to try to reuse database de- +L+ scriptions for multiple test cases where possible, to reduce +L+ the amount of programming and maintenance time. In this +L+ case, some redundant updates will be made before each test +L+ case - updates that our extensional approach will not bother +L+ to make. It is also the case that DBUnit makes its updates +L+ blindly, whether they are needed or not, whereas the inten- +L+ sional approach will be able to reuse much of the existing +L+ database state for each new test case. +L+ Given this, it seems likely that the performance of DBUnit +L+ will be better when the database state required for each +L+ test case is relatively small, but that the situation will be +L+ reversed when the database state grows much larger. In +L+ order to gauge the point at which this change occurs, we +L+ ran our two test suites (extensional and intensional) with +L+ databases of varying sizes, and measured the execution time +L+ taken to execute the whole test suite. +L+ In each case, we generated initial database states of vary- +L+ ing sizes at random - either populating the database directly +L+ (for the intensional test cases) or generating XML descrip- +L+ tions of the required state (for the extensional test cases). +L+ The results are shown in Figure 6. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 6: Comparison of Approaches as DB Size +L+ Increases +L+ </SectLabel_figureCaption> <SectLabel_bodyText> To our surprise, although the performance of DOT-Unit was +L+ initially worse than that of DBUnit, it overtook its com- +L+ petitor at a comparatively small database size of around 20 +L+ tuples per relation. Obviously, this experiment is a little +L+ unfair to DBUnit, since programmers are unlikely to create +L+ database descriptions consisting of 1000s of tuples per re- +L+ lation. However, tests of this scale will be needed at some +L+ point in the development cycle, in order to verify the be- +L+ haviour of the system on more realistic data sets. +L+ In order to assess the behaviour of DOT-Unit more pre- +L+ cisely, consider the graph in Figure 7, which shows the re- +L+ sults at small databases sizes in more detail. It can be ob- +L+ served that the performance of DOT-Unit first improves and +L+ then begins to degrade again at a database size of around +L+ 50 tuples per relation. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 7: Detailed Comparison of Approaches +L+ </SectLabel_figureCaption> <SectLabel_bodyText> One possible explanation for this initial improvement in per- +L+ formance is that, as the database size rises, so does the +L+ probability that the data needed for the test case is al- +L+ ready present in the database. For the very small states, +L+ a lot of preparation work is required to create the needed +L+ data, whereas less work is needed for a more fully populated +L+ database. As the database size increases further, however, +L+ the costs of making the queries needed to test the precondi- +L+ tions and formulate the preparation updates rises, pushing +L+ up the time required for the entire preparation step. This +L+ </SectLabel_bodyText> <SectLabel_page> 109 +L+ </SectLabel_page> <SectLabel_bodyText> behaviour may be a peculiarity of the particular test suite +L+ used, of course, and further, more extensive studies will be +L+ required in order to completely characterise the performance +L+ of the DOT-Unit test harness. +L+ From these initial results, however, DOT-Unit appears to +L+ scale well relative to database size, and the execution times +L+ are of the same order of magnitude as those resulting from +L+ DBUnit. This suggests that the intensional approach may +L+ provide a good compromise between saving expensive pro- +L+ grammer time in developing new test cases and expenditure +L+ of cheaper processing time in executing the test cases. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Effect of Constraint Complexity +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A further concern was the effect of increasing constraint +L+ complexity on the performance of DOT-Unit test cases. How +L+ much additional overhead is added for conditions involving +L+ a higher number of selection conditions and (most impor- +L+ tantly) joins? In order to assess this, we grouped the test +L+ cases into three groups, according to their complexity: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 A: queries with one or more selections and no joins, +L+ 9 B: queries with one or more selections and a join be- +L+ tween two relations, +L+ 9 C: queries with one or more selections and joins be- +L+ tween three relations. +L+ </SectLabel_listItem> <SectLabel_bodyText> This gave a test suite with 5 test cases in each of these +L+ categories, which we executed against a randomly generated +L+ database state with 500 tuples per relation that does not +L+ satisfy any of the test case pre-conditions. Figure 8 shows +L+ the results obtained for the three complexity categories. We +L+ measured the average time taken to execute the test cases +L+ in each category, including a breakdown of where the time +L+ is spent in each case: +L+ </SectLabel_bodyText> <SectLabel_listItem> 9 Test: the time required to execute the procedural as- +L+ pects of the test case; +L+ 9 Query: the time required to execute the query aspect +L+ of the test case condition; +L+ 9 Prepare the time required to execute the preparation +L+ aspect of the test case condition. +L+ </SectLabel_listItem> <SectLabel_bodyText> While the overall time required to execute the test cases rises +L+ as the complexity rises (unsurprisingly), the relative propor- +L+ tions of time spent in the various phases remains roughly the +L+ same. The preparation phase seems to account for slightly +L+ more than half of the time in each case, indicating that sig- +L+ nificant improvements could be achieved with a less-naive +L+ preparation algorithm. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have presented a new approach to the specification +L+ of test cases for database systems that attempts to reduce +L+ the amount of manual intervention required in between test +L+ case runs while also minimising the number of spurious test +L+ failures due to inappropriate input database states. The ap- +L+ proach has the further advantage that it sits naturally on top +L+ of test data sets taken from live databases, and this allows +L+ testing to be carried out using realistic data sets without re- +L+ quiring significant programmer effort to tailor the data set to +L+ the test cases. In effect, the intensional approach we have +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 8: The Affect of Changing Constraint Com- +L+ plexity +L+ </SectLabel_figureCaption> <SectLabel_bodyText> described allows software developers to trade programmer +L+ time for test execution time +L+ Our experience has indicated that intensional test cases +L+ are quick and natural to write for anyone who is familiar +L+ with SQL and database programming, although a study +L+ with an independent testing team would be necessary be- +L+ fore we can make any strong claims in this regard. How- +L+ ever, compared with what is involved in writing pure JDBC +L+ database test cases and DBUnit test cases, we found that +L+ the self-contained nature of the intensional test cases was a +L+ definite advantage. Writing DBUnit test cases requires the +L+ programmer to continually check that the test case is com- +L+ patible with the database description. Moreover, since it is +L+ common to try to reuse database descriptions for multiple +L+ test cases by combining their requirements into one database +L+ state, it becomes very easy to break one test case by chang- +L+ ing the database description in order to ready it for another. +L+ These problems do not arise with intensional testing, since +L+ all the information about the test case is present in a single +L+ file (the Java class file). +L+ We designed this first version of the preparation algorithm +L+ for simplicity and correctness rather than efficiency, and as +L+ such it performs rather stupidly in many cases. We are cur- +L+ rently exploring options for improving the algorithm, includ- +L+ ing more intelligent selection of the order in which the rela- +L+ tional algebra tree is traversed, alternating between passing +L+ query bindings and passing literal value bindings as is most +L+ efficient, and making use of modifications to existing tuples +L+ as well as simply adding and deleting tuples (both of which +L+ are comparatively expensive operations). The complexity of +L+ the conditions we can handle is at present limited by the +L+ capabilities of DBMonster, and can be expanded by devel- +L+ opment of a custom data generation facility. We also need +L+ to expand the range of queries that can be handled, beyond +L+ simple select-project-join queries. For example, standard +L+ SQL also allows aggregation and ordering within queries— +L+ both of which offer challenges in terms of automatic prepa- +L+ ration. +L+ A further problem with our current algorithm is that it +L+ may sometimes fail to find a solution to the database prepa- +L+ ration problem, even though one exists. This is due to the +L+ fact that updates are made at leaf nodes before the full set of +L+ constraints on those nodes has been encountered. It should +L+ </SectLabel_bodyText> <SectLabel_page> 110 +L+ </SectLabel_page> <SectLabel_bodyText> be possible to address the problem with more sophisticated +L+ querying techniques (this is an example of a fairly standard +L+ constrained search problem, after all), although this will add +L+ to the performance overhead. A thorough study of the trade- +L+ offs between spurious failures and more intelligent searching +L+ will need to be carried out before any concrete recommen- +L+ dations can be made. +L+ Finally, we note that where it is important to test large +L+ numbers of frame constraints (i.e. aspects of the original +L+ database state that are not affected by the execution of the +L+ program under test), it may be easier to express the test case +L+ using DBUnit, rather than cluttering up the intensional test +L+ with many such constraints. +L+ Our work presents a number of possible avenues for future +L+ work beyond the improvements mentioned above, of which +L+ the most urgent is the question of ordering of test cases +L+ within suites. This ordering can be in terms of reducing the +L+ cost of the modifications to database state or to maximise +L+ fault coverage. There is also the question of whether the +L+ modifications to database state should always persist be- +L+ tween test cases or under certain conditions discarded. For +L+ example, a test case may specify that a relation be empty +L+ and to satisfy the condition the content is discarded. How- +L+ ever, this relation may be required by later test cases and so +L+ by discarding its contents we increase the divide between the +L+ test state and the real world. This could be accomplished +L+ by either embedding the modifications inside of a transac- +L+ tion which can then be aborted or by using a hypothetical +L+ database engine. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We thank Leonardo Mariani and the anonymous reviewers +L+ for comments on earlier drafts of this paper. David Willmor +L+ is supported by a research studentship from the UK Engi- +L+ neering and Physical Sciences Research Council. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] M. Arenas, L. E. Bertossi, and J. Chomicki. +L+ Consistent query answers in inconsistent databases. In +L+ Proceedings of the 18th ACM +L+ SIGACT-SIGMOD-SIGART Symposium on Principles +L+ of Database Systems (PODS), pages 68–79. ACM +L+ Press, 1999. +L+ [2] L. E. Bertossi and J. Chomicki. Query answering in +L+ inconsistent databases. In J. Chomicki, R. van der +L+ Meyden, and G. Saake, editors, Logics for Emerging +L+ Applications of Databases, pages 43–83. Springer, +L+ 2003. +L+ [3] P. Bohannon, M. Flaster, W. Fan, and R. Rastogi. A +L+ cost-based model and effective heuristic for repairing +L+ constraints by value modification. In Proceedings of +L+ the SIGMOD Conference, pages 143–154. ACM, 2005. +L+ [4] L. Bravo and L. E. Bertossi. Logic programs for +L+ consistently querying data integration systems. In +L+ G. Gottlob and T. Walsh, editors, Proceedings of the +L+ 18th International Joint Conference on Artificial +L+ Intelligence (IJCAI), pages 10–15. Morgan Kaufmann, +L+ August 2003. +L+ [5] A. Cali, D. Lembo, and R. Rosati. On the decidability +L+ and complexity of query answering over inconsistent +L+ and incomplete databases. In Proceedings of the 22nd +L+ ACM SIGACT-SIGMOD-SIGART Symposium on +L+ Principles of Database Systems (PODS), pages +L+ 260–271. ACM, June 2003. +L+ [6] D. Chays, S. Dan, P. G. Frankl, F. I. Vokolos, and +L+ E. J. Weber. A framework for testing database +L+ applications. In Proceedings of the International +L+ Symposium on Software Testing and Analysis +L+ (ISSTA), pages 147–157, August 2000. +L+ [7] D. Chays, Y. Deng, P. G. Frankl, S. Dan, F. I. +L+ Vokolos, and E. J. Weyuker. An AGENDA for testing +L+ relational database applications. Software Testing, +L+ Verification and Reliability, 14(1):17–44, 2004. +L+ [8] J. Chomicki and J. Marcinkowski. On the +L+ computational complexity of minimal-change integrity +L+ maintenance in relational databases. In L. E. Bertossi, +L+ A. Hunter, and T. Schaub, editors, Inconsistency +L+ Tolerance, volume 3300 of Lecture Notes in Computer +L+ Science, pages 119–150. Springer, 2005. +L+ [9] S. S. Cosmadakis and C. H. Papadimitriou. Updates +L+ of relational views. Journal of the ACM, +L+ 31(4):742–760, 1984. +L+ [10] S. M. Embury, S. M. Brandt, J. S. Robinson, +L+ I. Sutherland, F. A. Bisby, W. A. Gray, A. C. Jones, +L+ and R. J. White. Adapting integrity enforcement +L+ techniques for data reconciliation. Information +L+ Systems, 26(8):657–689, 2001. +L+ [11] G. Greco, S. Greco, and E. Zumpano. A logical +L+ framework for querying and repairing inconsistent +L+ databases. IEEE Transactions on Knowledge and +L+ Data Engineering, 15(6):1389–1408, 2003. +L+ [12] A. Guessoum and J. W. Lloyd. Updating knowledge +L+ bases. New Generation Computing, 8(1):71–89, 1990. +L+ [13] F. Haftmann, D. Kossmann, and A. Kreutz. Efficient +L+ regression tests for database applications. In +L+ Proceedings of the 2nd Biennial Conference on +L+ Innovative Data Systems Research (CIDR), pages +L+ 95–106. Online Proceedings, January 2005. +L+ [14] G. M. Kapfhammer and M. L. Soffa. A family of test +L+ adequacy criteria for database-driven applications. In +L+ Proceedings of the 11th ACM SIGSOFT Symposium +L+ on Foundations of Software Engineering, pages +L+ 98–107. ACM, September 2003. +L+ [15] R. Langerak. View updates in relational databases +L+ with an independent scheme. ACM Transactions on +L+ Database Systems (TODS), 15(1):40–66, 1990. +L+ [16] P. Louridas. Junit: Unit testing and coding in +L+ tandem. IEEE Software, 22(4):12 – 15, July-Aug 2005. +L+ [17] J. Melton and A. R. Simon. SQL:1999 Understanding +L+ Relational Language Components. Morgan Kaufmann, +L+ 2002. +L+ [18] H. Shu. Using constraint satisfaction for view update. +L+ Journal of Intelligent Information Systems, +L+ 15(2):147–173, 2000. +L+ [19] D. Willmor and S. M. Embury. Exploring test +L+ adequacy for database systems. In Proceedings of the +L+ 3rd UK Software Testing Research Workshop +L+ (UKTest), pages 123–133. The University of Sheffield, +L+ September 2005. +L+ [20] D. Willmor and S. M. Embury. A safe regression test +L+ selection technique for database–driven applications. +L+ In Proceedings of the 21st International Conference on +L+ Software Maintenance (ICSM), pages 421–430. IEEE +L+ Computer Society, September 2005. +L+ </SectLabel_reference> <SectLabel_page> 111 +L+ </SectLabel_page>
<SectLabel_title> Analysis of Soft Handover Measurements +L+ in 3G Network +L+ </SectLabel_title> <SectLabel_author> Kimmo Raivio +L+ </SectLabel_author> <SectLabel_affiliation> Adaptive Informatics Research Centre +L+ Helsinki University of Technology +L+ </SectLabel_affiliation> <SectLabel_address> P.O. Box 5400, FIN-02015 HUT, Finland +L+ </SectLabel_address> <SectLabel_email> kimmo.raivio@hut.fi +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A neural network based clustering method for the analysis +L+ of soft handovers in 3G network is introduced. The method +L+ is highly visual and it could be utilized in explorative anal- +L+ ysis of mobile networks. In this paper, the method is used +L+ to find groups of similar mobile cell pairs in the sense of +L+ handover measurements. The groups or clusters found by +L+ the method are characterized by the rate of successful han- +L+ dovers as well as the causes of failing handover attempts. +L+ The most interesting clusters are those which represent cer- +L+ tain type of problems in handover attempts. By comparing +L+ variable histograms of a selected cluster to histograms of +L+ the whole data set an application domain expert may find +L+ some explanations on problems. Two clusters are investi- +L+ gated further and causes of failing handover attempts are +L+ discussed. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.4.3 [Information Systems Applications]: Communi- +L+ cations Applications—Information browsers; I.5.3 [Pattern +L+ Recognition]: Clustering +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Algorithms, Management, Performance +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> 3G network, soft handover, mobility management, data min- +L+ ing, hierarchical clustering, neural networks +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Mobility management is a great challenge in current and +L+ future radio access networks. In third generation (3G) net- +L+ works user experienced quality of service (QoS) under a +L+ move of mobile station (MS) from one mobile cell to an- +L+ other cell has been improved by implementing soft handover +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, to +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> MSWIM’06, October 2–6, 2006, Torremolinos, Malaga, Spain. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2006 ACM 1-59593-477-4/06/0010 ...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> (SHO). Soft handover makes it possible to have connections +L+ on several base stations (BS) simultaneously. +L+ In this paper, a set of measurements which can be used for +L+ soft handover decision making are analyzed and compared +L+ with other measurements in which statistics of successful- +L+ ness of handover attempts have been collected. We do not +L+ know exactly the parameters of used SHO algorithm. SHOs +L+ are investigated only on basis of data set and some general +L+ knowledge of 3G systems. Mobile cell pairs with handovers +L+ (HO) are divided in groups using clustering algorithm. Cell +L+ pairs in which SHOs are similar with each other fall in same +L+ group. Different types of SHO failures are analyzed using +L+ clustering information and distributions of measurements in +L+ each cluster. +L+ In Section 2 the soft handover concept, the measurements +L+ and used neural network algorithm are shortly introduced. +L+ Analysis methods which have been used are described in +L+ Section 3. Preliminary results are shown and discussed in +L+ Section 4. Finally, some conclusions are drawn in the last +L+ section. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. BACKGROUND +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, the basics of soft handover in 3G network +L+ is explained and the available data set is introduced. Neural +L+ network algorithm used in data clustering is also presented. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.1 Soft handover +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Soft handover is a state of MS being connected to several +L+ BSs simultaneously. In GSM networks, a fixed threshold for +L+ handover from one cell to another is used. In 3G networks, +L+ each MS is connected to a network via a set of BSs called +L+ active set. Members of active set are updated on basis of +L+ measurements made by MS. The advantage of having con- +L+ nections on several BS simultaneously is realized when MS +L+ is moving towards another BS, the MS should have a con- +L+ nection at least on one BS all the time. In GSM system, the +L+ older connection has to be terminated before the new one +L+ can be setup. The connection setup phases are the most +L+ vulnerable steps in a call. The connection between MS and +L+ BS is setup in a beginning of a call or later when handover +L+ occurs. If the setup is not successful, it is useful to have an +L+ existing connection to another BS or otherwise the call will +L+ be abnormally terminated. +L+ Handover can occur due to signal quality reasons or when +L+ the traffic capacity in a cell has reached its maximum or is +L+ approaching it. In the latter case, traffic load in the network +L+ can be distributed more uniformly by handing over some +L+ users from the most crowded cells. The above method is +L+ </SectLabel_bodyText> <SectLabel_page> 330 +L+ </SectLabel_page> <SectLabel_bodyText> called cell breathing. Use of cell breathing without giving +L+ the information to the analyzer increases the complexity of +L+ the analysis and can mix up a lot in the analysis process. +L+ For a user soft handover means power saving (in uplink) +L+ and less abnormally terminated calls. For an operator lower +L+ MS transmitting powers mean less interference. When MS +L+ is in SHO, several BSs listen the same uplink channel, but +L+ all BSs have their own downlink channel. The offered diver- +L+ sity is resource consuming in downlink direction. There is +L+ a tradeoff between better QoS in mobility management and +L+ consumption of resources. +L+ Decision of soft handover is made in mobile station by +L+ comparing the signal-to-noise ratios of active and candidate +L+ BSs Common Pilot Channel (CPICH) [2]. Members of ac- +L+ tive set are selected on basis of powers of this pilot signal [5, +L+ 12, 16] . +L+ BSs which are not in the active set but next from it in the +L+ sense of measured quantity are in candidate set. Candidate +L+ set BSs are constantly monitored whether their offer better +L+ connection than cells in active set. Cells not in active or +L+ candidate set are monitored less frequently whether their +L+ can enter the candidate set. Cell is either added to the +L+ active set if the maximum amount of cells in the active set +L+ is not reached or cell replaces the cell which offers the lowest +L+ quality connection. Cells which are no more able to offer +L+ a connection which is good enough are removed from the +L+ active set. +L+ Thresholds are used in adding, replacing and removing +L+ BSs from active set by BSs in candidate set to avoid ping +L+ pong effect. This means that a value of measured quantity +L+ should be with a certain threshold better than the old one +L+ for changing cells in active set. If measurement which is only +L+ slightly better (i.e. with zero threshold) is enough for chang- +L+ ing cells in sets, it is quite possible that the same change is +L+ performed in opposite direction very soon. Thus, the origi- +L+ nal update of the set was useless and resource consuming in +L+ the sense of all required signaling. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Data +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Three data sets of Key Performance Indicator (KPI) level +L+ measurements related on handover events are saved. Each +L+ set consists of measurements collected during one hour. KPI +L+ is considered as an important measure to be followed. It can +L+ be a measurement by itself or it has been computed from a +L+ group of raw counters [10]. One data vector consists of prob- +L+ abilities, means, sums and counters computed over one hour +L+ of one source target cell pair. Here, source refers on cell +L+ in active set and target on another cell which is measured +L+ and possibly added in active or candidate set. Measure- +L+ ments of target cell are compared with those of source cell. +L+ Handover decisions are made in MS on basis of measured +L+ and computed base stations received signal signal-to-noise +L+ ratios (Ec/N0). For each source and target cell pair mean +L+ of signal-to-noise ratio differences is computed using +L+ </SectLabel_bodyText> <SectLabel_equation> EcnoDiffMean = mean {[Ec/N0]target — [Ec/N0] source} +L+ </SectLabel_equation> <SectLabel_bodyText> Mean value and number off made comparisons (EcnoDiffNum) +L+ are saved. Four bin pdfs of these measurements are also +L+ stored with bin centers in -6, -3, 0 and 3dB, correspond- +L+ ingly. +L+ In addition to Ec/N0 measurements, averages of received +L+ pilot signal power ratios between BS pairs (av rscp ratio) +L+ have been computed and stored in database. The time and +L+ probability of being in SHO with each other have also been +L+ measured. Time of target and source cell being in SHO with +L+ each other simultaneously is counted in variable t act. Then, +L+ at least one MS is in SHO having both source and target cell +L+ in its active set. The measurement is symmetric for a switch +L+ of source and target cells. Time of target cell being in SHO +L+ with source cell is stored in t act dir. Cell total time in +L+ SHO is saved in tot time sho. It has been counted over all +L+ the targets of fixed source cell. Probability of target and +L+ source being in same active set is stored in variable p act. +L+ Total number of SHO attempts to add target to active +L+ set is stored in SHO total att. Ratio of successful SHO at- +L+ tempts which lead to addition of target cell in active set is +L+ saved in add ratio. In addition to those above, the num- +L+ ber of SHO failures is stored in pfail total and ratios of four +L+ different failure causes are saved. Failure occurs in setup +L+ or active time phase of SHO and it is either radio channel +L+ problem or not. Probability of cell being in monitored state +L+ is also measured (p4th 5th). All the measurements used in +L+ the analysis are shortly described in Table 1. +L+ A lot of data has been saved in data sets, but also some +L+ very important information is missing. Due to missing in- +L+ formation on cell capacities, their locations and performed +L+ manual and automatic tuning operations on network config- +L+ uration between successive data set saves, only preliminary +L+ analysis can be performed. The rest of the analysis process +L+ is described on theoretical level. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.3 Self-Organizing Map +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Self-Organizing Map (SOM) [8] is an unsupervised neu- +L+ ral network algorithm which adapts the codebook vectors +L+ of neurons so that they approximate the input data distri- +L+ bution. When the training has converged topological areas +L+ or domains corresponding to certain types of inputs can be +L+ found from the map. The topology and the size of the net- +L+ work is fixed before adaptation. +L+ In the SOM algorithm, the codebook vectors wj of the +L+ SOM are at first initialized. Then, the following steps are +L+ repeated for each input vector x: Find the index of best- +L+ matching or nearest codebook vector using +L+ </SectLabel_bodyText> <SectLabel_equation> i(x) = argminI Ix — wj I I, +L+ </SectLabel_equation> <SectLabel_bodyText> in which j goes through all the neurons in the map. Next, +L+ the codebook vectors of winner neuron and its neighbors are +L+ updated using +L+ </SectLabel_bodyText> <SectLabel_equation> wj (t + 1) = wj (t) + αhij (x)(x(t) — wj (t)). +L+ </SectLabel_equation> <SectLabel_bodyText> Here, α is the learning rate and hij (x) is the neighborhood +L+ function centered around the winner neuron. Input sample +L+ x defines the winner neuron and the topological distance +L+ between indexes i and j defines how much the neuron is +L+ updated. Neighborhood function is typically Gaussian or +L+ bubble function i.e. function which decrease monotonically +L+ and even goes to zero when the distance increases. +L+ In this paper, a batch version of the SOM algorithm is +L+ used. In batch SOM, all codebook vectors of the SOM are +L+ computed after the best-matching units of all input data +L+ vectors have been found. The same data set is used several +L+ times. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. METHODS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Handover related measurement from 3G network can be +L+ analyzed using standard data mining methods [1]. In this +L+ </SectLabel_bodyText> <SectLabel_page> 331 +L+ </SectLabel_page> <SectLabel_tableCaption> Table 1: Measurements in the analysis. Data set has one sample vector for each source target cell pair. +L+ </SectLabel_tableCaption> <SectLabel_table> Variable	Explanation	Type +L+ EcnoDiffNum	Computed Ec/N0 differences	number +L+ EcnoDiffMean	Computed Ec/N0 differences	mean +L+ EcnoDiffPdf-6.0	-6 dB bin of Ec/N0 difference pdf	ratio +L+ EcnoDiffPdf-3.0	-3 dB bin of Ec/N0 difference pdf	ratio +L+ EcnoDiffPdf0.0	0 dB bin of Ec/N0 difference pdf	ratio +L+ EcnoDiffPdf3.0	3 dB bin of Ec/N0 difference pdf	ratio +L+ t act	Target and source simultaneously in SHO	mean +L+ t act dir	Time of target being in SHO with source	mean +L+ tot time sho	Cell total time in SHO	sum +L+ p act	Target in active set of source	ratio +L+ SHO total att	SHO attempts to add Target to active set	number +L+ add ratio	Successful attempts leading to addition	ratio +L+ pfail total	Failures	number +L+ pfail ini	Setup phase failures due to non-radio	ratio +L+ pfail ini radio	Setup phase failures due to radio	ratio +L+ pfail act	Active time failures due to non-radio	ratio +L+ pfail act radio	Active time failures due to radio	ratio +L+ p4th 5th	Cell is in monitored state (=4th or 5th)	ratio +L+ av rscp ratio	Target / Source Received power ratio	mean +L+ r fail	Ratio pfail total / SHO total att	ratio +L+ r EcnoDNum	Ratio EcnoDiffNum / SHO total att	ratio +L+ * Variable defined in the analysis. +L+ </SectLabel_table> <SectLabel_bodyText> study, methods presented in Figure 1 are used. At first, +L+ the miner have to decide what could be interesting in this +L+ data. The analysis task has to be defined. On basis of that +L+ the first choice of variables will be done. Next, the selected +L+ variables are preprocessed, in order to be able to use them +L+ in later analysis. +L+ In data mining tasks, variable selection and preprocessing +L+ are the most critical phases, because in this step the miner +L+ decides which variables are important and how should they +L+ be processed. The whole data mining process consists of sev- +L+ eral cycles performed repeatedly. The cycles include testing +L+ how different variable selections and preprocessing methods +L+ effect on final results. The process has inner loops in which +L+ some tasks or parameters are fixed on basis of selections +L+ made in outer loop. The inner loops are performed more +L+ frequently. Loops with more general task like the defini- +L+ tion of mining task are repeated less frequently. When the +L+ mining task is defined the analyzer should be able to decide +L+ what is (s)he looking out for. +L+ Now, the analysis task is defined as finding groups of sim- +L+ ilarly behaving cell pairs in SHO situations. Importance of +L+ measurements can also be highlighted using proper weight- +L+ ing of variables. In addition to clustering, also other tasks +L+ for data analysis can be defined. One possibility is to try to +L+ find cells or cell pairs with anomalous behavior. Anomalies +L+ can also be found by clustering, but expert knowledge in +L+ variable selection and preprocessing steps are very impor- +L+ tant. +L+ Using different variables, preprocessing methods and weight- +L+ ing of variables different clustering results can be found. To +L+ find out which of them is useful, interpretation of clusters is +L+ needed. This can be done using histograms or rules defined +L+ by data samples falling in clusters. The results which have +L+ been found using clustering methods should be visualized +L+ together with spatial locations to be able to understand the +L+ usefulness of results. Methods should be performed repeat- +L+ edly to analyze successive data sets under the knowledge of +L+ performed tuning operations. Thus, there is a possibility to +L+ find explanations to changing results. In this study, results +L+ of only one data set are shown, because more information +L+ on application domain is needed to be able to combine and +L+ compare successive clustering results. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Preprocessing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Different preprocessing methods have been tested. The +L+ final method was selected on basis of histograms and the +L+ clusters which were found using the selected method. At +L+ the first step, the distributions are truncated. Outliers in +L+ the selected variables were replaced by their maximum per- +L+ mitted values. Two variables, pfail total and EcnoDiffNum, +L+ were scaled using the number of performed soft handover +L+ attempts (see Table 1). Logarithms of some of the variables +L+ were taken, but finally only scaled EcnoDiffNum was prepro- +L+ cessed with logarithmic function. Sample vectors with high +L+ amount of undefined measurements were canceled. Used +L+ clustering method (see section 3.2) allows using sample vec- +L+ tors in which some variables are undefined. However, they +L+ are not so useful when the rate of undefined values increases. +L+ Here, sample vectors with 15 or more missing values in 20 +L+ variables are canceled. +L+ In Figure 2 the histograms of the most interesting vari- +L+ ables preprocessed using selected methods are visualized. +L+ Some of the variables have quite high peaks in distributions, +L+ but due to the origin of variables no other preprocessing +L+ have been performed. For example, handover failure reasons +L+ pfail ini, pfail ini radio, pfail act radio and pfail act sum up +L+ to unity. However, pfail act is not analyzed because it is zero +L+ all the time in the first data set. +L+ </SectLabel_bodyText> <SectLabel_page> 332 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 1: Used data analysis method. Steps con- +L+ nected with solid arrows have been performed. +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 3.2 Clustering +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Cluster analysis is used to divide data vectors in groups. +L+ Data vectors falling in same cluster are similar with each +L+ other. Here, clustering is performed using a two-phase +L+ method [15]. In this method, data vectors are at first used +L+ to train a Self-Organizing Map. Neurons of the SOM adapt +L+ to incoming data so that the input data can in later analysis +L+ be represented by the codebook vectors of neurons. Number +L+ of these codebook vectors is much smaller than the number +L+ of original data vectors. Thus, computational complexity of +L+ the final crisp clustering algorithm is decreased. Another +L+ advantage of using a SOM based two-phase method instead +L+ of direct clustering of data vectors is the visualization capa- +L+ bility of SOM. +L+ In addition to preprocessing, SOM algorithm provides an- +L+ other possibility to emphasize important properties of data. +L+ Larger weights in distance computation are given to the +L+ most important properties defined by the analyzer. Smaller +L+ or even zero weight can be given to those variables which +L+ are not used in organization of the SOM i.e. in building clus- +L+ ters. However, values of them can be compared to those with +L+ larger weights using various visualization methods. Weight- +L+ ing by variable importance can also be built into SOM train- +L+ ing algorithm by utilizing learning distance metrics [7]. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: Logarithmic histograms after distribution +L+ cuts, logarithmic preprocessing of r EcnoDNum and +L+ scaling of all variables between [0,1] +L+ </SectLabel_figureCaption> <SectLabel_bodyText> The codebook vectors are further clustered using k-means +L+ or some hierarchical clustering method. In this paper, Ward +L+ agglomerative clustering method has been used [4]. In the +L+ beginning of hierarchical clustering, each codebook vector +L+ is a cluster of its own. In the next step, the most similar +L+ clusters are combined and this is continued until all vectors +L+ are in same cluster. The clustering results form a tree struc- +L+ ture called dendrogram. In visualization of a dendrogram, +L+ the clusters combined in each step and the distance between +L+ them are shown. Final clustering is selected by cutting this +L+ tree at certain level. The number of clusters can be selected +L+ manually or some cluster validation index can be utilized +L+ to find the optimum. In this paper, Davies-Bouldin vali- +L+ dation index has been used [3]. Similar clustering methods +L+ have earlier been used in the analysis of both GSM and 3G +L+ network BTSs [9, 11, 13]. +L+ As a result of clustering, each data vector is represented +L+ by index of one neuron or by the codebook vector stored in +L+ that neuron. Furthermore, the neuron and the data vectors +L+ the neuron represents belong to same cluster. On basis of +L+ the clustering result, some clusters can be selected for more +L+ specific analysis. Cluster selection is usually done on basis +L+ of found higher values of some critical variables. It is pos- +L+ sible to build a system in which rules are found for clusters +L+ [14, 9] and these are used to select interesting clusters au- +L+ tomatically. Here, interesting clusters are selected manually +L+ on basis of clusterwise variable mean values and histograms. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, handover measurement data is used to +L+ train a Self-Organizing Map of size 17 x 12. Then, the code- +L+ book vectors of the SOM are clustered using hierarchical +L+ Ward method. Results of clustering are described and two +L+ clusters are then selected for more specific analysis. Charac- +L+ teristics of sample vectors falling in those clusters are studied +L+ using histograms. +L+ Only the most interesting variables are used to find the +L+ </SectLabel_bodyText> <SectLabel_figure> Task definition +L+ Variable selection +L+ Clustering +L+ Interpretation +L+ of clusters +L+ Preprocessing +L+ Visualization +L+ with locations +L+ Parameter tuning +L+ </SectLabel_figure> <SectLabel_page> 333 +L+ </SectLabel_page> <SectLabel_bodyText> nearest neuron of input data vector. These variables have +L+ nonzero mask which can also be considered as a weighting +L+ factor in a search for the best-matching neuron. Rest of +L+ the variables have zero mask, which means that they can +L+ be visualized and updated using SOM algorithm, but they +L+ do not have an effect on organization of the SOM and on +L+ selection of the cluster in which the sample belongs to. +L+ In Figure 3 all other component planes of SOM with pos- +L+ itive mask are shown, except Ec/N0 difference distributions +L+ which are shown in Figure 6. In component plane visual- +L+ ization, distributions of components (or variables) of SOM +L+ codebook vectors are shown. Component values of one code- +L+ book vector are visualized using grayscaling and their locate +L+ in the same position at each plane. For example, values of +L+ one codebook vector are shown at upper right corner in each +L+ plane. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 3: Component planes of SOM with denor- +L+ malized scales. Shown variables have nonzero mask +L+ and they are not describing Ec/N0 difference distri- +L+ butions. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Some component values which were not used in SOM +L+ training (i.e. they were masked out) are shown in Figure +L+ 4. Although, they have no effect on SOM organization, they +L+ are adapted to be able to compare their distributions even +L+ with those used in organizing the SOM. +L+ By visual comparison of variables in Figures 3 and 4, it can +L+ be seen that the total number of SHO attempts +L+ (SHO total att) and Ec/N0 difference measurements +L+ (EcnoDiffNum) is higher in upper part of the SOM. How- +L+ ever, when the latter is scaled by total number of attempts, +L+ higher rate of measurements (r EcnoDNum) is in lower part +L+ of the map. Also, the total number of failuring SHO at- +L+ tempts (pfail total) is high in upper right corner, but scal- +L+ ing this by number of attempts tells us that the failure rate +L+ (r fail) in upper right corner is quite moderate. Instead, +L+ higher failure rates exists in both lower corners i.e. in clus- +L+ ters 5 and 8 (see Figure 5). +L+ Trained SOM codebook vectors are clustered using hier- +L+ archical Ward algorithm. The clustering result selected by +L+ Davies-Bouldin index is shown in Figure 5. Four bin Ec/N0 +L+ difference histograms are visualized on top of clustered SOM +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4: Denormalized component planes of vari- +L+ ables which were not used in SOM training. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> in Figure 6. When component values of SOM (see Figures +L+ 3, 4 and 6) are compared with clustering result (see Figure +L+ 5) several types of source target pairs can be found. Most of +L+ them are behaving as expected, but some of them represent +L+ handover attempts with certain type of problems. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5: SOM which is clustered using hierarchical +L+ Ward method and Davies-Bouldin validation index. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> To find out the most interesting clusters of the SOM for +L+ further investigations, distribution of data samples on SOM +L+ is visualized. In Figure 7a hits of all samples on SOM nodes +L+ are visualized and in Figure 7b hits of samples with SHO +L+ failure rate (r fail) larger than 22% are shown. Samples are +L+ distributed all over the map, only some edge nodes have +L+ slightly larger hit rate. Lower part of the map has more hits +L+ when samples with increased failure rate are considered. +L+ In Figure 8 hits of samples which represent two differ- +L+ </SectLabel_bodyText> <SectLabel_page> 334 +L+ </SectLabel_page> <SectLabel_figure> (a) All	(b) SHO failure rate > 22% +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 7: Sample vector hits on SOM nodes. Size of +L+ black hexagonal on SOM node denotes the number +L+ of hits. Maximum number of hits per node is shown +L+ above the plot. +L+ Figure 6: EcnoDiff distributions on top of clustered +L+ SOM. In each SOM node a four bin Ec/N0 histogram +L+ is shown. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> ent types of SHO failures are shown. Samples are from cell +L+ pairs in which the rate of selected type of failures is larger +L+ than 75%. However, handover initialization failures due to +L+ some other reason than radio channel resources (i.e. pfail ini +L+ type failures) are obviously more frequent than failures due +L+ to radio channel initialization problems (pfail ini radio type +L+ failures). Cell pairs with SHO failures originating mainly +L+ from these two reasons are mapped on separate clusters. +L+ All SHO failures due to radio channel initialization are in +L+ cluster 9 (see Figures 5 and 8b) and most of all other ini- +L+ tialization failures are in cluster 5 (see Figures 5 and 8a). In +L+ the following, these two clusters are studied in more detail. +L+ In Figures 9 and 10 histograms of samples which belong +L+ to clusters 5 and 9 are shown. These histograms should +L+ be compared with histograms of whole data set which were +L+ shown in Figure 2. In histograms of cluster 5 (see Figure +L+ 9), the average received signal power ratio (av rscp ratio) is +L+ slightly lower than in general. Distributions of three largest +L+ Ec/N0 difference measurement bins are completely different +L+ than corresponding distributions from the whole data set. +L+ In cluster 5 most of the samples have about 3dB Ec/N0 dif- +L+ ference (EcnodiffPdf3.0) which means that at least this mea- +L+ surement makes successful SHOs possible and SHO should +L+ be performed. Exceptional Ec/N0 difference measurements +L+ </SectLabel_bodyText> <SectLabel_figure> (a) pfail ini	(b) pfail ini radio +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 8: Hits of samples of two failure types. Sam- +L+ ples of which more than 75% are failuring due to +L+ selected cause are counted. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> of this cluster can also be seen in Figure 6. All the failing +L+ cell pairs fail in initialization due to other than radio chan- +L+ nel reasons (pfail ini). Total rate of failures is very high +L+ (r fail). One reason for high rate of failures can be that all +L+ the capacity is in use. +L+ In histograms of cluster 9 (see Figure 10), the average re- +L+ ceived power ratios are a bit higher than usual, but there are +L+ no samples with high rate of 3dB Ec/N0 differences (EcnoD- +L+ iffPdf3.0). However, in such a situation it should be possible +L+ to perform successful SHOs. The rate of initialization fail- +L+ ures in radio channels (pfail ini radio) is higher than usually, +L+ but because only a small part of samples in this cluster have +L+ above mentioned problems the total SHO failure rate is not +L+ higher than usually. The total number of samples or cell +L+ pairs with high rate of initialization failures in SHO is so +L+ small, that it is impossible to make any further inferences +L+ from these clusters. It is possible to check histograms of +L+ only those samples which fulfill the failure rate criteria, but +L+ the number of samples is anyway quite low. +L+ </SectLabel_bodyText> <SectLabel_page> 335 +L+ </SectLabel_page> <SectLabel_figureCaption> Figure 9: Histograms of data vectors of cluster 5. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Cell pairs with high rate of radio channel initialization +L+ failures in SHO attempts vary from data set to another, +L+ but without any information on network topology and with +L+ uncomplete information on performed tuning operations, it +L+ is impossible to make any further inferences. +L+ Figure 10: Histograms of data vectors of cluster 9. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, a data analysis method based on a neu- +L+ ral network has been presented. The method is utilized in +L+ data visualization and clustering. The presented method is +L+ only one possibility for finding data clusters. However, the +L+ benefits of the proposed method are the decrease in compu- +L+ tational complexity due to used two-phase clustering algo- +L+ rithm and the visualization capability of the method. Thus, +L+ it is well suitable for this kind of explorative data analysis. +L+ It is desirable to find clusters with characteristics which +L+ differ from one cluster to another. In the presented method, +L+ selection of variables and variable weighting factors have +L+ been used to find interesting clusters. In the preprocess- +L+ ing phase, also the number of permitted undefined measure- +L+ ment values in sample vector has an effect on found clusters. +L+ Sample vectors with high rate of missing values are not so +L+ usable and describable as samples without them. Vectors +L+ with missing values can be used in the SOM training but +L+ the benefit of using them decreases when the rate of unde- +L+ fined values increases. +L+ In this study, histograms are used both when preprocess- +L+ ing methods are decided and when an interpretation for the +L+ found clusters are looked for. However, clusters can also be +L+ compared using other visual methods, finding limiting rules +L+ for variable values in clusters or comparing distributions of +L+ variable values in clusters using more sophisticated distribu- +L+ tion comparison measures like Kullback-Leibler divergences +L+ [6]. +L+ The results which have been obtained using all available +L+ data sets differ slightly from each other, but due to uncom- +L+ plete information on network configuration and parameter +L+ tuning, further inferences cannot be made. However, adding +L+ this information would offer interesting possibilities to con- +L+ tinue this study. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] P. Chapman, J. Clinton, T. Khabaza, T. Reinartz, +L+ and R. Wirth. CRISP-DM 1.0 step-by-step data +L+ mining guide. Technical report, CRISM-DM +L+ consortium, 2000. http://www.crisp-dm.org. +L+ [2] Y. Chen. Soft Handover Issues in Radio Resource +L+ Management for 3G WCDMA Networks. PhD thesis, +L+ Queen Mary, University of London, 2003. +L+ [3] D. Davies and D. Bouldin. A cluster separation +L+ measure. IEEE Transactions on Pattern Analysis and +L+ Machine Intelligence, 1(2):224–227, April 1979. +L+ [4] B. Everitt. Cluster Analysis. Arnold, 1993. +L+ [5] V. K. Garg. Wireless Network Evolution: 2G to 3G. +L+ Prentice-Hall, Inc., 2002. +L+ [6] S. Haykin. Neural Networks, a Comprehensive +L+ Foundation. Macmillan, 1999. +L+ [7] S. Kaski and J. Sinkkonen. Metrics that learn +L+ relevance. In Proceedings of the International Joint +L+ Conference on Neural Networks, volume 5, pages +L+ 547–552, 2000. +L+ [8] T. Kohonen. Self-Organizing Maps. Springer-Verlag, +L+ Berlin, 1995. +L+ [9] J. Laiho, K. Raivio, P. Lehtim¨aki, K. H¨at¨onen, and +L+ O. Simula. Advanced analysis methods for 3G cellular +L+ networks. IEEE Transactions on Wireless +L+ Communications, 4(3):930–942, May 2005. +L+ [10] J. Laiho, A. Wacker, and T. Novosad, editors. Radio +L+ Network Planning and Optimisation for UMTS. John +L+ Wiley & Sons Ltd., 2001. +L+ [11] P. Lehtim¨aki and K. Raivio. A SOM based approach +L+ for visualization of GSM network performance data. +L+ In IEA/AIE, pages 588–598, 2005. +L+ [12] R. Prakash and V. Veeravalli. Locally optimal soft +L+ handoff algorithms. IEEE Transactions on Vehicular +L+ Technology, 52(2):347–356, March 2003. +L+ [13] K. Raivio, O. Simula, and J. Laiho. Neural analysis of +L+ mobile radio access network. In IEEE International +L+ </SectLabel_reference> <SectLabel_page> 336 +L+ </SectLabel_page> <SectLabel_reference> Conference on Data Mining, pages 457–464, San Jose, +L+ California, USA, November 29 - December 2 2001. +L+ [14] M. Siponen, J. Vesanto, O. Simula, and P. Vasara. An +L+ approach to automated interpretation of SOM. In +L+ N. Allinson, H. Yin, L. Allinson, and J. Slack, editors, +L+ Advances in Self-Organizing Maps, pages 89–94. +L+ Springer, 2001. +L+ [15] J. Vesanto and E. Alhoniemi. Clustering of the +L+ self-organizing map. IEEE Transactions on Neural +L+ Networks, 11(3):586–600, May 2000. +L+ [16] J. Zander. Radio Resource Management for Wireless +L+ Networks. Artech House, Inc., 2001. +L+ </SectLabel_reference> <SectLabel_page> 337 +L+ </SectLabel_page>
<SectLabel_title> Automated Rich Presentation of a Semantic Topic +L+ </SectLabel_title> <SectLabel_author> Lie Lu and Zhiwei Li +L+ </SectLabel_author> <SectLabel_affiliation> Microsoft Research Asia +L+ </SectLabel_affiliation> <SectLabel_email> {llu, zli}@microsoft.com +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> To have a rich presentation of a topic, it is not only expected that +L+ many relevant multimodal information, including images, text, +L+ audio and video, could be extracted; it is also important to +L+ organize and summarize the related information, and provide +L+ users a concise and informative storyboard about the target topic. +L+ It facilitates users to quickly grasp and better understand the +L+ content of a topic. In this paper, we present a novel approach to +L+ automatically generating a rich presentation of a given semantic +L+ topic. In our proposed approach, the related multimodal informa- +L+ tion of a given topic is first extracted from available multimedia +L+ databases or websites. Since each topic usually contains multiple +L+ events, a text-based event clustering algorithm is then performed +L+ with a generative model. Other media information, such as the +L+ representative images, possibly available video clips and flashes +L+ (interactive animates), are associated with each related event. A +L+ storyboard of the target topic is thus generated by integrating each +L+ event and its corresponding multimodal information. Finally, to +L+ make the storyboard more expressive and attractive, an incidental +L+ music is chosen as background and is aligned with the storyboard. +L+ A user study indicates that the presented system works quite well +L+ on our testing examples. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.5.3 [Information Interfaces and Presentation]: Group and +L+ Organization Interfaces - Organizational design; H.3.1 [Informa- +L+ tion Storage and Retrieval]: Content Analysis and Indexing - +L+ abstracting methods. +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Algorithms, Design, Management, Experimentation, Theory +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Rich presentation, multimodality, multimedia authoring, story- +L+ board, events clustering, multimedia fusion +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In the multimedia field, a major objective of content analysis is to +L+ discover the high-level semantics and structures from the low- +L+ level features, and thus to facilitate indexing, browsing, searching, +L+ and managing the multimedia database. In recent years, a lot of +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> MM’05, November 6–11, 2005, Singapore. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2005 ACM 1-59593-044-2/05/0011...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> technologies have been developed for various media types, +L+ including images, video, audio and etc. For example, various +L+ approaches and systems have been proposed in image content +L+ analysis, such as semantic classification [1], content-based image +L+ retrieval [2] and photo album management [3]. There are also a lot +L+ of research focuses on video analysis, such as video segmentation +L+ [4], highlight detection [5], video summarization [6][7], and video +L+ structure analysis [8], applied in various data including news +L+ video, movie and sports video. Since audio information is very +L+ helpful for video analysis, many research works on audio are also +L+ developed to enhance multimedia analysis, such as audio +L+ classification [9], and audio effect detection in different audio +L+ streams [10]. Most recently, there are more and more approaches +L+ and systems integrating multimodal information in order to +L+ improve analysis performance [11][12]. +L+ The main efforts of the above mentioned research have focused on +L+ understanding the semantics (including a topic, an event or the +L+ similarity) from the multimodal information. That is, after the +L+ multimedia data is given, we want to detect the semantics implied +L+ in these data. In this paper, we propose a new task, Rich +L+ Presentation, which is an inverse problem of the traditional +L+ multimedia content analysis. That is, if we have a semantic topic, +L+ how can we integrate its relevant multimodal information, +L+ including image, text, audio and video, to richly present the target +L+ topic and to provide users a concise and informative storyboard? +L+ In this paper, the so-called “semantic topic” is a generic concept. +L+ It could be any keyword representing an event or events, a +L+ person’s name, or anything else. For example, “World Cup 2002” +L+ and “US election” could be topics, as well as “Halloween” and +L+ “Harry Potter”. In this paper, our task is to find sufficient +L+ information on these topics, extract the key points, fuse the +L+ information from different modalities, and then generate an +L+ expressive storyboard. +L+ Rich presentation can be very helpful to facilitate quickly +L+ grasping and better understanding the corresponding topic. +L+ People usually search information from (multimedia) database or +L+ the Internet. However, what they get is usually a bulk of +L+ unorganized information, with many duplicates and noise. It is +L+ tedious and costs a long time to get what they want by browsing +L+ the search results. If there is a tool to help summarize and +L+ integrate the multimodal information, and then produce a concise +L+ and informative storyboard, it will enable users to quickly figure +L+ out the overview contents of a topic that they want to understand. +L+ Rich presentation provides such a tool, and thus it could have +L+ many potential applications, such as education and learning, +L+ multimedia authoring, multimedia retrieval, documentary movie +L+ production, and information personalization. +L+ In this paper, we will present the approach to rich presentation. In +L+ order to produce a concise and informative storyboard to richly +L+ present a target topic, we need to answer the following questions. +L+ 1) How to extract the relevant information regarding the target +L+ </SectLabel_bodyText> <SectLabel_page> 745 +L+ </SectLabel_page> <SectLabel_bodyText> topic? 2) How to extract the key points from the relevant +L+ information and build a concise and informative storyboard? 3) +L+ How to fuse all the information from different modality? and 4) +L+ how to design the corresponding rendering interface? +L+ </SectLabel_bodyText> <SectLabel_figure> A Target Topic +L+ Rich Presentation +L+ </SectLabel_figure> <SectLabel_figureCaption> Fig. 1 The system framework of rich presentation of a target +L+ semantic topic. It is mainly composed of three steps, relevant +L+ multimodal information extraction, media analysis, and rich +L+ presentation generation. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> In this paper, we propose a number of novel approaches to deal +L+ with the above issues and also present an example system. Fig. 1 +L+ illustrates the proposed system framework of rich presentation. It +L+ is mainly composed of three steps, relevant multimodal informa- +L+ tion extraction, media analysis including multiple events cluster- +L+ ing, representative media detection and music rhythm analysis; +L+ and the final storyboard generation and music synchronization. +L+ In the proposed system, given the semantic topic, the relevant +L+ information, including text, image, video and music, is first +L+ extracted from the available multimedia database or the web data- +L+ base. User interaction is also allowed to provide extra relevant +L+ material or give relevant feedback. Then, the information is +L+ summarized, with an event clustering algorithm, to give a concise +L+ representation of the topic and figure out the overview of the +L+ contents. Other multimedia materials, such as representative +L+ images (or image sequences) and geographic information, are +L+ subsequently associated with each event. In the next step, all the +L+ above information is integrated to generate a storyboard, in which +L+ each event is presented as one or multiple slides. An incidental +L+ music, which is also possibly relevant to the topic, is finally +L+ synchronized with the storyboard to improve its expressiveness +L+ and attractiveness. Thus, with these steps, a concise and +L+ informative rich presentation regarding the target topic is gener- +L+ ated. +L+ The rest of the paper is organized as follows. Section 2 discusses +L+ the relevant information extraction corresponding to the target +L+ topic. Section 3 presents our approach to the topic representation, +L+ including multiple events clustering, event description, and +L+ representative media selection. Section 4 describes the approach +L+ to rich presentation generation, including storyboard generation, +L+ incidental music analysis and synchronization. Experiments and +L+ evaluations are presented in the Section 5. Conclusions are given +L+ in the Section 6. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. OBTAINING RELEVANT INFORMATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> To obtain the multimodal information which is relevant to the +L+ input topic (keyword), generally, we could search them from +L+ various databases which have been indexed with the “state-of-the- +L+ art” multimedia analysis techniques. However, in current stage, +L+ there is lack of such publicly available multimedia databases. The +L+ public search engine like MSN or Google indexes all the Internet +L+ web-pages and can return a lot of relevant information, but the +L+ search results usually contain much noise. We could also build a +L+ private database for this system to provide more relevant and +L+ clean results, but it will be too much expensive to collect and +L+ annotate sufficient multimedia data for various topics. In order to +L+ obtain relatively accurate and sufficient data for an arbitrary topic, +L+ in our system, we chose to collect the relevant multimodal +L+ information of the given topic from the news websites such as +L+ MSNBC, BBC and CNN, instead of building an available +L+ database from the scratch. These news websites are usually well +L+ organized and managed; and contain various kinds of high quality +L+ information including text, image and news video clips. Although +L+ the news websites are used as the information sources in our +L+ system, other various multimedia databases can be also easily +L+ incorporated into the system if they are available. +L+ Instead of directly submitting the topic as a query and getting the +L+ returned results by using the search function provided by the +L+ websites, in our system, we crawled the news documents from +L+ these websites in advance and then build a full-text index. It +L+ enables us to quickly obtain the relevant documents, and also en- +L+ able us to use some traditional information retrieval technologies, +L+ such as query expansion [13], to remove the query ambiguousness +L+ and get more relevant documents. +L+ In our approach, user interaction is also allowed to provide more +L+ materials relevant to the topic, or give relevant feedback on the +L+ returned results. For example, from the above websites, we can +L+ seldom find a music clip relevant to the target topic. In this case, +L+ users could provide the system a preferred music, which will be +L+ further used as incidental music to accompany with the storyboard +L+ presentation. Users could also give some feedbacks on the +L+ obtained documents. For example, if he gives a thumb-up to a +L+ document, the relevant information of the document needs to be +L+ presented in the final storyboard. On the other side, users could +L+ also thumb-down a document to remove the related information. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. TOPIC REPRESENTATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> A semantic topic is usually a quite broad concept and it usually +L+ contains multiple events. For example, in the topic “Harry Potter”, +L+ the publication of each book and the release of each movie could +L+ be considered as an event; while in the topic “World Cup 2002”, +L+ each match could also be taken as an event. For each event, there +L+ are usually many documents reporting it. Therefore, in order to +L+ generate an informative and expressive storyboard to present the +L+ topic, it would be better to decompose the obtained information +L+ and cluster the documents into different events. +L+ However, event definition is usually subjective, different +L+ individuals may have different opinions. It is also confusing in +L+ which scale an event should be defined. Also take “World Cup” +L+ as an example, in a larger scale, “World Cup 2002” and “World +L+ Cup 2006” could also be considered as a big event. Therefore, +L+ due to the above vagueness, in this paper, we do not strictly define +L+ </SectLabel_bodyText> <SectLabel_figure> Relevant multimodal information Retrieval +L+ User +L+ Interaction +L+ Music +L+ Text +L+ Relevant Media +L+ Rhythm Analysis +L+ •	Onset/Beat Sequence +L+ •	Strength confidence +L+ Multiple Events Clustering +L+ •	Event summary (4w + time) +L+ •	Geographic information +L+ Media Association +L+ •	Representative images +L+ •	Relevant video clips +L+ Storyboard Generation +L+ Event presentation, multimodal information fusion, layout design +L+ Storyboard +L+ Music and storyboard synchronization +L+ </SectLabel_figure> <SectLabel_page> 746 +L+ </SectLabel_page> <SectLabel_bodyText> each event of the target topic. Following our previous works on +L+ news event detection [14], an event is assumed as some similar +L+ information describing similar persons, similar keywords, similar +L+ places, and similar time duration. Therefore, in our system, an +L+ event is represented by four primary elements: who (persons), +L+ when (time), where (locations) and what (keywords); and event +L+ clustering is to group the documents reporting similar primary +L+ elements. As for the scale of event, in the paper, it could be +L+ adaptively determined by the time range of the obtained +L+ documents or the required event number. +L+ In this section, we present a novel clustering approach based on a +L+ generative model proposed in [14], instead of using traditional +L+ clustering methods such as K-means. After event clusters are +L+ obtained, the corresponding event summary is then extracted and +L+ other representative media is associated with each event. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 Multiple Event Clustering +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To group the documents into different events, essentially, we need +L+ to calculate p(ej I xi), which represents the probability that a docu- +L+ ment xi belongs to an event ej. Here, as mentioned above, an +L+ event ej (and thus the document xi describing the event) is +L+ represented by four primary elements: who (persons), when (time), +L+ where (locations) and what (keywords). That is, +L+ </SectLabel_bodyText> <SectLabel_equation> Event / Docment = {persons, locations, keywords, time} +L+ </SectLabel_equation> <SectLabel_bodyText> Assuming that a document is always caused by an event [14] and +L+ the four primary elements are independent, to calculate the +L+ probability p(ej I xi), in our approach, we first determine the likeli- +L+ hood that the document xi is generated from event ej, p(xi I ej) +L+ which could be further represented by the following generative +L+ model, +L+ </SectLabel_bodyText> <SectLabel_equation> p(xi | ej) =p(namei | ej)p(loci | ej)p(keyi | ej)p(timei | ej) (1) +L+ </SectLabel_equation> <SectLabel_bodyText> where namei, loci, keyi, and timei are the feature vectors +L+ representing persons, locations, keywords and time in the +L+ document xi, respectively. In our approach, the above entities are +L+ extracted by the BBN NLP tools [15]. The tool can extract seven +L+ types of entities, including persons, organizations, locations, date, +L+ time, money and percent. In our approach, the obtained organiza- +L+ tion entity is also considered as a person entity; and all the words +L+ except of persons, locations, and other stop-words are taken as +L+ keywords. +L+ In more detail, namei (similarly, loci and keyi) is a vector <ci1, +L+ ci2, ..., ciNp>, where cin is the occurrence frequency of the personn +L+ appears in the document xi, and personn is the nth person in the +L+ person vocabulary, which is composed of all the persons appeared +L+ in all the obtained documents (similarly, we can define keyword +L+ vocabulary and location vocabulary). Assuming Np is the size of +L+ person vocabulary, p(nameiI ej) could be further expressed by +L+ </SectLabel_bodyText> <SectLabel_equation> Np(namei | ej) = n p(personn | ej )cin (2) +L+ n=1 +L+ </SectLabel_equation> <SectLabel_bodyText> Since the person, location and keyword are discrete variables +L+ represented by words, and the probability of the location and +L+ keyword can be also defined similarly as that of the person in (2), +L+ in the flowing sections, we will not discriminate them and +L+ uniformly represent the probability p(personn | ej) (correspond- +L+ ingly, the p(locationn | ej) and p(keywordn | ej)) as p(wn | ej), which +L+ denotes the probability that the word wn appears in the event ej +L+ On the other hand, the time of an event usually lasts a continuous +L+ duration. It is also observed, especially in the news domain, that +L+ the documents about an event usually increases at the beginning +L+ stage of the event and then decreases at the end. Therefore, in +L+ our approach, a Gaussian model N(uj, aj) is utilized to roughly +L+ represent the probability p(timei | ej), where uj and aj is the mean +L+ and standard deviation, respectively. +L+ To this end, in order to estimate the probability p(ej I xi), we need +L+ to estimate the parameters 6 = {p(wn | ej), uj, σj, 1!�j5K}, assuming +L+ K is the number of events (the selection of K is discussed in +L+ section 3.2). In our approach, the Maximum Likelihood is used to +L+ estimate the model parameters, as, +L+ </SectLabel_bodyText> <SectLabel_equation> θ* = argmaxθ log(p(X |θ)) = +L+ M	K +L+ =argmax θ ∑ log(∑ p(ej)p(xi | ej +L+ ia	j=1 +L+ where X represents the corpus of the obtained documents; M and +L+ K are number of documents and events, respectively. +L+ </SectLabel_equation> <SectLabel_bodyText> Since it is difficult to derive a close formula to estimate the +L+ parameters, in our approach, an Expectation Maximization (EM) +L+ algorithm is applied to maximize the likelihood, by running E-step +L+ and M-step iteratively. A brief summary of these two steps is +L+ listed as follows, and more details can be found in [14]. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	In E-step, the posterior probability p(ej | xi) is estimated as: +L+ </SectLabel_listItem> <SectLabel_equation> p(ej | xi)(t+1) =  p(xi | ej)(t)p(ej)(t)	(4) +L+ p( xi +L+ where the upper script (t) indicate the tth iteration. +L+ </SectLabel_equation> <SectLabel_listItem> •	In M-step, the model parameters are updated, as, +L+ </SectLabel_listItem> <SectLabel_bodyText> where tf(i,n) is the term frequency of the word wn in the +L+ document xi and N is the corresponding vocabulary size. It +L+ is noted that, in (5), the Laplace smoothing [ 16] is applied to +L+ prevent zero probability for the infrequently occurring word. +L+ At last, the prior of each event is updated as: +L+ </SectLabel_bodyText> <SectLabel_equation> M +L+ ∑p ( +L+ p(ej)(t+1) = i=1(8) +L+ M +L+ </SectLabel_equation> <SectLabel_bodyText> arg +L+ The algorithm can increase the log-likelihood consistently with +L+ the iterations; and then converge to a local maximum. Once the +L+ parameters are estimated, we can simply assign each document to +L+ an event, as following +L+ </SectLabel_bodyText> <SectLabel_equation> yi =argmaxj(p(ej |xi))	(9) +L+ where yi is the event label of the document xi. +L+ i=1 +L+ M +L+ t+1) +L+ ( +L+ ( +L+ ) +L+ (t +L+ u +L+ i=1 +L+ ej +L+ +1) +L+ | xi +L+ (6) +L+ ∑p +L+ timei +L+ tf (i, n) +L+ p(wn | ej)(t+1) = 	Mi=1jN	(5) +L+ i=1	s=1 +L+ 1+N+∑(p(e +L+  I x)	' +L+ ∑s)) +L+ tf 0 +L+ , +L+ M +L+ uj +L+ σ2(t+1) =  i=1 	/7) +L+ j	M +L+ l +L+ ( +L+ ∑p +L+ ) +L+ |xi +L+ i +L+ ej +L+ =1 +L+ ∑ p(ej | xi)(t+1) ⋅ (timei − +L+ t+1) 2 +L+ ) +L+ ( +L+ t+1) +L+ M +L+ |θ)) +L+ maxθ log(∏ p(xi +L+ (3) +L+ ,θ)) +L+ ej +L+ | xi +L+ ) (t+1) +L+ </SectLabel_equation> <SectLabel_page> 747 +L+ </SectLabel_page> <SectLabel_bodyText> The advantage of this generative approach is that it not only +L+ considers the temporal continuity of an event, it also can deal with +L+ the issue that some events overlap in some time durations. In this +L+ case, the Gaussian model of the event time can also be overlapped +L+ through this data-driven parameter estimation. From this view, +L+ the event clustering is also like a Gaussian mixture model (GMM) +L+ estimation in the timeline. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 Determining the Number of Events +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the above approach to event clustering, the event number K is +L+ assumed known (as shown in (3)-(8)). However, the event number +L+ is usually very difficult to be determined a priori. In our approach, +L+ an intuitive way is adopted to roughly estimate the event number +L+ based on the document distribution along with the timeline. +L+ As mentioned above, it is assumed that each document is caused +L+ by an event, and the document number of an event changes with +L+ the development of the event. According to this property, each +L+ peak (or the corresponding contour) of the document distribution +L+ curve might indicate one event [14], as the Fig. 2 shows. Thus, we +L+ can roughly estimate the event number by simply counting the +L+ peak number. However, the curve is quite noisy and there +L+ inevitably exist some noisy peaks in the curve. In order to avoid +L+ the noisy peaks, in our approach, only the salient peaks are +L+ assumed to be relevant to the event number. +L+ To detect the salient peaks, we first smooth the document curve +L+ with a half-Hamming (raised-cosine) window, and then remove +L+ the very small peaks with a threshold. Fig.2 illustrates a +L+ smoothed document distribution with the corresponding threshold, +L+ collected on the topic “US Election” in four months. In +L+ experiments, the threshold is adaptively set as Yd-σd/2, where Yd +L+ and ad are the mean and standard deviation of the curve, +L+ respectively. +L+ After the smoothing and tiny peaks removal, we further detect the +L+ valleys between every two contingent peaks. Thus, the range of +L+ an event (which is correlated to the corresponding peak) can be +L+ considered as the envelope in the two valleys. As shown in Fig2, +L+ the duration denoted by Li+Ri is a rough range of the event +L+ correlated to the peak Pi. Assuming an important event usually +L+ has more documents and has effects in a longer duration, the +L+ saliency of each peak is defined as, +L+ </SectLabel_bodyText> <SectLabel_equation> Si =( P )(Li +Ri) (10) +L+ Pavr Davr +L+ </SectLabel_equation> <SectLabel_bodyText> where Pi is the ith peak, Li and Ri is the duration from the ith peak +L+ to the previous and next valley; Pavr is the average peak value and +L+ Davr is average duration between two valleys in the curve. Si is the +L+ saliency value of the peak Pi. It could also be considered as the +L+ normalized area under peak Pi, and thus, it roughly represents the +L+ document number of the corresponding event. +L+ In our approach, the top K salient peaks are selected to determine +L+ the event number: +L+ </SectLabel_bodyText> <SectLabel_equation> K=argmaxk{∑;1Si/∑N1S ≤η}	(11) +L+ where S; is the sorted saliency value from large to small, N is +L+ </SectLabel_equation> <SectLabel_bodyText> total number of detected peaks and ii is a threshold. In our +L+ experiments, ii is set as 0.9, which roughly means that at least +L+ 90% documents will be kept in the further initialization of event +L+ clustering. This selection scheme is designed to guarantee there is +L+ no important information is missed in presentation. After the +L+ event number and initial clusters (the most salient peaks with their +L+ corresponding range) are selected, the event parameters could be +L+ initialized and then updated iteratively. +L+ </SectLabel_bodyText> <SectLabel_figure> 0 20 40 60 80 100 120 +L+ </SectLabel_figure> <SectLabel_figureCaption> Fig.2 Peak saliency definition. It also illustrates the smoothed +L+ document distribution (document number per day) with the +L+ corresponding threshold for tiny peak removal. Each peak Pi is +L+ assumed to be correlated with each event. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> It is noted that some technology such as Bayesian Information +L+ Criteria (BIC) or minimum description length (MDL) [17] could +L+ be used to estimate the optimal event number, by searching +L+ through a reasonable range of the event number to find the one +L+ which maximizes the likelihood in (3). However, these algo- +L+ rithms take long time, and it is usually not necessary to estimate +L+ the exact event number in our scenario of rich presentation. +L+ Actually, in our system, the most important point of event cluster- +L+ ing is that the clustered documents ‘really’ represent the same +L+ event, rather than the event number, as observed in the experi- +L+ ments. Moreover, in the step of synchronization between the +L+ music and storyboard (in the section 4.2), the number of presented +L+ events may be further refined, based on the user’s preference, in +L+ order to match the presentation duration with the music duration. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.3 Event Description +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> After obtaining the events and the corresponding documents, we +L+ not only need a concise event summary, but also need to extract +L+ some representative media to describe each event. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 3.3.1 Event Summary +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> A simple way to summarize an event is to choose some +L+ representative words on the persons, locations and keywords of +L+ the event. For example, for the event ej, the ‘leading actor’ could +L+ be chosen as the person with the maximum p(personn | ej), while +L+ the major location could be selected based on p(locationn | ej). +L+ However, such brief description might have a bad readability. +L+ Therefore, in order to increase the readability of the summary, in +L+ our system, we also provide an alterative way. That is, we choose +L+ a candidate document to represent an event. For example, the +L+ document with the highest p(xi| ej) is a good candidate represen- +L+ tative of the event ej. However, a document might be too long to +L+ be shown on the storyboard. Therefore, in our system, only the +L+ “title-brow” (the text between the news title and news body) of +L+ the document, which usually exists and is usually a good +L+ overview (summary) of the document based on our observation +L+ (especially true in our case of news document), is selected to +L+ describe the event. +L+ </SectLabel_bodyText> <SectLabel_figure> 20 +L+ 15 +L+ 10 +L+ 5 +L+ 0 +L+ Peaks relevant to event +L+ P;-1	P;+1 +L+ L;	R; +L+ P; +L+ #Doc +L+ Threshold +L+ ' +L+ 748 +L+ IV +L+ I +L+ III +L+ II +L+ </SectLabel_figure> <SectLabel_figureCaption> Fig. 3 The event template of the Storyboard, which illustrates (I) the representative media, (II)geographic information, (III) event summary, +L+ and (IV) a film strip giving an overview of the events in the temporal order. +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 3.3.2 Extracting Representative Media +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the obtained documents describing an event, there are usually +L+ many illustrational images, with possible flashes and video clips. +L+ These media information is also a good representative of the +L+ corresponding event. However, since the obtained documents are +L+ directly crawled from the news websites, they usually contain +L+ many noisy multimedia resources, such as the advertisements. +L+ Moreover, there also possible exist some duplicate images in +L+ different documents describing the same event. Therefore, to +L+ extract the representative media from the documents, we need to +L+ remove noisy media and possible duplicate images. Before this, +L+ we also performed a pre-filtering to remove all the images smaller +L+ than 50 pixels in height or width. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Noisy Media Detection. In our approach, a simple but +L+ efficient rule is used to remove the noisy media resources. +L+ We find almost all advertisements are provided by other +L+ agencies rather than these news websites themselves. That is, +L+ the hosts of advertisement resources are from different +L+ websites. Thus, in our approach, we extract the host names +L+ from the URLs of all multimedia resources, and remove +L+ those resources with different host name. +L+ •	Duplicate Detection. A number of image signature schemes +L+ </SectLabel_listItem> <SectLabel_bodyText> can be adopted here to accomplish duplicate detection. In +L+ our implementation, each image is converted into grayscale, +L+ and down-sampled to 8 × 8. That is, a 64-byte signature for +L+ each image is obtained. Then the Euclidean distance of the +L+ 64-byte signature are taken as the dissimilarity measure. +L+ Images have sufficiently small distance are considered as +L+ duplicates. +L+ Once removing the noisy resources and duplicate images, we +L+ simply select the 1-4 large images from the top representative +L+ documents (with the top largest p(xi|ej)), and take them as +L+ representative media of the corresponding event. The exact +L+ number of the selected images is dependent on the document +L+ number (i.e., the importance) of the event and the total image +L+ number the event has. It is noted that, in our current system, we +L+ only associates images with each event. However, other media +L+ like video and flashes can be chosen in a similar way. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. RICH PRESENTATION GENERATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In the proposed system, the above obtained information, including +L+ event summary and representative media, are fused to generate a +L+ concise and informative storyboard, in order to richly present the +L+ target topic. In this section, we will first describe the storyboard +L+ generation for the target topic, by presenting each event with the +L+ multimodal information. Then, we present the approach to +L+ synchronizing the storyboard with an incidental music. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Storyboard Generation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In our approach, a storyboard of a target topic is generated by +L+ presenting each event of the topic slide by slide. To describe an +L+ event, we have obtained the corresponding information including +L+ the person, time, location, event summary and other relevant +L+ images. Therefore, to informatively present each event, we need +L+ first to design an event template (i.e., an interface) to integrate all +L+ the information. +L+ Fig. 3 illustrates the event template used in our proposed system, +L+ with an example event in the topic ‘US Election”. First, the +L+ template presents the representative images in the largest area +L+ (part I), since the pictures are more vivid than the words. As for +L+ each representative picture, the title and date of the document from +L+ which it is extracted is also illustrated. In the Fig.3, there are 4 +L+ pictures extracted from 3 documents. Then, the corresponding +L+ event summaries of these three documents are presented (part III), +L+ where each paragraph refers to the summary of one document. If a +L+ user is interested in one document, he can click on the correspond- +L+ ing title to read more details. Moreover, the geographic informa- +L+ tion of the event is shown with a map in the top-left corner (part +L+ II), to give users a view of the event location. The map is obtained +L+ from “MapPoint Location” service [18], which can return a +L+ </SectLabel_bodyText> <SectLabel_page> 749 +L+ </SectLabel_page> <SectLabel_bodyText> corresponding map based on user’s location query. However, the +L+ mapping is usually difficult, especially when the event location is +L+ confusing so that the representative location is not accurately +L+ detected. For example, the event shown in the Fig 1 is mapped to +L+ Washington D.C. rather than New York where the republic +L+ convention is held, since Washington is the most frequently +L+ mentioned places in the documents. Finally, a film strip (part IV) +L+ is also presented, arranging each event in the temporal order, +L+ where each event is simply represented by a cluster of images, +L+ with the current event highlighted. It enables users to have a quick +L+ overview of the past and the future in the event sequence. +L+ By connecting various events slide by slide, we could get an +L+ informative storyboard regarding the target topic. In order to +L+ catch the development process of a topic, the events are ordered +L+ by their timestamps in the generated storyboard. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Synchronizing with Music +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To make the storyboard more expressive and attractive, and to +L+ provide a more relaxing way to read information, in the proposed +L+ system, we will accompany the storyboard with an incidental +L+ music and align the transitions between event slides with the +L+ music beats, following the idea in music video generation [19][20]. +L+ Sometimes, music could also provide extra information about the +L+ target topic. For example, when the target topic is a movie, the +L+ corresponding theme song could be chosen for the rich presenta- +L+ tion. In this sub-section, we will present our approach to music +L+ analysis and synchronization with the storyboard. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.1 Music Rhythm Analysis +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In the proposed system, we detect the onset sequences instead of +L+ the exact beat series to represent music rhythm. This is because +L+ the beat information is sometimes not obvious, especially in light +L+ music which is usually selected as incidental music. The strongest +L+ onset in a time window could be assumed as a “beat”. This is +L+ reasonable since there are some beat positions in a time window +L+ (for example, 5 seconds); thus, the most possible position of a beat +L+ is the position of the strongest onset. +L+ The process of onset estimation is illustrated in Fig. 4. After FFT +L+ is performed on each frame of 16ms-length, an octave-scale filter- +L+ bank is used to divide the frequency domain into six sub-bands, +L+ including [0, co0 /26), [co0 /26, co0 /25), ..., [co0 /22, co0 /2], where co0 +L+ refers to the sampling rate. +L+ </SectLabel_bodyText> <SectLabel_figure> Onset Curve +L+ </SectLabel_figure> <SectLabel_figureCaption> Fig. 4 The process of onset sequence estimation +L+ </SectLabel_figureCaption> <SectLabel_bodyText> After the amplitude envelope of each sub-band is extracted by +L+ using a half-Hamming window, a Canny operator is used for onset +L+ sequence detection by estimating its difference function, +L+ </SectLabel_bodyText> <SectLabel_equation> Di (n) = Ai (n) ⊗ C(n) (12) +L+ </SectLabel_equation> <SectLabel_bodyText> where Di(n) is the difference function in the ith sub-band, Ai(n) is +L+ the amplitude envelope of the ith sub-band, and C(n) is the Canny +L+ operator with a Gaussian kernel, +L+ </SectLabel_bodyText> <SectLabel_equation> C(n) = i e .2/2σ2 n∈ +L+ σ 2 c c +L+ </SectLabel_equation> <SectLabel_bodyText> where Lc is the length of the Canny operator and a is used to +L+ control the operator’s shape, which are set as 12 and 4 in our +L+ implementation, respectively. +L+ Finally, the sum of the difference curves of these six sub-bands is +L+ used to extract onset sequence. Each peak is considered as an +L+ onset, and the peak value is considered as the onset strength. +L+ Based on the obtained onsets, an incidental music is further +L+ segmented into music sub-clips, where a strong onset is taken as +L+ the boundary of a music sub-clip. These music sub-clips are then +L+ used as the basic timeline for the synchronization in the next step. +L+ Thus, to satisfy the requirement that the event slide transitions of +L+ the storyboard should occur at the music beats, we just need to +L+ align the event slide boundaries and music sub-clip boundaries. +L+ To give a more pleasant perception, the music sub-clip should not +L+ be too short or too long, also it had better not always keep the +L+ same length. In our implementation, the length of music sub-clips +L+ is randomly selected in a range of [tmin, tmax] seconds. Thus, the +L+ music sub-clips can be extracted in the following way: given the +L+ previous boundary, the next boundary is selected as the strongest +L+ onset in the window which is [tmin, tmax] seconds away from the +L+ previous boundary. In the proposed system, users can manually +L+ specify the range of the length of the music sub-clip. The default +L+ range in the system is set as [12, 18] seconds, in order to let users +L+ have enough time to read all the information on each event slide. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.2 Alignment Scheme +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> To synchronize the transitions between different event slides and +L+ the beats of the incidental music, as mentioned above, we actually +L+ need to align the slide boundaries and music sub-clip boundaries. +L+ To satisfy this requirement, a straightforward way is to set the +L+ length of each event slide be equal to the corresponding length of +L+ the sub-music clip. +L+ However, as Fig. 5 illustrates, the number of event slides is +L+ usually not equal to the number of music sub-clip. In this case, in +L+ our proposed system, we provide two schemes to solve this +L+ problem. +L+ </SectLabel_bodyText> <SectLabel_listItem> 1) Music Sub-clip Based. In this scheme, only the top N important +L+ events of the target topic are adaptively chosen and used in the +L+ rich presentation, where N is supposed as the number of music +L+ sub-clip in the corresponding incidental music, as the Fig.5 shows. +L+ Although a formal definition of event importance is usually hard +L+ and subjective, in our approach, the importance score of an event +L+ is simply measured by the number of documents reporting it, +L+ assuming that the more important the event, the more the +L+ corresponding documents. The assumption is quite similar as that +L+ in the definition of (10). +L+ </SectLabel_listItem> <SectLabel_figure> Acoustic Music Data +L+ FFT +L+ Difference curve +L+ Sub-Band 1 +L+ Envelope +L+ Extractor +L+ ...	...	... +L+ . +L+ . +L+ . +L+ . +L+ . +L+ . +L+ Difference curve +L+ Sub-Band N +L+ Envelope +L+ Extractor +L+ ] +L+ (13) +L+ </SectLabel_figure> <SectLabel_page> 750 +L+ </SectLabel_page> <SectLabel_listItem> 2) Specified Event Number Based. In this scheme, users can +L+ specify the number of the event he wants to learn. For example, a +L+ user could choose to show the top 30 important events or all the +L+ events. Thus, to accommodate all the events in the music duration, +L+ we will repeat the incidental music if it is needed and then fade out +L+ the music at the end. +L+ </SectLabel_listItem> <SectLabel_bodyText> Fig. 5 Music and storyboard synchronization: a music sub-slip +L+ based scheme, that is, only the top important events are presented +L+ to match the number of music sub-clips. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.2.3 Rendering +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> After the alignment between storyboard and incidental music, in +L+ our system, fifteen common transition effects, such as cross-fade, +L+ wipe and dissolve, are also randomly selected to connect the event +L+ slides, producing a better rich presentation in final rendering. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. EVALUATIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we evaluate the performance of the proposed +L+ approach to rich presentation and its key component, event +L+ clustering. In the experiments, we randomly select 8 topics of +L+ different types, including Earthquake, Halloween, Air Disaster, +L+ US Election, Nobel Prize, Britney Spears, David Beckham, and +L+ Harry Potter, from some hot news topics in the end of 2004 and +L+ beginning of 2005. Once the topic is selected, the topic name is +L+ used as a query and the relevant documents are collected from +L+ CNN, MSNBC and BBC. More details about the selected topics +L+ and the corresponding documents are shown in the Table 1, which +L+ lists the topic name, the time range of the collected documents, +L+ and the number of documents and its corresponding events. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 1. A list of testing topics in the rich presentation evaluations +L+ </SectLabel_tableCaption> <SectLabel_table> No.	Topic	Time	#doc	#event +L+ 1	Earthquake	1995-2004	976	17 +L+ 2	Halloween	1995-2004	762	9 +L+ 3	Air Disaster	1995-2004	210	13 +L+ 4	US Election	1995-2004	2486	— +L+ 5	Britney Spears	2000-2004	1311	— +L+ 6	Nobel Prize	1995-2004	186	— +L+ 7	David Beckham	1995-2004	877	— +L+ 8	Harry Potter	2000-2004	841	— +L+ Total	——	——	7649	— +L+ </SectLabel_table> <SectLabel_bodyText> It is noted that, in the table, only 3 topics have labeled events, +L+ while another 5 topics have not. This is because that, the labeling +L+ work of a topic is very subjective and usually hard for individuals +L+ to manually decide the event number of a given topic. Therefore, +L+ we only label the topics which are easily to be annotated based on +L+ the criterion in Topic Detection and Tracking (TDT) project [21]. +L+ For example, Halloween is a topic which is reported once a year, +L+ thus, each year's documents can be regarded as an event; as for +L+ Earthquake and Air Disaster, their events lists could be found +L+ from corresponding official websites. In the annotation, we +L+ remove the events which do not have or have few (less than 4) +L+ relevant documents, and also remove the documents not belonging +L+ to any events. +L+ After parsing the obtained documents, for each topic, we usually +L+ can obtain 3.8 images per document in average. With further +L+ duplicate detection, only 1.6 images per document are remained. +L+ Moreover, from each document, we could also obtain about 3.0 +L+ unique location entities and 2.8 unique name entities. Other words +L+ except of these entities are taken as keywords. Fig.6 shows a real +L+ representation of an example document with extracted entities in +L+ the XML format, from which the event clustering is performed. +L+ </SectLabel_bodyText> <SectLabel_figure> <URL>http://news.bbc.co.uk/1/hi/world/americas/4071845.stm </URL> +L+ <Abstract>The US battleground state of Ohio has certified the victory +L+ of President George W Bush's in last month's poll. </Abstract> +L+ <Date> 2004/12/6 </Date> +L+ <NLPRESULT> +L+ <LOCATION> +L+ <entity> Ohio </entity> <freq>4</freq> +L+ <entity> US </entity> <freq> 2 </freq> +L+ </LOCATION> +L+ <PERSON> +L+ <entity> Bush </entity> <freq> 3 </freq> +L+ <entity>David Cobb</entity> <freq>1</freq> +L+ ... +L+ </PERSON> +L+ ... +L+ <DATE> +L+ <entity> 6 December, 200</entity> <freq> 1 </freq> +L+ <entity> Friday </entity> <freq> 2 </freq> +L+ ... +L+ </DATE> +L+ <KEYWORDS> +L+ ... +L+ <entity> recount </entity> <freq>7</freq> +L+ <entity> elect </entity> <freq>3</freq> +L+ <entity> America </entity> <freq>3</freq> +L+ <entity> poll </entity> <freq>3</freq> +L+ ... +L+ </KEYWORDS> +L+ </NLPRESULT> +L+ </SectLabel_figure> <SectLabel_figureCaption> Fig. 6. XML representation of a document on “US Election” with +L+ extracted entities +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 5.1 Event Clustering +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As mentioned above, the evaluation of the approach to event +L+ clustering is evaluated on three topics, including Earthquake, Hal- +L+ loween, and Air Disaster, for which the corresponding event num- +L+ bers are determined and the documents are labeled using a similar +L+ method in the TDT project. However, in the proposed appraoch, +L+ we actually do not estimate the optimal event number, but use a +L+ much larger one. Therefore, in order to better evaluate the +L+ performance of the event clustering algorithm and compare with +L+ its counterpart, we use the event number in the ground truth to +L+ initialize the cluster number in the proposed clustering algorithm. +L+ </SectLabel_bodyText> <SectLabel_figure> Event +L+ Slide List +L+ Music +L+ Sub-Clip +L+ E1 +L+ S1 +L+ E2 +L+ S2 +L+ E3 +L+ S3 +L+ E4 +L+ S4 +L+ E5 +L+ S5 +L+ E6 +L+ E7 +L+ E8 +L+ </SectLabel_figure> <SectLabel_page> 751 +L+ </SectLabel_page> <SectLabel_bodyText> In the experiments, K-means, which is another frequently used +L+ clustering algorithm (as well in TDT [22]), is adopted to compare +L+ with the proposed approach. The comparison results of two +L+ clustering approaches are illustrated in Table 2, with precision and +L+ recall for each topic. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 2. The performance comparison between our approach and +L+ K-means on the event clustering +L+ </SectLabel_tableCaption> <SectLabel_table> 	Precision		Recall +L+ 	K-means	Ours	K-means	Ours +L+ Earthquake	0.74	0.87	0.63	0.74 +L+ Halloween	0.88	0.93	0.72	0.81 +L+ Air Disaster	0.57	0.68	0.55	0.61 +L+ Average	0.73	0.83	0.63	0.72 +L+ </SectLabel_table> <SectLabel_bodyText> From Table 2, it can be seen that the results of our approach are +L+ significantly better than those of K-means, both on precision and +L+ recall. On the three testing topics, the average precision of our +L+ approach is up to 0.83 and the average recall achieves 0.72, which +L+ is 10% and 9% higher than those of K-means, respectively. By +L+ tracing the process of K-means, we find that K-means usually +L+ assigns documents far away from each other on the timeline into +L+ the same cluster, since the time information affects little in K- +L+ means. It also indicates the advantages of our approach with time +L+ modeling. +L+ The algorithms also show different performance on different kind +L+ topics. As for the “Air disaster”, its performance is not as good as +L+ that of the other two, since the features (words and time) of its +L+ events are more complicated and intertwined in the feature space. +L+ As for the topics (4-8 in Table I) which could not have an +L+ objective evaluation, the clustering performance on these topics +L+ could be indirectly reflected by the subjective evaluation of the +L+ rich presentation presented in section 5.2. This is because users +L+ will be more satisfied when the grouped documents shown in each +L+ event slide really belong to the same event; while users are not +L+ satisfied if the documents from different events are mixed in one +L+ event slide. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Rich Presentation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> It is usually difficult to find a quantitative measure for rich +L+ presentation, since the assessment of the goodness of rich presen- +L+ tation is a strong subjective task. In this paper, we carry out a pre- +L+ liminary user study to evaluate the performance of the proposed +L+ rich presentation schemes. +L+ To indicate the performance of rich presentation, we design two +L+ measures in the experiments, including ‘informativeness’ and +L+ ‘enjoyablity’, following the criteria used in the work [7]. Here, the +L+ informativeness measures whether the subjects satisfy with the +L+ information obtained from the rich presentation; while enjoyablity +L+ indicates if users feel comfortable and enjoyable when they are +L+ reading the rich presentation. In evaluating the informativeness, +L+ we also provide the documents from which the rich presentation is +L+ generated. They are used as baseline, based on which the subjects +L+ can more easily evaluate if the important overview information +L+ contained in the documents is conveyed by the rich presentation. +L+ Moreover, in order to reveal the subjects’ opinion on the design of +L+ the storyboard template, like the one shown in Fig 3, we also ask +L+ the subjects to evaluate the ‘interface design’. +L+ In the user study, 10 volunteered subjects including 8 males and 2 +L+ females are invited. The subjects are around 20-35 years old, have +L+ much experience on computer manipulation, and usually read +L+ news on web in their leisure time. We ask them to give a +L+ subjective score between 1 and 5 for each measure of the rich +L+ presentation of each testing topic (an exception is ‘interface +L+ design’, which is the same for each rich presentation). Here, the +L+ score ‘1’ to ‘5’ stands for unsatisfied (1), somewhat unsatisfied (2), +L+ acceptable (3), satisfied (4) and very satisfied (5), respectively. +L+ In experiments, we first check with the ‘interface design’ measure. +L+ We find 7 out of 10 subjects satisfy with the event template design +L+ and the left three also think it is acceptable. The average score is +L+ up to 3.9. An interesting observation is that, some subjects like +L+ the template design very much at the first glance, but they feel a +L+ little boring after they finish all the user study since every slide in +L+ the rich presentation of each topic has the same appearance. It +L+ hints us that we had better design different templates for different +L+ topics to make the rich presentation more attractive. +L+ As for the other two measures, we average the score across all the +L+ subjects to represent the performance for each topic, and list the +L+ detailed results in Table 3. It can be seen that the average score of +L+ both enjoyablity and informativeness achieves 3.7, which indicates +L+ that most subjects satisfy the provided overview information of the +L+ target topic, and they enjoy themselves when reading these rich +L+ presentations. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 3. The evaluation results of rich presentation on each topic +L+ </SectLabel_tableCaption> <SectLabel_table> No.	Topic	Informative	Enjoyable +L+ 1	Earthquake	4.3	3.2 +L+ 2	Halloween	3.6	4.0 +L+ 3	Air Disaster	4.0	3.4 +L+ 4	US Election	4.1	4.0 +L+ 5	Britney Spears	3.6	4.1 +L+ 6	Nobel Prize	3.3	3.4 +L+ 7	David Beckham	3.4	4.0 +L+ 8	Harry Potter	3.3	3.4 +L+ Average		3.7	3.7 +L+ </SectLabel_table> <SectLabel_bodyText> In the experiments, we find informativeness is highly depended on +L+ the correlation between the presented documents and the target +L+ topic. If the presented information is consistent with the topic, +L+ subjects usually give a high score for informativeness, such as +L+ those on Earthquake and US Election; otherwise, they will give a +L+ low score, like those on David Beckham and Nobel Prize. It +L+ indicates that it is quite important to provide users clean +L+ information of the target topic with less noise. However, in +L+ current system, the documents are crawled from web and +L+ inevitably contain many noises. It affects much on the perform- +L+ ance of informativeness in the current system. We need to consider +L+ how to prone the information of the target topic in the future +L+ works. +L+ We also find that the enjoyablity score is usually related with +L+ informativeness. If the subjects do not get enough information +L+ from the rich presentation, they will be not enjoyable as well, such +L+ as the topics of Nobel Prize and Harry Potter. Enjoyablity is also +L+ topic-related, the subjects usually feel unconformable when they +L+ are facing with miserable topics, such as Earthquake and Air +L+ Disaster, although their informativeness is quite high. On the +L+ </SectLabel_bodyText> <SectLabel_page> 752 +L+ </SectLabel_page> <SectLabel_bodyText> contrary, users give a high score for enjoyablity on the interesting +L+ topics, such as Britney Spears and David Beckham, although their +L+ informative score is not high. This is because that there are +L+ usually many funny and interesting pictures in the presentation of +L+ these topics. Another finding is that users usually fell unenjoyable +L+ if the images and summaries in one event slide are not consistent +L+ with each other. From this view, the high enjoyablity score in our +L+ experiments also indicates that our event clustering algorithm +L+ works promisingly +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> To facilitate users to quickly grasp and go through the content of a +L+ semantic topic, in this paper, we have proposed a novel approach +L+ to rich presentation to generate a concise and informative +L+ storyboard for the target topic, with many relevant multimodal +L+ information including image, text, audio and video. In this +L+ approach, the related multimodal information of a given topic is +L+ first extracted from news databases. Then, the events are clustered, +L+ and the corresponding information, such as representative images, +L+ geographic information, and event summary, is obtained. The +L+ information is composed into an attractive storyboard which is +L+ finally synchronized with incidental music. A user study indicates +L+ that the presented system works well on our testing examples. +L+ There is still some room for improving the proposed approach. +L+ First, the proposed approach could be extended to other +L+ multimedia databases or more general websites. For example, +L+ some standard multimedia database like NIST TRECVID could +L+ provide a nice platform for the implementation and evaluation of +L+ event detection and rich presentation. Second, to integrate more +L+ relevant multimedia information (such as video clips and flashes) +L+ and more accurate information regarding the target topic is highly +L+ expected by users. Thus, more advanced information retrieval/ +L+ extraction techniques and other multimedia analysis techniques are +L+ needed to be exploited and integrated, such as relevance ranking, +L+ mapping schemes, important or representative video clips +L+ detection and video clip summarization. We also need to design a +L+ much natural way to incorporate video clips in the event template. +L+ Third, we also consider designing various storyboard templates for +L+ different kind of topics. For example, each topic may be belonging +L+ to different clusters such as politics, sports and entertainments, +L+ each of which can have a representative template. Forth, +L+ appropriate user interaction will be added to further make the +L+ storyboard more interactive and easy to control. Finally, a +L+ thorough evaluation will be implemented to evaluate the effect of +L+ each component in the framework and storyboard template. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] A. Vailaya, M.A.T. Figueiredo, A. K. Jain, and H.-J. Zhang. +L+ “Image classification for content-based indexing”. IEEE +L+ Transactions on Image Processing, Vol. 10, Iss.1, 2001 +L+ [2] F. J., M.-J. Li, H.-J. Zhang, and B. Zhang. “An effective +L+ region-based image retrieval framework”. Proc. ACM +L+ Multimedia’02, pp. 456-465, 2002 +L+ [3] J. Platt “AutoAlbum: Clustering Digital Photographs using +L+ Probabilistic Model Merging” Proc. IEEE Workshop on +L+ Content-Based Access of Image and Video Libraries, pp. 96– +L+ 100, 2000. +L+ [4] A. Hanjalic, R. L. Lagendijk, J. Biemond, “Automated high- +L+ level movie segmentation for advanced video-retrieval +L+ systems”, IEEE Trans on Circuits and Systems For Video +L+ Technology, Vol. 9, No. 4, pp. 580-588, 1999. +L+ [5] J. Assfalg and et al, “Semantic annotation of soccer videos: +L+ automatic highlights identification," CVIU'03, vol. 92, pp. +L+ 285-305, 2003. +L+ [6] A. Ekin, A. M. Tekalp, and R. Mehrotra, "Automatic soccer +L+ video analysis and summarization," IEEE Trans. on Image +L+ Processing, 12(7), pp. 796-807, 2003. +L+ [7] Y. -F. Ma, L. Lu, H. -J. Zhang, and M.-J Li. “A User +L+ Attention Model for Video Summarization”. ACM +L+ Multimeida’02, pp. 533-542, 2002. +L+ [8] L. Xie, P. Xu, S.F. Chang, A. Divakaran, and H. Sun, +L+ "Structure analysis of soccer video with domain knowledge +L+ and hidden markov models," Pattern Recognition Letters, +L+ vol. 25(7), pp. 767-775, 2004. +L+ [9] L. Lu, H. Jiang, H. J. Zhang, “A Robust Audio Classification +L+ and Segmentation Method,” Proc. ACM Multimedia’01, pp. +L+ 203-211, 2001 +L+ [10] R. Cai, L. Lu, H.-J. Zhang, and L.-H. Cai, “Highlight Sound +L+ Effects Detection in Audio Stream,” Proc. ICME’03 Vol.3, +L+ pp.37-40, 2003. +L+ [11] Y. Rui, A. Gupta, and A. Acero, “Automatically Extracting +L+ Highlights for TV Baseball Programs”, Proc. ACM Multi- +L+ media’00, pp. 105-115, 2000. +L+ [12] C. Snoek, and M. Worring. “Multimodal Video Indexing: A +L+ Review of the State-of-the-art”. Multimedia Tools and +L+ Applications, Vol. 25, No. 1 pp. 5 – 35, 2005 +L+ [13] E.M. Voorhees, “Query expansion using lexical-semantic +L+ relations” Proc. ACM SIGIR Conference on Research and +L+ Development in Information Retrieval , pp 61 - 69, 1994 +L+ [14] Z.-W. Li, M.-J. Li, and W.-Y. Ma. "A Probabilistic Model for +L+ Retrospective News Event Detection”, Proc. SIGIR +L+ Conference on Research and Development in Information +L+ Retrieval, 2005 +L+ [15] D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. “An +L+ Algorithm That Learns What’s in a Name”. Machine +L+ Learning, 34(1-3), 1999 +L+ [16] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. “Text +L+ Classification from Labeled and Unlabeled Documents using +L+ EM”. Machine Learning, 39(2-3), 2000 +L+ [17] T. Hastie, R. Tibshirani, and J. Friedman. “The Elements of +L+ Statistical Learning: Data Mining, Inference and Prediction”. +L+ Springer-Verlag, 2001 +L+ [18] MapPoint Web Service http://www.microsoft.com/mappoint/ +L+ products/ webservice/default.mspx +L+ [19] X.-S. Hua, L. Lu, H.-J. Zhang. "Automated Home Video +L+ Editing", Proc. ACM Multimedia’03, pp. 490-497, 2003 +L+ [20] J. Foote, M. Cooper, and A. Girgensohn. “Creating Music +L+ Videos Using Automatic Media Analysis”. ACM +L+ Multimedia’02, pp.553-560, 2002. +L+ [21] Topic Detection and Tracking (TDT) Project: http://www. +L+ nist.gov/speech/tests/tdt/ +L+ [22] J. Allan, R. Papka, and V. Lavrenko. “On-line New Event +L+ Detection and Tracking”. Proc. SIGIR Conference on +L+ Research and Development in Information Retrieval 98, +L+ pp.37-45, 1998 +L+ </SectLabel_reference> <SectLabel_page> 753 +L+ </SectLabel_page>
<SectLabel_title> Automatic Extraction of Titles from General Documents +L+ using Machine Learning +L+ </SectLabel_title> <SectLabel_author> Yunhua Hu1 +L+ </SectLabel_author> <SectLabel_affiliation> Computer Science Department +L+ Xi’an Jiaotong University +L+ </SectLabel_affiliation> <SectLabel_address> No 28, Xianning West Road +L+ Xi'an, China, 710049 +L+ </SectLabel_address> <SectLabel_email> yunhuahu@mail.xjtu.edu.cn +L+ </SectLabel_email> <SectLabel_author> Hang Li, Yunbo Cao +L+ </SectLabel_author> <SectLabel_affiliation> Microsoft Research Asia +L+ </SectLabel_affiliation> <SectLabel_address> 5F Sigma Center, +L+ No. 49 Zhichun Road, Haidian, +L+ Beijing, China, 100080 +L+ </SectLabel_address> <SectLabel_email> {hangli,yucao}@microsoft.com +L+ </SectLabel_email> <SectLabel_author> Dmitriy Meyerzon +L+ </SectLabel_author> <SectLabel_affiliation> Microsoft Corporation +L+ </SectLabel_affiliation> <SectLabel_address> One Microsoft Way +L+ Redmond, WA, +L+ USA, 98052 +L+ </SectLabel_address> <SectLabel_email> dmitriym@microsoft.com +L+ </SectLabel_email> <SectLabel_author> Qinghua Zheng +L+ </SectLabel_author> <SectLabel_affiliation> Computer Science Department +L+ Xi’an Jiaotong University +L+ </SectLabel_affiliation> <SectLabel_address> No 28, Xianning West Road +L+ Xi'an, China, 710049 +L+ </SectLabel_address> <SectLabel_email> qhzheng@mail.xjtu.edu.cn +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we propose a machine learning approach to title +L+ extraction from general documents. By general documents, we +L+ mean documents that can belong to any one of a number of +L+ specific genres, including presentations, book chapters, technical +L+ papers, brochures, reports, and letters. Previously, methods have +L+ been proposed mainly for title extraction from research papers. It +L+ has not been clear whether it could be possible to conduct +L+ automatic title extraction from general documents. As a case study, +L+ we consider extraction from Office including Word and +L+ PowerPoint. In our approach, we annotate titles in sample +L+ documents (for Word and PowerPoint respectively) and take them +L+ as training data, train machine learning models, and perform title +L+ extraction using the trained models. Our method is unique in that +L+ we mainly utilize formatting information such as font size as +L+ features in the models. It turns out that the use of formatting +L+ information can lead to quite accurate extraction from general +L+ documents. Precision and recall for title extraction from Word is +L+ 0.810 and 0.837 respectively, and precision and recall for title +L+ extraction from PowerPoint is 0.875 and 0.895 respectively in an +L+ experiment on intranet data. Other important new findings in this +L+ work include that we can train models in one domain and apply +L+ them to another domain, and more surprisingly we can even train +L+ models in one language and apply them to another language. +L+ Moreover, we can significantly improve search ranking results in +L+ document retrieval by using the extracted titles. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> H.3.3 [Information Storage and Retrieval]: Information Search +L+ and Retrieval - Search Process; H.4.1 [Information Systems +L+ </SectLabel_category> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, or +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> JCDL’05, June 7–11, 2005, Denver, Colorado, USA +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2005 ACM 1-58113-876-8/05/0006...$5.00. +L+ Applications]: Office Automation - Word processing; D.2.8 +L+ [Software Engineering]: Metrics - complexity measures, +L+ performance measures +L+ </SectLabel_copyright> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Algorithms, Experimentation, Performance. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> information extraction, metadata extraction, machine learning, +L+ search +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Metadata of documents is useful for many kinds of document +L+ processing such as search, browsing, and filtering. Ideally, +L+ metadata is defined by the authors of documents and is then used +L+ by various systems. However, people seldom define document +L+ metadata by themselves, even when they have convenient +L+ metadata definition tools [26]. Thus, how to automatically extract +L+ metadata from the bodies of documents turns out to be an +L+ important research issue. +L+ Methods for performing the task have been proposed. However, +L+ the focus was mainly on extraction from research papers. For +L+ instance, Han et al. [10] proposed a machine learning based +L+ method to conduct extraction from research papers. They +L+ formalized the problem as that of classification and employed +L+ Support Vector Machines as the classifier. They mainly used +L+ linguistic features in the model. +L+ In this paper, we consider metadata extraction from general +L+ documents. By general documents, we mean documents that may +L+ belong to any one of a number of specific genres. General +L+ documents are more widely available in digital libraries, intranets +L+ and the internet, and thus investigation on extraction from them is +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 The work was conducted when the first author was visiting +L+ Microsoft Research Asia. +L+ </SectLabel_footnote> <SectLabel_page> 145 +L+ </SectLabel_page> <SectLabel_bodyText> sorely needed. Research papers usually have well-formed styles +L+ and noticeable characteristics. In contrast, the styles of general +L+ documents can vary greatly. It has not been clarified whether a +L+ machine learning based approach can work well for this task. +L+ There are many types of metadata: title, author, date of creation, +L+ etc. As a case study, we consider title extraction in this paper. +L+ General documents can be in many different file formats: +L+ Microsoft Office, PDF (PS), etc. As a case study, we consider +L+ extraction from Office including Word and PowerPoint. +L+ We take a machine learning approach. We annotate titles in +L+ sample documents (for Word and PowerPoint respectively) and +L+ take them as training data to train several types of models, and +L+ perform title extraction using any one type of the trained models. +L+ In the models, we mainly utilize formatting information such as +L+ font size as features. We employ the following models: Maximum +L+ Entropy Model, Perceptron with Uneven Margins, Maximum +L+ Entropy Markov Model, and Voted Perceptron. +L+ In this paper, we also investigate the following three problems, +L+ which did not seem to have been examined previously. +L+ </SectLabel_bodyText> <SectLabel_listItem> (1) Comparison between models: among the models above, which +L+ model performs best for title extraction; +L+ (2) Generality of model: whether it is possible to train a model on +L+ one domain and apply it to another domain, and whether it is +L+ possible to train a model in one language and apply it to another +L+ language; +L+ (3) Usefulness of extracted titles: whether extracted titles can +L+ improve document processing such as search. +L+ </SectLabel_listItem> <SectLabel_bodyText> Experimental results indicate that our approach works well for +L+ title extraction from general documents. Our method can +L+ significantly outperform the baselines: one that always uses the +L+ first lines as titles and the other that always uses the lines in the +L+ largest font sizes as titles. Precision and recall for title extraction +L+ from Word are 0.810 and 0.837 respectively, and precision and +L+ recall for title extraction from PowerPoint are 0.875 and 0.895 +L+ respectively. It turns out that the use of format features is the key +L+ to successful title extraction. +L+ </SectLabel_bodyText> <SectLabel_listItem> (1) We have observed that Perceptron based models perform +L+ better in terms of extraction accuracies. (2) We have empirically +L+ verified that the models trained with our approach are generic in +L+ the sense that they can be trained on one domain and applied to +L+ another, and they can be trained in one language and applied to +L+ another. (3) We have found that using the extracted titles we can +L+ significantly improve precision of document retrieval (by 10%). +L+ </SectLabel_listItem> <SectLabel_bodyText> We conclude that we can indeed conduct reliable title extraction +L+ from general documents and use the extracted results to improve +L+ real applications. +L+ The rest of the paper is organized as follows. In section 2, we +L+ introduce related work, and in section 3, we explain the +L+ motivation and problem setting of our work. In section 4, we +L+ describe our method of title extraction, and in section 5, we +L+ describe our method of document retrieval using extracted titles. +L+ Section 6 gives our experimental results. We make concluding +L+ remarks in section 7. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2. RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> 2.1 Document Metadata Extraction +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Methods have been proposed for performing automatic metadata +L+ extraction from documents; however, the main focus was on +L+ extraction from research papers. +L+ The proposed methods fall into two categories: the rule based +L+ approach and the machine learning based approach. +L+ Giuffrida et al. [9], for instance, developed a rule-based system for +L+ automatically extracting metadata from research papers in +L+ Postscript. They used rules like “titles are usually located on the +L+ upper portions of the first pages and they are usually in the largest +L+ font sizes”. Liddy et al. [14] and Yilmazel el al. [23] performed +L+ metadata extraction from educational materials using rule-based +L+ natural language processing technologies. Mao et al. [16] also +L+ conducted automatic metadata extraction from research papers +L+ using rules on formatting information. +L+ The rule-based approach can achieve high performance. However, +L+ it also has disadvantages. It is less adaptive and robust when +L+ compared with the machine learning approach. +L+ Han et al. [10], for instance, conducted metadata extraction with +L+ the machine learning approach. They viewed the problem as that +L+ of classifying the lines in a document into the categories of +L+ metadata and proposed using Support Vector Machines as the +L+ classifier. They mainly used linguistic information as features. +L+ They reported high extraction accuracy from research papers in +L+ terms of precision and recall. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.2 Information Extraction +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Metadata extraction can be viewed as an application of +L+ information extraction, in which given a sequence of instances, we +L+ identify a subsequence that represents information in which we +L+ are interested. Hidden Markov Model [6], Maximum Entropy +L+ Model [1, 4], Maximum Entropy Markov Model [17], Support +L+ Vector Machines [3], Conditional Random Field [12], and Voted +L+ Perceptron [2] are widely used information extraction models. +L+ Information extraction has been applied, for instance, to part-of- +L+ speech tagging [20], named entity recognition [25] and table +L+ extraction [19]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.3 Search Using Title Information +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Title information is useful for document retrieval. +L+ In the system Citeseer, for instance, Giles et al. managed to +L+ extract titles from research papers and make use of the extracted +L+ titles in metadata search of papers [8]. +L+ In web search, the title fields (i.e., file properties) and anchor texts +L+ of web pages (HTML documents) can be viewed as ‘titles’ of the +L+ pages [5]. Many search engines seem to utilize them for web page +L+ retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with +L+ well-defined metadata are more easily retrieved than those without +L+ well-defined metadata [24]. +L+ To the best of our knowledge, no research has been conducted on +L+ using extracted titles from general documents (e.g., Office +L+ documents) for search of the documents. +L+ </SectLabel_bodyText> <SectLabel_page> 146 +L+ </SectLabel_page> <SectLabel_sectionHeader> 3. MOTIVATION AND PROBLEM +L+ SETTING +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We consider the issue of automatically extracting titles from +L+ general documents. +L+ By general documents, we mean documents that belong to one of +L+ any number of specific genres. The documents can be +L+ presentations, books, book chapters, technical papers, brochures, +L+ reports, memos, specifications, letters, announcements, or resumes. +L+ General documents are more widely available in digital libraries, +L+ intranets, and internet, and thus investigation on title extraction +L+ from them is sorely needed. +L+ Figure 1 shows an estimate on distributions of file formats on +L+ intranet and internet [15]. Office and PDF are the main file +L+ formats on the intranet. Even on the internet, the documents in the +L+ formats are still not negligible, given its extremely large size. In +L+ this paper, without loss of generality, we take Office documents as +L+ an example. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1. Distributions of file formats in internet and intranet. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> For Office documents, users can define titles as file properties +L+ using a feature provided by Office. We found in an experiment, +L+ however, that users seldom use the feature and thus titles in file +L+ properties are usually very inaccurate. That is to say, titles in file +L+ properties are usually inconsistent with the ‘true’ titles in the file +L+ bodies that are created by the authors and are visible to readers. +L+ We collected 6,000 Word and 6,000 PowerPoint documents from +L+ an intranet and the internet and examined how many titles in the +L+ file properties are correct. We found that surprisingly the accuracy +L+ was only 0.265 (cf., Section 6.3 for details). A number of reasons +L+ can be considered. For example, if one creates a new file by +L+ copying an old file, then the file property of the new file will also +L+ be copied from the old file. +L+ In another experiment, we found that Google uses the titles in file +L+ properties of Office documents in search and browsing, but the +L+ titles are not very accurate. We created 50 queries to search Word +L+ and PowerPoint documents and examined the top 15 results of +L+ each query returned by Google. We found that nearly all the titles +L+ presented in the search results were from the file properties of the +L+ documents. However, only 0.272 of them were correct. +L+ Actually, ‘true’ titles usually exist at the beginnings of the bodies +L+ of documents. If we can accurately extract the titles from the +L+ bodies of documents, then we can exploit reliable title information +L+ in document processing. This is exactly the problem we address in +L+ this paper. +L+ More specifically, given a Word document, we are to extract the +L+ title from the top region of the first page. Given a PowerPoint +L+ document, we are to extract the title from the first slide. A title +L+ sometimes consists of a main title and one or two subtitles. We +L+ only consider extraction of the main title. +L+ As baselines for title extraction, we use that of always using the +L+ first lines as titles and that of always using the lines with largest +L+ font sizes as titles. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2. Title extraction from Word document. +L+ Figure 3. Title extraction from PowerPoint document. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Next, we define a ‘specification’ for human judgments in title data +L+ annotation. The annotated data will be used in training and testing +L+ of the title extraction methods. +L+ Summary of the specification: The title of a document should be +L+ identified on the basis of common sense, if there is no difficulty in +L+ the identification. However, there are many cases in which the +L+ identification is not easy. There are some rules defined in the +L+ specification that guide identification for such cases. The rules +L+ include “a title is usually in consecutive lines in the same format”, +L+ “a document can have no title”, “titles in images are not +L+ considered”, “a title should not contain words like ‘draft’, +L+ </SectLabel_bodyText> <SectLabel_page> 147 +L+ </SectLabel_page> <SectLabel_bodyText> ‘whitepaper’, etc”, “if it is difficult to determine which is the title, +L+ select the one in the largest font size”, and “if it is still difficult to +L+ determine which is the title, select the first candidate”. (The +L+ specification covers all the cases we have encountered in data +L+ annotation.) +L+ Figures 2 and 3 show examples of Office documents from which +L+ we conduct title extraction. In Figure 2, ‘Differences in Win32 +L+ API Implementations among Windows Operating Systems’ is the +L+ title of the Word document. ‘Microsoft Windows’ on the top of +L+ this page is a picture and thus is ignored. In Figure 3, ‘Building +L+ Competitive Advantages through an Agile Infrastructure’ is the +L+ title of the PowerPoint document. +L+ We have developed a tool for annotation of titles by human +L+ annotators. Figure 4 shows a snapshot of the tool. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4. Title annotation tool. +L+ </SectLabel_figureCaption> <SectLabel_sectionHeader> 4. TITLE EXTRACTION METHOD +L+ 4.1 Outline +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Title extraction based on machine learning consists of training and +L+ extraction. The same pre-processing step occurs before training +L+ and extraction. +L+ During pre-processing, from the top region of the first page of a +L+ Word document or the first slide of a PowerPoint document a +L+ number of units for processing are extracted. If a line (lines are +L+ separated by ‘return’ symbols) only has a single format, then the +L+ line will become a unit. If a line has several parts and each of +L+ them has its own format, then each part will become a unit. Each +L+ unit will be treated as an instance in learning. A unit contains not +L+ only content information (linguistic information) but also +L+ formatting information. The input to pre-processing is a document +L+ and the output of pre-processing is a sequence of units (instances). +L+ Figure 5 shows the units obtained from the document in Figure 2. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5. Example of units. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> In learning, the input is sequences of units where each sequence +L+ corresponds to a document. We take labeled units (labeled as +L+ title_begin, title_end, or other) in the sequences as training data +L+ and construct models for identifying whether a unit is title_begin +L+ title_end, or other. We employ four types of models: Perceptron, +L+ Maximum Entropy (ME), Perceptron Markov Model (PMM), and +L+ Maximum Entropy Markov Model (MEMM). +L+ In extraction, the input is a sequence of units from one document. +L+ We employ one type of model to identify whether a unit is +L+ title_begin, title_end, or other. We then extract units from the unit +L+ labeled with ‘title_begin’ to the unit labeled with ‘title_end’. The +L+ result is the extracted title of the document. +L+ The unique characteristic of our approach is that we mainly utilize +L+ formatting information for title extraction. Our assumption is that +L+ although general documents vary in styles, their formats have +L+ certain patterns and we can learn and utilize the patterns for title +L+ extraction. This is in contrast to the work by Han et al., in which +L+ only linguistic features are used for extraction from research +L+ papers. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Models +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The four models actually can be considered in the same metadata +L+ extraction framework. That is why we apply them together to our +L+ current problem. +L+ Each input is a sequence of instances x1x2 L xk together with a +L+ sequence of labels y1 y2 L yk . xi and yi represents an instance +L+ and its label, respectively (i =1,2, L , k ). Recall that an instance +L+ here represents a unit. A label represents title_begin, title_end, or +L+ other. Here, k is the number of units in a document. +L+ In learning, we train a model which can be generally denoted as a +L+ conditional probability distribution P(Y1 L Yk | X1 L Xk) where +L+ Xi and Yi denote random variables taking instance xi and label +L+ yi as values, respectively ( i =1,2, L ,k). +L+ </SectLabel_bodyText> <SectLabel_figure> x11 x12 L x1k → y11y12 L y1k +L+ x21x22 L x2k → y21y22 L y2k +L+ L L +L+ xn1xn 2 L x1k → yn1yn2 L ynk +L+ Learning Tool +L+ xm1xm2 L xmk	Extraction Tool +L+ arg max P(ymLym k | xm1 L xmk +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6. Metadata extraction model. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> We can make assumptions about the general model in order to +L+ make it simple enough for training. +L+ Conditional +L+ Distribution +L+ </SectLabel_bodyText> <SectLabel_equation> P(Y1L Yk | X1LXk) +L+ ) +L+ </SectLabel_equation> <SectLabel_page> 148 +L+ </SectLabel_page> <SectLabel_bodyText> For example, we can assume that Y1 , ... , Yk are independent of +L+ each other given X 1 ,... ,X k . Thus, we have +L+ </SectLabel_bodyText> <SectLabel_equation> P(Y1 ... Yk|X1 ... Xk) +L+ =P ( Y 1 | X 1)... P(Yk | Xk) +L+ </SectLabel_equation> <SectLabel_bodyText> In this way, we decompose the model into a number of classifiers. +L+ We train the classifiers locally using the labeled data. As the +L+ classifier, we employ the Perceptron or Maximum Entropy model. +L+ We can also assume that the first order Markov property holds for +L+ Y1 , ... , Yk given X1 ,... ,Xk . Thus, we have +L+ Again, we obtain a number of classifiers. However, the classifiers +L+ are conditioned on the previous label. When we employ the +L+ Percepton or Maximum Entropy model as a classifier, the models +L+ become a Percepton Markov Model or Maximum Entropy Markov +L+ Model, respectively. That is to say, the two models are more +L+ precise. +L+ In extraction, given a new sequence of instances, we resort to one +L+ of the constructed models to assign a sequence of labels to the +L+ sequence of instances, i.e., perform extraction. +L+ For Perceptron and ME, we assign labels locally and combine the +L+ results globally later using heuristics. Specifically, we first +L+ identify the most likely title_begin. Then we find the most likely +L+ title _end within three units after the title _begin. Finally, we +L+ extract as a title the units between the title_begin and the title_end. +L+ For PMM and MEMM, we employ the Viterbi algorithm to find +L+ the globally optimal label sequence. +L+ In this paper, for Perceptron, we actually employ an improved +L+ variant of it, called Perceptron with Uneven Margin [13]. This +L+ version of Perceptron can work well especially when the number +L+ of positive instances and the number of negative instances differ +L+ greatly, which is exactly the case in our problem. +L+ We also employ an improved version of Perceptron Markov +L+ Model in which the Perceptron model is the so-called Voted +L+ Perceptron [2]. In addition, in training, the parameters of the +L+ model are updated globally rather than locally. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Features +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> There are two types of features: format features and linguistic +L+ features. We mainly use the former. The features are used for both +L+ the title-begin and the title-end classifiers. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 4.3.1 Format Features +L+ </SectLabel_subsubsectionHeader> <SectLabel_listItem> Font Size: There are four binary features that represent the +L+ normalized font size of the unit (recall that a unit has only one +L+ type of font). +L+ </SectLabel_listItem> <SectLabel_bodyText> If the font size of the unit is the largest in the document, then the +L+ first feature will be 1, otherwise 0. If the font size is the smallest +L+ in the document, then the fourth feature will be 1, otherwise 0. If +L+ the font size is above the average font size and not the largest in +L+ the document, then the second feature will be 1, otherwise 0. If the +L+ font size is below the average font size and not the smallest, the +L+ third feature will be 1, otherwise 0. +L+ It is necessary to conduct normalization on font sizes. For +L+ example, in one document the largest font size might be ‘12pt’, +L+ while in another the smallest one might be ‘18pt’. +L+ </SectLabel_bodyText> <SectLabel_listItem> Boldface: This binary feature represents whether or not the +L+ current unit is in boldface. +L+ Alignment: There are four binary features that respectively +L+ represent the location of the current unit: ‘left’, ‘center’, ‘right’, +L+ and ‘unknown alignment’. +L+ </SectLabel_listItem> <SectLabel_bodyText> The following format features with respect to ‘context’ play an +L+ important role in title extraction. +L+ </SectLabel_bodyText> <SectLabel_listItem> Empty Neighboring Unit: There are two binary features that +L+ represent, respectively, whether or not the previous unit and the +L+ current unit are blank lines. +L+ Font Size Change: There are two binary features that represent, +L+ respectively, whether or not the font size of the previous unit and +L+ the font size of the next unit differ from that of the current unit. +L+ Alignment Change: There are two binary features that represent, +L+ respectively, whether or not the alignment of the previous unit and +L+ the alignment of the next unit differ from that of the current one. +L+ Same Paragraph: There are two binary features that represent, +L+ respectively, whether or not the previous unit and the next unit are +L+ in the same paragraph as the current unit. +L+ </SectLabel_listItem> <SectLabel_subsubsectionHeader> 4.3.2 Linguistic Features +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The linguistic features are based on key words. +L+ </SectLabel_bodyText> <SectLabel_listItem> Positive Word: This binary feature represents whether or not the +L+ current unit begins with one of the positive words. The positive +L+ words include ‘title:’, ‘subject:’, ‘subject line:’ For example, in +L+ some documents the lines of titles and authors have the same +L+ formats. However, if lines begin with one of the positive words, +L+ then it is likely that they are title lines. +L+ Negative Word: This binary feature represents whether or not the +L+ current unit begins with one of the negative words. The negative +L+ words include ‘To’, ‘By’, ‘created by’, ‘updated by’, etc. +L+ </SectLabel_listItem> <SectLabel_bodyText> There are more negative words than positive words. The above +L+ linguistic features are language dependent. +L+ </SectLabel_bodyText> <SectLabel_listItem> Word Count: A title should not be too long. We heuristically +L+ </SectLabel_listItem> <SectLabel_bodyText> create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞ ) and define one +L+ feature for each interval. If the number of words in a title falls into +L+ an interval, then the corresponding feature will be 1; otherwise 0. +L+ </SectLabel_bodyText> <SectLabel_listItem> Ending Character: This feature represents whether the unit ends +L+ with ‘:’, ‘-’, or other special characters. A title usually does not +L+ end with such a character. +L+ </SectLabel_listItem> <SectLabel_sectionHeader> 5. DOCUMENT RETRIEVAL METHOD +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We describe our method of document retrieval using extracted +L+ titles. +L+ Typically, in information retrieval a document is split into a +L+ number of fields including body, title, and anchor text. A ranking +L+ function in search can use different weights for different fields of +L+ </SectLabel_bodyText> <SectLabel_equation> P (Y1... Yk | X1... Xk +L+ =X0... P(Yk | Yk,Xk) +L+ ) +L+ P(Y1 | +L+ </SectLabel_equation> <SectLabel_page> 149 +L+ </SectLabel_page> <SectLabel_bodyText> the document. Also, titles are typically assigned high weights, +L+ indicating that they are important for document retrieval. As +L+ explained previously, our experiment has shown that a significant +L+ number of documents actually have incorrect titles in the file +L+ properties, and thus in addition of using them we use the extracted +L+ titles as one more field of the document. By doing this, we attempt +L+ to improve the overall precision. +L+ In this paper, we employ a modification of BM25 that allows field +L+ weighting [21]. As fields, we make use of body, title, extracted +L+ title and anchor. First, for each term in the query we count the +L+ term frequency in each field of the document; each field +L+ frequency is then weighted according to the corresponding weight +L+ parameter: +L+ </SectLabel_bodyText> <SectLabel_equation> wtf, =∑wftfe +L+ f +L+ </SectLabel_equation> <SectLabel_bodyText> Similarly, we compute the document length as a weighted sum of +L+ lengths of each field. Average document length in the corpus +L+ becomes the average of all weighted document lengths. +L+ </SectLabel_bodyText> <SectLabel_equation> wdl =∑wfdlf +L+ f +L+ w +L+ t k1 ((1−b)+b wdl )+wtf	n +L+ avwdl +L+ </SectLabel_equation> <SectLabel_sectionHeader> 6. EXPERIMENTAL RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Inourexperiments we used +L+ </SectLabel_bodyText> <SectLabel_none> k1 +L+  =1.8, b = 0.75. Weightforcontent +L+ was 1.0, title was 10.0, anchorwas 10.0, andextractedtitle was +L+ 5.0. +L+ </SectLabel_none> <SectLabel_subsectionHeader> 6.1 Data Sets and Evaluation Measures +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We usedtwo datasets inourexperiments. +L+ First, we downloadedandrandomly selected5,000 Word +L+ documents and5,000 PowerPointdocuments fr +L+ om an intranet of +L+ Microsoft. We call it MS hereafter. +L+ Second, we downloadedandrandomlyselected500 Wordand500 +L+ PowerPointdocuments fromthe DotGov andDotComdomains on +L+ the +L+  internet, +L+  respectively. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 7 shows the distributions ofthe genres ofthedocuments. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> We see thatthe documents are indeed +L+  ‘generaldocuments’ +L+  as +L+  we +L+ define them. +L+ internet. +L+ d 500 PowerPoint documents +L+ in Chinese. +L+ Wemanuallylabeledthe titles ofall the documents, onthe basis +L+ ofourspecification. +L+ Notall the documents inthe two datasets have titles. Table 1 +L+ shows the percentages ofthe documents having titles. Wesee that +L+ DotComandDotGov have more PowerPointdocuments with titles +L+ thanMS. This mightbebecausePowerPointdocuments published +L+ onthe +L+ internet +L+ aremore formal thanthose onthe +L+ intranet. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 1. The portion of documents with titles +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Inourexperiments, we conductedevaluations ontitle extractionin +L+ terms ofprecision, recall, andF-measure. The evaluation +L+ measures aredefinedas +L+  follows: +L+ </SectLabel_bodyText> <SectLabel_equation> Precision:	P = A/ +L+  ( A +L+  + B ) +L+ Recall: +L+ R = A / ( A + C ) +L+ F-measure: +L+ F1 +L+  = 2PR/ +L+  ( P +L+  + +L+ R ) +L+ </SectLabel_equation> <SectLabel_bodyText> Here, A, B, C, andD are numbers ofdocuments as +L+  those defined +L+ in Table 2. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 2. Contingence table with regard to title extraction +L+ </SectLabel_tableCaption> <SectLabel_subsectionHeader> 6.2 Baselines +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Wetestthe accuracies ofthe two baselines describedinsection +L+ 4.2. Theyare denotedas +L+ ‘largest +L+ font +L+  size’ +L+  an +L+ d ‘first line’ +L+ respectively. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.3 Accuracy ofTitles in File Properties +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Weinvestigate howmanytitles inthe file properties ofthe +L+ documents arereliable. We viewthe titles annotatedbyhumans as +L+ true titles andtesthowmanytitles inthe fileproperties can +L+ approximatelymatch with the truetitles. We useEditDistance to +L+ conductthe approximate match. (Approximate match is onlyused +L+ inthis evaluation). This is becausesometimes humanannotated +L+ titles canbe slightlydifferentfromthe titles infile properties on +L+ the surface, e.g., containextraspaces). +L+ GivenstringA andstringB: +L+ if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then +L+ string +L+ A = +L+  string B +L+ D:	EditDistan +L+ ce between string A and string B +L+ La:	length of string A +L+ Lb:	length of string B +L+ </SectLabel_bodyText> <SectLabel_equation> e:	0.1 +L+ BM25F = ∑ 	`	× log( N) +L+ tf, (k, 1) +L+ </SectLabel_equation> <SectLabel_table> Domain		MSDotComDotGovType +L+ 		Word	75.7%	77.8%	75.6% +L+ 	PowerPoint	82.1%	93.4%	96.4% +L+ </SectLabel_table> <SectLabel_figureCaption> Figure 7. Distributions of document genres. +L+ </SectLabel_figureCaption> <SectLabel_table> 	Is title	Is not title +L+ Extracted	Third, adatasetinChinese was also downloadedfromthe	B +L+ 	A +L+ Itincludes 500 Worddocuments an	C	D +L+ Not extracted +L+ </SectLabel_table> <SectLabel_page> 150 +L+ </SectLabel_page> <SectLabel_tableCaption> Table 3. Accuracies of titles in file properties +L+ </SectLabel_tableCaption> <SectLabel_table> File Type	Domain	Precision	Recall	F1 +L+ Word	MS	0.299	0.311	0.305 +L+ 	DotCom	0.210	0.214	0.212 +L+ 	DotGov	0.182	0.177	0.180 +L+ PowerPoint	MS	0.229	0.245	0.237 +L+ 	DotCom	0.185	0.186	0.186 +L+ 	DotGov	0.180	0.182	0.181 +L+ </SectLabel_table> <SectLabel_subsectionHeader> 6.4 Comparison with Baselines +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We conducted title extraction from the first data set (Word and +L+ PowerPoint in MS). As the model, we used Perceptron. +L+ We conduct 4-fold cross validation. Thus, all the results reported +L+ here are those averaged over 4 trials. Tables 4 and 5 show the +L+ results. We see that Perceptron significantly outperforms the +L+ baselines. In the evaluation, we use exact matching between the +L+ true titles annotated by humans and the extracted titles. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 4. Accuracies of title extraction with Word +L+ </SectLabel_tableCaption> <SectLabel_table> 		Precision	Recall	F1 +L+ Model	Perceptron	0.810	0.837	0.823 +L+ Baselines	Largest font size	0.700	0.758	0.727 +L+ 	First line	0.707	0.767	0.736 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5. Accuracies of title extraction with PowerPoint +L+ </SectLabel_tableCaption> <SectLabel_table> 		Precision	Recall	F1 +L+ Model	Perceptron	0.875	0. 895	0.885 +L+ Baselines	Largest font size	0.844	0.887	0.865 +L+ 	First line	0.639	0.671	0.655 +L+ </SectLabel_table> <SectLabel_bodyText> We see that the machine learning approach can achieve good +L+ performance in title extraction. For Word documents both +L+ precision and recall of the approach are 8 percent higher than +L+ those of the baselines. For PowerPoint both precision and recall of +L+ the approach are 2 percent higher than those of the baselines. +L+ We conduct significance tests. The results are shown in Table 6. +L+ Here, ‘Largest’ denotes the baseline of using the largest font size, +L+ ‘First’ denotes the baseline of using the first line. The results +L+ indicate that the improvements of machine learning over baselines +L+ are statistically significant (in the sense p-value < 0.05) +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 6. Sign test results +L+ </SectLabel_tableCaption> <SectLabel_table> Documents Type	Sign test between	p-value +L+ Word	Perceptron vs. Largest	3.59e-26 +L+ 	Perceptron vs. First	7.12e-10 +L+ PowerPoint	Perceptron vs. Largest	0.010 +L+ 	Perceptron vs. First	5.13e-40 +L+ </SectLabel_table> <SectLabel_bodyText> We see, from the results, that the two baselines can work well for +L+ title extraction, suggesting that font size and position information +L+ are most useful features for title extraction. However, it is also +L+ obvious that using only these two features is not enough. There +L+ are cases in which all the lines have the same font size (i.e., the +L+ largest font size), or cases in which the lines with the largest font +L+ size only contain general descriptions like ‘Confidential’, ‘White +L+ paper’, etc. For those cases, the ‘largest font size’ method cannot +L+ work well. For similar reasons, the ‘first line’ method alone +L+ cannot work well, either. With the combination of different +L+ features (evidence in title judgment), Perceptron can outperform +L+ Largest and First. +L+ We investigate the performance of solely using linguistic features. +L+ We found that it does not work well. It seems that the format +L+ features play important roles and the linguistic features are +L+ supplements.. +L+ We conducted an error analysis on the results of Perceptron. We +L+ found that the errors fell into three categories. (1) About one third +L+ of the errors were related to ‘hard cases’. In these documents, the +L+ layouts of the first pages were difficult to understand, even for +L+ humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of +L+ the errors were from the documents which do not have true titles +L+ but only contain bullets. Since we conduct extraction from the top +L+ regions, it is difficult to get rid of these errors with the current +L+ approach. (3). Confusions between main titles and subtitles were +L+ another type of error. Since we only labeled the main titles as +L+ titles, the extractions of both titles were considered incorrect. This +L+ type of error does little harm to document processing like search, +L+ however. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.5 Comparison between Models +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To compare the performance of different machine learning models, +L+ we conducted another experiment. Again, we perform 4-fold cross +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 8. An example Word document. +L+ Figure 9. An example PowerPoint document. +L+ </SectLabel_figureCaption> <SectLabel_page> 151 +L+ </SectLabel_page> <SectLabel_bodyText> validation on the first data set (MS). Table 7, 8 shows the results +L+ of all the four models. +L+ It turns out that Perceptron and PMM perform the best, followed +L+ by MEMM, and ME performs the worst. In general, the +L+ Markovian models perform better than or as well as their classifier +L+ counterparts. This seems to be because the Markovian models are +L+ trained globally, while the classifiers are trained locally. The +L+ Perceptron based models perform better than the ME based +L+ counterparts. This seems to be because the Perceptron based +L+ models are created to make better classifications, while ME +L+ models are constructed for better prediction. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 7. Comparison between different learning models for +L+ </SectLabel_tableCaption> <SectLabel_table> title extraction with Word +L+ Model	Precision	Recall	F1 +L+ Perceptron	0.810	0.837	0.823 +L+ MEMM	0.797	0.824	0.810 +L+ PMM	0.827	0.823	0.825 +L+ ME	0.801	0.621	0.699 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 8. Comparison between different learning models for +L+ </SectLabel_tableCaption> <SectLabel_table> title extraction with PowerPoint +L+ Model	Precision	Recall	F1 +L+ Perceptron	0.875	0. 895	0. 885 +L+ MEMM	0.841	0.861	0.851 +L+ PMM	0.873	0.896	0.885 +L+ ME	0.753	0.766	0.759 +L+ </SectLabel_table> <SectLabel_subsectionHeader> 6.6 Domain Adaptation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We apply the model trained with the first data set (MS) to the +L+ second data set (DotCom and DotGov). Tables 9-12 show the +L+ results. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 9. Accuracies of title extraction with Word in DotGov +L+ </SectLabel_tableCaption> <SectLabel_table> 		Precision	Recall	F1 +L+ Model	Perceptron	0.716	0.759	0.737 +L+ Baselines	Largest font size	0.549	0.619	0.582 +L+ 	First line	0.462	0.521	0.490 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 10. Accuracies of title extraction with PowerPoint in +L+ </SectLabel_tableCaption> <SectLabel_table> DotGov +L+ 		Precision	Recall	F1 +L+ Model	Perceptron	0.900	0.906	0.903 +L+ Baselines	Largest font size	0.871	0.888	0.879 +L+ 	First line	0.554	0.564	0.559 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 11. Accuracies of title extraction with Word in DotCom +L+ </SectLabel_tableCaption> <SectLabel_table> 		Precisio	Recall	F1 +L+ 		n +L+ Model	Perceptron	0.832	0.880	0.855 +L+ Baselines	Largest font size	0.676	0.753	0.712 +L+ 	First line	0.577	0.643	0.608 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 12. Performance of PowerPoint document title +L+ </SectLabel_tableCaption> <SectLabel_table> extraction in DotCom +L+ 		Precisio	Recall	F1 +L+ 		n +L+ Model	Perceptron	0.910	0.903	0.907 +L+ Baselines	Largest font size	0.864	0.886	0.875 +L+ 	First line	0.570	0.585	0.577 +L+ </SectLabel_table> <SectLabel_bodyText> From the results, we see that the models can be adapted to +L+ different domains well. There is almost no drop in accuracy. The +L+ results indicate that the patterns of title formats exist across +L+ different domains, and it is possible to construct a domain +L+ independent model by mainly using formatting information. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.7 Language Adaptation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We apply the model trained with the data in English (MS) to the +L+ data set in Chinese. +L+ Tables 13-14 show the results. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 13. Accuracies of title extraction with Word in Chinese +L+ </SectLabel_tableCaption> <SectLabel_table> 		Precision	Recall	F1 +L+ Model	Perceptron	0.817	0.805	0.811 +L+ Baselines	Largest font size	0.722	0.755	0.738 +L+ 	First line	0.743	0.777	0.760 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 14. Accuracies of title extraction with PowerPoint in +L+ Chinese +L+ </SectLabel_tableCaption> <SectLabel_table> 		Precision	Recall	F1 +L+ Model	Perceptron	0.766	0.812	0.789 +L+ Baselines	Largest font size	0.753	0.813	0.782 +L+ 	First line	0.627	0.676	0.650 +L+ </SectLabel_table> <SectLabel_bodyText> We see that the models can be adapted to a different language. +L+ There are only small drops in accuracy. Obviously, the linguistic +L+ features do not work for Chinese, but the effect of not using them +L+ is negligible. The results indicate that the patterns of title formats +L+ exist across different languages. +L+ From the domain adaptation and language adaptation results, we +L+ conclude that the use of formatting information is the key to a +L+ successful extraction from general documents. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 6.8 Search with Extracted Titles +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We performed experiments on using title extraction for document +L+ retrieval. As a baseline, we employed BM25 without using +L+ extracted titles. The ranking mechanism was as described in +L+ Section 5. The weights were heuristically set. We did not conduct +L+ optimization on the weights. +L+ The evaluation was conducted on a corpus of 1.3 M documents +L+ crawled from the intranet of Microsoft using 100 evaluation +L+ queries obtained from this intranet’s search engine query logs. 50 +L+ queries were from the most popular set, while 50 queries other +L+ were chosen randomly. Users were asked to provide judgments of +L+ the degree of document relevance from a scale of 1to 5 (1 +L+ meaning detrimental, 2 – bad, 3 – fair, 4 – good and 5 – excellent). +L+ </SectLabel_bodyText> <SectLabel_page> 152 +L+ </SectLabel_page> <SectLabel_bodyText> Figure 10 shows the results. In the chart two sets of precision +L+ results were obtained by either considering good or excellent +L+ documents as relevant (left 3 bars with relevance threshold 0.5), or +L+ by considering only excellent documents as relevant (right 3 bars +L+ with relevance threshold 1.0) +L+ </SectLabel_bodyText> <SectLabel_figure> Name All +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 10. Search ranking results. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Figure 10 shows different document retrieval results with different +L+ ranking functions in terms of precision @10, precision @5 and +L+ reciprocal rank: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Blue bar – BM25 including the fields body, title (file +L+ property), and anchor text. +L+ •	Purple bar – BM25 including the fields body, title (file +L+ </SectLabel_listItem> <SectLabel_bodyText> property), anchor text, and extracted title. +L+ With the additional field of extracted title included in BM25 the +L+ precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, +L+ it is safe to say that the use of extracted title can indeed improve +L+ the precision of document retrieval. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7. CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we have investigated the problem of automatically +L+ extracting titles from general documents. We have tried using a +L+ machine learning approach to address the problem. +L+ Previous work showed that the machine learning approach can +L+ work well for metadata extraction from research papers. In this +L+ paper, we showed that the approach can work for extraction from +L+ general documents as well. Our experimental results indicated that +L+ the machine learning approach can work significantly better than +L+ the baselines in title extraction from Office documents. Previous +L+ work on metadata extraction mainly used linguistic features in +L+ documents, while we mainly used formatting information. It +L+ appeared that using formatting information is a key for +L+ successfully conducting title extraction from general documents. +L+ We tried different machine learning models including Perceptron, +L+ Maximum Entropy, Maximum Entropy Markov Model, and Voted +L+ Perceptron. We found that the performance of the Perceptorn +L+ models was the best. We applied models constructed in one +L+ domain to another domain and applied models trained in one +L+ language to another language. We found that the accuracies did +L+ not drop substantially across different domains and across +L+ different languages, indicating that the models were generic. We +L+ also attempted to use the extracted titles in document retrieval. We +L+ observed a significant improvement in document ranking +L+ performance for search when using extracted title information. All +L+ the above investigations were not conducted in previous work, and +L+ through our investigations we verified the generality and the +L+ significance of the title extraction approach. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 8. ACKNOWLEDGEMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We thank Chunyu Wei and Bojuan Zhao for their work on data +L+ annotation. We acknowledge Jinzhu Li for his assistance in +L+ conducting the experiments. We thank Ming Zhou, John Chen, +L+ Jun Xu, and the anonymous reviewers of JCDL’05 for their +L+ valuable comments on this paper. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 9. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A +L+ maximum entropy approach to natural language processing. +L+ Computational Linguistics, 22:39-71, 1996. +L+ [2] Collins, M. Discriminative training methods for hidden +L+ markov models: theory and experiments with perceptron +L+ algorithms. In Proceedings of Conference on Empirical +L+ Methods in Natural Language Processing, 1-8, 2002. +L+ [3] Cortes, C. and Vapnik, V. Support-vector networks. Machine +L+ Learning, 20:273-297, 1995. +L+ [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to +L+ information extraction from semi-structured and free text. In +L+ Proceedings of the Eighteenth National Conference on +L+ Artificial Intelligence, 768-791, 2002. +L+ [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia +L+ newsblaster: multilingual news summarization on the Web. +L+ In Proceedings of Human Language Technology conference / +L+ North American chapter of the Association for +L+ Computational Linguistics annual meeting, 1-4, 2004. +L+ [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov +L+ models. Machine Learning, 29:245-273, 1997. +L+ [7] Gheel, J. and Anderson, T. Data and metadata for finding and +L+ reminding, In Proceedings of the 1999 International +L+ Conference on Information Visualization, 446-451,1999. +L+ [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., +L+ Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a +L+ niche search engine for e-Business. In Proceedings of the +L+ 26th Annual International ACM SIGIR Conference on +L+ Research and Development in Information Retrieval, 413- +L+ 414, 2003. +L+ [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based +L+ metadata extraction from PostScript files. In Proceedings of +L+ the Fifth ACM Conference on Digital Libraries, 77-84, 2000. +L+ [ 10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and +L+ Fox, E. A. Automatic document metadata extraction using +L+ support vector machines. In Proceedings of the Third +L+ ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, +L+ 2003. +L+ [1 1 ] Kobayashi, M., and Takeda, K. Information retrieval on the +L+ Web. ACM Computing Surveys, 32:144-173, 2000. +L+ [ 12] Lafferty, J., McCallum, A., and Pereira, F. Conditional +L+ random fields: probabilistic models for segmenting and +L+ </SectLabel_reference> <SectLabel_figure> 0.45 +L+ BM25 AnchorTitle, Body +L+ BM25 AnchorTitle, Body ExtractedTitle +L+ 0.4 +L+ 0.35 +L+ 0.3 +L+ 0.25 +L+ 0.2 +L+ 0.15 +L+ 0.1 +L+ 0.05 +L+ 0 +L+ P@10	P@5	ReciprocalP@10	P@5	Reciprocal +L+ 0.5	1 +L+ RelevanceThreshold Data +L+ Description +L+ </SectLabel_figure> <SectLabel_page> 153 +L+ </SectLabel_page> <SectLabel_reference> labeling sequence data. In Proceedings of the Eighteenth +L+ International Conference on Machine Learning, 282-289, +L+ 2001. +L+ [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and +L+ Kandola, J. S. The perceptron algorithm with uneven margins. +L+ In Proceedings of the Nineteenth International Conference +L+ on Machine Learning, 379-386, 2002. +L+ [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., +L+ Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., +L+ and Silverstein, J. Automatic Metadata generation & +L+ evaluation. In Proceedings of the 25th Annual International +L+ ACM SIGIR Conference on Research and Development in +L+ Information Retrieval, 401-402, 2002. +L+ [15] Littlefield, A. Effective enterprise information retrieval +L+ across new content formats. In Proceedings of the Seventh +L+ Search Engine Conference, +L+ http://www.infonortics.com/searchengines/sh02/02prog.html, +L+ 2002. +L+ [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature +L+ generation system for automated metadata extraction in +L+ preservation of digital materials. In Proceedings of the First +L+ International Workshop on Document Image Analysis for +L+ Libraries, 225-232, 2004. +L+ [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy +L+ markov models for information extraction and segmentation. +L+ In Proceedings of the Seventeenth International Conference +L+ on Machine Learning, 591-598, 2000. +L+ [ 18] Murphy, L. D. Digital document metadata in organizations: +L+ roles, analytical approaches, and future research directions. +L+ In Proceedings of the Thirty-First Annual Hawaii +L+ International Conference on System Sciences, 267-276, 1998. +L+ [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B. Table +L+ extraction using conditional random fields. In Proceedings of +L+ the 26th Annual International ACM SIGIR Conference on +L+ Research and Development in Information Retrieval, 235- +L+ 242, 2003. +L+ [20] Ratnaparkhi, A. Unsupervised statistical models for +L+ prepositional phrase attachment. In Proceedings of the +L+ Seventeenth International Conference on Computational +L+ Linguistics. 1079-1085, 1998. +L+ [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 +L+ extension to multiple weighted fields, In Proceedings of +L+ ACM Thirteenth Conference on Information and Knowledge +L+ Management, 42-49, 2004. +L+ [22] Yi, J. and Sundaresan, N. Metadata based Web mining for +L+ relevance, In Proceedings of the 2000 International +L+ Symposium on Database Engineering & Applications, 113- +L+ 121, 2000. +L+ [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: +L+ An NLP system to automatically assign metadata. In +L+ Proceedings of the 2004 Joint ACM/IEEE Conference on +L+ Digital Libraries, 241-242, 2004. +L+ [24] Zhang, J. and Dimitroff, A. Internet search engines' response +L+ to metadata Dublin Core implementation. Journal of +L+ Information Science, 30:310-320, 2004. +L+ [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using +L+ named entities: focused named entity recognition using +L+ machine learning. In Proceedings of the 27th Annual +L+ International ACM SIGIR Conference on Research and +L+ Development in Information Retrieval, 281-288, 2004. +L+ [26] http://dublincore.org/groups/corporate/Seattle/ +L+ </SectLabel_reference> <SectLabel_page> 154 +L+ </SectLabel_page>
<SectLabel_title> A Dependability Perspective on Emerging Technologies +L+ </SectLabel_title> <SectLabel_author> Lucian Prodan	Mihai Udrescu	Mircea Vladutiu +L+ </SectLabel_author> <SectLabel_affiliation> Advanced Computing Systems and Architectures (ACSA) Laboratory, +L+ Computer Science and Engineering Department, “Politehnica” University of Timisoara, +L+ </SectLabel_affiliation> <SectLabel_address> 2 V.Parvan Blvd, 300223 Timisoara, Romania +L+ </SectLabel_address> <SectLabel_note> www.acsa.upt.ro +L+ </SectLabel_note> <SectLabel_address> +40-722-664779	+40-723-154989	+40-256-403258 +L+ </SectLabel_address> <SectLabel_email> lprodan@cs.upt.ro	mudrescu@cs.upt.ro	mvlad@cs.upt.ro +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Emerging technologies are set to provide further provisions for +L+ computing in times when the limits of current technology of +L+ microelectronics become an ever closer presence. A technology +L+ roadmap document lists biologically-inspired computing and +L+ quantum computing as two emerging technology vectors for novel +L+ computing architectures [43]. But the potential benefits that will +L+ come from entering the nanoelectronics era and from exploring +L+ novel nanotechnologies are foreseen to come at the cost of +L+ increased sensitivity to influences from the surrounding +L+ environment. This paper elaborates on a dependability perspective +L+ over these two emerging technology vectors from a designer’s +L+ standpoint. Maintaining or increasing the dependability of +L+ unconventional computational processes is discussed in two +L+ different contexts: one of a bio-inspired computing architecture +L+ (the Embryonics project) and another of a quantum computational +L+ architecture (the QUERIST project). +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Categories and Subject Descriptors +L+ </SectLabel_sectionHeader> <SectLabel_category> B.8.1 [Performance and Reliability]: Reliability, Testing, and +L+ Fault-Tolerance. +L+ C.4 [Performance of Systems]: Fault-Tolerance, Reliability, +L+ Availability, and Serviceability. +L+ </SectLabel_category> <SectLabel_sectionHeader> General Terms +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Design, Reliability, Theory. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Dependability, emerging technologies, evolvable hardware, bio- +L+ inspired computing, bio-inspired digital design, Embryonics, +L+ reliability, quantum computing, fault-tolerance assessment. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1. INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> High-end computing has reached nearly every corner of our +L+ present day life, in a variety of forms taylored to accommodate +L+ either general purpose or specialized applications. Computers +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CF’06, May 3–5, 2006, Ischia, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2006 ACM 1-59593-302-6/06/0005...$5.00. +L+ may be considerred as fine exponents of the present days’ +L+ technological wave – if not their finest, they certainly do count as +L+ solid, indispensable support for the finest. +L+ </SectLabel_copyright> <SectLabel_bodyText> From the very beginning of the computing advent, the main target +L+ was squeezing out any additional performance. The inception +L+ period was not always trouble-free, accurate computation results +L+ being required at an ever faster pace on a road that has become +L+ manifold: some applications do require computational speed as a +L+ top priority; others are set for the highest possible dependability, +L+ while still delivering sufficient performance levels. +L+ Several definitions for dependability have been proposed: “the +L+ ability of a system to avoid service failures that are more frequent +L+ or more severe than is acceptable” [2], or “the property of a +L+ computer system such that reliance can justifiably be placed on +L+ the service it delivers” [9][45]. Dependability is therefore a +L+ synthetic term specifying a qualitative system descriptor that can +L+ generally be quantified through a list of attributes including +L+ reliability, fault tolerance, availability, and others. +L+ In real world, a dependable system would have to operate +L+ normally over extended periods of time before experiencing any +L+ fail (reliability, availability) and to recover quickly from errors +L+ (fault tolerance, self-test and self-repair). The term “acceptable” +L+ has an essential meaning within the dependability’s definition, +L+ setting the upper limits of the damages that can be supported by +L+ the system while still remaining functional or computationally +L+ accurate. A dependability analysis should take into consideration +L+ if not quantitative figures for the acceptable damage limit, at least +L+ a qualitative parameter representation for its attributes. +L+ Dependable systems are therefore crucial for applications that +L+ prohibit or limit human interventions, such as long-term exposure +L+ to aggressive (or even hostile) environments. The best examples +L+ are long term operating machines as required by managing deep- +L+ underwater/nuclear activities and outer space exploration. +L+ There are three main concerns that should be posed through a +L+ system’s design in order to achieve high dependability [42]: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Specifying the dependability requirements: selecting the +L+ dependability requirements that have to be pursued in +L+ building the computing system, based on known or assumed +L+ goals for the part of the world that is directly affected by the +L+ computing system; +L+ 2. Designing and implementing the computing system so as to +L+ achieve the dependability required. However, this step is hard +L+ to implement since the system reliability cannot be satisfied +L+ </SectLabel_listItem> <SectLabel_page> 187 +L+ </SectLabel_page> <SectLabel_bodyText> simply from careful design. Some techniques can be used to +L+ help to achieve this goal, such as using fault injection to +L+ evaluate the design process. +L+ </SectLabel_bodyText> <SectLabel_listItem> 3. Validating a system: gaining confidence that a certain +L+ dependability requirement/goal has been attained. +L+ </SectLabel_listItem> <SectLabel_bodyText> This paper will address these main concerns through an attempt to +L+ provide an in-depth view over modern computing directions and +L+ paradigms, which we consider to be representative for the efforts +L+ involved in improving overall dependability. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 1.1 Motivations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We have listed some of the applications of dependable computing +L+ systems as linked to activities that take place in special +L+ environments, such as deep underwater or outer space. At a very +L+ first sight, these applications would appear specific enough to not +L+ encourage a specific design for dependability approach in +L+ computing. However, evidence suggest this is hardly the case; on +L+ the contrary, it is difficult to imagine a domain left unconquered +L+ by computer systems during times when industrial, transport, +L+ financial services and others do rely heavily on accurate computer +L+ operation at any given moment. If computer innacuracies could be +L+ more easily overlooked at home, professional environments +L+ cannot accept such missbehaviors. +L+ Yet the recent history of computing provides evidence that +L+ dependability is not a sine qua non feature. During their life +L+ cycle, electronic devices constantly suffer a number of influences +L+ that manifest predominantly over transient regimes, which in turn +L+ introduce a variety of errors unified in the literature under the +L+ name of transient faults, soft errors or single event upsets (SEUs). +L+ The rate electronic devices are affected with is known under the +L+ term of soft error rate or simply SER and is measured in fails per +L+ unit time. Because it relies on transient phenomena due to +L+ changing states and logical values, digital electronics makes up +L+ for a special category that is also affected by soft errors. No +L+ matter the name they are referred under, these errors affect the +L+ computing processes and are due to electromagnetic noise and/or +L+ external radiations rather than design or manufacturing flaws [28]. +L+ One cause at the origin of soft fails affecting digital devices is +L+ known to be due to radioactive decay processes. Radioactive +L+ isotopes, widely used for a range of purposes, might contaminate +L+ semiconductor materials leading to soft errors; evidence is +L+ available throughout the literature, both by empirical observations +L+ and experimental results [20]. Consequently, cosmic rays, +L+ containing a broad range of energized atomic/subatomic particles +L+ may lead to the appearance of soft fails. +L+ Computers therefore are susceptive to soft errors, an issue that +L+ will potentially become essential with the advent of emerging +L+ technologies. As acknowledged by the International Technology +L+ Roadmap for Semiconductors (ITRS), issued at the end of 2004 +L+ [43], the microelectronics industry faces a challenging task in +L+ going to and beyond 45nm scale in order to address “beyond +L+ CMOS” applications. Scaling down the technology will enable an +L+ extremely large number of devices to be integrated onto the same +L+ chip. However, the great challenge will be to ensure the new +L+ devices will be operational at this scale [6], since they will exhibit +L+ a sensitive behavior to soft fails. In order to address the negative +L+ effects brought by technology scaling, it is to be expected that +L+ significant control resources will need to be implemented [3]. +L+ Another challenging aspect concerning emerging technologies is +L+ to match the newly developed device technologies with new +L+ system architectures, a synergistic/collaborative development of +L+ the two being seen as likely to be very rewarding. The potential of +L+ biologically-inspired and quantum computing architectures is +L+ acknowledged by the ITRS report on emerging technologies [43] +L+ (see Figure 1). This paper will investigate the relevance of soft +L+ fails and attempt to provide means of harnessing their negative +L+ effects on modern computing in the context of biologically- +L+ inspired and quantum computing architectures. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1: Bio-inspired and quantum computing are +L+ </SectLabel_figureCaption> <SectLabel_bodyText> acknowledged as architectural technology vectors in emerging +L+ technologies [43] +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 1.2 Paper Outline +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> This paper is structured as follows. Section 2 will address the first +L+ main concern, that is, specifying and selecting dependability +L+ requirements that will have to be pursued when building a +L+ computational platform. Parameters that describe and quantify +L+ dependability attributes, such as reliability, will be introduced, +L+ with a highlight on their accepted models and their issues. A +L+ particular consideration will be given to the failure rate parameter, +L+ which is the basis of all reliability analyses. +L+ Section 3 will approach some of the means for design for +L+ dependability; it will therefore elaborate upon two emerging +L+ technology vectors, as seen by the ITRS report [43], which define +L+ two novel architectures, namely biologically-inspired (or bio- +L+ inspired) and quantum computing. We will introduce two projects +L+ and their corresponding architectures, called Embryonics (as a +L+ biologically-inspired computing platform) and QUERIST (as a +L+ quantum computing platform designed to allow and study error +L+ injection). These two architectures are representative for the +L+ coming age of nano-computing, where computational processes +L+ take place as encoded at the very inner core level of matter, be it +L+ semiconductor material (for nanoelectronics, targetted here by the +L+ Embryonics project) or atomic scale dynamics (for quantum +L+ computing, targetted here by the QUERIST project). This section +L+ will then introduce dependability aspects within bio-inspired +L+ computing (the Embryonics project being investigated in +L+ SubSection 3.1) and within quantum computing (the QUERIST +L+ project being investigated in SubSection 3.2). +L+ Finally, Section 4 will present the conclusions and prospects for +L+ designing emerging technology dependable computing systems, +L+ as we see them. +L+ </SectLabel_bodyText> <SectLabel_page> 188 +L+ </SectLabel_page> <SectLabel_sectionHeader> 2. DEPENDABILITY ATTRIBUTES +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> An important dependability attribute for any given system lies in +L+ its capacity to operate reliably for a given time interval, knowing +L+ that normal operation was delivered at initial time [8]. Reliability +L+ functions are modelled as exponential functions of parameter A, +L+ which is the failure rate. The reliability of a system is the +L+ consequence of the reliability of all of its subsystems. The +L+ heterogeneity of the system leads to a difficult quantitative +L+ assessment of its overall reliability; moreover, estimating the +L+ reliability functions is further made difficult because formal +L+ rigour is not commercially available, this being kept under +L+ military mandate [44]. +L+ The failure rate for a given system can be modelled as a function +L+ of the failure rates of its individual subsystems, suggestions being +L+ present in the MIL-HDBC-217 document, which is publicly +L+ available [44]. However, this document has been strongly +L+ criticized for its failure rate estimations based on the Arrhenius +L+ model, which relates the failure rate to the operating temperature: +L+ where K is a constant, KB is Boltzmann’s constant, T is the +L+ absolute temperature and E is the “activation energy” for the +L+ process [18]. Quantitative values for failure rates show significant +L+ differences between those predicted using MIL-HDBC-217 and +L+ those from testing real devices (see Figure 2). There are two +L+ conclusions that can be drawn from this: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. quantitative estimations for failure rate values are strongly +L+ dependant on the quality of information used; unfortunately, +L+ current reliable information about electronic devices is known +L+ to be lacking [44]; +L+ 2. despite differences between predicted and real values, the +L+ MIL-HDBC-217 methodology can be useful for qualitative +L+ analyses in order to take decisions regarding sub-system parts +L+ that should benefit from improved designs. +L+ </SectLabel_listItem> <SectLabel_figureCaption> Figure 2. Predicted vs real failure rates plotted against +L+ </SectLabel_figureCaption> <SectLabel_bodyText> temperature [18] +L+ So far the failure rate of digital devices has been considerred as +L+ due to internal causes. However, this is not always the case, soft +L+ fails being equally present due to the aggressive influences of the +L+ external environment, which also have to be modelled [22]. The +L+ external envirnment features highly dynamic changes in its +L+ parameters, which will eventually affect the normal operation of +L+ digital devices that lack sufficient protection or ability to adapt. +L+ Ideally, computing devices would behave in a consistent and +L+ accurate manner regardless of fluctuations in environmental +L+ parameters. This is either a consequence of soft error mitigation +L+ techniques or due to flexible hardware/software functionality that +L+ allow the system as a whole to adapt to environamental changes +L+ and tolerate induced faults. +L+ While certain soft error mitigation techniques are available, the +L+ technology scaling towards nanoelectronics affects their +L+ efficiency by integrating a larger number of devices per chip +L+ (which requires a larger amount of redundant/control logic or +L+ other measures), which feature, at the same time, smaller +L+ dimensions (which renders an electronic device much more +L+ senzitive to the influence of stray energetic particles that reach it +L+ as part of cosmic rays). Both aspects are involved in the +L+ development of the two emerging technology vectors mentioned +L+ in SubSection 1.1, although having slightly different motivations: +L+ while the nature of the quantum environment prohibits precise +L+ computation in the absence of fault tolerance techniques, such +L+ techniques are targetted by bio-inspired computing as means of +L+ improving the dependability of a computing platform. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 2.1 Bio-Inspired Computing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> If living beings may be considered to fulfill computational tasks, +L+ then Nature is the ultimate engineer: each of the living beings +L+ exhibit solutions that were successfully tested and refined in such +L+ ways human engineers will never afford. One reason is time: the +L+ testing period coinciding with the very existence of life itself. +L+ Another reason is variety and complexity: Nature has found and +L+ adapted a variety of solutions to address complex survivability +L+ issues in a dynamically changing environment. No matter how +L+ Nature approached the process of evolution, engineering could +L+ perhaps benefit most from drawing inspiration from its +L+ mechanisms rather from trying to develop particular techniques. +L+ Bio-inspired computing is not a new idea. John von Neumann was +L+ preoccupied to design a machine that could replicate itself and +L+ was quite interested in the study of how the behavior of the +L+ human brain could be implemented by a computer [13][14]. He +L+ also pioneered the field of dependable computing by studying the +L+ possibility of building reliable machines out of unreliable +L+ components [15]. Unfortunately, the dream of implementing his +L+ self-reproducing automata could not become true until the 1990s, +L+ when massively programmable logic opened the new era of +L+ reconfigurable computing. +L+ But when trying to adapt nature’s mechanisms in digital devices, +L+ it becomes most evident that biological organisms are rightfully +L+ the most intricate structures known to man. They continuously +L+ demonstrate a highly complex behavior due to massive, parallel +L+ cooperation between huge numbers of relatively simple elements, +L+ the cells. And considering uncountable variety of living beings, +L+ with a life span up to several hundreds (for the animal regnum) or +L+ even thousands (for the vegetal regnum) of years, it seems nature +L+ is the closest spring of inspiration for designing dependable, fault +L+ tolerant systems. +L+ Investigating the particularities of natural systems, a taxonomy of +L+ three categories of processes can be identified [32]: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Phylogenetic processes constitute the first level of +L+ organization of the living matter. They are concerned with the +L+ temporal evolution of the genetic heritage of all individuals, +L+ </SectLabel_listItem> <SectLabel_none> E +L+ </SectLabel_none> <SectLabel_equation> λ= Ke KBT	(1) +L+ − +L+ </SectLabel_equation> <SectLabel_page> 189 +L+ </SectLabel_page> <SectLabel_bodyText> therefore mastering the evolution of all species. The +L+ phylogenetic processes rely on mechanisms such as +L+ recombination and mutation, which are essentially +L+ nondeterministic; the error rate ensures here nature’s +L+ diversity. +L+ </SectLabel_bodyText> <SectLabel_listItem> 2. Ontogenetic processes represent the second level of +L+ organization of the living matter. They are also concerned +L+ with the temporal evolution of the genetic heritage of, in this +L+ case, a single, multicellular individual, therefore mastering an +L+ individual’s development from the stage of a single cell, the +L+ zygote, through succesive cellular division and specialization, +L+ to the adult stage. These processes rely on deterministic +L+ mechanisms; any error at this level results in malformations. +L+ 3. Epigenetic processes represent the third level of organization +L+ of the living matter. They are concerned with the integration +L+ of interactions with the surrounding environment therefore +L+ resulting in what we call learning systems. +L+ </SectLabel_listItem> <SectLabel_bodyText> This taxonomy is important in that it provides a model called POE +L+ (from Phylogeny, Ontogeny and Epigenesis) that inspires the +L+ combination of processes in order to create novel bio-inspired +L+ hardware (see Figure 3). We believe this is also important from a +L+ dependability engineering perspective, for the following reasons: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Phylogenetic processes were assimilated by modern +L+ computing as evolutionary computation, including genetic +L+ algorithms and genetic programming. The essence of any +L+ genetic algorithm is the derivation of a solution space based +L+ on recombination, crossover and mutation processes that +L+ spawn a population of individuals, each encoding a possible +L+ solution. One may consider that each such step, with the +L+ exception of discovering the solution, is equivalent to a +L+ process of error injection, which in turn leads to wandering +L+ from the optimal solution (or class of solutions). However, +L+ genetic algorithms prove to be successful despite this error +L+ injection, the fitness function being responsible for the +L+ successful quantification of the significance of the “error”. +L+ Therefore genetic computation is intrinsicaly resilient to +L+ faults and errors, largely due to the fact that they are part of +L+ the very process that generates the solutions. +L+ 2. Ontogenetic processes have been implemented in digital +L+ hardware with modular and uniform architectures. Such an +L+ architecture enables the implementation of mechanisms +L+ similar to the cellular division and cellular differentiation that +L+ take place in living beings [31]. These mechanisms bring the +L+ advantage of distributed and hierarchical fault tolerance +L+ strategies: the uniformity of the architecture also makes any +L+ module to be universal, that is, to be able to take over the role +L+ of any other damaged module. +L+ 3. Epigenetic processes were assimilated by modern computing +L+ mainly as artificial neural networks (or ANNs) as inspired by +L+ the nervous system, and much less as inspired by the immune +L+ or endocrine systems from superior multicellular living +L+ beings. ANNs are known to have a generalization capacity, +L+ that is, to respond well even if the input patterns are not part +L+ of the patterns used during the learning phase. This means that +L+ ANNs possess a certain ability to tolerante faults, whether +L+ they manifest at the inputs or inside their intenal architecture. +L+ </SectLabel_listItem> <SectLabel_bodyText> With the advent of field programmable logic (of which the most +L+ salient representative are the FPGAs) it is now possible to change +L+ hardware functionality through software, thus allowing +L+ information to govern matter in digital electronics. This is not +L+ dissimilar to what happens in nature: information coded in DNA +L+ affects the development of an organism. A special kind of such +L+ digital devices that change dynamically their behavior are known +L+ as evolvable or adaptive hardware; they are bio-inspired +L+ computing systems whose behaviors may change according to +L+ computational targets, or, if harsh or unknown environments are +L+ to be explored, for the purpose of maximizing dependability. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 3. The POE model of bio-inspired systems [32] +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> 2.2 Quantum Computing +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Error detection and correction techniques are vital in quantum +L+ computing due to the destructive effect of the environment, which +L+ therefore acts as an omnipresent error generator. Error detection +L+ and correction must provide a safe recovery process within +L+ quantum computing processes through keeping error propagation +L+ under control. Without such dependability techniques there could +L+ be no realistic prospect of an operational quantum computational +L+ device [19]. +L+ There are two main sources of errors: the first is due to the +L+ erroneous behavior of the quantum gate, producing the so-called +L+ processing errors; the second is due to the macroscopic +L+ environment that interacts with the quantum state, producing the +L+ storing and transmitting errors. +L+ The consistency of any quantum computation process can be +L+ destroyed by innacuracies and errors if the error probability in the +L+ basic components (qubits, quantum gates) excedes an accuracy +L+ threshold. This is a critical aspect since the microscopic quantum +L+ states are prone to frequent errors. +L+ The main error source is the decoherence effect [16]. The +L+ environment is constantly attempting to measure the sensitive +L+ quantum superposition state, a phenomenon that cannot be +L+ avoided technologically since it is not (yet) possible to isolate +L+ them perfectly. The superposition state will decay through +L+ measuring and will therefore become a projection of the state +L+ vector onto a basis vector (or eigenstate). The most insidious +L+ error, however, appears when decoherence affects the quantum +L+ amplitudes without destroying them; this is similar to small +L+ analog errors. Issues stated above are solved, on one hand, +L+ through intrinsic fault tolerance by technological implementation +L+ (topological interactions [1]) and, on the other hand, by error +L+ correcting techniques at the unitary (gate network) level. We will +L+ focus on the error detecting and correcting techniques, which are +L+ difficult to approach due to quantum constraints: the useful state +L+ </SectLabel_bodyText> <SectLabel_page> 190 +L+ </SectLabel_page> <SectLabel_bodyText> can neither be observed (otherwise it will decohere), nor can it be +L+ cloned. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 2.2.1 Background +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> As expressed in bra-ket notation [16], the qubit is a normalized +L+ vector in some Hilbert space H2 , { 0 , 1 } being the orthonormal +L+ basis: ψ = a0 0 + a1 1 ( a0, a1 ∈ C are the so-called quantum +L+ amplitudes, representing the square root of the associated +L+ measurement probabilities for the eigenstates +L+ respectively, with a0 2 + a1 2 =1). Therefore, the qubit can be +L+ affected by 3 types of errors: +L+ Bit flip errors are somewhat similar to classical bit flip errors. For +L+ a single qubit things are exactly the same as in classical +L+ computation: 0 H 1 , 1 H 0 . For 2 or more qubits, flip errors +L+ affecting the state may modify it or leave it unchanged. For +L+ instance,	if we consider the so-called cat state +L+ ψ Cat = 2 ( 00 + 11 ) [19], and the first qubit is affected by a +L+ bit flip error, the resulting state will be yr Cat H 2 ( 10 + 01) . +L+ But, if both qubits are affected by bit flips, there will be no +L+ change in the state: V Cat H 2 ( 11 + 00 ) = ψ Cat +L+ Phase errors affect the phase of one of the qubit's amplitudes and +L+ is expressed as 0 H 0 , 1 H − 1 . This type of error is very +L+ dangerous, due to its propagation behavior but it only makes +L+ sense when dealing with superposition states. If we consider an +L+ equally weighted qubit superposition state and inject a phase +L+ error, this results in 2 ( 0 + 1 )H 2 ( 0 − 1 ) . +L+ There is a strict correspondence between bit flip and phase error +L+ types due to the way they map onto Hilbert spaces with the same +L+ dimension but different basis. The bit flip is an error from the +L+ { 0 , 1 } , whereas the phase error appears in the +L+ same space with basis r	0 + 1, �(0 − 1 )⎫⎬⎭ or{ + +L+ The space basis conversion, in this case, is made by applying the +L+ Hadamard transform; Figure 4 shows an example of transforming +L+ a bit flip error into a phase error (A, and vice versa (B. +L+ Small amplitude errors: amplitudes a0 and a1 of the quantum bit +L+ can be affected by small errors, similar to analog errors. Even if +L+ such an error does not destroy the superposition and conserves the +L+ value of the superposed states, small amplitude errors could +L+ accumulate over time, eventually ruining the computation. In +L+ order to avoid this situation, specific methodologies for digitizing +L+ small errors are used to reduce them to a non-fault or a bit-flip +L+ [19]. +L+ Due to the quantum physics laws, fault tolerance techniques have +L+ to comply with the following computational constraints: +L+ </SectLabel_bodyText> <SectLabel_listItem> – The observation destroys the state. Since observation is +L+ equivalent to measurement, this leads to destroying the +L+ useful state superposition. +L+ – Information copying is impossible. Quantum physics renders +L+ the cloning of a quantum state impossible, meaning that a +L+ quantum state cannot be copied correctly. Therefore +L+ quantum error correction must address the following +L+ problems: +L+ </SectLabel_listItem> <SectLabel_bodyText> Non-destructive measurement. Despite the first constraint it is +L+ necessary to find a way to measure the encoded information +L+ without destroying it. Because the encoded state cannot be +L+ measured directly, one needs to properly prepare some scratch +L+ (ancilla) qubits, which can then be measured. +L+ Fault-tolerant recovery. Due to the high error rate in quantum +L+ computational devices, it is likely that the error recovery itself +L+ will be affected by errors. If the recovery process is not fault- +L+ tolerant, then any error coding becomes useless. +L+ Phase error backward propagation. If we consider the XOR gate +L+ from Figure 5(A, a flip error affecting the target qubit (b) will +L+ propagate backwards and also affect the source qubit. This is due +L+ to the gate network equivalence from Figure 5(B and the basis +L+ transformation described by Figure 4. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4. Correspondence between bit flip and phase errors +L+ Figure 5. (A The backward propagation of a phase error for +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the XOR gate; (B Gate network equivalence +L+ In order to deal with the problems described the next strategies +L+ have to be followed: +L+ Digitizing small errors. The presence of small errors is not a +L+ major concern, as they can be digitized using a special technique +L+ based on measuring auxiliary (ancilla) qubits [19]. +L+ Ancilla usage. Since qubit cloning is impossible, a majority +L+ voting strategy is difficult to implement. However, by using +L+ ancilla qubits, the eigenstate information can be duplicated inside +L+ the existing superposition, resulting in the entanglement of the +L+ ancilla with the useful data. Because any measurement performed +L+ on the ancilla could have repercussions on the useful qubits, the +L+ appropriate strategy will employ special coding for both data +L+ qubits and ancilla (data errors only will be copied onto the +L+ ancilla), followed by the computation of an error syndrome, +L+ which has to be obtained through measuring the ancilla (see +L+ Figure 6). +L+ Avoiding massive spreading of phase errors. As shown +L+ previously, a phase error on the target qubit will propagate on all +L+ source qubits. The solution is to use more ancilla qubits as targets, +L+ so that no ancilla qubit is used more than once. +L+ </SectLabel_bodyText> <SectLabel_none> 0 and 1 +L+ 	2 +L+ space with basis +L+ </SectLabel_none> <SectLabel_page> 191 +L+ </SectLabel_page> <SectLabel_none> 1 +L+ </SectLabel_none> <SectLabel_figureCaption> Figure 6. Fault-tolerant procedure with ancilla qubits +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Ancilla and syndrome accuracy. Setting the ancilla code to some +L+ known quantum state could be an erroneous process. Computing +L+ the syndrome is also prone to errors. Hence, on one hand, one has +L+ to make sure that the ancilla qubits are in the right state by +L+ verifying and recovering them if needed; on the other hand, in +L+ order to have a reliable syndrome, it must be computed +L+ repeatedly. +L+ Error recovery. As the small errors can be digitized (therefore, +L+ they are either corrected or transformed into bit flip errors), the +L+ recovery must deal only with bit flip and phase errors. A state that +L+ needs to be recovered is described by: +L+ Correcting a bit flip error 1means applying the negation unitarytransformation UN = ux = ° Oj to the affected qubit. To +L+ correct phase and combined errors, the following unitary +L+ operators	will	have	to	be	applied	respectively: +L+ </SectLabel_bodyText> <SectLabel_equation> ⎡1	0 ⎤	⎡0	−i ⎤ +L+ UZ=⎢0	−1], UY = UN⋅UZ	⎣i	. +L+ 			0 ⎥⎦ +L+ </SectLabel_equation> <SectLabel_subsubsectionHeader> 2.2.2 Quantum Error Correcting Codes +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Quantum error coding and correcting (QECC) is performed with +L+ special coding techniques inspired from the classic Hamming +L+ codes. The classical error coding is adapted so that it becomes +L+ suitable for the quantum strategy, allowing only the ancilla qubits +L+ to be measured. +L+ The state-of-the-art in QECC is represented by the stabilizer +L+ encoding, a particular case being the Steane codes (the Shor codes +L+ may also be used [29]). Steane's 7-qubit code is a single error +L+ correcting code inspired from classical Hamming coding and can +L+ be adapted for ancilla coding as well. Therefore it cannot recover +L+ from two identical qubit faults, but it can recover from a bit flip a +L+ phase flip. The Steane 7-qubit coding of 0 and 1 consists of +L+ an equally weighted superposition of all the valid Hamming 7-bit +L+ words with an even and odd number of 1s, respectively: +L+ </SectLabel_bodyText> <SectLabel_equation> = 1 +L+ S	3	odd⎞u0u1u2u3c0c1c2 +L+ ⎜	⎟ +L+ ⎝	⎠ +L+ =1 1111111 + 1101000 + 1010001 + 1000110 +L+ 232 +L+ + 0110100 + 0100011 + 0011010 + 0001101 +L+ </SectLabel_equation> <SectLabel_bodyText> Applying the Steane coding on an arbitrary given quantum state +L+ </SectLabel_bodyText> <SectLabel_equation> ψ = a0 0 +a1 1 transforms it into V S = a0 0 S + a1 1 S. This +L+ </SectLabel_equation> <SectLabel_bodyText> code was designed to correct bit-flip errors, but by changing the +L+ basis (through a Hadamard transform) the phase error transforms +L+ into a bit flip error, which can then be corrected: +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 2.2.3 Fault Tolerance Methodologies +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Quantum error-correcting codes exist for r errors, r ∈ ICY, r ≥ 1. +L+ Therefore a non-correctable error occurs if a number of r +1 +L+ errors occur simultaneously before the recovery process. +L+ If the probability of a quantum gate error or storage error in the +L+ time unit is of orderξ , then the probability of an error affecting +L+ the processed data block becomes of order �r+1 , which is +L+ negligible if r is sufficiently large. However, by increasing r the +L+ safe recovery also becomes more complex and hence prone to +L+ errors: it is possible that r +1 errors accumulate in the block +L+ before the recovery is performed. +L+ Considering the relationship between r and the number of +L+ computational steps required for computing the syndrome is +L+ polynomial of the order rp . It was proven that in order to reduce +L+ as much as possible the error probability r must be chosen so that +L+ </SectLabel_bodyText> <SectLabel_none> 1 +L+ − +L+ </SectLabel_none> <SectLabel_bodyText> r — e 1K p [7][19]. By consequence, if attempting to execute N +L+ cycles Sof error correction without any r+1 errors accumulating +L+ before the recovery ends, then N — exp⎜ξp +L+ </SectLabel_bodyText> <SectLabel_none> ⎛ − 1 +L+ </SectLabel_none> <SectLabel_bodyText> . Therefore the +L+ accuracy degree will be of the form � —(logN)−p , which is +L+ better than the accuracy degree corresponding to the no-coding +L+ case, � — N-1. However, there exists a Nmax so that if N > Nmax +L+ then non-correctable error becomes likely, which limits the length +L+ of the recovery process. Given the extremely large number of +L+ gates employed by a quantum algorithm implementation, Nmax +L+ also has to be very large; for Shor's algorithm Nmax must be +L+ higher than 3⋅109 [30]. +L+ As shown in Figure 7, the required accuracy degree approaches +L+ today's technological limits (tipically 10-3 for p=4) after N=105. +L+ For a fault tolerant encoding solution for Shor algorithm +L+ implementation this should have happened after N=109 [19][34]. +L+ +	(2)	Additional fault tolerance must be employed in order to preserve +L+ </SectLabel_bodyText> <SectLabel_figure> ⎯⎯⎯→⎨error ⎧ ⎪ ⎪ ⎪ ⎪ ⎩ +L+ a +L+ a1 0 + a0 1 for a flip error +L+ . +L+ a0 0 −a1 1 for a phase error +L+ a1 0 − a0 1 for both flip and phase errors +L+ 0 0 +a1 1 if no error occurs +L+ a00+a11 +L+ = 1 0000000 + 0010111 + 0101110 + 0111001 +L+ + 1001011 + 1011100 + 1100101 + 1110010 +L+ 232 +L+ ( +L+ ) +L+ 1 +L+ = +L+ S	232	evelu0`2u3c01c2) +L+ 0 +L+ u0u1u2u3c0c1c2 +L+ +	(3) +L+ ) +L+ 0 S =H⋅ 0S= 1 +L+ 1 S =H⋅ 1 S = 1 +L+ 2 +L+ 2 +L+ (0S+1S) +L+ (0S−1S) +L+ </SectLabel_figure> <SectLabel_none> (4) +L+ </SectLabel_none> <SectLabel_bodyText> reliable quantum computation over an arbitrary number of +L+ computational steps. Concatenated coding represents one such +L+ technique, which improves the reliability by shaping the size of +L+ </SectLabel_bodyText> <SectLabel_page> 192 +L+ </SectLabel_page> <SectLabel_bodyText> the code blocks and the number of code levels. It is also resource +L+ demanding and vulnerable to correlated errors [19][37]. +L+ Another approach, replacing the concatenated codes, is based on +L+ Reconfigurable Quantum Gate Arrays (RQGAs) [34][37], which +L+ are used for configuring ECC circuits based on stabilizer codes +L+ [7][33]. By using a quantum configuration register for the RQGA +L+ (i.e. a superposition of classical configurations), the +L+ reconfigurable circuit is brought to a state where it represents a +L+ simultaneous superposition of distinct ECC circuits. After +L+ measuring the configuration register, only one ECC circuit is +L+ selected and used; if k distinct ECC circuits were superposed and +L+ the gate error rate is � , then the overall gate error probability +L+ becomes �k (see Figure 8). As a result, the accuracy threshold +L+ value for the RQGA solution clearly dominates the technological +L+ accuracy limit, as shown in Figure 9 [37]. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 7. Accuracy plots: p=3 for xi1, p=4 for xi2, p=5 for xi3; +L+ </SectLabel_figureCaption> <SectLabel_bodyText> xi4 for no-coding, ref is the reference accuracy (i.e. the +L+ accuracy allowed by today's state of the art technology) +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 8. A quantum configuration register acts as a +L+ </SectLabel_figureCaption> <SectLabel_bodyText> superposition of k distinct circuits sharing the same input +L+ state and the same output qubits +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3. DEPENDABLE SYSTEM DESIGN +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In order to model the erroneous behavior of a device of system it +L+ is necessary to understand the causality of phenomena concerned. +L+ A defect affecting a device from a physical point of view is called +L+ a fault, or a fail. Faults may be put in evidence through logical +L+ misbehavior, in which case they transform into errors. Finally, +L+ errors accumulating can lead to system failure [8]. The fault- +L+ error-failure causal chain is essential to developping techniques +L+ that reduce the risk of error occurrence, even in the presence of +L+ faults, in order to minimize the probability of a system failure, +L+ and can be architecture specific. We will elaborate next on +L+ techniques used by a bio-inspired and by a quantum computing +L+ platform. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 9. Evolution of accuracy threshold value for RQHW +L+ </SectLabel_figureCaption> <SectLabel_bodyText> stabilizer codes (xir); the technological accuracy limit (dim) is +L+ also provided for a relevant comparison +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.1 The Embryonics Approach +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Several years before his untimely death John von Neumann began +L+ developping a theory of automata, which was to contain a +L+ systematic theory of mixed mathematical and logical forms, +L+ aimed to a better understanding of both natural systems and +L+ computers [14]. The essence of von Neumann’s message appears +L+ to entail the formula “genotype + ribotype = phenotype”. He +L+ provided the foundations of a self-replicating machine (the +L+ phenotype), consisting of its complete description (the genotype), +L+ which is interpreted by a ribosome (the ribotype). +L+ Embryonics (a contraction for embryonic electronics) is a long +L+ term research project launched by the Logic Systems Laboratory +L+ at the Swiss Federal Institute of Technology, Lausanne, +L+ Switzerland. Its aim is to explore the potential of biologically- +L+ inspired mechanisms by borrowing and adapting them from +L+ nature into digital devices for the purpose of endowing them with +L+ the remarkable robustness present in biological entities [39]. +L+ Though perhaps fuzzy at a first glance, analogies between biology +L+ and electronics are presented in Table 1 [12][31]. +L+ But if we consider that the function of a living cell is determined +L+ by the genome, and that a computer’s functionality is determined +L+ by the operating program, then the two worlds may be regarded as +L+ sharing a certain degree of similarity. Three fundamental features +L+ shared by living entities are required to be targetted by +L+ Embryonics in order to embody the formula “genotype + ribotype +L+ = phenotype” into digital hardware: +L+ </SectLabel_bodyText> <SectLabel_listItem> –multicellular organisms are made of a finite number of cells, +L+ which in turn are made of a finite number of chemically +L+ bonded molecules; +L+ –each cell (beginning with the original cell, the zygote) may +L+ generate one or several daughter cell(s) through a process +L+ called cellular division; both the parent and the daughter +L+ cell(s) share the same genetic information, called the genome; +L+ –different types of cells may exist due to cellular +L+ differentiation, a process through which only a part of the +L+ genome is executed. +L+ </SectLabel_listItem> <SectLabel_bodyText> These fundamental features led the Embryonics project to settle +L+ for an architectural hierarchy of four levels (see Figure 10). We +L+ will not delve very deep inside the Embryonics’phylosophy, as +L+ such details were broadly covered by literature [12][20][23][24] +L+ [25][40]; we will, however, introduce each of the four levels in +L+ </SectLabel_bodyText> <SectLabel_page> 193 +L+ </SectLabel_page> <SectLabel_bodyText> order to be able to see how this bio-inspired platform fits modern +L+ design for dependability efforts. +L+ </SectLabel_bodyText> <SectLabel_tableCaption> Table 1. Analogies present in Embryonics [12] +L+ </SectLabel_tableCaption> <SectLabel_table> Biology	Electronics +L+ Multicellular organism	Parallel computer systems +L+ Cell	Processor +L+ Molecule	FPGA Element +L+ </SectLabel_table> <SectLabel_figureCaption> Figure 10. Structural hierarchy in Embryonics [12] +L+ </SectLabel_figureCaption> <SectLabel_bodyText> The upmost level in Embryonics, bearing a certain similarity to +L+ what is found in nature, is the population level, composed of a +L+ number of organisms. One level down the hierarchy constitutes +L+ the organismic level, and corresponds to individual entities in a +L+ variety of functionalities and sizes. Each of the organisms may be +L+ further decomposed into smaller, simpler parts, called cells, which +L+ in turn may be decomposed in molecules. According to +L+ Embryonics, a biological organism corresponds in the world of +L+ digital systems to a complete computer, a biological cell is +L+ equivalent to a processor, and the smallest part in biology, the +L+ molecule, may be seen as the smallest, programmable element in +L+ digital electronics (see Table 1). +L+ An extremely valuable consequence of the Embryonics +L+ architecture is that each cell is "universal", containing a copy of +L+ the whole of the organism’s genetic material, the genome. This +L+ enables very flexible redundancy strategies, the living organisms +L+ being capable of self-repair (healing) or self-replication (cloning) +L+ [12]. Self-replication may be of great interest in the +L+ nanoelectronics era, where extremely large areas of +L+ programmable logic will probably render any centralized control +L+ very inefficient. Instead, the self-replication mechanism +L+ implemented in Embryonics will allow the initial colonization of +L+ the entire programmable array in a decentralized and distributed +L+ manner. Figure 11 presents an example of such colonization. At +L+ initial time the configuration bitstream (containing the genome) +L+ enters the bottom left corner of a programmable array and, at each +L+ clock cycle, the genome is pushed through and partitions the +L+ programmable space accordingly. +L+ From a dependability standpoint, the Embryonics hierarchical +L+ architecture offers incentives for an also hierarchical self-repair +L+ strategy. Because the target applications are those in which the +L+ failure frequency must be very low to be “acceptable”, two levels +L+ of self-repair are offered: at the molecular level (programmable +L+ logic is susceptible to soft fail occurrences) and at the cellular +L+ level (soft fails manifest at this level as soft errors). +L+ Let us consider an example of a simple cell made of 3 lines and 3 +L+ columns of molecules, of which one column contains spare +L+ molecules. If a fault occurs inside an active cell, it can be repaired +L+ through transferring its functionality toward the appropriate spare +L+ molecule, which will become active (see Figure 12). +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 11. Space colonization in Embryonics [11] +L+ Figure 12. Self-repair at the molecular level: faulty molecule +L+ E is replaced by spare molecule H, which becomes active [39] +L+ </SectLabel_figureCaption> <SectLabel_bodyText> The self-repair process at molecular level ensures the fault +L+ recovery as long as there are spare molecules left for repair. +L+ However, it is possible for a cell to experience a multiple error, in +L+ which case the self-repair mechanism at the molecular level can +L+ no longer reconfigure the inside of the cell successfully. If such a +L+ situation arises, then a second self-repair strategy is trigerred at a +L+ higher level. The cell will “die”, therefore trigerring the self- +L+ repair at the cellular level, the entire column containing the faulty +L+ cell (cell C in this example) being deactivated, its role being taken +L+ by the nearest spare column to the right (see Figure 13). +L+ A critique that could be addressed to the current Embryonics +L+ design would be its strategy of self-repair at the higher, cellular +L+ level: in case of a faulty cell, an entire column containing that cell +L+ will be deactivated, its role being transferred to the first available +L+ column of spares to the right (see Figure 13). There are two points +L+ in which this strategy could benefit: +L+ </SectLabel_bodyText> <SectLabel_page> 194 +L+ </SectLabel_page> <SectLabel_listItem> 1. Instead of deactivating a whole column of cells, it would be +L+ more efficient to only deactivate the faulty cell only (see +L+ Figure 14). The resources affected by the role transfer would +L+ be greatly reduced (one cell versus an entire column), +L+ coupled with the fact that particle flux generating soft fails is +L+ unlikely to be homogeneous and isotrope. This means +L+ regions assimilable more likely to cells rather than entire +L+ column of cells would be more affected by soft fails, not to +L+ mention that during genetic data transfer (required by taking +L+ over the role of the faulty cell) there is a greater risk of +L+ enduring a new soft fail (moving data is much more sensitive +L+ to soft fails than static data) [5][10]. +L+ 2. Such a strategy would be consistent with that used for the +L+ self-repair at the molecular level, which would simplify a +L+ thorough reliability analysis. Concatenated coding would +L+ also seem easier to be implemented and the strategy +L+ consistency would mean that concatenated coding would not +L+ be limited to a two-level hierarchy [20][21]. +L+ </SectLabel_listItem> <SectLabel_figureCaption> Figure 13. Molecular self-repair failure: the cell “dies” +L+ (bottom), triggering the cellular self-repair (top) [39] +L+ </SectLabel_figureCaption> <SectLabel_bodyText> We consider a cell of M lines and N columns, being composed of +L+ modules of M lines and n+s columns (for instance, the cell +L+ presented in Figure 12 consists of a single such module of two +L+ active columns and one spare column), of which s are spares. In +L+ order to meet certain reliability criteria, it is necessary to know +L+ what is the number s of spare columns of molecules that +L+ correspond to n columns of active molecules, that is, the +L+ horizontal dimensions for such a module. We will not provide a +L+ thorough reliability analysis, as this has been done previously +L+ [4][17][20][21]; instead, we will analyze the influences of the +L+ proposed consistent self-repair strategy at both molecular and +L+ cellular levels through the use of logic molecules. Therefore +L+ Equation (5) holds: +L+ </SectLabel_bodyText> <SectLabel_equation> k +L+ RModRow (t)=Prob{ no fails} (t) + ∑ Prob{ i fails} (t) +L+ i=1	(5) +L+ N=k(n+s) +L+ </SectLabel_equation> <SectLabel_bodyText> where RModRow(t) represents the reliability function for a row +L+ within a module. Then, considering the failure rate for one +L+ molecule λ, the probability of all molecules (both active and +L+ spare) to operate normally in a module’s row becomes: +L+ </SectLabel_bodyText> <SectLabel_equation> Prob{ no fails} (t) = e−"n+s)t	(6) +L+ </SectLabel_equation> <SectLabel_bodyText> The probability of a row enduring i fails in the active molecules +L+ part is the conditional probability of having n-i active molecules +L+ operating normally, while a number of s-i spare molecules are +L+ ready to activate (that is, they are not affected by errors +L+ themselves): +L+ </SectLabel_bodyText> <SectLabel_equation> Prob{ i fails} (t) = Prob{ i fails active} (t) +L+ ⋅Prob{i spares ok}(t) +L+ Prob{ i fails active} (t) = (n Je λ(n-i)t (1 − eλ(n-i)t)	(8) +L+ i	l +L+ Prob{ i spares ok} (t) = (k )e−λit (1 − a λ(k−i)t +L+ i +L+ </SectLabel_equation> <SectLabel_bodyText> Then the reliability function for an entire cell is the cummulated +L+ reliability functions for the total number of modules: +L+ </SectLabel_bodyText> <SectLabel_equation> RCell(t) = [RModRow(t)]MN`	(10) +L+ </SectLabel_equation> <SectLabel_figureCaption> Figure 14. Proposed reconfiguration strategy at the cellular +L+ </SectLabel_figureCaption> <SectLabel_bodyText> level +L+ A self-repair strategy that conserves the consistency between the +L+ molecular and the cellular level would allow for a more +L+ straightforward reliability analysis. Basically, it would be +L+ sufficient to substitute dimension parameters in Equations (5)- +L+ (10) with those approapriate to the analysis of an organism +L+ instead of a cell. To illustrate this, we will consider an organism +L+ of M* lines and N* columns, being composed of modules of M* +L+ lines and n*+s* columns, of which s* are spares; we will also use +L+ the organism partitioning into modules, similar to the partitioning +L+ of cells used before. Therefore Equation (5) transforms into +L+ Equation (11): +L+ </SectLabel_bodyText> <SectLabel_equation> k +L+ RCellMR (t)=Prob*{nofails}(t)+∑Prob* { ifails} (t) i = 1	(11) +L+ N*=k*(n +s ) +L+ </SectLabel_equation> <SectLabel_bodyText> where RCellMR (t) represents the reliability function for a row of +L+ cells within an organism module. In this case, the significance of +L+ the terms will be as follows: +L+ </SectLabel_bodyText> <SectLabel_equation> Prob {no fails} (t) = [RCell ( tj +s	(12) +L+ ⋅ +L+ (7) +L+ (9) +L+ </SectLabel_equation> <SectLabel_page> 195 +L+ </SectLabel_page> <SectLabel_bodyText> While Equation (7) continues to hold under the form of Equation +L+ (13), the significance of its terms will change according to the +L+ dimensions at the cellular level: +L+ </SectLabel_bodyText> <SectLabel_equation> ⎛ ⎞ − +L+ Prob*{ifailsactive}(t)=⎜ +L+  i +L+ (t)(1−R +L+ L +L+ ll(t +L+ ⎛ ⎞	− +L+ Prob*{isparesok}(t)=⎜k* +L+ J +L+ RCiell(t)(1−RCkelli(t))	(15) i +L+ t0, t1,..., tm−1 will be given by +L+ The outputs ofthe firstcycle, whichare also inputs forthe +L+ The used FTAMs are only valid if the relationship between the +L+ </SectLabel_equation> <SectLabel_bodyText> experimental ξsim and the assumed singular error rateξ is of the +L+ </SectLabel_bodyText> <SectLabel_equation> order gsim _ � 2 [19]. +L+ ) )(14) +L+ </SectLabel_equation> <SectLabel_bodyText> Finally, similar to Equation(10), the reliabilityfunctionforan +L+ entire organism is the cummulated re +L+ liability functions for the +L+ total number of its modules: +L+ </SectLabel_bodyText> <SectLabel_equation> ROrg(t)= +L+  [ +L+ RCellMR(t) +L+ ] +L+ MN +L+  � +L+ + +L+ s	(16) +L+ </SectLabel_equation> <SectLabel_bodyText> Equations (5) to (16) provide the basics forathorough reliability +L+ analysis for the proposed, uniformstrategy ofhierarchical +L+ reconfiguration, as opposedto the analysis providedby [21], +L+ whichspecifically targetted the currentEmbryonics architecture. +L+ Despite having settled the reliabilitymodel, bothanalyses are +L+ incomplete, inthatthe failure rate parameteris missing, which +L+ makes aprecise, quantitative dependability targetdifficultto +L+ meet. However, areliability analysis is still valuable from a +L+ qualitative pointofview, allowingadirectcomparison of +L+ differentsystems. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 3.2 The QUERISTApproach +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Inorderto deal with errors inducedby the constantinfluence of +L+ the external environmentuponcomputational processes, the +L+ following assumptions were made: errors appear randomly, are +L+ uncorrelated (neitherinspace, norintime), there are no storage +L+ errors, andthere are no leakage phenomenainvolved[19]. +L+ Classical HDL-based faultinjectionmethodologies can be +L+ mappedto simulatingquantumcircuits withoutintervention +L+ providedthatthe new errorand faultmodels are takeninto +L+ account[35]. Ofcourse, efficiencycriteriarequire thatthey be +L+ adaptedto one ofthe available efficientsimulationframeworks +L+ [36][38][41]. QUERIST(from QUantum ERrorInjection +L+ Simulation Tool) is the name ofsucha project, fostering +L+ simulated faultinjectiontechniques inquantum circuits [34]. +L+ Similarto classical computation, simulatedfaultinjection is used +L+ in orderto evaluate the employed FTAMS (FaultTolerance +L+ Algorithms andMethodologies) [26][27]. +L+ Anoverview ofthe QUERISTprojectis presented in Figure 15. +L+ The three cycles ofinitialization, simulation, anddata +L+ computationare commonto bothclassical andquantum +L+ approaches. The firstcycle takes the quantumcircuitHDL +L+ description as aninput. Two abstractinputs are considered, the +L+ HDL model andthe assumederrormodel; the firstinfluences how +L+ the HDLdescription is presented, while the secondone dictates +L+ the testscenario by definingthe start/stop simulationstates (since +L+ qubits are equallyprone to error, all the signals mustbe +L+ observed). HDLmodelingofquantumcircuits inorderto attain +L+ effi +L+ cient simulation is discussed in [34][35][36][38]. +L+ </SectLabel_bodyText> <SectLabel_page> 196 +L+ </SectLabel_page> <SectLabel_bodyText> secondcycle consists oftime diagrams forall qubits, from the +L+ startto the stop states. Useful information, extracted fromthe raw, +L+ bubble-bit-represented, qubittraces are comparedto correctqubit +L+ values, the resultbeingthe probabilistic accuracy thresholdvalue, +L+ inthe thirdcycle. The initialization andsimulation cycles depend +L+ on specific aspects ofquantum circuitsimulation [35]. The data +L+ processing cycle is independentfromthe specific simulation +L+ framework andis aimedatdeterminingthe accuracythresholdas +L+ the mainreliability measure thatalso defines the feasibility ofthe +L+ quantum circuitimplementations. +L+ Suppose that, atsimulationtime twe observe signals +L+ {s0,s1,...,sn} +L+  . In ouranalysis, +L+ si +L+  is the state observedduringnon- +L+ faulty simulation, so forthe same state ina faulty environmentwe +L+ will have the state +L+ si* . +L+ Forvalidationofthe quantum FTAMs, we needto compare +L+ si +L+ with +L+ si*. +L+  This can be done by using operator +L+  d +L+ if +L+ (si,s +L+ ; +L+ ) +L+  . This +L+ mean +L+ s that the total number of overall state errors at simulation +L+ n − 1 +L+ time tis +L+ . et=∑d +L+ if +L+ (si,s +L+ ; +L+ )The error rate on the overall observed +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 4. CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper presented arguments in favor of two novel computing +L+ architectures for the purpose of addressing the challenges raised +L+ by the forthcoming nanoelectronics era. Distributed self-testing +L+ and self-repairing will probably become a must in the next years +L+ as centralized control logic is expected to become unable to +L+ harness the extremely large number of devices, all equally prone +L+ to errors, that will be integrated onto the same chip. Bio-inspired +L+ computing brings valuable techniques that explore the potential of +L+ massively parallel, distributed computation and fault-tolerance +L+ that will likely provide an essential help to jumpstart new +L+ nanoelectronic architectures. As one of the representatives of bio- +L+ inspired computing, the Embryonics project presents a +L+ hierarchical architecture that achieves fault tolerance through +L+ implementing an also hierarchical reconfiguration. A similar +L+ approach for maximizing fault tolerance is present in quantum +L+ computing, the QUERIST project; even if bio-inspired and +L+ quantum computing may seem dissimilar at a first glance, they +L+ both achieve fault tolerance by adapting the same techniques from +L+ classical computing and using essentially the same error model. +L+ Nanoelectronics will potentially change the way computing +L+ systems are designed, not only because of the sheer number of +L+ devices that will coexist onto the same chip, but also because of +L+ the sensitivity of these devices. +L+ </SectLabel_bodyText> <SectLabel_equation> Prob* +L+ { i fails} (t) = Prob* { i fails active} (t) +L+ ⋅ +L+ ⋅ Prob* { i spares ok} (t) +L+ (13) +L+ i=0 +L+ </SectLabel_equation> <SectLabel_figureCaption> simulation cycle, consistofatestscenario and an executable HDL +L+ model withthe correspondingentanglementanalysis, dictatedby +L+ the bubble-bitencoded quantum states [36][38]. The outputofthe +L+ states at moments +L+ </SectLabel_figureCaption> <SectLabel_equation> 1 m−1 +L+ ξsim	∑ et • +L+ m +L+ j=0 � +L+ </SectLabel_equation> <SectLabel_figureCaption> Figure 15. An overview of the QUERIST project +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Therefore, if nanoelectronics is to be employed to build +L+ dependable computing machines (a certain contradiction +L+ notwithstanding), valuable expertise in design can be drawn from +L+ natural sciences. While biology provides countless examples of +L+ successfully implemented fault tolerance strategies, physics offers +L+ theoretical foundations, both of which were found to share +L+ common ground. It is perhaps a coincidence worth exploring in +L+ digital computing. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5. REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] Aharonov, D., Ben-Or, M. Fault Tolerant Quantum +L+ Computation with Constant Error. Proc. ACM 29th Ann. +L+ Symposium on Theory of Computing, El Paso, Texas, May +L+ 1997, pp. 176-188. +L+ [2] Avižienis, A., Laprie, J.C., Randell, B., Landwehr, C. Basic +L+ Concepts and Taxonomy of Dependable and Secure +L+ Computing. IEEE Transactions on Dependable and Secure +L+ Computing, 1, 1 (Jan-Mar 2004), 11-33. +L+ [3] Butts, M., DeHon, A., Golstein, S.C. Molecular Electronics: +L+ Devices, Systems and Tools for Gigagate, Gigabit Chips. +L+ Proc. Intl. Conference on CAD (ICCAD’02), 2002, pp. 433- +L+ 440. +L+ [4] Canham, R., Tyrrell, A. An Embryonic Array with Improved +L+ Efficiency and Fault Tolerance. Proc. IEEE NASA/DoD +L+ Conference on Evolvable Hardware, Chicago Il, 2003, 275- +L+ 282. +L+ [5] Gaisler, J. Evaluation of a 32-Bit Microprocessor with Built- +L+ In Concurrent Error Detection. Proc. 27th Annual Intl. +L+ Symposium on Fault-Tolerant Computing (FTCS-27), 1997, +L+ pp. 42-46. +L+ [6] Goldstein, S.C. The Challenges and Opportunities of +L+ Nanoelectronics. Proc. Government Microcircuit Applica- +L+ tions and Critical Technology Conference (GOMAC Tech - +L+ 04), Monterey, CA, March 2004. +L+ [7] Gottesman, D. Class of quantum error-correcting codes +L+ saturating the quantum Hamming bound. Phys. Rev. A 54, +L+ 1996, pp. 1862-1868. +L+ [8] Johnson, B.W. Design and Analysis of Fault-Tolerant +L+ Digital Systems. Addison-Wesley, 1989. +L+ [9] Laprie, J.-C. (Ed.). Dependability: Basic Concepts and +L+ Terminology. Dependable Computing and Fault-Tolerant +L+ Systems Series, Vol. 5, Springer-Verlag, Vienna, 1992. +L+ [10] Liden, P., Dahlgren, P., Johansson, R., Karlsson, J. On +L+ Latching Probability of Particle Induced Transients in +L+ Combinational Networks. Proc. Intl. Symposium on Fault- +L+ Tolerant Computing (FTCS-24), 1994, pp.340-349. +L+ [11] Mange, D., Sipper, M., Stauffer, A., Tempesti, G. Toward +L+ Robust Integrated Circuits: The Embryonics Approach. Proc. +L+ of the IEEE, vol. 88, No. 4, April 2000, pp. 516-541. +L+ [12] Mange, D. and Tomassini, M. eds. Bio -Inspired Computing +L+ Machines: Towards Novel Computational Architectures. +L+ Presses Polytechniques et Universitaires Romandes, +L+ Lausanne, Switzerland, 1998. +L+ [13] Von Neumann, J. The Computer and the Brain (2nd edition). +L+ Physical Science, 2000. +L+ [14] Von Neumann, J. The Theory of Self-Reproducing +L+ Automata. A. W. Burks, ed. University of Illinois Press, +L+ Urbana, IL, 1966. +L+ [15] Von Neumann, J. Probabilistic Logic and the Synthesis of +L+ Reliable Organisms from Unreliable Components. In C.E. +L+ Shannon, J. McCarthy (eds.) Automata Studies, Annals of +L+ Mathematical Studies 34, Princeton University Press, 1956, +L+ 43-98. +L+ [16] Nielsen, M.A., Chuang, I.L. Quantum Computation and +L+ Quantum Information. Cambridge University Press, 2000. +L+ [17] Ortega, C., Tyrrell, A. Reliability Analysis in Self-Repairing +L+ Embryonic Systems. Proc. 1st NASA/DoD Workshop on +L+ Evolvable Hardware, Pasadena CA, 1999, 120-128. +L+ [18] O’Connor, P.D.T. Practical Reliability Engineering. John +L+ Wiley & Sons, 4th edition, 2002. +L+ [19] Preskill, J. Fault Tolerant Quantum Computation. In H.K. +L+ Lo, S. Popescu and T.P. Spiller, eds. Introduction to +L+ Quantum Computation, World Scientific Publishing Co., +L+ 1998. +L+ </SectLabel_reference> <SectLabel_page> 197 +L+ </SectLabel_page> <SectLabel_reference> [20] Prodan, L. Self-Repairing Memory Arrays Inspired by +L+ Biological Processes. Ph.D. Thesis, “Politehnica” University +L+ of Timisoara, Romania, October 14, 2005. +L+ [21] Prodan, L., Udrescu, M., Vladutiu, M. Survivability Analysis +L+ in Embryonics: A New Perspective. Proc. IEEE NASA/DoD +L+ Conference on Evolvable Hardware, Washington DC, 2005, +L+ 280-289. +L+ [22] Prodan, L., Udrescu, M., Vladutiu, M. Self-Repairing +L+ Embryonic Memory Arrays. Proc. IEEE NASA/DoD +L+ Conference on Evolvable Hardware, Seattle WA, 2004, 130- +L+ 137. +L+ [23] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. +L+ Embryonics: Electronic Stem Cells. Proc. Artificial Life VIII, +L+ The MIT Press, Cambridge MA, 2003, 101-105. +L+ [24] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. +L+ Embryonics: Artificial Cells Driven by Artificial DNA. +L+ Proc. 4th International Conference on Evolvable Systems +L+ (ICES2001), Tokyo, Japan, LNCS vol. 2210, Springer, +L+ Berlin, 2001, 100-111. +L+ [25] Prodan, L., Tempesti, G., Mange, D., and Stauffer, A. +L+ Biology Meets Electronics: The Path to a Bio-Inspired +L+ FPGA. In Proc. 3rd International Conference on Evolvable +L+ Systems (ICES2000), Edinburgh, Scotland, LNCS 1801, +L+ Springer, Berlin, 2000, 187-196. +L+ [26] Rimen, M., Ohlsson, J., Karlsson, J., Jenn, E., Arlat, J. +L+ Validation of fault tolerance by fault injection in VHDL +L+ simulation models. Rapport LAAS No. 92469, December +L+ 1992. +L+ [27] Rimen, M., Ohlsson, J., Karlsson, J., Jenn, E., Arlat, J. +L+ Design guidelines of a VHDL-based simulation tool for the +L+ validation of fault tolerance. Rapport LAAS No931 70, Esprit +L+ Basic Research Action No. 6362, May 1993. +L+ [28] Shivakumar, P., Kistler, M., Keckler, S.W., Burger, D., +L+ Alvisi, L. Modelling the Effect of Technology Trends on the +L+ Soft Error Rate of Combinational Logic. Proc. Intl. +L+ Conference on Dependable Systems and Networks (DSN), +L+ June 2002, pp. 389-398. +L+ [29] Shor, P. Fault-tolerant quantum computation. +L+ arXiv.org:quant-ph/9605011, 1996. +L+ [30] Shor, P. Algorithms for Quantum Computation: Discrete +L+ Logarithms and Factoring. Proc. 35th Symp. on Foundations +L+ of Computer Science, 1994, pp. 124-134. +L+ [31] Sipper, M., Mange, D., Stauffer, A. Ontogenetic Hardware. +L+ BioSystems, 44, 3, 1997, 193-207. +L+ [32] Sipper, M., Sanchez, E., Mange, D., Tomassini, M., Perez- +L+ Uribe, A., Stauffer, A. A Phylogenetic, Ontogenetic and +L+ Epigenetic View of Bio-Inspired Hardware Systems. IEEE +L+ Transactions on Evolutionary Computation, 1, 1, April 1997, +L+ 83-97. +L+ [33] Steane, A. Multiple Particle Interference and Quantum Error +L+ Correction. Proc. Roy. Soc. Lond. A 452, 1996, pp. 2551. +L+ [34] Udrescu, M. Quantum Circuits Engineering: Efficient +L+ Simulation and Reconfigurable Quantum Hardware. Ph.D. +L+ Thesis, “Politehnica” University of Timisoara, Romania, +L+ November 25, 2005. +L+ [35] Udrescu, M., Prodan, L., Vladutiu, M. Simulated Fault +L+ Injection in Quantum Circuits with the Bubble Bit +L+ Technique. Proc. International Conference "Adaptive and +L+ Natural Computing Algorithms", pp. 276-279. +L+ [36] Udrescu, M., Prodan, L., Vladutiu, M. The Bubble Bit +L+ Technique as Improvement of HDL-Based Quantum Circuits +L+ Simulation. IEEE 38th Annual Simulation Symposium, San +L+ Diego CA, USA, 2005, pp. 217-224. +L+ [37] Udrescu, M., Prodan, L., Vladutiu, M. Improving Quantum +L+ Circuit Dependability with Reconfigurable Quantum Gate +L+ Arrays. 2nd ACM International Conference on Computing +L+ Frontiers, Ischia, Italy, 2005, pp. 133-144. +L+ [38] Udrescu, M., Prodan, L., Vladutiu, M. Using HDLs for +L+ describing quantum circuits: a framework for efficient +L+ quantum algorithm simulation. Proc. 1st ACM Conference +L+ on Computing Frontiers, Ischia, Italy, 2004, 96-110. +L+ [39] Tempesti, G. A Self-Repairing Multiplexer-Based FPGA +L+ Inspired by Biological Processes. Ph.D. Thesis No. 1827, +L+ Logic Systems Laboratory, The Swiss Federal Institute of +L+ Technology, Lausanne, 1998. +L+ [40] Tempesti, G., Mange, D., Petraglio, E., Stauffer, A., Thoma +L+ Y. Developmental Processes in Silicon: An Engineering +L+ Perspective. Proc. IEEE NASA/DoD Conference on +L+ Evolvable Hardware, Chicago Il, 2003, 265-274. +L+ [41] Viamontes, G., Markov, I., Hayes, J.P. High-performance +L+ QuIDD-based Simulation of Quantum Circuits. Proc. Design +L+ Autom. and Test in Europe (DATE), Paris, France, 2004, pp. +L+ 1354-1359. +L+ [42] Yu, Y., Johnson, B.W. A Perspective on the State of +L+ Research on Fault Injection Techniques. Technical Report +L+ UVA-CSCS-FIT-001, University of Virginia, May 20, 2002. +L+ [43] ***. ITRS – International Technology Roadmap for Semic- +L+ onductors, Emerging Research Devices, 2004, http://www. +L+ itrs.net/Common/2004Update/2004_05_ERD.pdf +L+ [44] ***. Society of Reliability Engineers, http://www.sre.org/ +L+ pubs/ +L+ [45] ***. http://www.dependability.org/wg10.4/ +L+ </SectLabel_reference> <SectLabel_page> 198 +L+ </SectLabel_page>
<SectLabel_title> A Distributed 3D Graphics Library +L+ </SectLabel_title> <SectLabel_author> Blair MacIntyre and Steven Feiner1 +L+ </SectLabel_author> <SectLabel_affiliation> Department of Computer Science +L+ Columbia University +L+ </SectLabel_affiliation> <SectLabel_sectionHeader> Abstract +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We present Repo-3D, a general-purpose, object-oriented library for +L+ developing distributed, interactive 3D graphics applications across +L+ a range of heterogeneous workstations. Repo-3D is designed to +L+ make it easy for programmers to rapidly build prototypes using a +L+ familiar multi-threaded, object-oriented programming paradigm. +L+ All data sharing of both graphical and non-graphical data is done +L+ via general-purpose remote and replicated objects, presenting the +L+ illusion of a single distributed shared memory. Graphical objects +L+ are directly distributed, circumventing the “duplicate database” +L+ problem and allowing programmers to focus on the application +L+ details. +L+ Repo-3D is embedded in Repo, an interpreted, lexically-scoped, +L+ distributed programming language, allowing entire applications to +L+ be rapidly prototyped. We discuss Repo-3D’s design, and introduce +L+ the notion of local variations to the graphical objects, which allow +L+ local changes to be applied to shared graphical structures. Local +L+ variations are needed to support transient local changes, such as +L+ highlighting, and responsive local editing operations. Finally, we +L+ discuss how our approach could be applied using other program- +L+ ming languages, such as Java. +L+ </SectLabel_bodyText> <SectLabel_category> CR Categories and Subject Descriptors: D.1.3 [Program- +L+ ming Techniques]: Concurrent Programming—Distributed Pro- +L+ gramming; H.4.1 [Information Systems Applications]: Office +L+ Automation—Groupware; I.3.2 [Computer Graphics]: Graphics +L+ Systems—Distributed/network graphics; I.3.6 [Computer Graph- +L+ ics]: Methodology and Techniques—Graphics data structures and +L+ data types; I.3.7 [Computer Graphics]: Three-Dimensional +L+ Graphics and Realism—Virtual reality. +L+ </SectLabel_category> <SectLabel_keyword> Additional Keywords and Phrases: object-oriented graphics, +L+ distributed shared memory, distributed virtual environments, +L+ shared-data object model. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> 1 INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Traditionally, distributed graphics has referred to the architecture +L+ of a single graphical application whose components are distributed +L+ over multiple machines [14, 15, 19, 27] (Figure 1a). By taking +L+ advantage of the combined power of multiple machines, and the +L+ particular features of individual machines, otherwise impractical +L+ applications became feasible. However, as machines have grown +L+ more powerful and application domains such as Computer +L+ </SectLabel_bodyText> <SectLabel_footnote> 1. {bm,feiner}@cs.columbia.edu, http://www.cs.columbia.edu/graphics +L+ </SectLabel_footnote> <SectLabel_figureCaption> Figure 1: Two meanings of distributed graphics: (a) a single logical +L+ graphics system with distributed components, and (b) multiple dis- +L+ tributed logical graphics systems. We use the second definition here. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> Supported Cooperative Work (CSCW) and Distributed Virtual +L+ Environments (DVEs) have been making the transition from +L+ research labs to commercial products, the term distributed graphics +L+ is increasingly used to refer to systems for distributing the shared +L+ graphical state of multi-display/multi-person, distributed, interac- +L+ tive applications (Figure 1b). This is the definition that we use here. +L+ While many excellent, high-level programming libraries are +L+ available for building stand-alone 3D applications (e.g. Inventor +L+ [35], Performer [29], Java 3D [33]), there are no similarly powerful +L+ and general libraries for building distributed 3D graphics applica- +L+ tions. All CSCW and DVE systems with which we are familiar +L+ (e.g., [1, 7, 11, 12, 16, 28, 30, 31, 32, 34, 37, 41]) use the following +L+ approach: A mechanism is provided for distributing application +L+ state (either a custom solution or one based on a general-purpose +L+ distributed programming environment, such as ISIS [4] or Obliq +L+ [8]), and the state of the graphical display is maintained separately +L+ in the local graphics library. Keeping these “dual databases” syn- +L+ chronized is a complex, tedious, and error-prone endeavor. In con- +L+ trast, some non-distributed libraries, such as Inventor [35], allow +L+ programmers to avoid this problem by using the graphical scene +L+ description to encode application state. Extending this “single data- +L+ base” model to a distributed 3D graphics library is the goal of our +L+ work on Repo-3D. +L+ Repo-3D is an object-oriented, high-level graphics package, +L+ derived from Obliq-3D [25]. Its 3D graphics facilities are similar to +L+ those of other modern high-level graphics libraries. However, the +L+ objects used to create the graphical scenes are directly distribut- +L+ able—from the programmer’s viewpoint, the objects reside in one +L+ large distributed shared memory (DSM) instead of in a single +L+ process. The underlying system replicates any of the fine-grained +L+ objects across as many processes as needed, with no additional +L+ effort on the part of the programmer. Updates to objects are +L+ automatically reflected in all replicas, with any required objects +L+ automatically distributed as needed. By integrating the replicated +L+ objects into the programming languages we use, distributed +L+ applications may be built using Repo-3D with little more difficulty +L+ than building applications in a single process. +L+ No matter how simple the construction of a distributed applica- +L+ tion may be, a number of differences between distributed and +L+ monolithic applications must be addressed. These include: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Distributed control. In a monolithic application, a single com- +L+ ponent can oversee the application and coordinate activities +L+ among the separate components by notifying them of changes +L+ to the application state. This is not possible in a non-trivial dis- +L+ tributed application. Therefore, we must provide mechanisms +L+ for different components to be notified of changes to the +L+ distributed state. +L+ •	Interactivity. Updates to distributed state will be slower than +L+ updates to local state, and the amount of data that can be +L+ distributed is limited by network bandwidth. If we do not want +L+ to sacrifice interactive speed, we must be able to perform some +L+ operations locally. For example, an object could be dragged +L+ locally with the mouse, with only a subset of the changes +L+ applied to the replicated state. +L+ •	Local variations. There are times when a shared graphical +L+ scene may need to be modified locally. For example, a +L+ programmer may want to highlight the object under one user’s +L+ mouse pointer without affecting the scene graph viewed by +L+ other users. +L+ </SectLabel_listItem> <SectLabel_bodyText> Repo-3D addresses these problems in two ways. First, a +L+ programmer can associate a notification object with any replicated +L+ object. The notification object’s methods will be invoked when the +L+ replicated object is updated. This allows reactive programs to be +L+ built in a straightforward manner. To deal with the second and third +L+ problems, we introduce the notion of local variations to graphical +L+ objects. That is, we allow the properties of a graphical object to be +L+ modified locally, and parts of the scene graph to be locally added, +L+ removed, or replaced. +L+ In Section 2 we describe how we arrived at the solution presented +L+ here. Section 3 discusses related work, and Section 4 offers a +L+ detailed description of the underlying infrastructure that was used. +L+ The design of Repo-3D is presented in Section 5, followed by +L+ some examples and concluding remarks in Sections 6 and 7. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 2 BACKGROUND +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Repo-3D was created as part of a project to support rapid prototyp- +L+ ing of distributed, interactive 3D graphical applications, with a +L+ particular focus on DVEs. Our fundamental belief is that by +L+ providing uniform high-level support for distributed programming +L+ in the languages and toolkits we use, prototyping and experiment- +L+ ing with distributed interactive applications can be (almost) as +L+ simple as multi-threaded programming in a single process. While +L+ care must be taken to deal with network delays and bandwidth +L+ limitations at some stage of the program design (the languages and +L+ toolkits ought to facilitate this), it should be possible to ignore such +L+ issues until they become a problem. Our view can be summarized +L+ by a quote attributed to Alan Kay, “Simple things should be +L+ simple; complex things should be possible.” +L+ This is especially true during the exploration and prototyping +L+ phase of application programming. If programmers are forced to +L+ expend significant effort building the data-distribution components +L+ of the application at an early stage, not only will less time be spent +L+ exploring different prototypes, but radical changes in direction will +L+ become difficult, and thus unlikely. For example, the implementa- +L+ tion effort could cause programs to get locked into using a commu- +L+ nication scheme that may eventually prove less than ideal, or even +L+ detrimental, to the program’s final design. +L+ Since we are using object-oriented languages, we also believe +L+ that data distribution should be tightly integrated with the +L+ language’s general-purpose objects. This lets the language’s type +L+ system and programming constructs reduce or eliminate errors in +L+ the use of the data-distribution system. Language-level integration +L+ also allows the system to exhibit a high degree of network data +L+ transparency, or the ability for the programmer to use remote and +L+ local data in a uniform manner. Without pervasive, structured, +L+ high-level data-distribution support integrated into our program- +L+ ming languages and libraries, there are applications that will never +L+ be built or explored, either because there is too much programming +L+ overhead to justify trying simple things (“simple things are not +L+ simple”), or because the added complexity of using relatively +L+ primitive tools causes the application to become intractable (“com- +L+ plex things are not possible”). +L+ Of the tools available for integrating distributed objects into +L+ programming languages, client-server data sharing is by far the +L+ most common approach, as exemplified by CORBA [26], +L+ Modula-3 Network Objects [5], and Java RMI [39]. Unfortunately, +L+ interactive graphical applications, such as virtual reality, require +L+ that the data used to refresh the display be local to the process +L+ doing the rendering or acceptable frame refresh rates will not be +L+ achieved. Therefore, pure client-server approaches are inappropri- +L+ ate because at least some of the shared data must be replicated. +L+ Furthermore, since the time delay of synchronous remote method +L+ calls is unsuitable for rapidly changing graphical applications, +L+ shared data should be updated asynchronously. Finally, when data +L+ is replicated, local access must still be fast. +L+ The most widely used protocols for replicated data consistency, +L+ and thus many of the toolkits (e.g., ISIS [4] and Visual-Obliq [3]), +L+ allow data updates to proceed unimpeded, but block threads read- +L+ ing local data until necessary updates arrive. The same reason we +L+ need replicated data in the first place—fast local read access to the +L+ data—makes these protocols unsuitable for direct replication of the +L+ graphical data. Of course, these protocols are fine for replicating +L+ application state that will then be synchronized with a parallel +L+ graphical scene description, but that is what we are explicitly try- +L+ ing to avoid. Fortunately, there are replicated data systems (e.g., +L+ Orca [2] or COTERIE [24]) that provide replicated objects that are +L+ well suited to interactive applications, and it is upon the second of +L+ these systems that Repo-3D is built. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 3 RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> There has been a significant amount of work that falls under the +L+ first, older definition of distributed graphics. A large number of +L+ systems, ranging from established commercial products (e.g., IBM +L+ Visualization Data Explorer [21]) to research systems (e.g., +L+ PARADISE [19] and ATLAS [14]), have been created to distribute +L+ interactive graphical applications over a set of machines. However, +L+ the goal of these systems is to facilitate sharing of application data +L+ between processes, with one process doing the rendering. While +L+ some of these systems can be used to display graphics on more +L+ than one display, they were not designed to support high-level +L+ sharing of graphical scenes. +L+ Most high-level graphics libraries, such as UGA [40], Inventor +L+ [35] and Java 3D [33], do not provide any support for distribution. +L+ Others, such as Performer [29], provide support for distributing +L+ components of the 3D graphics rendering system across multiple +L+ processors, but do not support distribution across multiple +L+ machines. One notable exception is TBAG [13], a high-level +L+ constraint-based, declarative 3D graphics framework. Scenes in +L+ TBAG are defined using constrained relationships between time- +L+ varying functions. TBAG allows a set of processes to share a +L+ single, replicated constraint graph. When any process asserts or +L+ retracts a constraint, it is asserted or retracted in all processes. +L+ However, this means that all processes share the same scene, and +L+ that the system’s scalability is limited because all processes have a +L+ copy of (and must evaluate) all constraints, whether or not they are +L+ interested in them. There is also no support for local variations of +L+ the scene in different processes. +L+ Machiraju [22] investigated an approach similar in flavor to ours, +L+ but it was not aimed at the same fine-grained level of interactivity +L+ and was ultimately limited by the constraints of the implementa- +L+ tion platform (CORBA and C++). For example, CORBA objects +L+ are heavyweight and do not support replication, so much of their +L+ effort was spent developing techniques to support object migration +L+ and “fine-grained” object sharing. However, their fine-grained +L+ objects are coarser than ours, and, more importantly, they do not +L+ support the kind of lightweight, transparent replication we desire. +L+ A programmer must explicitly choose whether to replicate, move, +L+ or copy an object between processes when the action is to occur (as +L+ opposed to at object creation time). Replicated objects are indepen- +L+ dent new copies that can be modified and used to replace the origi- +L+ nal—simultaneous editing of objects, or real-time distribution of +L+ changes as they are made is not supported. +L+ Of greater significance is the growing interest for this sort of sys- +L+ tem in the Java and VRML communities. Java, like Modula-3, is +L+ much more suitable as an implementation language than C or C++ +L+ because of its cross-platform compatibility and support for threads +L+ and garbage collection: Without the latter two language features, +L+ implementing complex, large-scale distributed applications is +L+ extremely difficult. Most of the current effort has been focused on +L+ using Java as a mechanism to facilitate multi-user VRML worlds +L+ (e.g., Open Communities [38]). Unfortunately, these efforts +L+ concentrate on the particulars of implementing shared virtual +L+ environments and fall short of providing a general-purpose shared +L+ graphics library. For example, the Open Communities work is +L+ being done on top of SPLINE [1], which supports only a single +L+ top-level world in the local scene database. +L+ Most DVEs [11, 12, 16, 31, 32] provide support for creating +L+ shared virtual environments, not general purpose interactive 3D +L+ graphics applications. They implement a higher level of abstrac- +L+ tion, providing support for rooms, objects, avatars, collision detec- +L+ tion, and other things needed in single, shared, immersive virtual +L+ environments. These systems provide neither general-purpose +L+ programming facilities nor the ability to work with 3D scenes at a +L+ level provided by libraries such as Obliq-3D or Inventor. Some use +L+ communication schemes that prevent them from scaling beyond a +L+ relatively small number of distributed processes, but for most the +L+ focus is explicitly on efficient communication. SIMNET [7], and +L+ the later NPSNet [41], are perhaps the best known large-scale +L+ distributed virtual-environment systems. They use a fixed, well- +L+ defined communication protocol designed to support a single, +L+ large-scale, shared, military virtual environment. +L+ The techniques for object sharing implemented in recent CSCW +L+ toolkits [28, 30, 34, 37] provide some of the features we need, +L+ particularly automatic replication of data to ease construction of +L+ distributed applications. However, none of these toolkits has +L+ integrated the distribution of data into its programming language’s +L+ object model as tightly as we desire. As a result, they do not pro- +L+ vide a high enough level of network data transparency or suffi- +L+ ciently strong consistency guarantees. In groupware applications, +L+ inconsistencies tend to arise when multiple users attempt to per- +L+ form conflicting actions: the results are usually obvious to the +L+ users and can be corrected using social protocols. This is not an +L+ acceptable solution for a general-purpose, distributed 3D graphics +L+ toolkit. Furthermore, none of these CSCW systems provides any +L+ support for asynchronous update notification, or is designed to +L+ support the kind of large-scale distribution we have in mind. +L+ Finally, while distributed games, such as Quake, have become +L+ very popular, they only distribute the minimum amount of applica- +L+ tion state necessary. They do not use (or provide) an abstract, high- +L+ level distributed 3D graphics system. +L+ Network +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: The architecture of Repo-3D. Aside from native graphics +L+ libraries (X, Win32, OpenGL, Renderware) the Modula-3 runtime +L+ shields most of the application from the OS. The Replicated Object +L+ package uses an Event communication package and the Network +L+ Object package. DistAnim-3D is implemented on top of a variety of +L+ native graphics libraries and Replicated Objects. Repo exposes most of +L+ the useful Modula-3 packages, as well as using Network Objects and +L+ Replicated Objects to present a distributed shared memory model to +L+ the programmer. +L+ </SectLabel_figureCaption> <SectLabel_sectionHeader> 4 UNDERLYING INFRASTRUCTURE +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our work was done in the Modula-3 programming language [18]. +L+ We decided to use Modula-3 because of the language itself and the +L+ availability of a set of packages that provide a solid foundation for +L+ our infrastructure. Modula-3 is a descendant of Pascal that corrects +L+ many of its deficiencies, and heavily influenced the design of Java. +L+ In particular, Modula-3 retains strong type safety, while adding +L+ facilities for exception handling, concurrence object-oriented +L+ programming, and automatic garbage collection . One of its most +L+ important features for our work is that it gives us uniform access to +L+ these facilities across all architectures. +L+ Repo-3D relies on a number of Modula-3 libraries, as illustrated +L+ in Figure 2. Distributed data sharing is provided by two packages, +L+ the Network Object client-server object package [5], and the +L+ Replicated Object shared object package [24] (see Section 4.1). +L+ DistAnim-3D is derived from Anim-3D [25], a powerful, non- +L+ distributed, general-purpose 3D library originally designed for 3D +L+ algorithm animation (see Section 4.2). Finally, Repo itself is a +L+ direct descendant of Obliq [8], and uses the Replicated Object +L+ package to add replicated data to Obliq (see Section 4.3). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.1 Distributed Shared Memory +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Repo-3D’s data sharing mechanism is based on the Shared Data- +L+ Object Model of Distributed Shared Memory (DSM) [20]. DSM +L+ allows a network of computers to be programmed much like a mul- +L+ tiprocessor, since the programmer is presented with the familiar +L+ paradigm of a common shared memory. The Shared Data-Object +L+ Model of DSM is particularly well suited to our needs since it is a +L+ high-level approach that can be implemented efficiently at the +L+ application level. In this model, shared data is encapsulated in +L+ user-defined objects and can only be accessed through those +L+ objects’ method calls. The DSM address space is partitioned +L+ implicitly by the application programmer, with an object being the +L+ smallest unit of sharing. All shared data is fully network transpar- +L+ </SectLabel_bodyText> <SectLabel_footnote> 2. The Modula-3 compiler we used is available from Critical Mass, Inc. as +L+ part of the Reactor programming environment. The compiler, and thus +L+ our system, runs on all the operating systems we have available (plus +L+ others): Solaris, IRIX, HP-UX, Linux, and Windows NT and 95. +L+ </SectLabel_footnote> <SectLabel_figure> Repo-3D +L+ Modula-3 Runtime +L+ Operating System Services +L+ Repo +L+ Network Objects +L+ Replicated Objects +L+ Events +L+ DistAnim-3D +L+ Native +L+ Graphics +L+ </SectLabel_figure> <SectLabel_bodyText> ent because it is encapsulated within the programming language +L+ objects. +L+ Distribution of new objects between the processes is as simple as +L+ passing them back and forth as parameters to, or return values +L+ from, method calls—the underlying systems take care of the rest.3 +L+ Objects are only distributed to new processes as necessary, and (in +L+ our system) are removed by the garbage collector when they are no +L+ longer referenced. Furthermore, distributed garbage collection is +L+ supported, so objects that are no longer referenced in any process +L+ are removed completely. +L+ There are three kinds of distributed object semantics in our DSM: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Simple objects correspond to normal data objects, and have no +L+ special distributed semantics. When a simple object is copied +L+ between processes, a new copy is created in the destination +L+ process that has no implied relationship to the object in the +L+ source process. +L+ •	Remote objects have client-server distribution semantics. When +L+ a remote object is copied between processes, all processes +L+ except the one in which the object was created end up with a +L+ proxy object that forwards method invocations across the +L+ network to the original object. +L+ •	Replicated objects have replicated distribution semantics. +L+ When a replicated object is passed between processes, a new +L+ replica is created in the destination process. If any replica is +L+ changed, the change is reflected in all replicas. +L+ </SectLabel_listItem> <SectLabel_bodyText> The Network Object package provides support for remote +L+ objects. It implements distributed garbage collection, exception +L+ propagation back to the calling site, and automatic marshalling and +L+ unmarshalling of method arguments and return values of virtually +L+ any data type between heterogeneous machine architectures. The +L+ package is similar to other remote method invocation (RMI) pack- +L+ ages developed later, such as the Java RMI library [39]. All method +L+ invocations are forwarded to the original object, where they are +L+ executed in the order they are received. +L+ The Replicated Object package supports replicated objects. Each +L+ process can call any method of an object it shares, just as it can +L+ with a simple or remote object. We will describe the Replicated +L+ Object package in more detail, as Repo-3D relies heavily on its +L+ design, and the design of a replicated object system is less straight- +L+ forward than a remote one. The model supported by the Replicated +L+ Object package follows two principles: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	All operations on an instance of an object are atomic and +L+ serializable. All operations are performed in the same order on +L+ all copies of the object. If two methods are invoked simulta- +L+ neously, the order of invocation is nondeterministic, just as if +L+ two threads attempted to access the same memory location +L+ simultaneously in a single process. +L+ •	The above principle applies to operations on single objects. +L+ Making sequences of operations atomic is up to the program- +L+ mer. +L+ </SectLabel_listItem> <SectLabel_bodyText> The implementation of the Replicated Object package is based +L+ on the approach used in the Orca distributed programming +L+ language [2]. A full replication scheme is used, where a single +L+ object is either fully replicated in a process or not present at all. +L+ Avoiding partial replication significantly simplifies the implemen- +L+ tation and the object model, and satisfies the primary rationale for +L+ replication: fast read-access to shared data. To maintain replication +L+ consistency an update scheme is used, where updates to the object +L+ are applied to all copies. +L+ </SectLabel_bodyText> <SectLabel_footnote> 3. An important detail is how the communication is bootstrapped. In the +L+ case of the Network and Replicated Object packages, to pass a first +L+ object between processes, one of them exports the object to a special +L+ network object demon under some known name on some known +L+ machine. The second process then retrieves the object. +L+ </SectLabel_footnote> <SectLabel_bodyText> The method of deciding what is and is not an update is what +L+ makes the Orca approach particularly interesting and easy to +L+ implement. All methods are marked as either read or update meth- +L+ ods by the programmer who creates the object type. Read methods +L+ are assumed to not change the state of the object and are therefore +L+ applied immediately to the local object without violating consis- +L+ tency. Update methods are assumed to change the state. To distrib- +L+ ute updates, arguments to the update method are marshalled into a +L+ message and sent to all replicas. To ensure all updates are applied +L+ in the same order, the current implementation of the Replicated +L+ Object package designates a sequencer process for each object. +L+ There may be more than one sequencer in the system to avoid +L+ overloading one process with all the objects (in this case, each +L+ object has its updates managed by exactly one of the sequencers.) +L+ The sequencer is responsible for assigning a sequence number to +L+ each message before it is sent to all object replicas. The replicas +L+ then execute the incoming update messages in sequence. The pro- +L+ cess that initiated the update does not execute the update until it +L+ receives a message back from the sequencer and all updates with +L+ earlier sequence numbers have been executed. +L+ There are three very important reasons for choosing this +L+ approach. First, it is easy to implement on top of virtually any +L+ object-oriented language, using automatically generated object +L+ subtypes and method wrappers that communicate with a simple +L+ runtime system. We do this in our Modula-3 implementation, and it +L+ would be equally applicable to an implementation in C++ or Java. +L+ For example, the JSDT [36] data-sharing package in Java uses a +L+ similar approach. +L+ Second, the Replicated Object package does not pay attention to +L+ (or even care) when the internal data fields of an object change. +L+ This allows the programmer great flexibility in deciding exactly +L+ what constitutes an update or not, and what constitutes the shared +L+ state 4. For example, objects could have a combination of global +L+ and local state, and the methods that change the local state could +L+ be classified as read methods since they do not modify the global +L+ state. Alternatively, read methods could do some work locally and +L+ then call an update method to propagate the results, allowing time- +L+ consuming computation to be done once and the result distributed +L+ in a clean way. We took advantage of both of these techniques in +L+ implementing Repo-3D. +L+ Finally, the immediate distribution of update methods ensures +L+ that changes are distributed in a timely fashion, and suggests a +L+ straightforward solution to the asynchronous notification problem. +L+ The Replicated Object package generates a Notification Object +L+ type for each Replicated Object type. These new objects have +L+ methods corresponding to the update methods of their associated +L+ Replicated Object. The arguments to these methods are the same as +L+ the corresponding Replicated Object methods, plus an extra +L+ argument to hold the Replicated Object instance. These notifiers +L+ can be used by a programmer to receive notification of changes to +L+ a Replicated Object in a structured fashion. To react to updates to a +L+ Replicated Object instance, a programmer simply overrides the +L+ methods of the corresponding Notification Object with methods +L+ that react appropriately to those updates, and associates an instance +L+ </SectLabel_bodyText> <SectLabel_footnote> 4. Of course, it falls squarely on the shoulders of the programmer to +L+ ensure that the methods provided always leave the object in a consistent +L+ state. This is not significantly different than what needs to be done +L+ when building a complex object that is simultaneously accessed by +L+ multiple threads in a non-distributed system. For example, if a +L+ programmer reads an array of numbers from inside the object and then +L+ uses an update method to write a computed average back into the +L+ object, the internal array may have changed before the average is +L+ written, resulting in a classic inconsistency problem. In general, +L+ methods that perform computations based on internal state (rather than +L+ on the method arguments) are potentially problematic and need to be +L+ considered carefully. +L+ </SectLabel_footnote> <SectLabel_figure> RootGO +L+ ChoiceGroupGO +L+ OrthoCameraGO +L+ PerspCameraGO +L+ AmbientLightGO +L+ VectorLightGO +L+ PointLightGO +L+ SpotLightGO +L+ GO +L+ IndexedLineSetGO +L+ NonSurfaceGO +L+ Text2DGO +L+ PolygonGO +L+ BoxGO +L+ SphereGO +L+ CylinderGO +L+ DiskGO +L+ TorusGO +L+ QuadMeshGO +L+ IndexedPolygonSetGO +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 3: The Repo-3D GO class hierarchy. Most of the classes are +L+ also in Obliq-3D; the italicized ones were added to Repo-3D. +L+ Figure 4: The relationship between properties, names, values, and +L+ behaviors. Each oval represents an object and arrows show contain- +L+ ment. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> of it with the Replicated Object instance. Each time an update +L+ method of the Replicated Object is invoked, the corresponding +L+ method of the Notifier Object is also invoked. Notification Objects +L+ eliminate the need for object polling and enable a “data-driven” +L+ flow of control. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.2 Obliq-3D +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Obliq-3D is composed of Anim-3D, a 3D animation package +L+ written in Modula-3, and a set of wrappers that expose Anim-3D to +L+ the Obliq programming language (see Section 4.3). Anim-3D is +L+ based on three simple and powerful concepts: graphical objects for +L+ building graphical scenes, properties for specifying the behavior of +L+ the graphical objects, and input event callbacks to support interac- +L+ tive behavior. Anim-3D uses the damage-repair model: whenever a +L+ graphical object or property changes (is damaged), the image is +L+ repaired without programmer intervention. +L+ Graphical objects (GOs) represent all the logical entities in the +L+ graphical scene: geometry (e.g., lines, polygons, spheres, polygon +L+ sets, and text), lights and cameras of various sorts, and groups of +L+ other GOs. One special type of group, the RootGO, represents a +L+ window into which graphics are rendered. GOs can be grouped +L+ together in any valid directed acyclic graph (DAG). The GO class +L+ hierarchy is shown in Figure 3. +L+ A property is a defined by a name and a value. The name deter- +L+ mines which attribute is affected by the property, such as “Texture +L+ Mode” or “Box Corner1”. The value specifies how it is affected +L+ and is determined by its behavior, a time-variant function that +L+ takes the current animation time and returns a value. Properties, +L+ property values, and behaviors are all objects, and their relation- +L+ ships are shown in Figure 4. When a property is created, its name +L+ and value are fixed. However, values are mutable and their behav- +L+ ior may be changed at any time. There are four kinds of behaviors +L+ for each type of properties: constant (do not vary over time), +L+ synchronous (follow a programmed set of requests, such as “move +L+ from A to B starting at time t=1 and taking 2 seconds”), asynchro- +L+ nous (execute an arbitrary time-dependent function to compute the +L+ value) and dependent (asynchronous properties that depend on +L+ other properties). Synchronous properties are linked to animation +L+ handles and do not start satisfying their requests until the anima- +L+ tion handle is signalled. By linking multiple properties to the same +L+ handle, a set of property value changes can be synchronized. +L+ Associated with each GO g is a partial mapping of property +L+ names to values determined by the properties that have been asso- +L+ ciated with g. A property associated with g affects not only g but +L+ all the descendants of g that do not override the property. A single +L+ property may be associated with any number of GOs. It is perfectly +L+ legal to associate a property with a GO that is not affected by it; for +L+ example, attaching a “Surface Color” property to a GroupGO does +L+ not affect the group node itself, but could potentially affect the +L+ surface color of any GO contained in that group. A RootGO sets an +L+ initial default value for each named property. +L+ There are three types of input event callbacks in Anim-3D, corre- +L+ sponding to the three kinds of interactive events they handle: +L+ mouse callbacks (triggered by mouse button events), motion call- +L+ backs (triggered by mouse motion events) and keyboard callbacks +L+ (triggered by key press events). Each object has three callback +L+ stacks, and the interactive behavior of an object can be redefined +L+ by pushing a new callback onto the appropriate stack. Any event +L+ that occurs within a root window associated with a RootGO r will +L+ be delivered to the top handler on r’s callback stack. The handler +L+ could delegate the event to one of r’s children, or it may handle it +L+ itself, perhaps changing the graphical scene in some way. +L+ DistAnim-3D is a direct descendant of Anim-3D. In addition to +L+ the objects being distributed, it has many additional facilities that +L+ are needed for general-purpose 3D graphical applications, such as +L+ texture mapping, indexed line and polygon sets, choice groups, +L+ projection and transformation callbacks, and picking. Since +L+ DistAnim-3D is embedded in Repo instead of Obliq (see +L+ Section 4.3), the resulting library is called Repo-3D. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 4.3 Obliq and Repo +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Obliq [8] is a lexically-scoped, untyped, interpreted language for +L+ distributed object-oriented computation. It is implemented in, and +L+ tightly integrated with, Modula-3. An Obliq computation may +L+ involve multiple threads of control within an address space, multi- +L+ ple address spaces on a machine, heterogeneous machines over a +L+ local network, and multiple networks over the Internet. Obliq uses, +L+ and supports, the Modula-3 thread, exception, and garbage-collec- +L+ tion facilities. Its distributed-computation mechanism is based on +L+ Network Objects, allowing transparent support for multiple +L+ processes on heterogeneous machines. Objects are local to a site, +L+ while computations can roam over the network. Repo [23] is a +L+ descendant of Obliq that extends the Obliq object model to include +L+ replicated objects. Therefore, Repo objects have state that may be +L+ local to a site (as in Obliq) or replicated across multiple sites. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 5 DESIGN OF REPO-3D +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Repo-3D’s design has two logical parts: the basic design and local +L+ variations. The basic design encompasses the changes to Obliq-3D +L+ to carry it into a distributed context, and additional enhancements +L+ that are not particular to distributed graphics (and are therefore not +L+ discussed here). Local variations are introduced to handle two +L+ issues mentioned in Section 1: transient local changes and respon- +L+ sive local editing. +L+ </SectLabel_bodyText> <SectLabel_figure> . . . +L+ Name +L+ Property +L+ Value	Behavior +L+ Request +L+ Request +L+ GroupGO +L+ CameraGO +L+ LightGO +L+ SurfaceGO +L+ LineGO +L+ MarkerGO +L+ TextGO +L+ </SectLabel_figure> <SectLabel_subsectionHeader> 5.1 Basic Repo-3D Design +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The Anim-3D scene-graph model is well suited for adaptation to a +L+ distributed environment. First, in Anim-3D, properties are attached +L+ to nodes, not inserted into the graph, and the property and child +L+ lists are unordered (i.e., the order in which properties are assigned +L+ to a node, or children are added to a group, does not affect the final +L+ result). In libraries that insert properties and nodes in the graph and +L+ execute the graph in a well-defined order (such as Inventor), the +L+ siblings of a node (or subtree) can affect the attributes of that node +L+ (or subtree). In Anim-3D, and similar libraries (such as Java 3D), +L+ properties are only inherited down the graph, so a node’s properties +L+ are a function of the node itself and its ancestors—its siblings do +L+ not affect it. Therefore, subtrees can be added to different scene +L+ graphs, perhaps in different processes, with predictable results. +L+ Second, the interface (both compiled Anim-3D and interpreted +L+ Obliq-3D) is programmatical and declarative. There is no “graphi- +L+ cal scene” file format per se: graphical scenes are created as the +L+ side effect of executing programs that explicitly create objects and +L+ manipulate them via the object methods. Thus, all graphical +L+ objects are stored as the Repo-3D programs that are executed to +L+ create them. This is significant, because by using the Replicated +L+ Object library described in Section 4.1 to make the graphical +L+ objects distributed, the “file format” (i.e., a Repo-3D program) is +L+ updated for free. +L+ Converting Anim-3D objects to Replicated Objects involved +L+ three choices: what objects to replicate, what methods update the +L+ object state, and what the global, replicated state of each object is. +L+ Since replicated objects have more overhead (e.g., method execu- +L+ tion time, memory usage, and latency when passed between +L+ processes), not every category of object in Repo-3D is replicated. +L+ We will consider each of the object categories described in +L+ Figure 4.2 in turn: graphical objects (GOs), properties (values, +L+ names, behaviors, animation handles) and callbacks. For each of +L+ these objects, the obvious methods are designated as update meth- +L+ ods, and, as discussed in Section 4. 1, the global state of each object +L+ is implicitly determined by those update methods. Therefore, we +L+ will not go into excessive detail about either the methods or the +L+ state. Finally, Repo-3D’s support for change notification will be +L+ discussed. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 5.1.1 Graphical Objects +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> GOs are the most straightforward. There are currently twenty-one +L+ different types of GOs, and all but the RootGOs are replicated. +L+ Since RootGOs are associated with an onscreen window, they are +L+ not replicated—window creation remains an active decision of the +L+ local process. Furthermore, if replicated windows are needed, the +L+ general-purpose programming facilities of Repo can be used to +L+ support this in a relatively straightforward manner, outside the +L+ scope of Repo-3D. A GO’s state is comprised of the properties +L+ attached to the object, its name, and some other non-inherited +L+ property attributes.5 The methods that modify the property list are +L+ update methods. Group GOs also contain a set of child nodes, and +L+ have update methods that modify that set. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 5.1.2 Properties +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Properties are more complex. There are far more properties in a +L+ graphical scene than there are graphical objects, they change much +L+ more rapidly, and each property is constructed from a set of +L+ Modula-3 objects. There are currently 101 different properties of +L+ </SectLabel_bodyText> <SectLabel_footnote> 5. Some attributes of a GO, such as the arrays of Point3D properties that +L+ define the vertices of a polygon set, are not attached to the object, but +L+ are manipulated through method calls. +L+ </SectLabel_footnote> <SectLabel_bodyText> seventeen different types in Repo-3D, and any of them can be +L+ attached to any GO. A typical GO would have anywhere from two +L+ or three (e.g., a BoxGO would have at least two properties to +L+ define its corners) to a dozen or more. And, each of these proper- +L+ ties could be complex: in the example in Section 6, a single +L+ synchronous property for a long animation could have hundreds of +L+ requests enqueued within it. +L+ Consider again the object structure illustrated in Figure 4. A +L+ property is defined by a name and a value, with the value being a +L+ container for a behavior. Only one of the Modula-3 objects is +L+ replicated, the property value. Property values serve as the repli- +L+ cated containers for property behaviors. To change a property, a +L+ new behavior is assigned to its value. The state of the value is the +L+ current behavior. +L+ Animation handles are also replicated. They tie groups of related +L+ synchronous properties together, and are the basis for the interac- +L+ tion in the example in Section 6. In Anim-3D, handles have one +L+ animate method, which starts an animation and blocks until it +L+ finishes. Since update methods are executed everywhere, and block +L+ access to the object while they are being executed, they should not +L+ take an extended period of time. In creating Repo-3D, the +L+ animate method was changed to call two new methods: an update +L+ method that starts the animation, and a non-update method that +L+ waits for the animation to finish. We also added methods to pause +L+ and resume an animation, to retrieve and change the current rela- +L+ tive time of an animation handle, and to stop an animation early. +L+ The state of an Animation handle is a boolean value that says if it is +L+ active or not, plus the start, end, and current time (if the handle is +L+ paused). +L+ Most of the Modula-3 objects that comprise a property are not +L+ replicated, for a variety of reasons: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Properties represent a permanent binding between a property +L+ value and a name. Since they are immutable, they have no syn- +L+ chronization requirements and can simply be copied between +L+ processes. +L+ •	Names represent simple constant identifiers, and are therefore +L+ not replicated either. +L+ •	Behaviors and requests are not replicated. While they can be +L+ modified after being created, they are treated as immutable +L+ data types for two reasons. First, the vast majority of behaviors, +L+ even complex synchronous ones, are not changed once they +L+ have been created and initialized. Thus, there is some justifica- +L+ tion for classifying the method calls that modify them as part +L+ of their initialization process. The second reason is practical +L+ and much more significant. Once a scene has been created and +L+ is being “used” by the application, the bulk of the time-critical +L+ changes to it tend to be assignments of new behaviors to the +L+ existing property values. For example, an object is moved by +L+ assigning a new (often constant) behavior to its +L+ GO _T rans fo rm property value. Therefore, the overall perfor- +L+ mance of the system depends heavily on the performance of +L+ property value behavior changes. By treating behaviors as +L+ immutable objects, they can simply be copied between +L+ processes without incurring the overhead of the replicated +L+ object system. +L+ </SectLabel_listItem> <SectLabel_subsubsectionHeader> 5.1.3 Input Callbacks +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In Repo-3D, input event callbacks are not replicated. As discussed +L+ in Section 4.2, input events are delivered to the callback stacks of a +L+ RootGO. Callbacks attached to any other object receive input +L+ events only if they are delivered to that object by the programmer, +L+ perhaps recursively from another input event callback (such as the +L+ one attached to the RootGO). Therefore, the interactive behavior of +L+ a root window is defined not only by the callbacks attached to its +L+ RootGO, but also by the set of callbacks associated with the graph +L+ rooted at that RootGO. Since the RootGOs are not replicated, the +L+ </SectLabel_bodyText> <SectLabel_figure> (a)	(b) +L+ (c) (d) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: Simultaneous images from a session with the distributed CATHI animation viewer, running on four machines, showing an anima- +L+ tion of an engine. (a) Plain animation viewer, running on Windows NT. (b) Overview window, running on Windows 95. (c) Animation viewer +L+ with local animation meter, running on IRIX. (d) Animation viewer with local transparency to expose hidden parts, running on Solaris. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> callbacks that they delegate event handling to are not replicated +L+ either. If a programmer wants to associate callbacks with objects as +L+ they travel between processes, Repo’s general-purpose program- +L+ ming facilities can be used to accomplish this in a straightforward +L+ manner. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> 5.1.4 Change Notification +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The final component of the basic design is support for notification +L+ of changes to distributed objects. For example, when an object’s +L+ position changes or a new child is added to a group, some of the +L+ processes containing replicas may wish to react in some way. For- +L+ tunately, as discussed in Section 4.1, the Replicated Object +L+ package automatically generates Notification Object types for all +L+ replicated object types, which provide exactly the required +L+ behavior. The Notification Objects for property values allow a +L+ programmer to be notified of changes to the behavior of a property, +L+ and the Notification Objects for the various GOs likewise allow +L+ notification of updates to them. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> 5.2 Local Variations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Repo-3D’s local variations solve a set of problems particular to the +L+ distributed context in which Repo-3D lives: maintaining interactiv- +L+ ity and supporting local modifications to the shared scene graph. +L+ If the graphical objects and their properties were always strictly +L+ replicated, programmers would have to create local variations by +L+ copying the objects to be modified, creating a set of Notification +L+ Objects on the original objects, the copies of those objects, and all +L+ their properties (to be notified when either change), and reflecting +L+ the appropriate changes between the instances. Unfortunately, +L+ while this process could be automated somewhat, it would still be +L+ extremely tedious and error prone. More seriously, the overhead of +L+ creating this vast array of objects and links between them would +L+ make this approach impractical for short transient changes, such as +L+ highlighting an object under the mouse. +L+ To overcome this problem, Repo-3D allows the two major +L+ elements of the shared state of the graphical object scene—the +L+ properties attached to a GO and the children of a group—to have +L+ local variations applied to them. (Local variations on property +L+ values or animation handles are not supported, although we are +L+ considering adding support for the latter.) +L+ Conceptually, local state is the state added to each object (the +L+ additions, deletions, and replacements to the properties or +L+ children) that is only accessible to the local copies and is not +L+ passed to remote processes when the object is copied to create a +L+ new replica. The existence of local state is possible because, as +L+ discussed in Section 4. 1, the shared state of a replicated object is +L+ implicitly defined by the methods that update it 6. Therefore, the +L+ new methods that manipulate the local variations are added to the +L+ GOs as non-update methods. Repo-3D combines both the global +L+ and local state when creating the graphical scene using the under- +L+ lying graphics package. +L+ As mentioned above, local variations come in two flavors: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Property variations. There are three methods to set, unset, and +L+ get the global property list attached to a GO. We added the +L+ following methods to manipulate local variations: add or +L+ remove local properties (overriding the value normally used for +L+ the object), hide or reveal properties (causing the property +L+ value of the parent node to be inherited), and flush the set of +L+ local variations (removing them in one step) or atomically +L+ apply them to the global state of the object. +L+ •	Child variations. There are five methods to add, remove, +L+ replace, retrieve, and flush the set of children contained in a +L+ group node. We added the following ones: add a local node, +L+ remove a global node locally, replace a global node with some +L+ other node locally, remove each of these local variations, flush +L+ the local variations (remove them all in one step), and atomi- +L+ cally apply the local variations to the global state. +L+ </SectLabel_listItem> <SectLabel_bodyText> This set of local operations supports the problems local variations +L+ were designed to solve, although some possible enhancements are +L+ discussed in Section 7. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 6 EXAMPLE: AN ANIMATION EXAMINER +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> As an example of the ease of prototyping distributed applications +L+ with Repo-3D, we created a distributed animation examiner for the +L+ CATHI [6] animation generation system. CATHI generates short +L+ informational animation clips to explain the operation of technical +L+ devices. It generates full-featured animation scripts, including +L+ camera and object motion, color and opacity effects, and lighting +L+ setup. +L+ It was reasonably straightforward to modify CATHI to generate +L+ Repo-3D program files, in addition to the GeomView and Render- +L+ Man script files it already generated. The resulting output is a +L+ Repo-3D program that creates two scene DAGs: a camera graph +L+ and a scene graph. The objects in these DAGs have synchronous +L+ behaviors specified for their surface and transformation properties. +L+ An entire animation is enqueued in the requests of these behaviors, +L+ lasting anywhere from a few seconds to a few minutes. +L+ We built a distributed, multi-user examiner over the course of a +L+ weekend. The examiner allows multiple users to view the same +L+ animation while discussing it (e.g., via electronic chat or on the +L+ phone). Figure 5 shows images of the examiner running on four +L+ </SectLabel_bodyText> <SectLabel_footnote> 6. The local state is not copied when a replicated object is first passed to a +L+ new process because the Repo-3D objects have custom serialization +L+ routines (or Picklers, in Modula-3 parlance). These routines only pass +L+ the global state, and initialize the local state on the receiving side to +L+ reasonable default values corresponding to the empty local state. +L+ </SectLabel_footnote> <SectLabel_bodyText> machines, each with a different view of the scene. The first step +L+ was to build a simple “loader” that reads the animation file, creates +L+ a window, adds the animation scene and camera to it, and exports +L+ the animation on the network, requiring less than a dozen lines of +L+ Repo-3D code. A “network” version, that imports the animation +L+ from the network instead of reading it from disk, replaced the lines +L+ of code to read and export the animation with a single line to +L+ import it. Figure 5(a) shows an animation being viewed by one of +L+ these clients. +L+ The examiner program is loaded by both these simple clients, and +L+ is about 450 lines long. The examiner supports: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Pausing and continuing the animation, and changing the +L+ current animation time using the mouse. Since this is done by +L+ operating on the shared animation handle, changes performed +L+ by any viewer are seen by all. Because of the consistency guar- +L+ antees, all users can freely attempt to change the time, and the +L+ system will maintain all views consistently. +L+ •	A second “overview” window (Figure 5(b)), where a new +L+ camera watches the animation scene and camera from a distant +L+ view. A local graphical child (representing a portion of the +L+ animation camera’s frustum) was added to the shared anima- +L+ tion camera group to let the attributes of the animation camera +L+ be seen in the overview window. +L+ •	A local animation meter (bottom of Figure 5(c)), that can be +L+ added to any window by pressing a key, and which shows the +L+ current time offset into the animation both graphically and +L+ numerically. It was added in front of the camera in the anima- +L+ tion viewer window, as a local child of a GO in the camera +L+ graph, so that it would be fixed to the screen in the animation +L+ viewer. +L+ •	Local editing (Figure 5(d)), so that users can select objects and +L+ make them transparent (to better see what was happening in the +L+ animation) or hide them completely (useful on slow machines, +L+ to speed up rendering). Assorted local feedback (highlighting +L+ the object under the mouse and flashing the selected object) +L+ was done with local property changes to the shared GOs in the +L+ scene graph. +L+ </SectLabel_listItem> <SectLabel_bodyText> Given the attention paid to the design of Repo-3D, it was not +L+ necessary to be overly concerned with the distributed behavior of +L+ the application (we spent no more than an hour or so). Most of that +L+ time was spent deciding if a given operation should be global or a +L+ local variation. The bulk of programming and debugging time was +L+ spent implementing application code. For example, in the overview +L+ window, the representation of the camera moves dynamically, +L+ based on the bounding values of the animation’s scene and camera +L+ graphs. In editing mode, the property that flashes the selected node +L+ bases its local color on the current global color (allowing a user +L+ who is editing while an animation is in progress to see any color +L+ changes to the selected node.) +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> 7 CONCLUSIONS AND FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have presented the rationale for, and design of, Repo-3D, a +L+ general-purpose, object-oriented library for developing distributed, +L+ interactive 3D graphics applications across a range of heteroge- +L+ neous workstations. By presenting the programmer with the +L+ illusion of a large shared memory, using the Shared Data-Object +L+ model of DSM, Repo-3D makes it easy for programmers to rapidly +L+ prototype distributed 3D graphics applications using a familiar +L+ object-oriented programming paradigm. Both graphical and +L+ general-purpose, non-graphical data can be shared, since Repo-3D +L+ is embedded in Repo, a general-purpose, lexically-scoped, distrib- +L+ uted programming language. +L+ Repo-3D is designed to directly support the distribution of graph- +L+ ical objects, circumventing the “duplicate database” problem and +L+ allowing programmers to concentrate on the application function- +L+ ality of a system, rather than its communication or synchronization +L+ components. We have introduced a number of issues that must be +L+ considered when building a distributed 3D graphics library, espe- +L+ cially concerning efficient and clean support for data distribution +L+ and local variations of shared graphical scenes, and discussed how +L+ Repo-3D addresses them. +L+ There are a number of ways in which Repo-3D could be +L+ improved. The most important is the way the library deals with +L+ time. By default, the library assumes all machines are running a +L+ time-synchronization rotocol, such as NTP, and uses an internal +L+ animation time offset�(instead of the system-specific time offset) +L+ because different OSs (e.g., NT vs. UNIX) start counting time at +L+ different dates. Hooks have been provided to allow a programmer +L+ to specify their own function to compute the “current” animation +L+ time offset within a process. Using this facility, it is possible to +L+ build inter-process time synchronization protocols (which we do), +L+ but this approach is not entirely satisfactory given our stated goal +L+ of relieving the programmer of such tedious chores. Future +L+ systems should integrate more advanced solutions, such as adjust- +L+ ing time values as they travel between machines, so that users of +L+ computers with unsynchronized clocks can collaborate8. This will +L+ become more important as mobile computers increase in popular- +L+ ity, as it may not be practical to keep their clocks synchronized. +L+ The specification of local variations in Repo-3D could benefit +L+ from adopting the notion of paths (as used in Java 3D and Inventor, +L+ for example). A path is an array of objects leading from the root of +L+ the graph to an object; when an object occurs in multiple places in +L+ one or more scene graphs, paths allow these instances to be differ- +L+ entiated. By specifying local variations using paths, nodes in the +L+ shared scene graphs could have variations within a process as well +L+ as between processes. One other limitation of Repo-3D, arising +L+ from our use of the Replicated Object package, is that there is no +L+ way to be notified when local variations are applied to an object. +L+ Recall that the methods of an automatically generated Notification +L+ Object correspond to the update methods of the corresponding +L+ Replicated Object. Since the methods that manipulate the local +L+ variations are non-update methods (i.e., they do not modify the +L+ replicated state), there are no corresponding methods for them in +L+ the Notification Objects. Of course, it would be relatively straight- +L+ forward to modify the Replicated Object package to support this, +L+ but we have not yet found a need for these notifiers. +L+ A more advanced replicated object system would also improve +L+ the library. Most importantly, support for different consistency +L+ semantics would be extremely useful. If we could specify +L+ semantics such as “all updates completely define the state of an +L+ object, and only the last update is of interest,” the efficiency of the +L+ distribution of property values would improve significantly; in this +L+ case, updates could be applied (or discarded) when they arrive, +L+ without waiting for all previous updates to be applied, and could be +L+ applied locally without waiting for the round trip to the sequencer. +L+ There are also times when it would be useful to have support for +L+ consistency across multiple objects, either using causal ordering +L+ (as provided by systems such as ISIS and Visual-Obliq), or some +L+ kind of transaction protocol to allow large groups of changes to be +L+ applied either as a unit, or not at all. It is not clear how one would +L+ provide these features with a replicated object system such as the +L+ one used here. +L+ While a library such as Repo-3D could be built using a variety of +L+ underlying platforms, the most likely one for future work is Java. +L+ Java shares many of the advantages of Modula-3 (e.g., threads and +L+ garbage collection are common across all architectures) and the +L+ </SectLabel_bodyText> <SectLabel_footnote> 7. Computed as an offset from January 1, 1997. +L+ 8. Implementation details of the combination of Network and Replicated +L+ Objects made it difficult for us to adopt a more advanced solution. +L+ </SectLabel_footnote> <SectLabel_bodyText> packages needed to create a Repo-3D-like toolkit are beginning to +L+ appear. While Java does not yet have a replicated object system as +L+ powerful as the Replicated Object package, a package such as +L+ JSDT [36] (which focuses more on data communication than high- +L+ level object semantics) may be a good starting point. Work is also +L+ being done on interpreted, distributed programming languages on +L+ top of Java (e.g., Ambit [9]). Finally, Java 3D is very similar to +L+ Anim-3D, even though its design leans toward efficiency instead of +L+ generality when there are trade-offs to be made. For example, the +L+ designers chose to forgo Anim-3D’s general property inheritance +L+ mechanism because it imposes computational overhead. By com- +L+ bining packages such as Java 3D, JSDT, and Ambit, it should be +L+ possible to build a distributed graphics library such as Repo-3D in +L+ Java. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Acknowledgments +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We would like to thank the reviewers for their helpful comments, +L+ as well as the many other people who have contributed to this +L+ project. Andreas Butz ported CATHI to use Repo-3D and helped +L+ with the examples and the video. Clifford Beshers participated in +L+ many lively discussions about the gamut of issues dealing with +L+ language-level support for 3D graphics. Tobias Höllerer and +L+ Steven Dossick took part in many other lively discussions. Xinshi +L+ Sha implemented many of the extensions to Obliq-3D that went +L+ into Repo-3D. Luca Cardelli and Marc Najork of DEC SRC +L+ created Obliq and Obliq-3D, and provided ongoing help and +L+ encouragement over the years that Repo and Repo-3D have been +L+ evolving. +L+ This research was funded in part by the Office of Naval Research +L+ under Contract N00014-97-1-0838 and the National Tele-Immer- +L+ sion Initiative, and by gifts of software from Critical Mass and +L+ Microsoft. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> References +L+ </SectLabel_sectionHeader> <SectLabel_reference> [1] D. B. Anderson, J. W. Barrus, J. H. Howard, C. Rich, C. Shen, and +L+ R. C. Waters. Building Multi-User Interactive Multimedia Environ- +L+ ments at MERL. Technical Report Research Report TR95-17, Mit- +L+ subishi Electric Research Laboratory, November 1995. +L+ [2] H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: A Language for +L+ Parallel Programming of Distributed Systems. IEEE Transactions on +L+ Software Engineering, 18(3):190–205, March 1992. +L+ [3] K. Bharat and L. Cardelli. Migratory Applications. In ACM UIST '95, +L+ pages 133-142, November 1995. +L+ [4] K. P. Birman. The Process Group Approach to Reliable Distributed +L+ Computing. CACM, 36(12):36–53, Dec 1993. +L+ [5] A. Birrell, G. Nelson, S. Owicki, and E. Wobber. Network Objects. +L+ In Proc. 14th ACM Symp. on Operating Systems Principles, 1993. +L+ [6] A Butz, Animation with CATHI, In Proceedings ofAAAI/IAAI '97, +L+ pages 957–962, 1997. +L+ [7]	J. Calvin, A. Dickens, B. Gaines, P. Metzger, D. Miller, and +L+ D. Owen. The SIMNET Virtual World Architecture. In Proc. IEEE +L+ VRAIS ’93, pages 450–455, Sept 1993. +L+ [8] L. Cardelli. A Language with Distributed Scope. Computing Sys- +L+ tems, 8(1):27–59, Jan 1995. +L+ [9] L. Cardelli and A. Gordon. Mobile Ambients. In Foundations of +L+ Software Science and Computational Structures, Maurice Nivat +L+ (Ed.), LNCE 1378, Springer, 140–155. 1998. +L+ [10] R. Carey and G. Bell. The Annotated VRML 2.0 Reference Manual. +L+ Addison-Wesley, Reading, MA, 1997. +L+ [11] C. Carlsson and O. Hagsand. DIVE—A Multi-User Virtual Reality +L+ System. In Proc. IEEE VRAIS ’93, pages 394–400, Sept 1993. +L+ [12] C. F. Codella, R. Jalili, L. Koved, and J. B. Lewis. A Toolkit for +L+ Developing Multi-User, Distributed Virtual Environments. In Proc. +L+ IEEE VRAIS ’93, pages 401–407, Sept 1993. +L+ [13] C. Elliott, G. Schechter, R. Yeung and S. Abi-Ezzi. TBAG: A High +L+ Level Framework for Interactive, Animated 3D Graphics +L+ Applications, In Proc. ACM SIGGRAPH 94, pages 421–434, August, +L+ 1994. +L+ [14] M. Fairen and A. Vinacua, ATLAS, A Platform for Distributed +L+ Graphics Applications, In Proc. VI Eurographics Workshop on Pro- +L+ gramming Paradigms in Graphics, pages 91–102, September, 1997. +L+ [15] S. Feiner, B. MacIntyre, M. Haupt, and E. Solomon. Windows on the +L+ World: 2D Windows for 3D Augmented Reality. In Proc. ACM UIST +L+ ’93, pages 145–155, 1993. +L+ [16] T. A. Funkhouser. RING: A Client-Server System for Multi-User +L+ Virtual Environments. In Proc. 1995 ACM Symp. on Interactive 3D +L+ Graphics, pages 85–92, March 1995. +L+ [17] G. Grimsdale. dVS—Distributed Virtual Environment System. In +L+ Proc. Computer Graphics ’91 Conference, 1991. +L+ [18] S. P. Harbison. Modula-3. Prentice-Hall, 1992. +L+ [19] H.W. Holbrook, S.K. Singhal and D.R. Cheriton, Log-Based +L+ Receiver-Reliable Multicast for Distributed Interactive Simulation, +L+ Proc. ACM SIGCOMM ’95, pages 328–341, 1995. +L+ [20] W. Levelt, M. Kaashoek, H. Bal, and A. Tanenbaum. A Comparison +L+ of Two Paradigms for Distributed Shared Memory. Software +L+ Practice and Experience, 22(11):985–1010, Nov 1992. +L+ [21] B. Lucas. A Scientific Visualization Renderer. In Proc. IEEE +L+ Visualization '92, pp. 227-233, October 1992. +L+ [22] V. Machiraju, A Framework for Migrating Objects in Distributed +L+ Graphics Applications, Masters Thesis, University of Utah, Depart- +L+ ment of Computer Science, Salt Lake City, UT, June, 1997. +L+ [23] B. MacIntyre. Repo: Obliq with Replicated Objects. Programmers +L+ Guide and Reference Manual. Columbia University Computer +L+ Science Department Research Report CUCS-023-97, 1997.} +L+ [24] B. MacIntyre, and S. Feiner. Language-level Support for Exploratory +L+ Programming of Distributed Virtual Environments. In Proc. ACM +L+ UIST ’96, pages 83–94, Seattle, WA, November 6–8, 1996. +L+ [25] M. A. Najork and M. H. Brown. Obliq-3D: A High-level, Fast-turn- +L+ around 3D Animation System. IEEE Transactions on Visualization +L+ and Computer Graphics, 1(2):175–145, June 1995. +L+ [26] R. Ben-Natan. CORBA: A Guide to the Common Object Request +L+ Broker Architecture, McGraw Hill, 1995. +L+ [27] D. Phillips, M. Pique, C. Moler, J. Torborg, D. Greenberg. Distribut- +L+ ed Graphics: Where to Draw the Lines? Panel Transcript, +L+ SIGGRAPH 89, available at: +L+ http://www.siggraph.org:443/publications/panels/siggraphi89/ +L+ [28] A. Prakash and H. S. Shim. DistView: Support for Building Efficient +L+ Collaborative Applications Using Replicated Objects. In Proc. ACM +L+ CSCW ’94, pages 153–162, October 1994. +L+ [29] J. Rohlf and J. Helman, IRIS Performer: A High Performance +L+ Multiprocessing Toolkit for Real-Time {3D} Graphics, In Proc. +L+ ACM SIGGRAPH 94, pages 381–394, 1994. +L+ [30] M. Roseman and S. Greenberg. Building Real-Time Groupware with +L+ GroupKit, a Groupware Toolkit. ACM Transactions on Computer- +L+ Human Interaction, 3(1):66–106, March 1996. +L+ [31] C. Shaw and M. Green. The MR Toolkit Peers Package and +L+ Experiment. In Proc. IEEE VRAIS ’93, pages 18–22, Sept 1993. +L+ [32] G. Singh, L. Serra, W. Png, A. Wong, and H. Ng. BrickNet: Sharing +L+ Object Behaviors on the Net. In Proc. IEEE VRAIS ’95, pages 19–25, +L+ 1995. +L+ [33] H. Sowizral, K. Rushforth, and M. Deering. The Java 3D API +L+ Specification, Addison-Wesley, Reading, MA, 1998. +L+ [34] M. Stefik, G. Foster, D. G. Bobrow, K. Kahn, S. Lanning, and +L+ L. Suchman. Beyond The Chalkboard: Computer Support for +L+ Collaboration and Problem Solving in Meetings. CACM, 30(1):32– +L+ 47, January 1987. +L+ [35] P. S. Strauss and R. Carey, An Object-Oriented 3D Graphics Toolkit, +L+ In Computer Graphics (Proc. ACM SIGGRAPH 92), pages 341–349, +L+ Aug, 1992. +L+ [36] Sun Microsystems, Inc. The Java Shared Data Toolkit, 1998. +L+ Unsupported software, available at: +L+ http://developer.javasoft.com/developer/earlyAccess/jsdt/ +L+ [37] I. Tou, S. Berson, G. Estrin, Y. Eterovic, and E. Wu. Prototyping +L+ Synchronous Group Applications. IEEE Computer, 27(5):48–56, +L+ May 1994. +L+ [38] R. Waters and D. Anderson. The Java Open Community Version 0.9 +L+ Application Program Interface. Feb, 1997. Available online at: +L+ http://www.merl.com/opencom/opencom-java-api.html +L+ [39] A. Wollrath, R. Riggs, and J. Waldo. A Distributed Object Model for +L+ the Java System, In Proc. USENIX COOTS ’96, pages 219–231, July +L+ 1996. +L+ [40] R. Zeleznik, D. Conner, M. Wloka, D. Aliaga, N. Huang, +L+ P. Hubbard, B. Knep, H. Kaufman, J. Hughes, and A. van Dam. An +L+ Object-oriented Framework for the Integration of Interactive +L+ Animation Techniques. In Computer Graphics (SIGGRAPH '91 +L+ Proceedings), pages 105–112, July, 1991. +L+ [41 ] M. J. Zyda, D. R. Pratt, J. G. Monahan, and K. P. Wilson. NPSNET: +L+ Constructing a 3D Virtual World. In Proc. 1992 ACM Symp. on +L+ Interactive 3D Graphics, pages 147–156, Mar. 1992. +L+ </SectLabel_reference>
<SectLabel_note> CHI 2008 Proceedings · Socio-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> Ambient Social TV: +L+ Drawing People into a Shared Experience +L+ </SectLabel_title> <SectLabel_author> Gunnar Harboe, Crysta J. Metcalf, Frank Bentley, +L+ Joe Tullio, Noel Massey, Guy Romano +L+ </SectLabel_author> <SectLabel_affiliation> Motorola Labs +L+ </SectLabel_affiliation> <SectLabel_address> 1295 E. Algonquin Rd., Schaumburg, IL 60196 +L+ </SectLabel_address> <SectLabel_email> {gunnar.harboe, crysta.metcalf, f.bentley, joe.tullio, noel.massey, guy} @motorola.com +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We examine how ambient displays can augment social +L+ television. Social TV 2 is an interactive television solution +L+ that incorporates two ambient displays to convey to +L+ participants an aggregate view of their friends’ current TV- +L+ watching status. Social TV 2 also allows users to see which +L+ television shows friends and family are watching and send +L+ lightweight messages from within the TV-viewing +L+ experience. Through a two-week field study we found the +L+ ambient displays to be an integral part of the experience. +L+ We present the results of our field study with a discussion +L+ of the implications for future social systems in the home. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Social television, interactive television, social presence +L+ awareness, ambient displays, field trial +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> H5.m. Information interfaces and presentation (e.g., HCI): +L+ Miscellaneous. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Although Internet use is gaining importance, TV watching +L+ is still the primary recreational activity of American adults, +L+ accounting for half of all leisure time [7]. Although there is +L+ great social potential in TV watching as a shared activity +L+ and a topic for conversation [20], at least half of all people +L+ usually watch alone [10]. +L+ Many technologies coming out of the interactive television +L+ (iTV) field, such as Video-On-Demand and Personal Video +L+ Recorders (PVRs), focus on providing personalization and +L+ greater individual control. They cater to viewers as isolated +L+ individuals, and fragment audiences further [16]. +L+ However, in recent years ‘social television,’ the idea of +L+ using communication technology to connect TV viewers, in +L+ order to create remotely shared experiences around TV +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00 +L+ </SectLabel_copyright> <SectLabel_bodyText> content, has received much attention. Proponents of this +L+ idea see television as a social experience capable of +L+ reinforcing bonds in strong-tie relationships. TV +L+ programming can provide the common, shared experience +L+ that serves as a basis for socialization, even for groups who +L+ are already fairly close [29]. Social television systems +L+ typically integrate some combination of text chat, voice +L+ chat or video chat with TV programming, and use presence +L+ to provide awareness of the status and context of other users +L+ of the system [5, 9, 15, 23]. These efforts tie into earlier +L+ CMC work on telepresence and co-viewing, but come at the +L+ design and research questions from a different angle. +L+ In previous work we noted that users of social television +L+ systems would benefit from features that indicate favorable +L+ times for shared viewing [15]. One solution is to use +L+ always-on ambient displays to let users be aware of when +L+ others are watching TV. From a research standpoint, we can +L+ then examine how this kind of awareness helps or +L+ encourages people to get in touch through their social +L+ televisions, and in particular, how it is used to initiate and +L+ escalate communication sessions. As no extended field +L+ trials of social television systems have been reported to +L+ date, these important issues remain largely unexplored. +L+ In this paper, we present results from a two-week field +L+ study of Social TV 2, an experimental social television +L+ system that incorporates an ambient display component to +L+ provide awareness of remote viewers. Two groups of five +L+ households participated, with the members of each group +L+ comprising an existing social circle. In addition to standard +L+ social television functionality, Social TV 2 incorporates +L+ ambient displays in order to keep users aware of the +L+ participation of their friends and family. We found that the +L+ ambient displays proved to be a defining component of the +L+ system. The displays were effective indicators of good +L+ times to use Social TV 2, increased participants’ awareness +L+ of others’ TV-viewing schedules, and encouraged +L+ participation in the system. This participation often began +L+ as a glance at the ambient display – a quick check on the +L+ status of a participant’s social network, but would +L+ frequently escalate to deeper or more extended +L+ communication. In this way, participants began treating TV +L+ watching as a fundamentally social activity. Thus, the +L+ </SectLabel_bodyText> <SectLabel_page> 1 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Soci-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 1: The Social TV 2 system in use. The ambient +L+ orb is visible in the upper left, above the television. +L+ addition of ambient displays helped to fulfill the design +L+ objectives of the system. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> In the following sections, we describe the design of our +L+ Social TV 2 prototype, which we then put in the context of +L+ existing work in both the social television and ambient +L+ display literature. We outline unanswered questions that led +L+ us to initiate this work and describe the mixed-method +L+ approach to our field study. We then present our results, +L+ describing how the ambient devices functioned as +L+ traditional presence displays and as components of a more +L+ complex communication system. Lastly, we show how +L+ these findings bear on other social applications in the home, +L+ and outline our future goals for social television research +L+ and design. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> SOCIAL TV 2 +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Social TV 2 follows Social TV, the system previously +L+ described in Harboe et al. [15]. It is a prototype system that +L+ allows remotely located friends and family to experience +L+ some of the benefits of sitting next to one another on the +L+ couch and watching a TV program together (Figure 1). To +L+ provide this experience, we supplement typical TV +L+ functions with two additional features: TV presence +L+ information, and the ability to send and receive lightweight +L+ messages. The features were selected to specifically +L+ investigate people’s behaviors at the boundaries of the +L+ social television experience, as they begin and end their +L+ interactions, rather than the details of how they acted while +L+ fully engaged in the communication experience. +L+ The prototype is implemented as a PC application running +L+ on top of GBPVR1 media center software. The system uses +L+ on-screen displays to communicate which of the user’s +L+ friends or family (‘buddies’) are currently watching, what +L+ they are watching, and what they have watched in the past. +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 http://www.gbpvr.com/ +L+ The laptop running the Social TV 2 software is connected +L+ to a television, and all interaction with the system is +L+ performed with a standard remote control. For our field +L+ tests we chose to use a TiVo® remote that had several keys +L+ relabeled to correspond to the features of our system. +L+ </SectLabel_footnote> <SectLabel_subsectionHeader> TV Presence +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A key requirement for encouraging participation in social +L+ television is the ability to make users aware of when their +L+ friends and family are logged into the system and watching +L+ programs on TV. This information can be viewed from an +L+ on-screen buddy list (Figure 2). The deployed system has +L+ two presence states: ‘watching TV’ and ‘away’. The ‘away’ +L+ state is intended to inform buddies that a user is not +L+ currently watching TV. It is set automatically when the TV +L+ is turned off or when no interactions with the system are +L+ detected for some time after a program has ended. Users +L+ can also disconnect from the social component of the +L+ system if they wish to watch TV in privacy, and will then +L+ appear to others to be ‘away.’ However, in this state many +L+ of the features of the system are disabled, preserving a ‘see +L+ and be seen’ information reciprocity. +L+ Users can change to the same program that their buddy is +L+ watching from the buddy list. Additionally, whenever the +L+ channel is changed, a list of buddies who are also watching +L+ that program is displayed as part of the transient channel +L+ information banner. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Ambient Devices +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> While Social TV 2’s on-screen display provides presence +L+ awareness on the television, we wanted another means to +L+ convey this information when the TV wasn’t on, or when +L+ users were in a different room and unable to see the TV. +L+ We chose to use two separate displays capable of +L+ communicating information unobtrusively, visible from +L+ anywhere in a room, and which would fit in as household +L+ objects. The system has two different display devices to +L+ meet these needs (Figure 3). +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: The Social TV 2 on-screen buddy list. +L+ </SectLabel_figureCaption> <SectLabel_page> 2 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Socio-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 3: The orb and Chumby devices displaying a +L+ purple color (more than one friend is watching TV). +L+ </SectLabel_figureCaption> <SectLabel_bodyText> As our primary ambient display, we chose the Ambient +L+ Orb, a color changing lamp from Ambient Devices,2 and +L+ configured it to display the current number of buddies +L+ watching television. Different colors are used to indicate +L+ whether one (blue), more than one (purple), or no other +L+ buddies (yellow) are currently watching TV. +L+ The orb was connected via serial cable to our prototype, +L+ and therefore had to be placed close to the TV. As people +L+ often spend much of their time outside of the living room +L+ [1], we wanted to include a second display for times when +L+ users could not see the primary ambient display. The +L+ Chumby3 is a WiFi-enabled internet appliance with a 3.5" +L+ LCD color display Though we would have preferred to use +L+ a second orb, the Chumby provided faster updates and more +L+ reliable wireless reception than the orb’s pager-based +L+ network alternative. We wrote a Flash widget for the +L+ Chumby that follows the color of the living room orb in +L+ near-synchronization (within 15 seconds). +L+ Besides providing information about the number of buddies +L+ online, we wanted to use the displays to attract users to the +L+ system when a friend or family member invited them to +L+ watch a show with them. To signal this, we designed the +L+ displays to pulse slowly between the current color and +L+ black. We did not want the flashing to be too distracting, +L+ but wanted users to notice it so that they could come to their +L+ televisions if they desired. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Messaging +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We provided no voice or freeform text communication +L+ features. However, users can send ‘suggestions’ to invite a +L+ buddy to watch their current show together. If the buddy +L+ accepts the suggestion, the system automatically changes +L+ the channel to that program. +L+ When two or more buddies are watching the same program, +L+ the prototype allows them to send messages to each other. +L+ In the first round of this study, the system allowed only +L+ three expressions, in the form of graphical thumbs-up, +L+ thumbs-down, or ‘shout-out’ emoticons. After this +L+ deployment, we received feedback that our participants +L+ wanted to send a wider variety of messages. Therefore, for +L+ </SectLabel_bodyText> <SectLabel_footnote> 2 http://www.ambientdevices.com/ +L+ 3 http://www.chumby.com/ +L+ </SectLabel_footnote> <SectLabel_bodyText> the second group, we implemented a new feature that +L+ replaced the generic ‘shout-out’ message with +L+ approximately 20 pre-determined messages such as “How +L+ is this show?”, “This sucks!”, or “Call me”. A number of +L+ possible replies are available (“Good!”, “Bad!”, etc.), as +L+ well as the thumbs-up and thumbs-down emoticons. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> Social Television Systems and Studies +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The idea of communicating and sharing awareness through +L+ the television set has been explored by a number of +L+ different researchers. However, social television systems as +L+ yet remain confined to labs and limited trials. AmigoTV, +L+ described by Coppens et al. [9], was an early social +L+ television system that included presence, a buddy list, +L+ invitations, emoticons, and voice chat, but no facilities for +L+ indicating presence while offline. Telebuddies, by Luyten et +L+ al. [21], takes a different approach to the challenge of +L+ drawing users into the experience, using a friend-of-a-friend +L+ profile of interests and social relationships to match users +L+ with suitable communication partners. A more +L+ comprehensive review of social television systems can be +L+ found in Harboe et al. [14]. +L+ Geerts [14] and Baillie et al. [2] conducted user studies of +L+ AmigoTV, both of them lab experiments comparing voice +L+ communication to other modalities. Weisz et al. [29] and +L+ Regan and Todd [24] examined groups or pairs of friends +L+ and strangers using text chat while watching videos or TV +L+ together. Similarly, Oehlberg et al. studied groups of +L+ friends and acquaintances watching television, both in +L+ collocated groups and connected via an audio link [23]. +L+ Finally, In Harboe et al. we examined friends and family +L+ watching TV in a voice chat setting; this in the participants’ +L+ homes rather than the lab [15]. +L+ All these studies were based on single-session events. None +L+ of them looked at the process by which a shared viewing +L+ experience is initiated or how the systems are used over +L+ time. However, Boertjes has announced a prolonged in- +L+ home study of a social television system called ConnecTV +L+ [5], and we await those results with interest. +L+ Building on this existing work, we are extending the social +L+ television concept by including additional facilities for +L+ offline awareness of TV presence information. In addition, +L+ we are presenting results from the field that that throw light +L+ on the real-world use of television-based sociability, and +L+ which suggest design considerations for future social +L+ television systems. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Social Awareness Systems for the Home +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Hindus et al. created and evaluated a number of social and +L+ communication systems, with varying levels of media +L+ richness, for the home [18]. They note that consumers +L+ wanted devices with multiple communication modes. +L+ In the context of domestic video calling over the TV, +L+ Hemmeryckx-Deleersnijder and Thorne integrated an +L+ </SectLabel_bodyText> <SectLabel_page> 3 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Socio-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> awareness component that displayed images automatically +L+ captured from the environment [17]. In their scenario, they +L+ seek to provide background awareness and help negotiate +L+ conversation engagement. +L+ De Ruyter et al. compared different degrees of peripheral +L+ awareness of remote friends (displayed as video) while +L+ watching a shared TV program [27]. The system in their +L+ study only supported awareness while in a shared viewing +L+ session. Markopoulos et al. provide an overview of other +L+ awareness systems [22]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Evaluation of Ambient Displays +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> While the ambient display literature contains a wealth of +L+ design examples, it contains few reports of field +L+ evaluations. CareNet [8], a combination ambient/interactive +L+ display for elder care, arrived at a number of useful findings +L+ for the design of effective ambient displays for the home. +L+ Rowan et al. showed how an ambient display can foster +L+ connectedness and promote communication between elders +L+ and their adult children [26]. In addition to these studies, +L+ other researchers have shown how ambient displays can +L+ maintain a sense of connectedness between friends and +L+ loved ones [12, 28]. The Whereabouts clock [6] shows how +L+ simple location and messaging capabilities can enhance +L+ connectedness and allow family members make inferences +L+ about each others’ activities using their existing knowledge +L+ of one another. In this work, we build on these findings to +L+ demonstrate how ambient displays can be designed into a +L+ larger social application to promote use of that system and +L+ fulfill its goals of strengthening social bonds. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> METHODS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In the design of our field study, we were interested to see +L+ whether the ambient displays met our design goal of +L+ encouraging social television participation during those +L+ periods when a participant’s friends and family were also +L+ using the system, if it led to other forms of communication, +L+ and how it would ultimately affect the feelings of +L+ connectedness between our participants. Striving to +L+ understand these patterns of behavior is an essentially +L+ qualitative question, and our approach was largely +L+ exploratory. We ran two separate in-home trials. Five +L+ households were recruited for each trial, and each trial +L+ lasted 14 days. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Recruitment +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Participants were recruited using an independent recruiting +L+ agency that was instructed to find social groups in which +L+ the various household members were mutual friends, and +L+ all had strong ties with one another. The actual relationships +L+ between the recruited households varied (Figure 4). In both +L+ trials there was a central ‘hub’ (A1, B1), a person who +L+ knew and recruited all the other participants, but there were +L+ also at least two households who were more peripheral, +L+ without strong ties to the rest of the group. In both trials the +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4: Social ties between households in our field +L+ trials. Dashed lines represent acquaintances. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> central hub was a woman, and the four friends she recruited +L+ female. However, in each group some of the husbands +L+ knew each other, and in the second group two of the +L+ husbands were best friends. +L+ Group A, our first group, consisted of four couples living in +L+ their own households, two with (very young) children. The +L+ fifth participant was recently engaged, but still living with +L+ her parents and brother. Ages of the main participants +L+ ranged from 26 to 33. A2 and A5 are sisters, and A3 is the +L+ cousin of A1’s husband. A5 was not friends with anyone +L+ else in the study except her sister, and A3 and A4 knew +L+ each other only tangentially from having attended the same +L+ parties. +L+ In Group B, our second group, all five nuclear families +L+ were living in their own households, and all five households +L+ included teenage children. Ages of the main participants +L+ ranged from 46 to 53. B1 was very close friends with B2 +L+ and B4, and knew B3 and B5 well. Again we had a pair of +L+ sisters: B4 and B5, but B2, B3, and B4 only see each other +L+ at the parties of mutual friends, and B5 did not remember +L+ ever meeting B2 and B3. +L+ While there were some husbands in both groups who knew +L+ each other well, and some of the teenage children in Group +L+ B knew each other, their relationships, for the most part, +L+ mirrored those of our female main participants. Thus, we +L+ had a number of different kinds of relationships represented +L+ in the two social groups, but neither group was uniformly +L+ tight-knit. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Deployment +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> For each household, the Social TV 2 system was installed +L+ in the room participants reported as being the most common +L+ place for watching TV. As mentioned earlier, Groups A and +L+ B received slightly different versions of the software, as a +L+ result of an iterative design process on the basis of Group +L+ A’s feedback. Each household was given one orb and one +L+ Chumby. We asked participants to place the Chumby in any +L+ part of their home where they spent a significant amount of +L+ time or passed by frequently. Six participants put it in the +L+ </SectLabel_bodyText> <SectLabel_page> 4 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Soci-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> kitchen, two in a hallway, and one in a bedroom. The last +L+ Chumby malfunctioned and was not used. Participants were +L+ given a brief (20 minute) introduction to the system and its +L+ features, and a phone number for technical support that they +L+ could call at any time. We asked each participant to use the +L+ system the way they would if they were not in a study, and +L+ explained that they did not have to use every feature, or any +L+ feature, unless they wanted to. Other household members +L+ were also welcome to use the system, and generally did so. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Data Collection +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We used multiple methods for data collection, including +L+ interviews, usage logs, and voice mail diaries. +L+ There were three sets of semi-structured interviews for each +L+ household. The initial interview lasted about half an hour +L+ and was used to collect background information. In a phone +L+ interview after the first week, lasting between 15 and 30 +L+ minutes, we gathered data about the participants’ use of and +L+ reactions to the prototype during the first week. And the +L+ final interview, lasting from an hour to an hour and half, +L+ was structured to collect more detailed information on a +L+ number of topics. Whenever possible, we asked other +L+ household members to participate in the interviews. We +L+ recorded a total of 22 hours of interview data, but much of +L+ the material falls outside the scope of this paper. +L+ We logged all interactions with the system, in order to +L+ document actual usage and to allow us to examine +L+ interesting incidents in detail later. The logs, then, provide +L+ support for some of the events described in the interviews. +L+ Cameras or sensors in the home would have been useful to +L+ put the system use in context, but we felt this would be too +L+ intrusive and technically complex for this study. +L+ Instead, voice mail diaries were used to collect information +L+ on behaviors we could neither log nor directly observe, and +L+ which we were afraid would be forgotten prior to the +L+ interviews. We devised questions about behaviors +L+ surrounding the ambient presence/awareness features, +L+ communications with other people in the study, and their +L+ reactions to, as well as use of, the various features. So that +L+ the questions would not influence our participants’ behavior +L+ in advance, the participants were given 14 sealed envelopes, +L+ one for each day of the study, each containing that day’s +L+ questions. Thus, participants had the questions in front of +L+ them as they provided their feedback. Participants left 3 +L+ hours and 41 minutes of voice mail messages in total. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Analysis +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> From the log data we extracted some basic measures of +L+ system use (Table 1). Given the small scale and non- +L+ experimental design of the study, we did not attempt further +L+ statistical treatment of the quantitative data. +L+ The bulk of the analysis was instead qualitative, using a +L+ variation of the affinity diagram method. We reviewed the +L+ interview and voice mail data, and extracted observations, +L+ statements (which we transcribed) and behavioral +L+ </SectLabel_bodyText> <SectLabel_table> Activity	A	B +L+ TV watched, connected (hrs)	154.27	180.77 +L+ TV watched, disconnected (hrs)	21.81	0.34 +L+ Buddy list views	185	390 +L+ Joined show through buddy list	62	160 +L+ Emoticons sent	59	120 +L+ Canned messages sent	N/A	185 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Aggregate usage data by group. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> descriptions relating to ambient presence and awareness. +L+ These items were printed out as sticky notes. +L+ Then, working as a team, we put these items together into +L+ groups or categories, a process of “comparison, contrast, +L+ and integration” [19]. To efficiently organize the data we +L+ used an affinity-like post-it chart [4]. Here we followed +L+ Bernard: As the categories were identified we would “pull +L+ all the data (that is, exemplars) from those categories +L+ together and compare them, considering not only what +L+ [items belong] in each emerging category but also how the +L+ categories are linked together” [3]. The patterns that +L+ emerged from this analysis process form the basis of the +L+ results that follow. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Our analysis identified several themes around the effect and +L+ effectiveness of our ambient displays. We first discuss their +L+ basic ability to communicate presence information, and +L+ then go on to talk about how they functioned as part of the +L+ larger Social TV 2 system. Throughout, we use quotes from +L+ the interviews and voice mail diaries to illustrate each +L+ particular theme in our participants’ own words. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Effectiveness as Ambient Displays +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In considering how the ambient information interacted with +L+ the other components of the Social TV 2 system, the orb +L+ and Chumby would only be relevant if they actually +L+ functioned as ambient displays. Fortunately, they did. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Noticing and interpreting the displays +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Our participants saw and noticed the changing colors of the +L+ orb, and by and large understood what they signified. B3: “I +L+ notice when the color’s on, whether it’s purple or blue, I +L+ know that someone else is actually on the system.” In fact, +L+ several participants commented on the orb consistently +L+ drawing their attention. B 1 mentioned in the final interview +L+ that “every time I passed by, a thousand times a day, I +L+ would look to see what color it was.” Only one participant, +L+ B4, could not account for the meaning of the different +L+ colors. When asked if she knew what they signified, she +L+ answered: “[My daughter] does, I don’t.” +L+ The pulsing signal that the ambient devices sent when a +L+ suggestion was received was less widely understood. Many +L+ </SectLabel_bodyText> <SectLabel_page> 5 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Soci-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> participants confused pulsing with just changing colors. For +L+ example, B3 said “I’m not sure if it was pulsating or just +L+ changing purple to blue, to purple to blue [ ... ] I honestly +L+ don’t know...” This may partly have been a problem of +L+ terminology. More substantially, at least part of the reason +L+ for the confusion was that few participants saw the pulsing +L+ happen. Most invitations were sent to people who were +L+ already watching TV, and who accepted or declined them +L+ before they had a chance to notice the ambient devices +L+ pulsing. Another likely contributing factor was the subtlety +L+ of the signal: “I love the orb, but when it flashes, I wish it +L+ flashed a different color or did something crazy. I can’t tell +L+ when it’s flashing sometimes.” (A2) +L+ A number of participants reported that they were less aware +L+ of the Chumby than of the orb. As B3 said: “I think it's kind +L+ of a non-entity, the Chumby.” A few different factors +L+ appeared to contribute to this. In daylight, the Chumby’s +L+ small LCD screen was only clearly readable from certain +L+ angles, so participants were unable to see it from some +L+ areas of the room where it was installed. In a few +L+ households, the Chumby was redundant because +L+ participants could also see the orb from most places where +L+ they could see the Chumby. Finally, the Chumbies used +L+ were pre-production models prone to technical glitches and +L+ service interruptions, so for nearly all of the participants the +L+ Chumby did not work properly for parts of the study. +L+ Nevertheless, the Chumby played an important part of the +L+ experience for certain participants: For example, A1 told us +L+ “Since I had [the Chumby] in the kitchen it was just while I +L+ was cooking...it was like ‘oh, I wonder who’s on.’ ” +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Placement +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> As noted by others [8, 26], we found that the ambient +L+ displays needed to be located where they would frequently +L+ be seen. B1 mentioned, “The physical positioning in my +L+ house [for the Chumby] was the perfect spot, because I +L+ spend most of my day in the kitchen when I’m home.” In +L+ two households, participants chose locations for the +L+ Chumby where they rarely spent time or passed by. “The +L+ orb was a lot more helpful than the Chumby. [ ... ] Because +L+ we’re down here more.” (B2) A5 did not regard the +L+ displays as particularly useful. This may have been the +L+ result of physical context, as both were put in her brother’s +L+ bedroom. When we asked her if she ever saw the orb go +L+ purple when she wasn’t watching TV, she replied “Uh, no, +L+ because if I was in that room, I knew I’m gonna be +L+ watching TV.” +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Acceptability of form factor +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The majority of our participants liked the form factor of the +L+ displays. We received numerous comments such as “I love +L+ the orb” (B5), and “I like the orb, I like the Chumby... the +L+ colors and the blinking” (A2). A3 and A4 both called the +L+ orb “cool” and B4 said it was “neat.” Even when the +L+ novelty of the system wore off and some participants were +L+ saying that interacting with the on-screen application was a +L+ “chore” (B5), their enjoyment of the orb continued to the +L+ end. It should be noted that the acceptance wasn’t universal; +L+ A1’s husband complained that the orb did not go with their +L+ décor, but “if it was a little bit smaller I think it would look +L+ fine.” +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Drawing Users into Communication +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Although the ambient devices provided effective social +L+ presence, our participants felt that the orb and Chumby +L+ would not be useful by themselves without the rest of the +L+ information the Social TV 2 system provided. “It would be +L+ pretty, but I couldn’t see what people were doing,” said A3. +L+ B1 commented that “There’d be no reason to have an orb +L+ and then I can’t turn it on and see who it’s connected to.” +L+ A2 thought that without the additional information of who +L+ was watching, and what they were watching, “it would be a +L+ tease.” Balancing this, several of our participants said that +L+ at times they were just interested in learning that a friend +L+ was at home and available, not specifically whether he or +L+ she was watching television. +L+ We saw further evidence that the ambient information did +L+ help to involve them in the social television experience. +L+ Many of our participants told us of occasions when they +L+ turned on the TV because the orb indicated that others were +L+ watching. For example, A2 told us “as soon as I come into +L+ the house or I wake up or come into the room, that’s the +L+ first thing. It draws my attention, and the first thing I do is +L+ turn on the TV.” In a voice mail, A3 told us that the orb +L+ “has made me a little bit more aware, makes me want to, +L+ when it does change colors, to see which of my buddies are +L+ on.” +L+ In some cases the participants switched on the TV just to +L+ see who was there, then turned it off. However, more often +L+ they would contact or be contacted by one of their friends, +L+ or simply go on watching the TV show. A2 told us that she +L+ would turn on the TV “just to see who’s on, and then most +L+ of the time we would all end up watching the same thing.” +L+ This usually involved exchanging lightweight messages: +L+ “Find out what other people are watching, flip to what +L+ they’re watching, and then talking to some of the people to +L+ throw comments out.” (B3) +L+ In a number of cases, the people who described watching +L+ TV with their buddies and messaging them were people +L+ who were doing similar things prior to the study. For +L+ example, the sisters A2 and A5 told us that they often +L+ watched TV shows together, and A2 mentioned receiving a +L+ text message from her sister while they were both watching +L+ the season finale of Gilmore Girls. Regarding our system, +L+ she said: “The first thing I do is see what my buddies are +L+ watching, and then I tend to sometimes watch what they’re +L+ watching and then give suggestions or thumbs up or down +L+ depending on how I feel toward their program.” +L+ Interestingly, people who used to watch TV with others +L+ remotely found themselves doing so again once the Social +L+ TV 2 system was in their homes. B 1 told us how, years ago, +L+ she and her close friend B2 would “watch Saturday Night +L+ </SectLabel_bodyText> <SectLabel_page> 6 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Socio-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_table> 8:01:48 PM B5 watching TV, B1 turns TV on +L+ 8:06:24 PM B5 joins B1 at Desperate Housewives +L+ 8:07:00 PM B 1: “How are you?” +L+ 8:08:38 PM B5: “Good!” +L+ 8:09:05 PM B1: t, [thumbs-up] +L+ 8:09:18 PM B5: t, [thumbs-up] +L+ 8:10:20 PM B5: “I really like this!” +L+ 8:17:26 PM B1: “Call me!” +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Sample interaction sequence from logs. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Live together,” while they talked on the phone. However, +L+ they haven’t done anything like that since their children +L+ were little. Until they participated in this study: +L+ </SectLabel_bodyText> <SectLabel_construct> “I noticed the orb was blue, so I knew somebody had their +L+ television on, and sure enough, it was [B2’s household]. +L+ And I knew that her husband’s at work and her kids were at +L+ school, so I deduced it was [B2]. And so without even +L+ saying ‘Who’s there?’ I immediately went to her channel, +L+ which was Oprah, and I sent her a thumbs-up. And then +L+ she thumbs-upped me, and then two seconds later I said [to +L+ myself], ‘This is dumb!’ And then [I called her, and] we +L+ had a whole conversation.” +L+ </SectLabel_construct> <SectLabel_bodyText> In this story, we can see that the ambient display serves to +L+ initially draw the participant into an interaction which then +L+ becomes progressively deeper, culminating in a direct voice +L+ conversation. This is a pattern we can see repeated with +L+ different variations in the logs (Table 2). +L+ It should be pointed out that although turning the TV on in +L+ response to the orb was one of the most commonly reported +L+ behavior patterns in our study, some participants did not +L+ have a desire to turn on the television and dig deeper into +L+ the presence information. B3 said: “[The orb is] kind of +L+ interesting, but once again, what do I care if somebody is +L+ watching TV?” +L+ Other participants were curious, but had little interest in +L+ talking about it. B2’s husband said he would “turn it on just +L+ to see what they’re watching, and then probably send them +L+ a message saying that ‘it sucks’ or something.” To put this +L+ in context, he rarely socializes around TV content anyway; +L+ although B1’s husband is his best friend, he has only once +L+ been over there to watch TV, for a Super Bowl party. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Meeting the Goal of Connectedness +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our participants reported that when they saw the ambient +L+ colors, they found themselves thinking about the fact that +L+ others were watching TV. In some cases this translated into +L+ a feeling that they could “know what’s going on,” (B2’s +L+ husband) with others in the study. Said A3: “I still think the +L+ orb is kinda the coolest part of the whole thing because it’s +L+ really neat to see it change color and then know that +L+ something is going on on the other end to cause it to do +L+ that.” The husband of A1 said: “Even before I turned on the +L+ TV, I knew that someone was on there.” B1 put it this way: +L+ “I liked the different colors, I liked coming in the house and +L+ saying ‘oh, someone’s home watching TV too now.’ I don’t +L+ know, it was like a friendly feeling, like someone else is +L+ home and I’m not the only one home tonight.” +L+ While B1 knew all the other participants in her trial, that +L+ was not true for the others. Participants on the periphery of +L+ the group could only imagine what it would be like to use +L+ the system with people closer to them. For example B5, +L+ who expressed dismay with the system overall, said “it’s a +L+ love connection. You have feelings for these people and +L+ you care more. My parents could be blue, my sisters could +L+ be pink, orange, green.” While B3 did not find the orb +L+ useful during the study, when she was asked if she would +L+ be more interested if the orb was telling her about different +L+ people, she answered: +L+ </SectLabel_bodyText> <SectLabel_construct> “Oh, yeah, yeah, yeah. Probably. If it’s somebody I was +L+ closer to. [ ... ] It’s almost like a communication light +L+ bulb. You know what I mean? Like the fire bell goes off, +L+ and ‘Ding, ding, ding!’ If I saw that, I’d be ‘Hmm, Dad’s +L+ watching,’ or, I feel like ‘Hmm, I’m connecting with +L+ somebody.’ It sounds so stupid, but maybe you’re +L+ connecting with somebody.” +L+ </SectLabel_construct> <SectLabel_subsectionHeader> Inference and Learning about Presence +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Despite the fact that our ambient displays only showed +L+ three colors, our participants were able to combine the +L+ information conveyed with previous social knowledge in +L+ order to draw rich inferences about the other people in the +L+ study. In some cases it inspired them to leaps of +L+ imagination, like when B2’s husband saw the orb turn blue +L+ when someone turned the TV on at 2 am: “I just figured +L+ somebody was watching, maybe fell asleep on the couch, +L+ maybe some guy got thrown out of the bedroom for the +L+ night.” At other times the speculations were more mundane, +L+ as when B 1 looked at the orb one morning: “The orb was +L+ yellow and no one was on, so I'm assuming everyone was +L+ already at work.” This statement also exemplifies another +L+ use our participants found for the ambient information: +L+ Whether or not someone was watching TV was used as a +L+ proxy for whether they were home. Naturally, the presence +L+ information in the buddy list was a helpful supplement to +L+ the ambient devices for this purpose. A5 put it this way: +L+ “It’d be interesting when I’d be on at night, like ‘Oh, let me +L+ see if she’s on. Is she watching TV, is she home, is she +L+ out?’ So that was one way to know, ‘Yeah, she’s home, +L+ she’s watching TV.’ ” +L+ We saw evidence that over the course of the study, our +L+ participants learned more about each other’s viewing habits, +L+ and used that knowledge to interpret the ambient signals +L+ with more confidence, guessing who might be on at any +L+ given time. Most of our participants knew little about the +L+ TV viewing habits of the rest of their group before the +L+ study commenced. To take just one example, A1 was only +L+ able to make general guesses about her friends’ viewing +L+ habits when the study started. At the end, on the other hand, +L+ she told us: “Yeah, I think as the two weeks progressed I +L+ kinda had a feel; like on Fridays I know [A4] works from +L+ </SectLabel_bodyText> <SectLabel_page> 7 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Soci-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> home so, like, this past Friday the light was blue, and I was +L+ like ‘the only person who’s going to be watching TV right +L+ now is her.’ So I kinda got a feel. Late night would be [A5], +L+ daytime would be [A3] because of the kids.” +L+ By allowing participants to leverage their existing +L+ knowledge of one another to learn more detail about their +L+ viewing habits, the ambient information conveyed by Social +L+ TV 2 became more powerful. Participants were soon able to +L+ use the orb to glean (with some degree of uncertainty) +L+ information that they had initially needed to look up in the +L+ buddy list. One indication of the meaning that our +L+ participants attributed to the ambient devices’ output is the +L+ power the displays had to puzzle them. When B1 sat down +L+ to watch TV on a rainy evening, she noted to her surprise +L+ that no one else was logged on: “The oddest thing, it’s a +L+ nice night to be in... and not one person is on Social TV +L+ tonight!” +L+ Presence seemed to lead to an expectation that someone +L+ who was watching TV was available for interaction. +L+ Unfortunately, our participants were often disappointed. B2 +L+ told us in a voice mail: “We’ve not really heard back from +L+ people, the orb is blue, I don’t know if they’re getting them +L+ or if they’re just not sending back or what the problem is.” +L+ In the phone interview she said that when people weren’t +L+ responding, perhaps “they just wanted to watch their show, +L+ and they were ignoring you.” B1 said that when she notices +L+ the orb turning blue or purple and turns on the TV, she felt +L+ like “they’re on for a few minutes, we say hello, and then +L+ someone turns it off or they have to go.” +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> An Integral Component of Social TV 2 +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> If we consider the ambient devices in isolation, they appear +L+ as fairly ordinary displays for peripheral presence +L+ awareness. And indeed, the results show that they +L+ successfully functioned as such. This, along with the fact +L+ that the orb, at least, was on the whole so well liked, goes +L+ some way towards validating our design, but it is not by +L+ itself a particularly novel finding. Far more interesting are +L+ the ways in which the ambient devices and the other Social +L+ TV 2 features interacted. Our findings reveal and hint at a +L+ number of interesting behaviors emerging, and these are +L+ particularly relevant to the design and understanding of +L+ future social systems in the home. +L+ Our system was designed so that the ambient devices only +L+ worked while the Social TV 2 client remained connected to +L+ the network. And as we have noted, people, in general, +L+ liked the ambient lights, both because of the information +L+ they provided and for the aesthetic appeal. Although the +L+ design of our study didn’t allow us to conclusively test this, +L+ it does seem to indicate that the ambient devices would +L+ therefore encourage people to stay logged on, and serve as a +L+ reminder, if they ever do leave, to return as soon as +L+ possible, thereby ensuring the presence conduit remains +L+ open. +L+ The orb and the Chumby were effective at conveying when +L+ other people were watching TV, and this allowed our +L+ participants to be aware of others’ availability even when +L+ their own TV sets were turned off and they were engaged in +L+ other activities. In this way, it reduced the risk of an +L+ opportunity for interaction going by unnoticed. +L+ Together, these effects expand the interface between the +L+ system and the environment. For one thing, the chances of +L+ making contact are greatly increased. Also, the social +L+ television experience is no longer something that only takes +L+ place during the time you are actually watching TV. The +L+ ambient devices keep users engaged with the system while +L+ they go about other activities, thereby creating an ‘out-of- +L+ the-box’ social experience. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Multiple Levels of Engagement +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our findings indicate that while it extended the social +L+ experience beyond the TV, the awareness provided by the +L+ ambient displays also helped draw our participants into the +L+ television experience and encourage them to use the +L+ system. This result supports previous efforts to use +L+ peripheral social awareness to lower the barriers to +L+ communication [17, 11]. +L+ We can generalize the steps reported by our users (and +L+ corroborated by the logs): the participants become aware of +L+ their friends’ availability through the ambient devices, turn +L+ on their TV (thereby themselves showing up as available to +L+ the other participants), look at their buddy list, and either +L+ join what one of their friends were watching or suggest that +L+ their friend join them. Once they were viewing together, +L+ they would usually send messages or emoticons, and +L+ sometimes this would culminate with a phone call. From +L+ this idealized flow, we can define different stages of +L+ interaction (Table 3). This can be viewed as an extension of +L+ Eggen et al.’s three interaction states [13]. +L+ We can see that as we move down the levels, the user +L+ becomes progressively more engaged with the experience, +L+ going from peripheral presence awareness to immersive +L+ participation through a number of intermediate levels. Part +L+ of the reason for this is that at each stage, the user has +L+ access to more detailed information. This is similar to a +L+ “ramping interface” model of information design and +L+ interaction [25], and allows users to drill down to the level +L+ they are interested in. In particular, each step provides more +L+ specific presence and richer contextual awareness. The +L+ ambient device provides only aggregate presence, and only +L+ conveys that a TV is on. The buddy list shows presence per +L+ household, and what they are watching. By watching the +L+ same thing, common ground is established, and by +L+ communicating it is possible to identify the other person at +L+ an individual level. However, the most interesting thing to +L+ note may be that as users become more deeply engaged, +L+ they also become increasingly present to, and eventually +L+ connected to, their buddies. +L+ </SectLabel_bodyText> <SectLabel_page> 8 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Soci-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_table> Interaction	Information	Presence specificity	Context detail	What others experience	Degree of +L+ 	amount				participation +L+ Noticing ambient display	Bite	Aggregate buddy list (+inferences)	TV is on/off	No presence info	Peripheral awareness +L+ 					Full involvement +L+ Viewing buddy list	Snack	Aggregate household (+inferences)	The program(s) being watched	TV is on/watching program +L+ Co-viewing program			Shared viewing context (common ground)	Sharing the viewing experience +L+ Exchanging lightweight messages	Meal	Aggregate	Reactions to show, responses to	In contact, communicating +L+ 		household or individual	messages +L+ Talking on the phone	Feast	Individual	Two-way audio stream	Rich conversation +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3: Different stages of interaction with the system, along with some notable characteristics. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Users can move freely between the various interaction +L+ levels, as their needs and interest dictate. However, each +L+ stage provides impulses that encourage deeper engagement, +L+ playing on such traits as curiosity and desire to express +L+ opinions, and guides users towards the ultimate state of live +L+ conversation, which here takes the form of phone calls. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Improving the Quality of Time Spent Watching TV +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We see that the ambient information is intimately bound up +L+ with the social functionality available on the TV. Although +L+ our participants found the ambient data interesting in the +L+ context of the Social TV 2 system, the information would +L+ be nearly meaningless on its own; “a tease.” If the ambient +L+ displays extend the social television experience outside of +L+ the TV set, the other features help users interpret the +L+ ambient signals and provide the information with a purpose. +L+ They make it actionable. +L+ Furthermore, as social presence information is made +L+ actionable through the TV, turning on the TV becomes +L+ redefined as a social act: “Even before I turned on the TV, I +L+ knew that someone was on there.” Because the ambient +L+ devices provide users with at least peripheral awareness of +L+ their buddies’ presence information, there is now an +L+ unavoidable social dimension to pressing that button, such +L+ as expectations of availability. And even if users should be +L+ oblivious, turning on the TV affects their buddies’ ambient +L+ displays and presence view, making them aware. We saw +L+ this social consideration give rise to new behaviors such as +L+ turning on the TV when the ambient devices showed other +L+ people online. +L+ The methods of our study do not allow us to say with +L+ confidence whether Social TV 2 caused our participants to +L+ watch more TV, or keep their TV on more. For that, a +L+ control condition would be required. Certainly some of our +L+ findings suggest that it might be the case. However, in light +L+ of the above, direct comparisons may not be particularly +L+ meaningful. To watch Social TV 2 is not merely to +L+ consume entertainment, but to engage in communication +L+ with friends and family that can bring you closer together, +L+ reaffirm social ties, and let you get to know each other +L+ better. The ambient displays provide a first point of contact +L+ for TV-based conversations, and help establish a social +L+ mindset around the very notion of television. +L+ This insight suggests that different systems could use +L+ ambient displays to emphasize specific features, by +L+ preparing the user’s frame of mind in advance of their +L+ active interactions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper we have presented results from testing a social +L+ television system with an ambient component for social +L+ presence. On the whole, our participants liked the idea of +L+ having the TV watching activities of their social groups +L+ represented in ambient displays. Presence information was +L+ understood and used as a proxy for who was home and +L+ available. More importantly, it worked to support the +L+ experience of communicating through the television. +L+ Most social television research to date has focused on what +L+ happens once people are engaged in a social television +L+ experience. There has been little research into how such +L+ sessions would be initiated, and how they can be made to fit +L+ into the context of everyday activities. Our study addresses +L+ these issues: Designs based on ambient displays offer a +L+ credible answer, and our field study showed one such +L+ design to perform well in practice. Since the same concerns +L+ are relevant to a wide class of other in-home social systems, +L+ this finding holds more general interest. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> One of the most interesting questions raised by this study is +L+ how the addition of social awareness will affect pre-existing +L+ patterns of behavior around TV viewing. The effect is +L+ profound, and some simple examples are clearly evident in +L+ the data. However, we suspect that the altered social +L+ dynamic of TV viewing could have far more complex and +L+ subtle effects, especially as the presence awareness +L+ increases the visibility of TV viewing habits to oneself and +L+ </SectLabel_bodyText> <SectLabel_page> 9 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Soci-Cultural Impact	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> others. Understanding these changes in perception and +L+ behavior is a rich area for further research. +L+ While our data collection provided a great deal of +L+ qualitative information on how participants used Social TV +L+ 2, we have no data on when participants were home, when +L+ they were in a room from which they could observe the +L+ ambient indicators, or who was using the system at any +L+ particular moment in time. This limits our ability to +L+ interpret the log data. In upcoming studies we plan to +L+ include a control condition, to provide a baseline against +L+ which changes in behavior can be detected. +L+ We continue to iterate on the design of the Social TV +L+ prototype. Given the tendency of our participants to seek +L+ progressively deeper engagement and richer +L+ communication, we have now integrated support for voice +L+ and text chatting, and are currently preparing another field +L+ trial with these features included. We are also considering +L+ other form factors for the ambient devices, and changing +L+ the kinds of information these devices display. However, +L+ one change we are not going to make is eliminating the +L+ ambient information. Our findings spoke clearly to us about +L+ the attractive nature of ambient awareness, and we intend to +L+ keep using this feature to promote TV-based sociability. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We thank Ambient Devices, Inc. and Chumby Industries for +L+ providing the ambient devices, and Seonyoung Park and +L+ Elaine Huang for their contributions to the study and paper. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Aipperspach, R., Rattenbury, T., Woodruff, A., Canny, J. +L+ (2006) A Quantitative Method for Revealing and +L+ Comparing Places in the Home. Proc. Ubicomp 2006. +L+ 2. Baillie, L., Fröhlich, P., Schatz, R. (2007) Exploring Social +L+ TV. In Proc. ITI 2007: 215–220. +L+ 3. Bernard, H.R. (1998) Handbook of Methods in Cultural +L+ Anthropology. Walnut Creek, CA : Altamira Press. +L+ 4. Beyer, H., & Holtzblatt, K. (1998). Contextual design: +L+ Defining customer-centered systems. San Francisco, CA : +L+ Morgan Kaufmann Publishers. +L+ 5. Boertjes, E. (2007) ConnecTV: Share the Experience. In +L+ EuroITV 2007 Adj. Proc. +L+ 6. Brown, B., Taylor, A., Izadi, S., Sellen, A., Kaye, J. (2007) +L+ Locating Family Values: A Field Trial of the Whereabouts +L+ Clock. In Proc. Ubicomp 2007. +L+ 7. Bureau of Labor Statistics (2006) American Time Use +L+ Survey. ftp://ftp.bls.gov/pub/news.release/atus.txt +L+ 8. Consolvo, S., Roessler, P., Shelton, B.E. (2004) The +L+ CareNet Display: Lessons Learned from an In Home +L+ Evaluation of an Ambient Display. Proc. Ubicomp 2004. +L+ 9. Coppens, T., Trappeniers, L. Godon, M. (2004). +L+ AmigoTV: towards a social TV experience. In Proc. +L+ EuroITV 2004. +L+ 10. Crispell, D. (1997) “TV soloists – Statistics on number of +L+ television sets owned in households from SRI Consulting’s +L+ Media Futures Program”, American Demographics, May +L+ 1997. +L+ 11. DeGuzman, E.S., Yau, M., Gagliano, A., Park, A., Dey, +L+ A.K. (2004) Exploring the design and use of peripheral +L+ displays of awareness information. CHI 04 Ext. Abstracts. +L+ 12. Dey, A.K. and DeGuzman, E.S. (2006) From Awareness to +L+ Connectedness: The Design and Deployment of Presence +L+ Displays. In Proc. CHI ’06: 899–908. +L+ 13. Eggen, B., Rozendaal, M., Schimmel, O. (2003) Home +L+ Radio: Extending the Home Experience beyond the +L+ Physical Boundaries of the House. In Proc. HOIT 2003. +L+ 14. Geerts, D. (2006) Comparing voice chat and text chat in a +L+ communication tool for interactive television. In Proc. +L+ NordiCHI 2006: 461–464. +L+ 15. Harboe, G., Massey, N., Metcalf, C.J., Wheatley, D., +L+ Romano, G. (2008) The Uses of Social Television. In +L+ Comput. Entertain. 6, 1 (Jan 2008). +L+ 16. Harrison, C. and Amento, B. (2007) CollaboraTV – +L+ Making TV Social Again. In EuroITV 2007 Adj. Proc. +L+ 17. Hemmeryckx-Deleersnijder, B. and Thorne, J.M. (2007) +L+ Awareness and Conversational Context Sharing to Enrich +L+ TV Based Communication, In Proc. EuroITV 2007: 1–10. +L+ 18. Hindus, D., Mainwaring, S.D., Leduc, N., Hagström, A.E., +L+ Bayley, O. (2001) Casablanca: Designing Social +L+ Communication Devices for the Home. In Proc. CHI ’01. +L+ 19. LeCompte, M. and Schensul, J. (1999) Designing and +L+ Conducting Ethnographic Research. Walnut Creek, CA : +L+ Altamira Press. +L+ 20. Lull, J. (1990). Inside family viewing: Ethnographic +L+ research on television's audiences. London: Routledge. +L+ 21. Luyten, K., Thys, K., Huypens, S., Coninx, K. (2006) +L+ Telebuddies: Social Stitching with Interactive Television. +L+ In CHI 2006 Extended Abstracts. +L+ 22. Markopoulos, P., de Ruyter, B., Mackay, W.E. (2005) +L+ Awareness Systems: Known Results, Theory, Concepts +L+ and Future Challenges. CHI 05 Ext. Abstracts. +L+ 23. Oehlberg, L., Ducheneaut, N., Thornton, J. D., Moore, R. +L+ J., Nickell, E. (2006). Social TV: Designing for +L+ Distributed, Sociable Television Viewing. In Proc. +L+ EuroITV 2006: 25–26. +L+ 24. Regan, T. and Todd, I. (2004) Media Center Buddies: +L+ Instant Messaging around a Media Center. In Proc. +L+ NordiCHI 2004: 141–144. +L+ 25. Rhodes, B. and Maes, P. (2000) Just-in-time information +L+ retrieval agents. IBM Systems Journal 39(3–4). +L+ 26. Rowan, J. and Mynatt, E. D. (2005) Digital Family Portrait +L+ Field Trial: Support for Aging in Place. In Proc. CHI ’05: +L+ 521–530. +L+ 27. de Ruyter, B., Huijnen, C., Markopoulos, P., IJsselstein, +L+ W. (2003) Creating social presence through peripheral +L+ awareness. In Proc. HCI International 2003. +L+ 28. Vetere et al. (2005) Mediating Intimacy: Designing +L+ Technologies to Support Strong-Tie Relationships. In +L+ Proc. CHI ’05: 471–480. +L+ 29. Weisz, J. D., Kiesler, S., Zhang, H., Ren, Y., Kraut, R. E., +L+ Konstan, J. A. (2007) Watching together: integrating text +L+ chat with video. In Proc. CHI ’07: 877–886. +L+ </SectLabel_reference> <SectLabel_page> 10 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Help Me Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> Conversation Pivots and Double Pivots +L+ </SectLabel_title> <SectLabel_author> Daniel Xiaodan Zhou, Nathan Oostendorp, Michael Hess, Paul Resnick +L+ </SectLabel_author> <SectLabel_affiliation> CommunityLab* +L+ University of Michigan School of Information +L+ </SectLabel_affiliation> <SectLabel_address> Ann Arbor, MI 48109 +L+ </SectLabel_address> <SectLabel_email> {mrzhou, oostendo, mlhess, presnick}@umich.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Many sites on the web offer collaborative databases that +L+ catalog items such as bands, events, products, or software +L+ modules. Conversation pivots allow readers to navigate +L+ from pages about these items to conversations about them +L+ on the same site or elsewhere on the Internet. Double pivots +L+ allow readers to navigate from item pages to pages about +L+ other items mentioned in the same conversations. Using +L+ text mining techniques specific to the collection it is +L+ possible to find references to collected items in online +L+ conversations. We implemented conversation pivots for the +L+ CPAN archive of Perl modules, and for Drupal.org, the +L+ reference site for the Drupal content management system. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Online Discussion, Conversation, Recommender, Pivot, +L+ Drupal, Perlmonks +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> H5.4 Hypertext/Hypermedia Navigation. H5.2. User +L+ Interfaces +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Many websites maintain collections of pages about people, +L+ places, and things. These item pages typically include +L+ structured data. The sites also frequently include online +L+ forums, with an abundant and unstructured repository of +L+ user-contributed data about the same items. Drenner et al +L+ [1] describe these areas as “item-land” and “forum-land”, +L+ and describe how the site MovieLens was able to cross-link +L+ them. Movie item pages have links to conversation threads +L+ mentioning the movies and conversation pages link to the +L+ referenced movie pages. +L+ More generally, item-land describes a large variety of +L+ online collections: +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00 +L+ </SectLabel_copyright> <SectLabel_listItem> •	Musician profiles on MySpace.com or Last.fm +L+ •	Concert or other event listing in a public calendar +L+ •	Wikipedia entries for people, places, or things +L+ •	Product pages (e.g., on Amazon.com) +L+ •	Software module pages on a repository such as PEAR +L+ or CPAN +L+ </SectLabel_listItem> <SectLabel_bodyText> It is easy to imagine how bridges between “item-land” and +L+ “forum-land” could be very useful in these contexts, +L+ especially when “forum-land” might be separate from the +L+ item collection (i.e., on a separate website.) +L+ While previous work has explored the connections in the +L+ case of movie items, we explore them in the case of item +L+ pages that describe software modules. We explain how the +L+ unique features of software modules and conversation +L+ threads can be used in inferring links between them. We +L+ also show how these links can be used not only to help +L+ navigate from software modules to related conversation but +L+ also from software modules to related modules. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONVERSATION PIVOTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Links between items and forums are instances of a more +L+ general class of navigation aids that we call pivots. A pivot +L+ enables navigation from an object to a set of other objects +L+ that share some attribute in common. Perhaps the most +L+ familiar instantiation of the pivot concept is the ability, at +L+ sites such as del.icio.us and Flickr and many individual +L+ blogs, to click on a “tag” in order to move from one page or +L+ photo to a set of others that have been classified with the +L+ same tag. Many other pivots are possible. For example, a +L+ pivot can allow navigation from a message to other +L+ messages by the same author, or from an event +L+ announcement to other events at the same venue, or other +L+ events at the same time, or other events featuring the same +L+ speaker.1 +L+ </SectLabel_bodyText> <SectLabel_footnote> * CommunityLab is a collaborative project of the University of +L+ Minnesota, University of Michigan, and Carnegie Mellon +L+ University. http://www.communitylab.org/ +L+ 1 We borrow this usage of the term pivot from spreadsheet +L+ pivot tables and from the description of features of the Ning +L+ platform for building social applications. [2] +L+ </SectLabel_footnote> <SectLabel_page> 1009 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Help Me Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_table> 		Messages +L+ 		T4	T2	T3	T4a	T4b +L+ 	S1	0	1	0	1	0 +L+ 	S2	1	0	1	0	0 +L+ 	S3	1	0	1	0	1 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1. A matrix representing the pivot from software +L+ modules to conversation messages that reference them. +L+ Target messages T4a and T4b are from the same thread. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> More formally, we define a pivot as a function that maps +L+ from source items to subsets of a set of target items. The +L+ pivots can be represented as a matrix P, with one row for +L+ each source item and one column for each target item. Each +L+ cell in the matrix indicates whether the source and target are +L+ related (i.e., share a “common attribute”). +L+ In the case of conversation pivots for software modules, the +L+ source items are pages describing software modules and the +L+ targets are messages. A cell’s value encodes whether the +L+ message mentions the software module. For example, as +L+ shown in Table 1, message T3 refers to the software +L+ module that page S1 describes but not the modules +L+ described by pages S2 and S3. +L+ Depending on the application, it may be useful to +L+ automatically display on a source page a pivot block with +L+ links to a few targets. In other applications, users may need +L+ to explicitly request that related targets be displayed. This is +L+ common, for example, in tagging interfaces, where a user +L+ has to click on a tag before the set of related items is +L+ displayed. In either case, if too many target items are +L+ related to a single source, it is helpful to order them based +L+ on which are likely to be most useful when navigating from +L+ that source item. To accommodate that, a cell in the pivot +L+ matrix can contain a similarity or relevance score based on +L+ frequency or prominence of the reference, rather than just a +L+ binary indicator of a reference to the source item. +L+ We have implemented, but not yet publicly released +L+ conversation pivot blocks for two popular software +L+ platforms, Drupal and CPAN. On the Drupal.org website, +L+ there is a page for each of the hundreds of available add-on +L+ modules. The Related Discussion block, on the top right in +L+ Figure 1, will add links to related conversations that occur +L+ in the Drupal forums, elsewhere on the site. Each link is to +L+ an entire thread, scrolled to the first message in the thread +L+ that references the module. +L+ For the programming language Perl, there are thousands of +L+ software libraries, also referred to as modules, that +L+ programmers can download from a site called CPAN. Each +L+ module gets its own page on CPAN, with information about +L+ its history and status and a link to download the actual code. +L+ We have focused on an experimental mirror site called +L+ AnnoCPAN that displays user annotations on the module +L+ pages. The CPAN and AnnoCPAN sites do not host +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1. The Drupal.org website with (A) pivot to +L+ related conversations and (B) double-pivot to other +L+ </SectLabel_figureCaption> <SectLabel_bodyText> conversations about Perl or Perl modules. One popular +L+ venue for such conversations is a website called Perlmonks. +L+ We have implemented a conversation pivot block for +L+ AnnoCPAN that shows links to related conversation threads +L+ on the Perlmonks website, as shown in Figure 2. +L+ We exploit the structure and chronology of conversation +L+ threads to help determine which messages are most +L+ important. Among messages that reference a software +L+ module, conversations that are longer and more recent are +L+ more likely to be useful to display in a pivot block. +L+ Although each link in a pivot block points to a particular +L+ message in a thread, we compute the message’s importance +L+ based on features of the entire thread. Our initial +L+ implementation simply computes a timestamp based on the +L+ time of the most recent message posted in the thread. +L+ Among messages that reference the source item, those +L+ whose threads have more recent activity are shown first. +L+ Without the conversation pivot blocks, people could use a +L+ search engine to seek forum references to particular +L+ software modules. This would require significant user +L+ effort. Moreover, we are able to tune our search algorithm +L+ to take advantage of the structure of software module pages +L+ and conversation threads, in a way that would be difficult +L+ for users to simulate if they had to construct their own +L+ search queries. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Double Pivots +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Any pivot matrix relating source items to subsets of target +L+ items can be used to generate another pivot matrix relating +L+ the source items back to subsets of those same source items. +L+ For example, from the page for a software module, we can +L+ display a pivot block of other related software modules, as +L+ shown in the “Related Modules” blocks in Figures 1 and 2. +L+ This can help users identify complementary modules and +L+ </SectLabel_bodyText> <SectLabel_figure> A +L+ B +L+ </SectLabel_figure> <SectLabel_page> 1010 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Help Me Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> especially substitutes that may be preferable to the current +L+ module. +L+ Conceptually, the double pivot block automatically follows +L+ the pivot to related conversation threads and then pivots +L+ again to a set of modules that are also referenced by those +L+ threads. Such a process would be tedious, however, for +L+ users to perform manually. By automatically aggregating +L+ the other module references from the set of messages that +L+ reference the current module, the body of contributing +L+ content is much larger than just the few messages that can +L+ be displayed to a user on a page. +L+ The double-pivot technique is closely related to the +L+ technique of item-item collaborative filtering [3], where +L+ two items are related if they are highly rated (or purchased) +L+ by the same people. It is also analogous to co-citation +L+ analysis in bibliometrics, where two papers are related if +L+ they are cited in the same other paper [4]. +L+ In computing conversation double pivots, we first collapse +L+ messages into threads. Column T4a in Table 1 is a message +L+ that might have said something like, “Module S 1 doesn’t +L+ quite do what I need. Anyone have suggestions?” T4b is a +L+ reply suggesting module S3 as an alternative. Other users, +L+ when visiting the page for module S1, might then benefit +L+ from a double pivot link to S3, even though no single +L+ message mentions both S1 and S3. Thus, we treat two +L+ modules as related if they are referenced in the same thread. +L+ Table 2 illustrates the double pivot matrix resulting from +L+ the original matrix from Table 1. It indicates that S1 and S2 +L+ are never referenced in the same thread, but S2 and S3 are +L+ both referenced in two different threads, and S1 and S3 in +L+ one thread. Thus, the double pivot block for the page about +L+ S2 would include a link to S3 but not to S1. +L+ More sophisticated implementations of the double pivot +L+ would account for the overall popularity of source items. +L+ For example, when two modules that are rarely referenced +L+ are referenced in the same thread, it is a stronger indicator +L+ that the two are related than when the same outcome occurs +L+ for a pair of frequently referenced modules. +L+ </SectLabel_bodyText> <SectLabel_table> 		Software Module +L+ 		S1	S2	S3 +L+ 	S1	2	0	1 +L+ 	S2	0	2	2 +L+ 	S3	1	2	3 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2. Module by Module Associations from a double pivot +L+ </SectLabel_tableCaption> <SectLabel_sectionHeader> DETECTING CONVERSATION SUBJECTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The pivot matrix is constructed by examining the potential +L+ target threads to see which source items they reference. One +L+ possibility would be to rely on message authors to explicitly +L+ link to source items rather than using natural language to +L+ identify them. Tools such as auto-complete could simplify +L+ this task for message authors. Even so, it seems unwise to +L+ depend on authors to do this, since the primary beneficiaries +L+ would not be the authors or even the readers of the +L+ messages, but unknown and unseen future visitors to +L+ module pages. +L+ Instead, a software program can automatically infer +L+ references to items as they are naturally expressed in +L+ messages, albeit with some error. The errors will be +L+ reduced to the extent that authors refer to items using +L+ distinctive canonical identifiers, such as ISBN numbers. For +L+ example, the PHOAKS project mined Usenet for references +L+ to web pages, where the natural way to refer to a site in +L+ conversation was to put in a complete URL [5]. +L+ Two factors make it practical to automatically mine +L+ conversations for software module references. The first is a +L+ high tolerance for errors of omission: detecting even a small +L+ fraction of the actual references to software modules may +L+ be sufficient to generate useful pivot and double-pivot +L+ blocks for navigation. The second factor is that there are +L+ regularities in module names and in patterns of reference to +L+ them that can be exploited when creating a reference index. +L+ Some software systems employ a naming system that +L+ allows exact string matching on module titles to perform +L+ well. Perl modules follow a hierarchical naming convention +L+ with “::” as a separator found rarely in normal conversation. +L+ If the text string “Time::ParseDate” appears in a message, +L+ the author almost certainly meant to refer to the Perl +L+ module of that name, so that exact matching on module +L+ titles will have high precision. Moreover, a social norm has +L+ emerged so that an author who wants to refer to that module +L+ will typically use that text string to refer to it, rather than a +L+ shorter alias such as ParseDate. Thus, exact text matching +L+ will retrieve the correct references with high recall as well, +L+ although it will occasionally miss matches due to +L+ misspellings. +L+ </SectLabel_bodyText> <SectLabel_figure> A +L+ B +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 2. CPAN with a (A) conversation pivot to +L+ Perlmonks.org, and a (B) double-pivot to related modules +L+ </SectLabel_figureCaption> <SectLabel_page> 1011 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Help Me Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> In mining Perlmonks conversations, we used the exact +L+ matching on module titles technique. Of 550,679 total +L+ comments that had been posted on the Perlmonks site, 16% +L+ contained module references. Some messages contained +L+ long lists of modules; it seemed unlikely that a user +L+ examining any particular module would find it useful to +L+ navigate to a message containing a long list that happened +L+ to contain the source module, so we discarded those +L+ messages. Even after discarding 35 outliers that referenced +L+ more than ten modules each, 41% of the 4,548 total CPAN +L+ modules were cited on Perlmonks, and those had a median +L+ of 5 references. +L+ Based on those detected module references, excluding 35 +L+ outliers, we computed the double pivot matrix. Of the 1,884 +L+ modules that were cited at all, 1,702 had at least one other +L+ module co-cited in the same conversation. Of these, the +L+ median number of “related modules” was 6. +L+ String matching on titles can be extended to include +L+ searching for distinctive aliases. For example, most authors +L+ refer to the Drupal module titled “Content Construction Kit +L+ (CCK)” as “CCK” rather than using the full title. Even if it +L+ is unrealistic to expect all message authors to tag all their +L+ references to modules, it may be quite reasonable to expect +L+ the authors of the module pages to identify aliases that are +L+ frequently used in conversation. The authors of those pages, +L+ typically the people responsible for maintaining the +L+ software modules, have both the knowledge of commonly +L+ used aliases and the incentive to enter them in to the +L+ system, in order to help users find what others are saying +L+ about the modules. +L+ String matching on titles can also be narrowed to handle +L+ situations where it would yield too many false positives. +L+ Some software systems use module titles that are +L+ potentially ambiguous. For example, Drupal modules +L+ generally use common words such as “Event” or “Upload” +L+ for titles. Simple text matching on these titles would yield +L+ many false positives, conversation messages that use these +L+ words but not in reference to the software modules. Real +L+ references to the Upload module, however, frequently +L+ contain the word “module” in close proximity to the word +L+ “Upload”. Thus, a matching algorithm that searches for the +L+ title adjacent to the magic word “module” will likely +L+ generate many fewer false positives, though possibly +L+ missing more correct references. +L+ We used this method to find module references in the +L+ Drupal.org conversation forums. Of 292,139 messages, +L+ 47,794 contained at least one module reference. After +L+ discarding 21 outliers that each referred to more than ten +L+ modules, 915 of the 1,590 modules were cited in at least +L+ one message and those had a median of 6 references. +L+ Based on those detected module references, we computed +L+ the double pivot matrix. Of the 915 modules that were cited +L+ at all, 747 had at least one other module co-cited in the +L+ same conversation. Of these, the median number of “related +L+ modules” was 6. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSIONS AND FUTURE RESEARCH +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The results from our implementations with Drupal.org and +L+ AnnoCPAN/Perlmonks lead us to believe that conversation +L+ pivots and double pivots hold a great deal of promise for +L+ making online collections more useful for users. By +L+ automatically mining forum data for item references and +L+ generating recommendations of other modules we provide +L+ users with a shortcut through time-intensive manual search. +L+ The vision of the semantic web [6] is that information for +L+ human consumption will also be tagged in a way that +L+ computers can process in useful ways. When semantic +L+ markup is not available, however, it may be possible to +L+ infer it imperfectly, but well enough to enable particular +L+ kinds of processing. We have developed techniques for +L+ detecting references to software modules in online +L+ conversations that are sufficient to enable the creation of +L+ navigation aids in the form of conversation pivots and +L+ double pivots. The techniques may be extensible to creating +L+ conversation pivots for other types of items. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGEMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This material is based upon work supported by the National +L+ Science Foundation under Grant Nos. 0308006 and +L+ 0325837. We wish to thank the administrators of +L+ Drupal.org and Perlmonks.org for allowing us access to +L+ their data sets and to Ivan Tubert-Brohman for allowing us +L+ to add the pivot feature to the AnnoCPAN site. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Drenner, S., Harper, M., Frankowski, D., Riedl, J., and +L+ Terveen, L. Insert movie reference here: a system to +L+ bridge conversation and item-oriented web sites. In +L+ Proceedings of the SIGCHI Conference on Human +L+ Factors in Computing Systems CHI '06 (2006) +L+ 2. Ning Platform Pivots Feature. http://blog.ning.com/ +L+ 2005/11/new_on_the_ning_pivot.html +L+ 3. Linden, G., Smith, B., York, J. Amazon.com +L+ recommendations: item-to-item collaborative filtering, +L+ Internet Computing, IEEE, 7, 1 (2003), 76-80. +L+ 4. Small, H. Co-citation in the scientific literature: A new +L+ measure of the relationship between two documents. +L+ Journal of the American Society for Information +L+ Science, vol. 24, 4 (1973), 265 – 269. +L+ 5. Terveen, L., Hill, W., Amento, B., McDonald, D., and +L+ Creter, J. PHOAKS: a system for sharing +L+ recommendations. Communications of the ACM. 40, 3 +L+ (1997), 59 – 62. +L+ 6. Berners-Lee, T., Hendler, J., and Lassila,O.,The +L+ semantic web. Scientific American. 284 (2001), 5, 34. +L+ </SectLabel_reference> <SectLabel_page> 1012 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> Harvesting with SONAR - The Value of Aggregating Social +L+ Network Information +L+ </SectLabel_title> <SectLabel_author> Ido Guy, Michal Jacovi, Elad Shahar, +L+ Noga Meshulam, Vladimir Soroka +L+ </SectLabel_author> <SectLabel_affiliation> IBM Haifa Research Lab +L+ </SectLabel_affiliation> <SectLabel_address> Mt. Carmel, Haifa 31905, Israel +L+ </SectLabel_address> <SectLabel_email> {ido, jacovi, elads, noga, vladi} @il.ibm.com +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Web 2.0 gives people a substantial role in content and +L+ metadata creation. New interpersonal connections are +L+ formed and existing connections become evident. This +L+ newly created social network (SN) spans across multiple +L+ services and aggregating it could bring great value. In this +L+ work we present SONAR, an API for gathering and sharing +L+ SN information. We give a detailed description of SONAR, +L+ demonstrate its potential value through user scenarios, and +L+ show results from experiments we conducted with a +L+ SONAR-based social networking application within our +L+ organizational intranet. These suggest that aggregating SN +L+ information across diverse data sources enriches the SN +L+ picture and makes it more complete and useful for the end +L+ user. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Social networks, SN, social network analysis, SNA, +L+ aggregation. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> H.5.3 Group and Organizational Interfaces – Computer- +L+ supported cooperative work +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Social software – software that has people as its focal point +L+ – is the core of Web 2.0. From blogs and wikis through +L+ recommender systems to social bookmarking and personal +L+ network systems – social applications proliferate. In +L+ continuation to its dominance on the internet, social +L+ software has recently emerged in organizations, as a mean +L+ of connecting employees in a better way and enhancing +L+ knowledge management and expertise location. Blogging +L+ systems [14], social bookmarking [19], and people tagging +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00. +L+ </SectLabel_copyright> <SectLabel_author> Stephen Farrell +L+ </SectLabel_author> <SectLabel_affiliation> IBM Almaden Research Center +L+ </SectLabel_affiliation> <SectLabel_address> 650 Harry Road, San Jose, California +L+ </SectLabel_address> <SectLabel_email> sfarrell@almaden.ibm.com +L+ </SectLabel_email> <SectLabel_bodyText> [7], are examples of social applications that became part of +L+ organizations’ intranets in the purpose of promoting intra- +L+ organizational interaction. +L+ Many of these social applications expose interesting +L+ information about people’s relationships. For example, by +L+ analysing blog commenters, or bookmarking similarities, +L+ connections among people can become evident. By +L+ extracting this information and aggregating it across +L+ multiple sources, a comprehensive and often intriguing +L+ picture of individual and organizational social networks +L+ may be revealed. +L+ Potential sources of social information are very diverse. +L+ Different users make use of different tools, and social +L+ information is scattered among many services and +L+ applications. As these applications rarely interoperate, each +L+ is typically only aware of its own social data and cannot +L+ benefit from other applications’ data. +L+ The diversity of sources of social networking data also +L+ brings a variety of semantics to interpersonal connections. +L+ While some of these semantics are straightforward and +L+ derived from the nature of the connection (brother, close +L+ friend, manager, etc.), others are more complex. Many +L+ researchers recognized that people are connected through +L+ artifacts. For example, many studies have investigated +L+ networks where two people are connected if they have co- +L+ authored a paper [22]. Newer examples of artifacts that +L+ connect people include email messages [3,28], and web +L+ pages [14]. Affiliation networks [29] present people’s +L+ connections through groups in which they co-participate, +L+ such as a board of directors, or a movie cast [22]. The +L+ above examples imply that aggregating social network data +L+ presents the challenge of creating a single framework, +L+ general yet informative, to fit all types of connections. +L+ To address the above challenges, we introduce SONAR +L+ (Social Networks Architecture) – an API for sharing social +L+ network data and aggregating it across applications to show +L+ who is related to whom and how. Applications +L+ implementing the SONAR API (SONAR providers) should +L+ provide internal information about how strongly people are +L+ connected and by what means. SONAR clients can use the +L+ SONAR API to access data from a single provider that +L+ implements the API. However, the more compelling case is +L+ </SectLabel_bodyText> <SectLabel_page> 1017 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> where an intermediate component, an aggregator, is used +L+ by clients, with the very same SONAR API, to consolidate +L+ the data from different providers. This way, one can choose +L+ multiple providers and assign an appropriate weight to each +L+ of them. It is expected that federating more data providers +L+ will make the resulting answers to queries more complete. +L+ Clients of SONAR may vary from expertise miners, +L+ through network visualizers, to user interface widgets. +L+ SONAR answers questions such as “who does this person +L+ communicate with most?”, “what are all the artifacts co- +L+ authored by these two individuals?”, “whom should I invite +L+ to a brainstorm on a certain topic?” +L+ SONAR includes two types of data sources: personal +L+ (private) and public. Personal sources, such as email and +L+ instant messaging (IM), are only available to their owner +L+ and reflect the owner’s personal, or egocentric, social +L+ network (i.e., all nodes in the network are directly related to +L+ the owner). Public data sources, such as blogs and +L+ organizational charts, are available to all users and reflect +L+ their extended, or sociocentric, network. SONAR maintains +L+ the privacy model of its data sources: only those users who +L+ have access to a certain piece of data by the original +L+ provider will have access to the social information extracted +L+ from this data by SONAR. +L+ As of now, we implemented the SONAR API for over ten +L+ sources, public and personal, within the IBM intranet. +L+ SONAR’s ultimate goal is to be widely used by social +L+ networking and Web 2.0 services on the internet and to +L+ define a standard, analogous to RSS [11]. SONAR is based +L+ on the REST design pattern [9] and uses standard data +L+ formats such as Atom [23] and JSON [13]. +L+ When used for creating a sociocentric view of a social +L+ network, SONAR is based solely on public sources. Any +L+ user may use this view to examine publicly visible +L+ connections within any group of people. When used for +L+ creating an egocentric view of a user’s network, SONAR +L+ also makes use of the user’s personal sources. Enriching the +L+ egocentric network, as reflected in personal sources, with +L+ information from public sources, opens up new +L+ opportunities for learning about one’s extended network +L+ (i.e., one’s connections and their connections with others). +L+ Consider, for example, Alice who seeks a social connection +L+ to Cindy. Cindy may not appear at all in Alice’s egocentric +L+ network based on personal sources. However, examining +L+ the extended network, Alice may discover that Bob – who +L+ appears on her egocentric network by her personal data – is +L+ related to Cindy according to public sources. Alice will then +L+ be able to discover a social path to Cindy through Bob, +L+ based on aggregation of her personal and public sources. +L+ In order to verify the fundamental concepts on which +L+ SONAR relies, namely aggregation and the usage of public +L+ sources, we conducted three experiments. The first +L+ experiment examines different social networks derived +L+ from four of SONAR’s implemented public data sources. +L+ The second experiment involves a user study in which over +L+ a hundred users evaluated different buddylists derived from +L+ 24 different combinations of SONAR public as well as +L+ personal data sources. For the third experiment we +L+ interviewed 12 users about their usage of a SONAR UI and +L+ their thoughts on the different buddylists. The experiments +L+ examine the diversity of the public data sources, their value +L+ to the user, and whether they add value over personal data +L+ sources. Finally, we checked whether there exists an ideal +L+ weighting scheme of the sources, which may serve as the +L+ system’s default, and followed users as they were +L+ composing their own ideal weighting scheme. +L+ We note that the experiments in this paper examine +L+ buddylists and social networks generally, while, in practice, +L+ different semantics may yield different social networks. For +L+ example, the network consisting of users’ friends may be +L+ different from the network of people with whom users +L+ communicate most frequently, which may be different from +L+ the network of individuals with whom users share similar +L+ interests. +L+ The rest of the paper is organized as follows. The next +L+ section surveys related work, followed by a more detailed +L+ description of SONAR and typical usage scenarios. We +L+ then describe our hypotheses, research method, and results. +L+ The final section discusses conclusions and future work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The formal discipline that studies interpersonal connections +L+ is called social network analysis (SNA) [29]. A social +L+ network (SN) is a graph that represents social entities and +L+ relationships between them. SNs have often been studied +L+ through the use of social science tools such as surveys and +L+ interviews, which require a great deal of human labor. The +L+ evolution of the Web, which is often referred to as Web 2.0 +L+ [24], has introduced new possibilities for SN research. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Gathering SN Data from Computer Applications +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> There are several popular SN services on the Web, which +L+ help to connect friends and business acquaintances1. These +L+ services define an explicit SN. Users directly specify who +L+ their friends are and often manually state the nature of the +L+ connection. The manual nature of these networks is perhaps +L+ their main disadvantage. Only part of the user’s actual SN +L+ will be registered in any such application, and since +L+ explicitly entering social data is tedious, even users who are +L+ registered are likely to have incomplete information about +L+ their network. These applications are useful for SONAR, +L+ providing very accurate, even if partial, social connections. +L+ The wealth of information in computer databases and +L+ applications is a good source for automatically obtaining +L+ various kinds of SN data without burdening users with the +L+ manual management of their network. For example, +L+ Wellman views computer networks as SNs and surveys +L+ how computer networks reflect and affect traditional SNs +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 {myspace, facebook, linkedin, orkut, friendster}.com +L+ </SectLabel_footnote> <SectLabel_page> 1018 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> [30]. In addition, data mining can discover subtle details of +L+ a relationship that a person may not be able to provide +L+ accurately. For example, the rate of email communication +L+ can be used to estimate the strength of social ties [19]. +L+ Email is commonly used to extract SNs. Extraction of the +L+ sociocentric SN from email logs has been demonstrated in +L+ [28] in order to automatically identify communities. Other +L+ tools, such as ContactMap [19] and Personal Map [8], +L+ analyze emails to extract the user’s egocentric SN. Email is +L+ one of the important sources for SONAR, but it is definitely +L+ not the only one. SONAR’s philosophy states that there is +L+ much important social information outside the inbox. +L+ Another common source for SN information is the Web. +L+ Adamic et al. [1] describe techniques for mining links and +L+ text of homepages to predict social relationships. The +L+ strength of a connection between two people can be +L+ estimated by querying a search engine with their names and +L+ checking for web pages where they co-occur [14,17]. Since +L+ various data sources such as papers, organizational charts, +L+ and net-news archives are available on the internet or +L+ intranet, they too could be mined by web searching [14]. +L+ SN extraction has been conducted on many other sources, +L+ including Usenet data [27] and Instant Messaging logs [25]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Public vs. Personal Data Sources +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Sociocentric network approaches may encounter difficulties +L+ using personal data sources due to privacy concerns. For +L+ example, mining email, even when results are displayed in +L+ aggregated forms, might expose private information [14]. +L+ One solution is to limit data mining to public sources. For +L+ example, Aleman-Meza et al. [2] chose to aggregate only +L+ publicly available SN data due to privacy concerns. +L+ However, ignoring private information may exclude +L+ important data. An alternative solution is to have users opt- +L+ in to explicitly give permission to make certain private +L+ information public. For example, Smarr [26] suggests +L+ requiring users to opt-in to publish their information to a +L+ public Friend of a Friend (FOAF) file [10]. However, opt-in +L+ requires action and motivation on the part of users – so +L+ those publishing their FOAF files will cover only a small +L+ percentage of organization members. +L+ SONAR can aggregate both private and public sources, +L+ without exposing private data to other people, or requiring +L+ users to opt-in. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Aggregating SN Data from Different Sources +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> No single archive or tool captures all our social relations +L+ with others. Aggregating SN data from several sources may +L+ improve the completeness of constructed SNs. For example, +L+ the ContactMap developers plan to extend their email +L+ mining tool to use sources such as voice mail and phone +L+ logs due to user complaints on the absence of phone-based +L+ contacts [19]. +L+ Several tools combine different sources to generate better +L+ SNs. In [5], an email database is used to extract people +L+ names and email addresses – these are then searched on the +L+ Web, to extract keywords and to find additional related +L+ people for which the search is recursively applied. Web +L+ mining, face-to-face communication, and manually entering +L+ one’s SN are combined in [12] to construct the SN in a +L+ Japanese conference. +L+ A basic problem of aggregation is deciding on the +L+ algorithm to be used for combining data from various +L+ sources. A simple approach is to compute the aggregation +L+ as a linear combination of the individual sources. Cai et al. +L+ [4] propose a method for learning the optimal linear +L+ combination given input from the user that describes the +L+ user’s expectation. However, this method requires the user +L+ to specify the query in ways which may prove to be +L+ complex. Matsuo et al. [16] extract and integrate SNs from +L+ several different sources. They provide a rough sketch for +L+ integrating the networks into a single one by using a linear +L+ combination of the different networks. Our research tests +L+ whether there is a weighting scheme which is appropriate to +L+ most users, for a specific scenario. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Evaluation of Social Network Quality +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A common approach for validating an automatically +L+ constructed SN is to ask the people in the network about its +L+ correctness, usually by questionnaires that require rating +L+ various aspects of the SNs and/or providing open-ended +L+ feedback [8,19,28]. +L+ Another possible approach used in SNA is to compare the +L+ automatically constructed network to external data. For +L+ example, Aleman-Meza et al. [2] detect conflict of interests +L+ between paper authors and referees by integrating data from +L+ several SNs. Their evaluation included comparison to data +L+ from an external source: a different existing system for +L+ detecting conflict of interest. The problem with this method +L+ of evaluation is that it is possible only if there exists a +L+ relevant external source, which can be used for comparison. +L+ This paper includes three types of evaluation. In the +L+ absence of an external source to compare to when +L+ evaluating an aggregated network, our first evaluation +L+ compares the networks obtained from individual sources to +L+ the other sources, to measure their uniqueness. The second +L+ evaluation is a user evaluation, where we evaluate and +L+ compare numerous linear combinations of different sources. +L+ The third evaluation employs interviews in order to receive +L+ user feedback on individual sources and their aggregation. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> SONAR IN DETAIL +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The purpose of the SONAR API is to provide open +L+ interfaces to SN data, “locked up” in a multitude of +L+ systems. SONAR specifies a way to share weighted SNs as +L+ relation lists. Clients may retrieve information about how +L+ people are connected based on different parameters. Like +L+ RSS, our goal is to make a read-only interface, simple to +L+ implement by the provider and consume by the client. +L+ </SectLabel_bodyText> <SectLabel_page> 1019 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> The first premise behind the SONAR API is that it is +L+ necessary to present the strength of ties between people. In +L+ contrast to APIs for specific SN applications like Facebook +L+ [6], SONAR does not model any specific semantics of the +L+ underlying system like “friending” or “communities”. +L+ Instead, it asks providers to boil down these semantics into +L+ floating point numbers between 0 and 1. SONAR +L+ aggregators combine results from multiple systems using a +L+ simple weighted average. This approach enables diverse +L+ applications from instant messaging clients through +L+ publication databases to SN sites to provide data supporting +L+ an aggregated view of relationships among people. SONAR +L+ clients are oblivious to the types of relations – when +L+ querying for strength of a relationship, all that the client +L+ sees is people and the weight of their associations. +L+ Users of aggregated SN data frequently want to understand +L+ how people are connected, or why a connection is stronger +L+ than another. To support this need, SONAR allows queries +L+ for evidence. Evidence is essentially a time-ordered log of +L+ entries, originating from each of the providers. It may +L+ include comments posted by one user in the other user’s +L+ blog, email messages or chat transcripts between them, or +L+ web sites that they both bookmarked. According to our +L+ privacy model, users do not have access to any private +L+ material of other people through this interface—it just +L+ organizes information they already had access to before. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> SONAR API Specification +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We have implemented SONAR as a REST API. The API +L+ has four methods. The first three are fetching weighted +L+ people relationships, while the fourth provides evidence for +L+ connections. The methods are summarized in Table 1. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> SONAR Aggregator +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> SONAR is designed to enable an aggregator component +L+ that merges results from multiple providers. Like an HTTP +L+ proxy, the aggregator protocol is in most ways identical to +L+ the protocol for interacting with a SONAR provider. In fact, +L+ clients communicate with aggregators the exact same way +L+ they do with primary providers – through the SONAR API. +L+ The aggregator is configured to connect to one or more +L+ providers. When a request is received, the aggregator +L+ forwards it to each provider. It then processes the results by +L+ computing a weighted average and returns the result to the +L+ user. The original results from the different sources remain +L+ transparent to the user. +L+ </SectLabel_bodyText> <SectLabel_table> Name	Parameters*	Output +L+ Strength	source (user) , target (user)	Float (0.0 to 1.0) +L+ Relations	user(s), limit, offset	<list of people> +L+ Network	user(s), degrees, threshold	<graph of people> +L+ Evidence	users(s), limit, offset	<list of entries> +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1. SONAR API methods* +L+ </SectLabel_tableCaption> <SectLabel_footnote> * All methods also accept parameters since and until. Since limits results to +L+ those after the given date, until limits to those before the given date. +L+ </SectLabel_footnote> <SectLabel_subsectionHeader> SONAR Providers +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We have implemented SONAR providers of over ten +L+ sources in our organizational intranet. The following four +L+ systems, which serve as public sources, were used in our +L+ experiments: BlogCentral (IBM’s corporate blogging +L+ system [14]), Fringe, for people tagging and friending [7], +L+ Dogear, for social bookmarking [19], and the IBM +L+ organizational chart. +L+ For the blog system, social relations are derived from the +L+ comments made to one’s blog. This information is an +L+ indication of the people who leave a trace in a blog, which +L+ is likely to imply that the author is aware of them. Fringe +L+ supports extraction of social information of both friending +L+ and tagging. Friending is a reciprocal action: one person +L+ invites the other to be friends and they are defined friends +L+ only if the invitation is accepted. Tagging people is one +L+ sided, yet indicates some level of connection. The SONAR +L+ provider that extracts SNs from Dogear is based on +L+ bookmark similarity information. The connections returned +L+ by this provider are those of people who bookmark the +L+ same pages. From the organizational chart we extracted, for +L+ each user, the user’s manager as well as the user’s direct +L+ peers - all employees who have the same manager. +L+ We have implemented several client-side SONAR +L+ providers that have access to the user’s private data. The +L+ experiments in this paper use two of these – email and chat +L+ transcripts. The outcome of these providers is only visible +L+ to the owner, visualizing an egocentric map of connections, +L+ but not revealing any private information to others. +L+ For the email information, our client requests the user’s +L+ password and then crawls the mailbox and collects details +L+ of people the user corresponds with. The chat information is +L+ easily accessible to our SONAR client, as the client is +L+ implemented as a plugin of Lotus Sametime, IBM’s chat +L+ system. We extract social information from the history of +L+ chat transcripts, as these indicate the people a person +L+ actually chats with. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> SONAR Usage Scenarios +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To demonstrate the potential usage of the SONAR API, we +L+ created SonarBuddies – a plugin for Lotus Sametime. The +L+ plugin presents an alternative buddylist, which consists of +L+ the people most strongly related to the user, ordered by +L+ their strength of connection (see Figure 1(a)). +L+ Additional features include showing related people to any +L+ buddy on the list, the connection points (evidence) with a +L+ buddy (Figure 1(b)), and people who are connected to both +L+ the user and a buddy (Figure 1(c)). +L+ The SonarBuddies extension has a preference page in which +L+ the user may choose the relative weight of each data source, +L+ the number of buddies to display, and the number of days in +L+ history to consider. When adjusting the preferences, the +L+ user may see a preview of the buddylist. This enables fine- +L+ tuning the selection of weights (see Figure 2). +L+ </SectLabel_bodyText> <SectLabel_page> 1020 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 1. SONARBuddies UI +L+ </SectLabel_figureCaption> <SectLabel_bodyText> SonarBuddies is just one example of a potential SONAR +L+ client. Below, are a few examples of more advanced +L+ scenarios that SONAR may support: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Expertise location (e.g., [14,18,31]) – SONAR, integrated +L+ with search, may be used for scenarios of expertise location, +L+ such as the basic “who knows about <topic>?”, but also +L+ “who do I know that knows about <topic>?”, and the related +L+ “who do I mostly communicate with about <topic>?” +L+ •	Automatic completion of names (e.g., [21]) and groups– +L+ completing a single string to a name may sort alternatives by +L+ strength of social ties and relevance to the context. Moreover, +L+ the completion of a whole group can be supported - e.g., if +L+ one participates in a project of 10 people, typing the names of +L+ 3 of them may automatically be completed to the entire +L+ group. This may also be useful for resolving “who's missing +L+ from the mail I’m about to send?” +L+ •	Finding social paths to someone who is not directly related +L+ to the user (e.g., [15]) +L+ •	Enhancing SN services by recommending people to connect +L+ to based on other evidence and by enriching information +L+ about existing friends: connection strength, evidence, and +L+ temporal characteristics of relationships +L+ </SectLabel_listItem> <SectLabel_sectionHeader> SONAR EXPERIMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> SonarBuddies has been made available for download at the +L+ IBM intranet and over 1800 users downloaded and used it. +L+ SonarBuddies is mainly an egocentric application and it +L+ could be assumed that its success is due to heavy usage of +L+ private data sources by the users. However, from the usage +L+ data it is evident that public sources are explored. We +L+ envision the SONAR API as being heavily used by +L+ sociocentric applications and thus the interest in public data +L+ sources was encouraging. +L+ This section states our hypotheses, describes our +L+ experiments, and discusses the results. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Hypotheses +L+ </SectLabel_subsectionHeader> <SectLabel_subsubsectionHeader> Public Data Sources +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The first group of hypotheses focuses on public data +L+ sources and their influence on the users’ SN. Public data +L+ sources are different in nature, ranging from social +L+ bookmarking systems to blogs and therefore we assume: +L+ </SectLabel_bodyText> <SectLabel_listItem> (1) Public data sources provide diverse SN information. +L+ </SectLabel_listItem> <SectLabel_bodyText> There is no single public data source that holds all SN +L+ information, and each public source makes a +L+ significant contribution to the overall SN information. +L+ Moreover, as part of the user activities are performed +L+ “outside the mailbox”, the public sources provide valuable +L+ SN information which is not reflected in private sources. +L+ We thus raise the following 2 hypotheses: +L+ </SectLabel_bodyText> <SectLabel_listItem> (2) Public data sources provide SN information that is +L+ valuable to the user. +L+ (3) Public data sources enrich egocentric SN information. +L+ By combining private sources with public sources, one +L+ can potentially get a more complete picture of the SN. +L+ </SectLabel_listItem> <SectLabel_subsubsectionHeader> Data Source Aggregation +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> An additional hypothesis focuses on aggregation of SN +L+ information. A key concept behind SONAR is the ability to +L+ consolidate social information from multiple data sources, +L+ assuming that there is real value in such aggregation. The +L+ following hypothesis is explored: +L+ </SectLabel_bodyText> <SectLabel_listItem> (4) Aggregated SN information is of greater value to users +L+ than information that originates from any single source. +L+ </SectLabel_listItem> <SectLabel_bodyText> A SONAR client based on this hypothesis would need to +L+ use some weight combination in order to aggregate the +L+ different sources. While the user may have control over the +L+ weights, a SONAR client should have a default weight +L+ combination that would be reasonably good for all users for +L+ the most common scenario (such as finding the people the +L+ user communicates with the most). During our experiments, +L+ we wish to study the following hypothesis: +L+ </SectLabel_bodyText> <SectLabel_listItem> (5) For a basic scenario, there exists a weighting scheme +L+ by which an aggregation of data sources most +L+ reasonably represents most users’ SN. +L+ </SectLabel_listItem> <SectLabel_bodyText> If this hypothesis is correct, our experiments may reveal the +L+ weighting scheme that we should use as default. +L+ Finally, an even more valuable aggregation of SNs can be +L+ achieved if users would share some of their private SN with +L+ others. While people are hesitant to share their private +L+ information, they may agree to share the buddylists created +L+ based on it, and thus allow a sociocentric view that is +L+ enhanced by private information. We hypothesize: +L+ </SectLabel_bodyText> <SectLabel_listItem> (6) People would be willing to share the buddylists created +L+ based on their private sources +L+ </SectLabel_listItem> <SectLabel_subsectionHeader> Research Method +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In order to examine our hypotheses, we conducted three +L+ experiments on SN information collected by SONAR. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Experiment 1: Information from Public Sources +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> For the first experiment we gathered information from the +L+ four public sources. Our goal was to compare the lists of +L+ connected people from the different sources (hypothesis +L+ (1)) and show that no source covers the others and may thus +L+ serve as a single source of information (hypothesis (4)). The +L+ collaboration tools in IBM, like many Web 2.0 services on +L+ </SectLabel_bodyText> <SectLabel_page> 1021 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> the internet, are still in their diffusion phase and are not yet +L+ used by all. However, they are gaining momentum and have +L+ become quite prevalent. As we wanted to compare all +L+ sources, we decided to focus on users who use all of the +L+ chosen data sources. While such users are not a statistical +L+ sample, we refer to them as the early adopters of the +L+ technologies, and use their figures as a reflection of the +L+ potential of SN information that may be extracted from +L+ public sources as Web 2.0 technologies become ubiquitous +L+ [24]. +L+ We started by locating the top 1000 heavy users of each of +L+ the tools (BlogCentral, Fringe, and Dogear). For blogs, we +L+ defined heavy users as those who received most comments +L+ in their blog. For Fringe we took the 1000 users with the +L+ largest lists of connections. For Dogear, the 1000 people +L+ with most bookmarks. Once we had these three lists, we +L+ took their union and received a list of 1761 users. We then +L+ obtained results from the different SONAR providers for all +L+ 1761 users, and examined those users who had a nonempty +L+ result in all four sources (the fourth source being the +L+ organizational chart). We ended up with a list of 273 such +L+ users. For every one of the four public sources and every +L+ one of the 273 users, we calculated the number of unique +L+ contributions of this source over the union of all other three +L+ sources. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Experiment 2: Online Questionnaire for Ranking Buddylists +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> For the second experiment we implemented a dedicated +L+ plugin for Lotus Sametime. The plugin was easy to install +L+ and presented a questionnaire containing three sets of up to +L+ eight buddylists that were aggregated from the different +L+ sources and tailored specifically for the user. The user could +L+ not tell what the sources of the different lists had been. +L+ Before starting the experiment, we asked the user to +L+ imagine setting up an “ideal” buddylist for communication +L+ inside IBM. By this request, we framed the experiment to a +L+ basic scenario. The user was then asked to rank the +L+ buddylists by how close they were to representing the ideal +L+ buddylist. The scale for ranking was 1-4 (where “1” is +L+ good, and “4” is bad). In addition, we asked the user to +L+ mark a single buddylist as the “best” buddylist – relative to +L+ the other lists in that set. +L+ The first set of up to eight buddylists was composed solely +L+ from public sources. Our goal with this set was to examine +L+ the value of extracting SN information from public sources +L+ (hypothesis (2)), and to compare the quality of lists created +L+ by aggregation of different sources vs. the quality of lists +L+ created from a single source. The weight combinations of +L+ sources used in this set are displayed in the leftmost +L+ columns of Table 2 (1-8). We use the term “up to eight +L+ lists”, since if two different combinations created two +L+ identical lists (in both content and order), we only presented +L+ them once. If the user voted “best” for a list that was +L+ created from more than one combination, we added a vote +L+ to all these combinations. +L+ The second set of buddylists was also focused on public +L+ sources. The goal with this set was to learn whether a +L+ specific weight combination is preferred by most users and +L+ may serve as a default (hypothesis (5)). The weight +L+ combinations of sources used in this set are displayed in the +L+ rightmost columns of Table 2 (9-16). +L+ The last set of buddylists introduced information gathered +L+ from the user’s private sources. The goal of this set was to +L+ examine the value of information public sources add over +L+ private sources (hypothesis (3)), as well as to study the +L+ effect aggregation has on the lists (hypothesis (4)) – +L+ aggregation of private sources, and aggregation of a mix of +L+ private and public sources. The weight combinations of +L+ sources used in this set are displayed in Table 5 (17-24). +L+ Our plugin reported the user ranking of the buddylists to a +L+ dedicated server that produced a report with all results. The +L+ results visible to us did not contain any private information +L+ nor could we see the buddylists, we only examined the +L+ ranks (1-4) and the vote for best list in each set. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Experiment 3: Sliders UI for Personal Weight Combination +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> A set of interviews we conducted, helped us examine +L+ hypotheses (2), (4), and (6), as well as hypothesis (5). It +L+ also gave us some insight about how people perceive their +L+ SN and what they feel about our UI. +L+ The preferences-page of the SONAR plugin allows +L+ modifying the weight combination of different sources with +L+ sliders and simultaneously seeing a preview of the buddylist +L+ created from this combination. The user interface of this +L+ feature is shown in Figure 2. +L+ </SectLabel_bodyText> <SectLabel_table> 	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16 +L+ bookmarking	1.0				0.5		0.1	0.25	0.1	0.3	0.3	0.3	0.1	0.1	0.4	0.5 +L+ people tagging		1.0			0.5		0.2	0.25	0.3	0.1	0.3	0.3	0.2	0.1	0.3	0.3 +L+ blogs			1.0			0.5	0.3	0.25	0.3	0.3	0.1	0.3	0.3	0.3	0.2	0.1 +L+ org-chart				1.0		0.5	0.4	0.25	0.3	0.3	0.3	0.1	0.4	0.5	0.1	0.1 +L+ average score +L+ # of “best” votes	0	23	1	63	6	57	58	46	70	48	46	47	63	61	46	43 +L+ # of score “1”	0	12	0	18	5	14	17	9	20	4	6	9	20	20	7	6 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2. Weight combinations and results of the public sources in the first two sets of buddylists +L+ </SectLabel_tableCaption> <SectLabel_page> 1022 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 2. Weight combination user interface +L+ </SectLabel_figureCaption> <SectLabel_bodyText> We conducted personal interviews with 12 of the users who +L+ took part in experiment 2, in order to learn about their +L+ experience with the sliders and follow their line of thought +L+ while they are examining the different buddylists. Each +L+ interviewee was asked to first reset all sliders and start with +L+ an empty list. In order to examine hypotheses (2) and (4), +L+ each slider was moved separately, to compare the lists +L+ based on single sources to an ideal list (as in the framed +L+ scenario of experiment 2). Once all sources were examined, +L+ the interviewees were asked to fiddle with the sliders in +L+ order to compose a list that is closest to their ideal list. We +L+ retrieved the selected weight combinations from our logs +L+ and examined them in order to validate hypothesis (5). +L+ Finally, we posed the question about sharing the buddylists +L+ based on private sources (hypothesis (6)). The question is a +L+ multiple choice question: to share automatically vs. share +L+ after manual editing; to share with anyone, or only with +L+ friends, or only with a specific individual. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Experimental Results +L+ </SectLabel_subsectionHeader> <SectLabel_subsubsectionHeader> Results of Experiment 1 +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In this experiment we examined the 273 users who had a +L+ nonempty result in all four public sources. For each of the +L+ four public sources and each of the 273 users, we calculated +L+ the number of connections extracted from the source, and +L+ the number of unique contributions of this source over the +L+ union of all other three sources. The averages of these +L+ figures appear in Table 3. +L+ Examining the average numbers of unique contributions is +L+ interesting. One would assume that the commonalities of +L+ the lists from the different sources would be large; that a +L+ person would mostly friend with peers from the +L+ organizational group; that a person’s friends would be the +L+ ones commenting in the blogging system; and even that +L+ working in the same group would imply similar interests +L+ and thus bookmarking the same web pages. However, our +L+ org chart		friending		blogs		bookmarks +L+ </SectLabel_bodyText> <SectLabel_table> #	>	#	>	#	>	#	> +L+ 15.73	13.91	24.64	20.27	8.57	5.74	423.1	417.7 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3. Results of experiment 1: average number of +L+ connections (#) and unique contributions (>) +L+ </SectLabel_tableCaption> <SectLabel_table> contribution	org chart	friending	blogs	bookmarks +L+ full list	107	76	83	55 +L+ 	(39.2%)	(27.8%)	(30.4%)	(20.1%) +L+ nothing	41 0	22 0 (8.1%)	370	00 +L+ 	(4.0%)		(13.6/0)	(0.0%) +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4. Unique contribution over all other sources +L+ </SectLabel_tableCaption> <SectLabel_bodyText> results reveal a different picture. It seems that for each and +L+ every source, the average number of unique contributions +L+ over the other three sources is quite close to the average +L+ number of connections, implying that the information +L+ extracted from the different sources is indeed diverse. +L+ Table 4 shows two statistics of the unique contributions of +L+ the different sources over all other sources. The first row in +L+ the table shows the number of people for whom the source +L+ contributed its full list – meaning that the lists from other +L+ sources had no intersection with this source. For instance, +L+ for 107 of the people (39.19%) – the lists of blog +L+ commenters, tagging friends, or similar bookmarkers did +L+ not contain anyone from their organizational group. As can +L+ be seen on Table 4, these numbers are rather large – 76 for +L+ Fringe, 83 for BlogCentral, and 55 for Dogear – indicating +L+ the diversity of the sources. There was not even a single +L+ person, for whom a single source covered all other sources, +L+ proving that aggregation creates a broader picture than any +L+ single source. The second row in the table shows the +L+ number of people for whom a source contributed nothing +L+ over the other lists. These figures indicate cases in which a +L+ single source may be dismissed, as the other three sources +L+ cover the information it provides. As may be seen on the +L+ table, these figures are rather small: up to 37, for +L+ BlogCentral, and as low as 0 for Dogear. +L+ For 33 out of the 273 people examined (12.08%), there was +L+ no intersection between any of the sources – each of their +L+ public sources provided a completely different list. +L+ The results of this experiment validate hypothesis (1) and +L+ support hypothesis (4), showing the diversity of information +L+ from public sources, that no single source holds all SN +L+ information, and thus that aggregation is likely to be of +L+ greater value than information from any single source. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Results of Experiments 2 and 3 +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Our questionnaire plugin collected information from 116 +L+ users who responded to all three stages of the experiment. +L+ Out of the 116 users, 65 are from the US and Canada, 49 +L+ are from Europe and the Middle East, and two are from +L+ Asia Pacific. 73 of the users who responded are using +L+ Fringe, 29 are bloggers, and 62 use Dogear. We believe +L+ these users represent a wide range of IBM employees and +L+ are thus a good test bed for our hypotheses. In addition, +L+ results collected from the in depth interviews with 12 users +L+ strengthen some of our hypotheses. +L+ The bottom part of Table 2 shows the results of the first two +L+ sets of buddylists. On the first step, shown on the bottom +L+ left of the table, the list that got the most “best” votes (63) +L+ is the one based on a single source: the organizational chart. +L+ </SectLabel_bodyText> <SectLabel_page> 1023 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> The average score of this list is 2.36 (on a scale of 1=good, +L+ 4=bad). The list that got the best average score is the one +L+ based on aggregation of all four sources by weight +L+ combination 7 on Table 2. The average score of this list is +L+ 2.34 and its number of “best” votes is 58. Over 40% of the +L+ users (48) got in this aggregated list an identical list to the +L+ one based on the organizational chart. As the score of the +L+ aggregated list is better, we may conclude that when the +L+ lists were not identical, the aggregated list received better +L+ scores. Finally, note that in Table 2, the total number of +L+ “best” votes given to aggregations (i.e., combinations 5 +L+ through 8) was significantly higher than the combined +L+ number of “best” votes for all single sources (1 through 4). +L+ We consider the above findings as supporting hypothesis +L+ (4) – the value of aggregation. +L+ The results of the second step are shown on the bottom +L+ right of Table 2. All lists of this step received scores that +L+ are higher than 2 but lower than 3, and all lists received +L+ quite a few “best” votes (over 40). It appears that no +L+ optimal weighting scheme exists that may serve a good +L+ default for most users. We were therefore unable to prove +L+ our hypothesis (5). While conducting the interviews during +L+ experiment 3, we had another chance to see how different +L+ people prefer different sources. For example, when +L+ examining the list from Fringe, one of the users said: +L+ “Completely off. Only 7 people, out of them only 3 are +L+ familiar” while another said: “Accurate, very accurate +L+ actually. [ ...] that would be my ideal buddylist”. +L+ The results of the third step are shown on the bottom of +L+ Table 5. In this step we compared lists from private sources +L+ with lists from public sources and with lists based on a mix +L+ of private and public sources. For this step our plugin +L+ requested access to users’ private data. Only 55 users +L+ granted us access to their private data. We therefore based +L+ our analysis on the responses of these 55 users only. +L+ As can be expected, the lists based on private sources +L+ received better average scores (1.62-1.76) than those based +L+ solely on public sources. The list with most “best” votes +L+ (17) on is the list based on the (private) chat system. +L+ </SectLabel_bodyText> <SectLabel_table> 	17	18	19	20	21	22	23	24 +L+ Bookmarking	0.25		0.16	0.1	0.2			0.1 +L+ people tagging	0.25		0.16	0.1	0.2			0.1 +L+ blogs	0.25		0.16	0.1	0.2			0.1 +L+ org-chart	0.25		0.16	0.1	0.2			0.1 +L+ email		0.5	0.16	0.3	0.1	1.0		0.4 +L+ chat		0.5	0.16	0.3	0.1		1.0	0.2 +L+ average score +L+ # of “best” votes	0	12	2	9	0	10	17	9 +L+ # of score “1”	3	30	6	23	4	25	26	23 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5. Weight combinations and results of public and +L+ private sources in the third set of buddylists +L+ </SectLabel_tableCaption> <SectLabel_bodyText> However, 17 votes are only 30.9%, indicating that no list +L+ significantly outvoted the others. The list with best average +L+ score (1.62) is the list based on combination 18 in Table 5 +L+ (email and chat), supporting the value of aggregating +L+ information (private in this case) – hypothesis (4). Yet +L+ another supporting point for hypothesis (4) was received +L+ during the interviews, when no single user had chosen a list +L+ based solely on one source. This observation is less strong, +L+ since the experiment setting encouraged people to play with +L+ aggregations, yet they clearly had a choice to disable all +L+ other sources and stay with one, but did not. +L+ Table 2 and Table 5 also show the number of times each of +L+ the buddylists in this experiment received score 1. List 18 +L+ (email and chat) received score 1 for the largest number of +L+ times – over 54% of the people granted it a perfect score. +L+ Other lists which obtained many high scores are list 20 (mix +L+ of public and private), list 22 (email), list 23 (chat), and list +L+ 24 (anther mix of public and private). It is obvious from the +L+ table that lists based on private sources receive score 1 +L+ more often, as expected. The value of public sources is +L+ evident from the bottom line of Table 2: the lists based +L+ solely on public sources received a perfect score a +L+ considerable amount of times, supporting hypothesis (2). In +L+ experiment 3, seven of 11 used some combination of public +L+ and private data sources (the twelfth user had no access to +L+ private data) and two of them even preferred the public +L+ sources slightly over private ones (see Table 6). +L+ Two of the combinations that mixed all six sources (number +L+ 20 and number 24 in Table 5) received 9 “best” votes each. +L+ Examining our results reveals that each such vote was given +L+ by a different user, implying that for 18 people, they created +L+ a buddylist that is preferred over the buddylists based solely +L+ on private sources. Both these lists received an average +L+ score of 1.75, which is identical to the average score of the +L+ list based on chat, and they both received a perfect score 23 +L+ times. This implies that for quite a few people the mix with +L+ public sources creates buddylists of high quality. Together +L+ with the fact that in experiment 3 most users chose to +L+ combine private with public data, we conclude that +L+ hypothesis (3) is true - information from public sources +L+ may provide a more complete picture of one’s SN. +L+ Finally, Experiment 3 revealed what people think about +L+ sharing lists coming from their private data sources (see +L+ Table 6). Most of the people (10) said that they will be +L+ </SectLabel_bodyText> <SectLabel_table> 	1	2	3	4	5	6	7	8	9	10	11	12 +L+ prv	85	00	100	48	63	100	100	100	69	48	54	54 +L+ pub	15	100	00	52	37	00	00	00	31	52	46	46 +L+ shr	2	3	2	2	2	3	2	2	3	3	1	0 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 6. Experiment 3 results* +L+ </SectLabel_tableCaption> <SectLabel_footnote> * First row shows percentage of private sources, second row percentage of +L+ public sources, third row – sharing list preference (3 – automatically with +L+ anyone, 2- after editing with anyone, 1 – after editing with friends, 0 – will +L+ not share) +L+ </SectLabel_footnote> <SectLabel_page> 1024 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> willing to share their lists with anyone. Seven stated that +L+ they would want to edit first. “I am worried that SONAR +L+ results are not accurate enough and would like to [ ...] +L+ make sure the people who see the lists get good lists”, said +L+ one user. Another user said, at first, “[I am] worried about +L+ what my buddies would say about me sharing their names”, +L+ but, after thinking about it he decided that it was harmless +L+ enough and said he would share his lists. Four users showed +L+ great openness by declaring that they would share their list +L+ automatically, without any editing, with anyone. Only one +L+ user said that he will not share his lists with anyone. One of +L+ the users summarized these results nicely: “... I think it +L+ should always be left up to the individual as there are +L+ dangerous things about SN. [but]... within a company, you +L+ have to realize you are probably not going to be that +L+ private”. All in all, hypothesis (6) is supported to a high +L+ extent. Our results suggest that there is a good chance +L+ people will be willing to share their private-based +L+ buddylists after applying some editing to it. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Discussion +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Our experiments show that information from public sources +L+ is very diverse and no single source may provide all SN +L+ information. While some sources provide communication +L+ data, others provide similarity data. While reacting to lists +L+ generated from Dogear, users said: “Far from ideal list”, +L+ “Over 50% are strangers”. But another user, while +L+ discussing her usage of SNs described: “if my goal was to +L+ search for expertise, then I would lean it heavily towards +L+ social bookmarking and blogs”. It suggests that diverse +L+ sources can become valuable for diverse tasks. It also +L+ explains why our hope to locate an optimal default +L+ combination of weights for aggregation was not fulfilled - +L+ different users with different views and different needs may +L+ require different combinations of the sources. +L+ The information extracted from public sources is shown to +L+ be of value, and while private sources provide better +L+ information, public sources do contribute additional +L+ information and create a more complete picture of SNs. The +L+ value of aggregation is proven both by the diversity of +L+ public sources and by user votes for aggregated lists. +L+ We saw in our interviews that aggregation and collection of +L+ SN information becomes crucial in global organizations. +L+ While analyzing the organizational chart data source one of +L+ the users noted: “It just looks at people within my world, +L+ and I deal with a lot of people outside of my function, if you +L+ will”. Another user commented: “Names that were missing +L+ on email appear now. Top person on ideal list does not +L+ appear here”. It shows that no single data source is +L+ sufficient for creating one’s ideal buddylist and thus +L+ demonstrates the value of aggregation. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSIONS AND FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper, we present the motivation and challenge for +L+ aggregating SN information from multiple data sources. We +L+ describe SONAR, an API for exposing relations embedded +L+ in numerous applications or services. SONAR allows +L+ building weighted networks with evidence for each +L+ connection, showing how strongly people are connected. +L+ SONAR was implemented for various sources, public and +L+ personal, within IBM, and demonstrated through a plugin +L+ for Lotus Sametime: SONARBuddies. +L+ Our experiments indicate that aggregation produces a more +L+ comprehensive SN. Information coming from public data +L+ sources is shown to be relevant and diverse. Public sources +L+ mainly represent new emerging social technologies on the +L+ Web. We believe that people’s involvement in these +L+ technologies will continue to grow, while new technologies +L+ appear. Hence, more quality social information will be +L+ available for frameworks like SONAR. For some users, +L+ public sources make a significant contribution over private +L+ ones, which may have been considered the predominant +L+ sources. It is extremely interesting to continue examining +L+ the potential of public sources to actually replace SN +L+ information currently extracted from private sources, and +L+ thus relieving privacy issues. +L+ This paper focuses on general SNs. In practice, there are +L+ different types of SNs, which reflect different semantics of +L+ connections, like friendship, interpersonal communication, +L+ or similarity. Such networks tend to be semantically +L+ different even for the same user. Shared bookmarks, for +L+ instance, reflect similarity between users rather than a direct +L+ connection. The scenario used in our experiments asked the +L+ users to rank buddylists, typically used for communication. +L+ It was natural that Dogear, which exposes similarity, +L+ received low scores by our users. One could think of +L+ different scenarios, such as finding potential people for a +L+ community on a specific topic, where this similarity +L+ network would be a perfect networking tool. It would be +L+ interesting to identify different scenarios and examine the +L+ contribution of different sources to them. +L+ Another interesting direction is deriving contextual +L+ networks – networks that are related to a specific context or +L+ term. For example, the list of people with whom one +L+ communicates most frequently about Java, is likely to be +L+ different from the list of people with whom one +L+ communicates mostly about SN analysis. +L+ Our plans for future work on the SONAR API include +L+ extending it to support different types of relations (e.g., +L+ familiarity vs. similarity). We also plan to allow a +L+ specification of a search term to support contextual queries +L+ such as: “who is most related to <person> w.r.t. <topic>?” +L+ These extensions would allow us to further explore the +L+ variety of SNs, the differences and relations among them, +L+ the value they bring to users, and the patterns of their usage. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGEMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We would like to acknowledge all those who installed +L+ SONARBuddies and provided us with feedback about its +L+ usage. We are especially grateful to those who participated +L+ in our experiments and interviews. We thank James Snell +L+ </SectLabel_bodyText> <SectLabel_page> 1025 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> for providing us data about the usage of BlogCentral, and +L+ Jonathan Feinberg for data about Dogear. Sigalit Ur and +L+ Inbal Ronen participated in numerous discussions and +L+ provided enlightening comments, we are indebted to them. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Adamic, L. A., and Adar, E. Friends and neighbors on +L+ the Web, Social Networks, 25, 3 (2003), 211-230. +L+ 2. Aleman-Meza, B., Nagarajan, M., Ramakrishnan, C., +L+ Sheth, A., Arpinar, I., Ding, L., Kolari, P., Josi, A., and +L+ Finin, T. Semantic analytics on social networks: +L+ Experiences in addressing the problem of conflict of +L+ interest detection. Proc. WWW '06, ACM Press (2006), +L+ 407-416. +L+ 3. Bar-Yossef, Z., Guy, I., Lempel, R., Maarek, Y. S., and +L+ Soroka, V. Cluster ranking with an application to +L+ mining mailbox networks. Proc. ICDM '06, IEEE +L+ Computer Society (2006), 63-74. +L+ 4. Cai, D., Shao, Z., He, X., Yan, X., and Han, J. Mining +L+ hidden community in heterogeneous social networks. In +L+ Proc. of the 3rd International Workshop on Link +L+ Discovery (LinkKDD 2005),ACM Press (2005), 58 – 65. +L+ 5. Culotta, A., Bekkerman, R., and McCallum, A. +L+ Extracting social networks and contact information from +L+ email and the Web. First Conference on Email and Anti- +L+ Spam (CEAS 2004) (2004). +L+ 6. Facebook Developers – Documentation. +L+ http://developers.facebook.com/documentation.php . +L+ 7. Farrell, S., and Lau, T. Fringe Contacts: People-Tagging +L+ for the Enterprise. Workshop on Collaborative Web +L+ Tagging, WWW’06, (2006). +L+ 8. Farnham, S., Portnoy, W., Turski, A., Cheng, L., and +L+ Vronay, D. Personal Map: Automatically modeling the +L+ user’s online social network. Proc. INTERACT’03, IOS +L+ Press (2003), 567-574. +L+ 9. Fielding, R.T. Architectural styles and the design of +L+ network-based software architectures. PhD thesis, +L+ University of California, Irvine, CA, (2000). +L+ 10. Friend of a Friend (FOAF) project. +L+ http://www.foaf-project.org/ . +L+ 11. HammerSley, B. Content Syndication with RSS, (2003). +L+ 12. Hope, T., Nishimura, T., and Takeda, H. An integrated +L+ method for social network extraction. Proc. WWW ’06, +L+ ACM Press (2006), 845-846. +L+ 13. Introducing JSON. http://www.json.org/ . +L+ 14. Jackson, A., Yates, J., Orlikowski, W. “Corporate +L+ Blogging: Building community through persistent +L+ digital talk”. Proc. 40th Annual Hawaii International +L+ Conference on System Sciences HICSS'07, (2007), p. 80 +L+ 15. Kautz, H., Selman, B., and Shah., M. ReferralWeb: +L+ Combining social networks and collaborative filtering. +L+ Communications of the ACM 40, 3 (1997), 63-65. +L+ 16. Matsuo, Y., Hamasaki, M. et al. Spinning multiple +L+ social networks for semantic Web. Proc. AAAI '06 +L+ (2006). +L+ 17. Matsuo, Y., Mori, J., Hamasaki, M., Takeda, H., +L+ Nishimura, T., Hasida, K., and Ishizuka, M. +L+ POLYPHONET: An advanced social network extraction +L+ system. Proc. WWW ‘06, ACM Press (2006), 397-406. +L+ 18. McDonald D.W. and Ackerman M.S. Expertise +L+ recommender: a flexible recommendation system and +L+ architecture. Proc. CSCW’00, (2000), 231–240. +L+ 19. Millen, D.R., Feinberg, J., and Kerr, B. Dogear: Social +L+ Bookmarking in the Enterprise. Proc. CHI 2006, (2006), +L+ 111-120. +L+ 20. Nardi, B.A., Whittaker, S., Issacs, E., Creech, M., +L+ Johnson, J., and Hainsworth, J. Integrating +L+ communication and information through contact map. +L+ Communications of the ACM 45, 4 (2002) 89-95. +L+ 21. Neustaedter, C., Brush, A., Smith, M., and Fisher, D. +L+ The social network and relationship finder: Social +L+ sorting for email triage. Proc. CEAS 2005. +L+ 22. Newman, M. E. J. Scientific collaboration networks, +L+ part I. Network construction and fundamental results. +L+ Physical Review E, 64, 016131, (2001). +L+ 23. Nottingham, M., and Sayre, R. RFC 4287 – The Atom +L+ Syndication Format (Proposed Standard). +L+ http://tools.ietf.org/html/rfc4287 +L+ 24. O'Reilly, T. What is Web 2.0. +L+ http://www.oreillynet.com/go/web2 . +L+ 25. Resig, J., Dawara, S., Homan, C. M., and Teredesai, A. +L+ Extracting social networks from instant messaging +L+ populations, Proc. of the 7th ACM SIGKDD Workshop +L+ on Link KDD, (2004). +L+ 26. Smarr, J. Technical and privacy challenges for +L+ integrating FOAF into existing applications. In the 1st +L+ Workshop on Friend of a Friend, Social Networking and +L+ the Semantic Web, (2004). +L+ 27. Smith, M. Invisible crowds in cyberspace: Measuring +L+ and mapping the social structure of Usenet. In Smith, +L+ M., and Kollock, P. Eds., Communities in Cyberspace. +L+ Routledge Press, (1999). +L+ 28. Tyler, J.R., Wilkinson, D.M., and Huberman, B.A. +L+ Email as spectroscopy: Automated discovery of +L+ community structure within organizations. In +L+ Communities and Technologies, Huysman, M., Wenger, +L+ E., and V. Wulf, Eds. (2003), 81-96. +L+ 29. Wasserman, S., and Faust, K. Social Network Analysis. +L+ (1994). +L+ 30. Wellman, B. Computer networks as social networks. +L+ Science 293 (2001) 2031-2034. +L+ 31. Zhang, J., Ackerman M.S. Searching for expertise in +L+ social networks: a simulation of potential strategies. +L+ Proc. GROUP 2005, ACM Press (2005), 71-80. +L+ </SectLabel_reference> <SectLabel_page> 1026 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> ‘Looking at’, ‘Looking up’ or ‘Keeping up with’ People? +L+ Motives and Uses of Facebook +L+ </SectLabel_title> <SectLabel_author> Adam N. Joinson +L+ </SectLabel_author> <SectLabel_affiliation> School of Management +L+ University of Bath +L+ </SectLabel_affiliation> <SectLabel_address> Bath +L+ United Kingdom +L+ BA2 7AA +L+ </SectLabel_address> <SectLabel_email> A.Joinson@Bath.ac.uk +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This paper investigates the uses of social networking site +L+ Facebook, and the gratifications users derive from those +L+ uses. In the first study, 137 users generated words or +L+ phrases to describe how they used Facebook, and what they +L+ enjoyed about their use. These phrases were coded into 46 +L+ items which were completed by 241 Facebook users in +L+ Study 2. Factor analysis identified seven unique uses and +L+ gratifications: social connection, shared identities, content, +L+ social investigation, social network surfing and status +L+ updating. User demographics, site visit patterns and the use +L+ of privacy settings were associated with different uses and +L+ gratifications. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Social networking sites, uses and gratifications, motivation +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> H1. Models and Principles: User/Machine Systems; H5.m. +L+ Information interfaces and presentation: Miscellaneous. +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Social networking sites such as MySpace, LinkedIn and +L+ Facebook have become hugely popular in the last few +L+ years. In July 2007, social networking sites occupied five of +L+ the top fifteen visited websites according to Alexa.com. On +L+ July 10, 2007, Facebook.com reported signing up its 30 +L+ millionth user, with a year on year increase in unique users +L+ of 89% [12]. In the UK, use of Facebook increased by +L+ 500% between November 2006 and May 2007 [19]. +L+ MySpace is reported (although disputed [10]) to have over +L+ 100 million users [4]. +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that +L+ copies bear this notice and the full citation on the first page. To copy +L+ otherwise, or republish, to post on servers or to redistribute to lists, +L+ requires prior specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> Social networking sites typically provide users with a +L+ profile space, facilities for uploading content (e.g. photos, +L+ music), messaging in various forms and the ability to make +L+ connections to other people. These connections (or +L+ ‘friends’) are the core functionality of a social network site +L+ [5, 6] although most also provide opportunities for +L+ communication, the forming of groups, hosting of content +L+ and small applications. +L+ Given the growth of social networking sites, it is perhaps +L+ unsurprising that their use has garnered media attention, +L+ including the seemingly now obligatory scare stories +L+ involving predatory child sex offenders [20], identity theft +L+ [1], workplace usage levels [9] and even addiction [8]. +L+ In many recent cases, this coverage has focused on +L+ Facebook.com, which was originally restricted to users with +L+ an ‘.edu’ e-mail address. In September 2006, Facebook +L+ opened registration to non-college based users. This change +L+ led to rapid growth in the number of users, as well as +L+ almost viral growth within non-educational organizations. +L+ For instance, the British Broadcasting Corporation (BBC) +L+ network (which requires a BBC email address) has circa +L+ 10,000 members, approximately 50% of employees [21]. +L+ Since May 2007, Facebook has also allowed the +L+ development and implementation of third-party applications +L+ (see dev.facebook.com). +L+ Before opening to non-academic (and non-US-based) users, +L+ Facebook.com was peculiar amongst social networking +L+ sites since many of the social networks its users built were +L+ based on offline, geographically confined groups (e.g. a +L+ campus). Termed ‘networks’ by the site (which have +L+ recently expanded to include non-university based +L+ geographic areas and workplaces), this reflection of the +L+ offline community in the online environment may have led +L+ to unique forms of use amongst users [17]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> User motivation and social networking sites +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Social networks serve a number of functions in offline life – +L+ for instance, providing social and emotional support, +L+ information resources and ties to other people [25]. Similar +L+ kinds of social networks have been identified in online +L+ </SectLabel_bodyText> <SectLabel_page> 1027 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> communities [7, 25], with users turning online for both +L+ emotional support and as an information resource (e.g. via a +L+ mailing list). In both cases, an online social network may +L+ provide users with social capital [7]. +L+ Online social networking sites may also serve a number of +L+ other purposes [5, 16]. Lampe et al. [16] draw a distinction +L+ between the use of Facebook for ‘social searching’ – +L+ finding out information about offline contacts, and ‘social +L+ browsing’ – the use of the site to develop new connections, +L+ sometimes with the aim of offline interaction. A survey of +L+ over 2,000 students, found evidence that the primary use of +L+ Facebook was for ‘social searching’ – that is, using +L+ Facebook to find out more about people who they have met +L+ offline, or who they attend class or share a dormitory with +L+ [16]. The use of Facebook for ‘social browsing’, for +L+ instance, to meet someone via the site with the intention of +L+ a later offline meeting, or to attend an event organized +L+ online, scored relatively low amongst their sample. The +L+ main use reported by the sample studied by Lampe et al. +L+ [16, see also 7] was to, “keep in touch with an old friend or +L+ someone I knew from high school”, an activity that while +L+ expressing the offline aspects of social searching, also +L+ suggests a social capital function for Facebook. Golder et +L+ al. [11] report that while the vast majority of messages are +L+ sent to friends (90.6%), a large proportion (41.6%) is sent +L+ to friends outside of one’s local network. This suggests that +L+ messaging is used to maintain and build social ties across +L+ distances. In comparison, ‘pokes’ (a form of content-free +L+ messaging) were primarily exchanged within a network / +L+ school (98.3% of all pokes were within a network). Golder +L+ et al. [11] argue that friendship ties require little effort or +L+ investment to maintain, while messaging with +L+ geographically distant friends is used to build social capital +L+ [7]. +L+ According to Lampe et al. [16], social networking sites like +L+ Facebook may also serve a surveillance function, allowing +L+ users to “track the actions, beliefs and interests of the larger +L+ groups to which they belong” (p. 167). The surveillance and +L+ ‘social search’ functions of Facebook may, in part, explain +L+ why so many Facebook users leave their privacy settings +L+ relatively open [13]. If ‘social searching’ is a public good, +L+ then reciprocity rules would dictate that by enabling a +L+ degree of surveillance of oneself, one would should also be +L+ able to engage in reciprocal surveillance of others. For +L+ instance, Gross and Acquisti [13] report that only 1.2% of +L+ users changed the default ‘search’ privacy setting, and less +L+ than 1/2% of users changed the default ‘profile visibility’ +L+ privacy settings. +L+ Enabling Facebook users who are not currently linked as +L+ friends to view personal aspects of one’s profile may also +L+ be a strategy to increase the size of one’s social network. In +L+ support of this view, [17] report that users completion of +L+ profile fields that share a common referent (e.g. class, +L+ hometown) is positively associated with more friends, +L+ perhaps because such information encourages the +L+ development of ties based on shared experiences. Profile +L+ elements that focused on individual likes and dislikes did +L+ not have an association with the number of friends. +L+ As noted earlier, Facebook.com has undergone radical +L+ change over the last twelve months. By moving outside of +L+ the US-academic environment and embracing users +L+ globally and outside of academia, it has not only changed +L+ the profile of its users, but also the potential motivations for +L+ their use. While tightly controlled, geographically bounded +L+ networks based on university affiliation still exist, they are +L+ dwarfed by networks based outside of academia – for +L+ instance, as of September 2007, the ‘London’ network has +L+ over 1 million members, New York over 355,000 and +L+ Toronto over 800,000. The present paper examines the +L+ motivations of Facebook users using a ‘uses and +L+ gratifications’ framework. +L+ Uses and gratifications refer to the ‘how and why’ of media +L+ use [23]. Specifically, ‘uses and gratifications’ refer to the +L+ motivations of specific uses, and the satisfaction people +L+ gain from such use. These gratifications can be divided into +L+ those based on the content of the media (content +L+ gratifications) and those based on the actual experience of +L+ using the media (process gratifications). Typically, content +L+ gratifications are held to be related to the repeated use of a +L+ media [18] which for the designers of such systems relates +L+ to a site’s ‘stickiness’. However, the Internet, and social +L+ networking sites in particular, also provide communication +L+ and interaction, unlike many ‘old media’ (e.g. television). +L+ This led Stafford et al. [23] to propose a third form of +L+ gratification arising from Internet use: as a social +L+ environment. +L+ In the present study, the usual two stage approach to +L+ studying uses and gratification is adopted [3]. In Study 1, +L+ Facebook users are asked to generate lists of words or +L+ phrases that describe their uses and gratifications in an +L+ exploratory way. In Study 2, these terms are subjected to +L+ factor analysis in order to form grouped profiles of specific +L+ uses and gratifications. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> STUDY 1: EXPLORATORY STAGE +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> Participants +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Participants were 137 Facebook users who responded to a +L+ request to complete a short online study. The sample +L+ comprised 53 males and 88 females (Mean age = 26.3 +L+ years). Participants were recruited through a number of +L+ different methods: postings to the ‘wall’ of three network +L+ homepages on Facebook (two universities, one regional), a +L+ paid flyer shown 10,000 times across all networks, and +L+ links on academic survey websites. The survey was open +L+ during the first two weeks of July 2007. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Materials +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The online survey comprised a series of basic demographic +L+ questions (e.g. age, gender, occupation, location), alongside +L+ some measures of use of Facebook (time spent on site each +L+ week, number of friends linked on site, history of use). +L+ </SectLabel_bodyText> <SectLabel_page> 1028 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> Following this, participants were asked to respond to the +L+ following questions adapted from [23] using free text entry: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	What is the first thing that comes to mind when +L+ you think about what you enjoy most when using +L+ Facebook? +L+ •	What other words describe what you enjoy about +L+ using Facebook? +L+ •	Using single, easy-to-understand terms, what do +L+ you use Facebook for? +L+ •	What uses of Facebook are most important to you? +L+ </SectLabel_listItem> <SectLabel_subsectionHeader> Results +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Two raters clustered the descriptive items and phrases +L+ developed by Facebook users in response to the first +L+ question. The raters worked collaboratively to develop the +L+ clusters, and were instructed to ‘identify responses that are +L+ related’. The author then discussed the themes with the +L+ raters, and named them accordingly. The main themes +L+ identified are outlined in Table 1. +L+ </SectLabel_bodyText> <SectLabel_table> Theme (sample user generated items)	Number +L+ 	of +L+ 	mentions +L+ ‘Keeping in touch’	52 +L+ Contacting friends who are away from home Chatting to people I otherwise would have lost contact with +L+ Passive contact, social surveillance Virtual people-watching.	19 +L+ ‘Re-acquiring lost contacts’	15 +L+ Reconnecting with people I’ve lost contact with +L+ Finding people you haven't seen for a while +L+ ‘Communication’ Being poked	15 +L+ Private messages Writing on walls +L+ Photographs	11 +L+ Tagged in picture Posting pictures Sharing pictures +L+ Design related Ease of use	4 +L+ Perpetual contact	4 +L+ Seeing what people have put as their 'status' The continuous updates +L+ Seeing what my friends have been up to today +L+ ‘Making new contacts’ Talking to singles	5 +L+ Getting new friends +L+ Joining groups +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1: Frequency of mentions (Question 1) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> In keeping with previous research [e.g. 16], the use of +L+ Facebook to ‘keep in touch’ received the largest number of +L+ mentions, with the use of the site to make new contacts +L+ receiving a small number of mentions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> STUDY 2: IDENTIFYING USES AND GRATIFICATIONS +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> Item generation +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A sample of items from each use and gratification proposed +L+ by users was extracted from the exploratory list developed +L+ in Study 1. Participants’ responses to items 2–4 were +L+ examined, and any occurrences of other uses or +L+ gratifications not mentioned in response to the first item +L+ were added to the list. This led to a total of 46 items. Where +L+ possible, the item was taken word for word from participant +L+ responses to Study 1. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Participants +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Participants were 241 Facebook users recruited using the +L+ same methods outlined in Study 1. In addition, e-mails were +L+ sent to selected mailing lists with a request for participation +L+ (e.g. AIR-L). Participants were 80 males (33.2%) and 161 +L+ (66.8%) females (mean age = 25.97 years (SD = 9.30, range +L+ 15-66 years old). The majority of the sample were full time +L+ students (n = 151, 62.7%), 6.6% (n = 16) were part-time +L+ students and worked part- or full-time (or had carer +L+ responsibilities), and 30.7% were in full-time work and not +L+ studying (n = 78). The study was open during the final +L+ week in July, and throughout August. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Measures +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The same demographic and Facebook use measures +L+ described in Study 1 were used in Study 2. Participants also +L+ completed an item related to their use of Facebook privacy +L+ settings, specifically if they had changed the default +L+ settings, and if so, the degree to which they had made them +L+ more private or more open. +L+ Participants were finally asked to rate, using a 7-point +L+ Likert scale, the 46 uses and gratifications derived from +L+ Study 1 using the metric, “How important are the following +L+ uses of Facebook to you personally?’ The scale was +L+ anchored at 1 (very unimportant) and 7 (very important). +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Results +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Participants had an average of 124 friends linked to their +L+ Facebook profile (Range 1-1000, Median = 85, SD = +L+ 129.97). Around half of the participants had been registered +L+ on the site for less than six months (6.3% for less than one +L+ month, 9.6% for between one and two months and 29.2% +L+ for between two and six months). The remaining +L+ participants had been signed up for between six months and +L+ a year (21.7%), more than one year, but less than two +L+ (21.7%) or for more than two years (10.8%). The majority +L+ of participants visited the site either daily (38.8%) or more +L+ than once a day (27.5%). Almost a quarter visited Facebook +L+ several times a week (22.5%), with 6.7% visiting once a +L+ week on average, and 4.2% visiting less than once a week. +L+ </SectLabel_bodyText> <SectLabel_page> 1029 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> Amongst all respondents, the most common responses for +L+ the time spent on the site each week were between 1 and 2 +L+ hours (33.3%) and between 2 and 5 hours (32.5%). A +L+ relatively small proportion of users claimed to spend either +L+ less than 1 hour a week (16.9%) or between 5 and 10 hours +L+ (11.0%) on the site. The proportion of users claiming more +L+ than 10 hours Facebook use per week was small (5.4%). +L+ Unlike previous research [13], the majority of users claimed +L+ to have changed the default privacy settings in Facebook, +L+ with 25.6% (n = 61) reporting making their profile +L+ ‘somewhat’ more private, 21% (n = 50) ‘much more’ +L+ private and 10.9% (n = 26) making it ‘as private as +L+ possible. A smaller group claimed to have made their +L+ profile either more ‘open’ (9.2%, n = 22) or ‘as open to +L+ others as possible (9.2%, n = 22). The number of people +L+ making no changes to their profile (23.5%, n = 56) was +L+ substantially lower than that reported in previous studies +L+ [ 14]. +L+ The most important uses of Facebook tended to be related +L+ to the ‘social searching’ and surveillance functions (see +L+ Tables 2-8), identified by [16]. Specifically, the use of the +L+ site to learn about old friends and maintain or re-connect +L+ relations scored consistently highly. This pattern repeats +L+ previous findings from student samples [7, 16]. +L+ To investigate the nature of the various uses and +L+ gratifications of Facebook in more depth, exploratory factor +L+ analysis was conducted. The initial factor analysis (varimax +L+ rotation) yielded 9 components with eigenvalues over 1, +L+ explaining 64.8% of the variance. Examination of the scree +L+ plot and unique loadings suggested that seven components +L+ (explaining 59% of the variance) should be retained for +L+ further analysis. Only four items did not load on any of the +L+ factors: one was related to privacy settings, two about use +L+ of the ‘poke’ facility and one about leaving messages on the +L+ ‘wall’. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> INTERPRETATION OF FACTORS AND SCALE +L+ DEVELOPMENT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> To aid further analysis, scales were developed from each +L+ factor. As a preliminary check, score distributions on each +L+ item were examined to ensure that none suffered from +L+ restricted range (i.e., the full range of response options was +L+ being used). This was the case for all items. Items were +L+ identified as markers of each factor based on the commonly +L+ used benchmark of a loading greater than .5. Items that had +L+ significant loadings on other factors were discounted as +L+ marker items [22]. +L+ Application of these criteria led to identification of eight +L+ marker items for Factor 1, three for Factors 2, four for +L+ Factor 3, four each for Factor 4 and 5, and three each for +L+ Factor 6 and 7. +L+ Factor 1 (Table 2) contains items predominantly concerned +L+ with ‘keeping in touch’ (the most often mentioned use of +L+ Facebook in Study 1, and by [16]). The items have a clear +L+ focus on re-connecting with lost contacts and maintaining +L+ contact with existing friends. Some of the items loading on +L+ this factor also clearly relate to the ‘surveillance’ function +L+ identified by [16], for instance, ‘Finding out what old +L+ friends are doing now”. Others are more closely related to +L+ the creation or maintenance of ‘weak ties’ (e.g. +L+ “Maintaining relationships with people you may not get to +L+ see very often”). Because of the combination of +L+ surveillance and social capital functions, this factor and +L+ related scale is labeled ‘social connection’. Two items +L+ loaded on this factor, but did not meet the criteria for factor +L+ purity: ‘Reading messages on your wall’ and ‘Seeing how +L+ old acquaintances look’). +L+ </SectLabel_bodyText> <SectLabel_table> Factor 1: Social connection (Cronbach’s Alpha = .89)	Item Mean (SD)	Loading +L+ Finding out what old friends are doing now	5.08 (1.71)	.753 +L+ Reconnecting with people you’ve lost contact with	5.29 (1.79)	.783 +L+ Connecting with people you otherwise would have lost contact with	5.53 (1.61)	.842 +L+ Receiving a friend request	4.86 (1.68)	.601 +L+ Finding people you haven’t seen for a while	5.41 (1.66)	.850 +L+ Maintaining relationships with people you may not get to see very often	5.71 (1.56)	.764 +L+ Contacting friends who are away from home	5.46 (1.83)	.522 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2: Items and loading (Factor 1) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> The second factor is comprised of three items related to the +L+ joining of groups, organization of events and meeting of +L+ ‘like-minded people’ (see Table 3). These activities are akin +L+ to ‘social browsing’ identified by Lampe et al.; although +L+ there is no reason to assume that they are necessarily +L+ motivated by a desire to meet offline eventually. It also +L+ contains related to the discovery of new music and new +L+ groups via friends. As such, it seems to represent a ‘shared +L+ identities’ function. Two items (‘Seeing what kinds of +L+ networks and special interest groups your friends have’ and +L+ ‘Learning about new music’) loaded on the factor, but did +L+ not meet the factor purity criteria. +L+ </SectLabel_bodyText> <SectLabel_table> Factor 2: Shared identities (Cronbach’s alpha .74)	Item Mean (SD)	Loading +L+ Organizing or joining events	3.42 (1.82)	.699 +L+ Joining groups	3.52 (1.63)	.727 +L+ Communication with likeminded people	3.82 (1.76)	.638 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 3: Items and loading (Factor 2) +L+ </SectLabel_tableCaption> <SectLabel_page> 1030 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> The third factor is related to the posting and viewing of +L+ photographs (see Table 4), although many of the items also +L+ had loadings in the .3 region on the first factor. This +L+ suggests that these activities within Facebook may fulfill a +L+ number of gratifications. Specifically, the social uses of +L+ photographs (e.g. sharing, tagging) may also play an +L+ important role in ‘social connection’. However, by forming +L+ a unique factor, they may also be a content gratification in +L+ their own right. +L+ </SectLabel_bodyText> <SectLabel_table> Factor 3: Photographs (Cronbach’s alpha =.89)	Item Mean (SD)	Loading +L+ Viewing photos	5.03 (1.72)	.609 +L+ Being tagged in photos	4.24 (1.90)	.668 +L+ Tagging photos	3.96 (1.89)	.734 +L+ Sharing / posting photographs	4.58 (1.89)	.701 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 4: Items and loading (Factor 3) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Factor four contains items related to content within +L+ Facebook – for instance, applications and quizzes (see +L+ Table 5). This relates to the usual ‘content gratification’ +L+ identified in previous media research. It is worthwhile +L+ noting that while these items form a unique factor, the mean +L+ scores are relatively low. A single item (‘Looking at posted +L+ items’) loaded on the factor, but did not meet factor purity +L+ criteria. +L+ </SectLabel_bodyText> <SectLabel_table> Factor 4: Content (Cronbach’s alpha = .74)	Item Mean (SD)	Loading +L+ Applications within Facebook	2.85 (1.65)	.826 +L+ Playing games	1.86 (1.40)	.559 +L+ Discovering apps because you see friends have added them	2.64 (1.58)	.756 +L+ Quizzes	1.85 1.30)	.638 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 5: Items and loading (Factor 4) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Factor five contains items akin to both social searching and +L+ social browsing identified by Lampe et al [16]. The items +L+ comprising this factor cover both the use of Facebook to +L+ meet or view new people and to find out more about people +L+ who are met offline (see Table 6). +L+ </SectLabel_bodyText> <SectLabel_table> Factor 5: Social investigation (Cronbach’s alpha =.75)	Item Mean (SD)	Loading +L+ Virtual people watching	3.31 (1.90)	.574 +L+ Using advanced search to look for specific types of people	2.56 (1.70)	.508 +L+ Meeting new people	2.91 (1.83)	.509 +L+ Stalking other people	2.13 (1.71)	.755 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 6: Items and loading (Factor 5) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> One item (‘Looking up the profile of people you meet +L+ offline’) loaded on the factor but did not meet the purity +L+ criteria. The items do share a targeted investigation of +L+ others, however. As such, the factor is termed ‘social +L+ investigation’. +L+ Factor six comprises items related to a unique affordance of +L+ social networking sites – the ability to view other people’s +L+ social networks and friends (see Table 7). This ability to +L+ find out more about one’s acquaintances through their +L+ social networks forms another important surveillance +L+ function, and may also be a method for increasing the size +L+ of one’s own social network. This specific use is termed +L+ ‘Social network surfing’ here to signify the ability of users +L+ to move from one person to another via friend links, +L+ although it may also relate closely to a ‘process +L+ gratification’. +L+ </SectLabel_bodyText> <SectLabel_table> Factor 6: Social network surfing (Cronbach’s alpha =.79)	Item Mean (SD)	Loading +L+ Looking at the profiles of people you don’t know	2.48 (1.53)	.719 +L+ Viewing other people’s friends	3.34 (1.74)	.785 +L+ Browsing your friends’ friends	3.89 (1.65)	.724 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 7: Items and loading (Factor 6) +L+ </SectLabel_tableCaption> <SectLabel_bodyText> The final factor comprises items related to the newsfeed +L+ and status updates within Facebook. The newsfeed provides +L+ updates on both ‘friends’ status, alongside recent activity +L+ (e.g. the addition or removal of applications, changes in +L+ relationship status, addition of ‘friends’). Given the outcry +L+ when the newsfeed was introduced [2], the relative high +L+ scores for this use suggest an increasing degree of +L+ acceptance. Interestingly, a gratification (‘to keep up with +L+ the latest gossip’) also loaded on this factor (although only +L+ at the .4 level), suggesting a clear motivation for viewing +L+ the newsfeed. +L+ </SectLabel_bodyText> <SectLabel_table> Factor 7: Status updates (Cronbach’s alpha = .71)	Item Mean (SD)	Loading +L+ Updating your own status	3.85 (1.77)	.568 +L+ The news feed	3.79 (1.83)	.531 +L+ Seeing what people have put as their status	3.84 (1.79)	.698 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 8: Items and loading (Factor 7) +L+ The pattern of loadings and internal reliability (Cronbach +L+ alpha scores) suggests that the seven factors should be +L+ considered suitable for use in further analysis, on the +L+ assumption that they are interpretable. Scales were +L+ developed for each factor by creating the mean score across +L+ the marker items. +L+ </SectLabel_tableCaption> <SectLabel_page> 1031 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_subsectionHeader> Inter-relations of uses and gratifications +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Despite the fact that these factors arise from an orthogonal +L+ rotation and are separable in terms of item loadings, they +L+ are correlated (see Table 9). +L+ The Spearman correlations between the factors suggest that +L+ the uses and gratifications identified are related, in some +L+ cases relatively strongly. +L+ </SectLabel_bodyText> <SectLabel_table> Shared Identities	.24**	1 +L+ Photographs	.62**	.32**	1 +L+ Content	.03	.31**	.06	1 +L+ Social investigation	.37**	.43**	.42**	.37**	1 +L+ Social network surfing	.28**	.33**	.29**	.29**	.54**	1 +L+ Status updates	.30**	.49**	.34**	.34**	.38**	.28** +L+ </SectLabel_table> <SectLabel_tableCaption> Table 9: Spearman correlations between scales (n = 241) +L+ </SectLabel_tableCaption> <SectLabel_subsectionHeader> User demographics and uses and gratifications +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A MANOVA test found a significant difference between +L+ males and females on their scores across the seven uses and +L+ gratifications scales (F (7, 233) = 2.662, p < 0.02). Further +L+ analysis of the between-subjects effects on the dependent +L+ variables showed that scores were significantly different on +L+ the first factor – ‘social connection’ (F (1, 239) = 16.16, +L+ p<0.001, η2= .063), with females scoring higher on the +L+ scale (M = 5.40, SD = 1.22) compared to males (M = 4.70, +L+ SD = 1.37), on the third factor - ‘photographs’ (F (1, 239) = +L+ 8.95, p<0.01, 112= .036), with females again scoring higher +L+ on the scale (M = 4.67, SD = 1.61) compared to males (M = +L+ 4.02, SD = 1.51). There was a marginally significant +L+ difference across the seventh factor – ‘status updates’ (F (1, +L+ 239) = 3.26, p=.072, r)2= .0 13), with females scoring higher +L+ on the scale (M = 3.94, SD = 1.40) compared to males (M = +L+ 3.58, SD = 1.47). +L+ A one-way between subjects ANOVA found a significant +L+ effect of gender on profile privacy settings (F (1, 236) = +L+ 12.29, p < .01), with females more likely to report making +L+ their profile more private (Mean = 4.83, SD = 1.60) +L+ compared to males (Mean = 4.01, SD = 1.86) +L+ A further MANOVA test was conducted to compare +L+ responses to the items in light of occupational status (i.e. +L+ full-time student, full-time employed, part-time +L+ student/employed). Given the relatively low number of +L+ people working part-time / studying part-time, this group +L+ was excluded from the analysis. The results showed a +L+ significant overall effect of occupational status on uses and +L+ gratifications of Facebook (F (7, 217) = 4.93, p < .001), +L+ with significant effects for Factor 1 (social connection – F +L+ (1, 223) = 7.3 1, p<.01), Factor 2 (shared identities – F (1, +L+ 223) = 4.90, p<.05), and Factor 3 (photographs – F (1, 223) +L+ = 7.85, p< .01). +L+ Full-time students scored higher on social connection and +L+ photographs, and lower on shared identities, compared to +L+ those in full-time work (Factor 1 Mean = 5.38 (SD = 1.16) +L+ for students, Mean = 4.89 (SD = 1.45) for full-time +L+ employment; Factor 2 Mean = 3.47 (SD = 1.37) for +L+ students, Mean = 3.90 (SD = 1.40) for full-time +L+ employment; Factor 3 Mean = 4.71 (SD = 1.53) for +L+ students, Mean = 4.09 (SD = 1.57) for full-time +L+ employment). +L+ Age also correlated negatively with their scores on social +L+ connection (rs(225) = -.27, p<.001), and photographs (rs(225) = +L+ -.32, p<.001), with younger respondents scoring higher on +L+ both scales. Age did not correlate with the other factor- +L+ derived scales. +L+ Age also correlated with the length of time users had been +L+ registered on Facebook (rs(241) = -.17, p<0.01), the regularity +L+ with which they visited the site (rs(241) = -.18, p<.01), the +L+ number of hours they used the site in a week (rs(241) = -.22, +L+ p<.01) and the number of friends they had linked to their +L+ profile (rs(219) = -.37, p<.001). In all cases, a younger user +L+ was associated with higher usage levels, and a greater +L+ number of ‘friends’. +L+ Age was also negatively correlated with the use of privacy +L+ settings (rs(238) = -.17, p<.01), such that younger users report +L+ that they were more likely to have increased the privacy of +L+ their profile. In part this may be due to the higher number +L+ of friends amongst younger users. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Predicting Facebook use +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A number of earlier researchers have predicted that certain +L+ uses and gratifications of Facebook may be associated with +L+ greater use of the site. For instance, [17] note that +L+ completion of certain profile elements is associated with a +L+ greater number of ‘friends’, while the findings of [16]) +L+ suggest that the use of Facebook for social searching and +L+ surveillance motivates use. +L+ To examine possible motivators for use of Facebook, a +L+ series of multiple regression equations were calculated +L+ using scores on the seven factor-based scales to predict both +L+ the frequency of visits to the site and the time spent on +L+ Facebook during an average week. Age, occupation and +L+ gender were also entered as covariates (part-time excluded). +L+ The results of the regression analyses to predict the +L+ frequency of site use are shown in Table 10. The overall +L+ model was significant (F (10, 213) = 4.77, p<0.001, R2 = +L+ .15). +L+ A second regression equation examined the same variables +L+ predicting the amount of time spent on the site (see Table +L+ </SectLabel_bodyText> <SectLabel_page> 1032 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> 11). Again, the overall model was significant (F (10, 210) = +L+ 3.85, p<0.001, R2 = .12). +L+ </SectLabel_bodyText> <SectLabel_table> Variable	J6	t	Sig +L+ Sex	.179	2.638	.009 +L+ Age	.126	1.479	.141 +L+ Occupation	.036	.430	.667 +L+ F1 – ‘social connection’	-.055	-.619	.536 +L+ F2 - ‘shared identities’	.015	.200	.842 +L+ F3 – ‘photographs’	-.208	-2.295	.023 +L+ F4 – ‘content gratifications’	.032	.455	.649 +L+ F5 – ‘social investigation’	.156	1.819	.070 +L+ F6 – ‘social network surfing’	-.043	-.561	.576 +L+ F7 – ‘status updates’	-.296	-3.848	.000 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 10: Predicting frequency of visits to Facebook +L+ </SectLabel_tableCaption> <SectLabel_bodyText> The regression equations show a differential pattern of uses +L+ and gratifications motivating frequency of visits to the site, +L+ and the time spent on the site. Gender (females visit more +L+ frequently) and scores on the ‘photographs’ and ‘status +L+ updates’ factors predict the frequency of visits to the site. +L+ Higher scores on both scales predicted more frequent visits. +L+ There was a marginally significant effect of ‘social +L+ investigation’ (higher scores related to less frequent visits). +L+ However, participants age (younger spend more time) and +L+ scores on the content gratification scale predict the actual +L+ number of hours spent online. This suggests that +L+ surveillance gratifications motivate repeat visits, but that +L+ content gratifications motivate people to spend longer on +L+ the site when they do visit. +L+ </SectLabel_bodyText> <SectLabel_table> Variable	J6	t	Sig +L+ Sex	-.031	-.440	.660 +L+ Age	-.265	-3.029	.003 +L+ Occupation	.058	.669	.504 +L+ F1 – ‘social connection’	-.090	-.983	.327 +L+ F2 - ‘shared identities’	.011	.145	.885 +L+ F3 – ‘photographs’	.134	1.442	.151 +L+ F4 – ‘content gratifications’	.213	2.962	.003 +L+ F5 – ‘social investigation’	-.040	-.448	.655 +L+ F6 – ‘social network surfing’	.117	1.481	.140 +L+ F7 – ‘status updates’	.086	1.092	.276 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 11: Predicting time spent (hours) on Facebook +L+ </SectLabel_tableCaption> <SectLabel_bodyText> A final regression equation was calculated to predict the +L+ number of ‘friends’ users reported on Facebook (see Table +L+ 12). Given that the number of friends should be related to +L+ the length of time users had been registered on Facebook, +L+ and the intensity of their use, the usage measures (length of +L+ time, frequency of visit, time spent on site) were entered +L+ alongside the remaining variables. +L+ The overall model was significant (F (15, 196) = 8.48, p < +L+ .001, R2 = .31). +L+ As might be expected, age was associated with the number +L+ of ‘friends’ (younger have more ‘friends’), as was the +L+ amount of time users had been registered on the site and the +L+ frequency of their site visits (longer time registered, and +L+ more frequent visits, associated with more friends). +L+ Interestingly, scores on the ‘social connection’ scale were +L+ not associated with ‘friend’ numbers, while scores on the +L+ ‘content gratification’ scale were negatively associated with +L+ the number of ‘friends’ (i.e. higher scores associated with +L+ smaller number of ‘friends’). Scores on the ‘social +L+ investigation’ scale were positively associated with the +L+ number of friends, while scores on the ‘photographs’ scale +L+ were marginally significantly associated with an increased +L+ number of friends. +L+ </SectLabel_bodyText> <SectLabel_table> Variable	J6	T	Sig +L+ Sex	.036	.584	.560 +L+ Age	-.213	-2.936	.004 +L+ Occupation	-.041	-.584	.560 +L+ Time registered on site	.289	4.725	.000 +L+ Frequency of visit	-.184	-2.805	.006 +L+ Time spent on site	.062	.909	.364 +L+ F1 – ‘social connection’	-.081	-.978	.329 +L+ F2 - ‘shared identities’	.090	1.286	.200 +L+ F3 – ‘photographs’	.138	1.610	.109 +L+ F4 – ‘content gratifications’	-.139	-2.058	.041 +L+ F5 – ‘social investigation’	.169	2.123	.035 +L+ F6 – ‘social network surfing’	-.048	-.679	.498 +L+ F7 – ‘status updates’	-.047	-.641	.523 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 12: Predicting number of ‘friends’ on Facebook +L+ </SectLabel_tableCaption> <SectLabel_subsectionHeader> Use of Facebook Privacy Settings and meeting new +L+ people +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A final set of analyses were conducted to examine the +L+ relationship between specific uses and respondents’ +L+ reported privacy profile settings. The privacy settings of +L+ users were grouped, according to their responses, into those +L+ who reported making their profile less private (n=44), those +L+ </SectLabel_bodyText> <SectLabel_page> 1033 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> who reported leaving it at the default setting (n = 56), and +L+ those who reported making it more private (n = 137). In the +L+ main, privacy settings in Facebook allow users to hide their +L+ profile from people who are neither listed as ‘friends’ or +L+ members of the user’s own network. However, if the +L+ motive for using Facebook is to meet new people, then such +L+ privacy settings would be somewhat counter productive. To +L+ test this proposition, a MANOVA was conducted with +L+ privacy settings as the independent variable, and the +L+ responses to items related to meeting new people set as the +L+ dependent variables. The results showed a significant link +L+ between privacy settings and the responses to the items (F +L+ (8, 454) = 2.11, p < .05). Analysis of the between subjects +L+ effects found no difference in responses to the ‘joining +L+ groups’ or ‘joining events’ items and privacy settings (ps > +L+ .3), but a significant effect of reported privacy settings on +L+ responses to the item ‘meeting new people’ (F (2, 229) = +L+ 4.16, p < .02), and a marginally significant effect on the +L+ item ‘using advanced search to look for specific types of +L+ people’ (F (2, 229) = 2.48, p = .08). The means for the +L+ ‘meeting new people’ item across the three privacy groups +L+ are shown in Figure 1. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1: Scores on ‘meeting new people’ by privacy +L+ settings +L+ </SectLabel_figureCaption> <SectLabel_bodyText> These results suggest that for users wishing to use +L+ Facebook to meet new people, the privacy settings may be +L+ set at too stringent a level. Further analyses confirmed no +L+ links between the social connection scales and privacy +L+ settings, suggesting that a primary motivation for making +L+ one’s profile less private is the desire to meet new people. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Social networking sites pose a number of challenges for +L+ HCI researchers and practitioners. First, the actual uses and +L+ gratifications of such sites are not well understood. The +L+ present paper presents the first study of a social networking +L+ site using a ‘uses and gratifications’ framework, and also +L+ provides an empirically developed measurement tool for +L+ future research. +L+ Second, previous research that has been conducted has +L+ tended to focus on campus-based use of Facebook [e.g. 7, +L+ 13, 6, 11, 17], which may limit the generalizability of any +L+ findings. However, the results of the present research +L+ support many of the conclusions of earlier research +L+ conducted on student populations. For instance, the +L+ distinction previously drawn between ‘social searching’ and +L+ ‘social browsing’ uses of Facebook [16] was similarly +L+ evident in the present research. Moreover, in keeping with +L+ prior student-users research [e.g. 7, 16], the use of +L+ Facebook to ‘keep in touch’ was the most commonly +L+ mentioned term in Study 1, and formed a large proportion +L+ of the items comprising the first factor in Study 2. +L+ However, the adoption of a uses and gratifications approach +L+ enables us to begin to probe in more depth the exact nature +L+ of ‘keeping in touch’ as both a use and a gratification. The +L+ results of the present study suggest that ‘keeping in touch’ +L+ comprises two main functions. The first is a surveillance +L+ function as identified by Lampe and colleagues [16]. +L+ Facebook is used to see what old contacts and friends are +L+ ‘up to’, how they look and how they behave. In keeping +L+ with this use, there is evidence that Facebook profiles serve +L+ an important self-presentation tool [26]. Associated with +L+ this use is the social capital building gratification, where +L+ Facebook is used to build, invest in and maintain ties with +L+ distant friends and contacts [7, 11]. +L+ The ‘social search’ and ‘social browsing’ uses of Facebook +L+ identified by Lampe and colleagues [16] were closely +L+ related in the present study. The use of Facebook to search +L+ for new people loaded on the same factor as the use of +L+ Facebook to research offline contacts. This ‘virtual people +L+ watching’ was represented in both Factors 5 and 6, with the +L+ important distinction that Factor 6 relied primarily on +L+ ‘friend of friend’ connections, while Factor 5 represented +L+ targeted investigation of people met offline, or searched for. +L+ Symptomatic of this distinction is the difference between +L+ ‘looking up’ (Factor 5) and ‘looking at’ (Factor 6) people. +L+ In the present study, only social investigation was +L+ associated with a higher number of ‘friends’, not social +L+ network browsing. +L+ Interestingly, an increased score on the content gratification +L+ scale was negatively related to the number of ‘friends’ +L+ reported to be linked to one’s profile. This perhaps suggests +L+ a sub-set of users gain gratification through the use of +L+ applications within Facebook, rather than through the +L+ accrual of ‘friends’. However, many of the applications +L+ available in Facebook are social in nature (e.g. scrabble +L+ games, ways to rate friends). But, at present these +L+ applications tend to rely on existing contacts, rather than the +L+ accrual of new ‘friends’. As such, they may serve to +L+ strengthen social ties, rather than acting to increase the +L+ overall size of a social network. Thus, investment of time +L+ and effort in social applications within Facebook may be +L+ akin to messaging between friends [11] – it solidifies ties, +L+ rather than creating new links. +L+ Users responses on the scales created from the factors also +L+ predicted their pattern of use of the site. In keeping with +L+ </SectLabel_bodyText> <SectLabel_figure> 4 +L+ 3.5 +L+ 3 +L+ 2.5 +L+ 2 +L+ 1.5 +L+ 1 +L+ 0.5 +L+ 0 +L+ Less private	Default	More private +L+ </SectLabel_figure> <SectLabel_page> 1034 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> earlier work on traditional media, content gratification +L+ predicted the amount of time spent on the site. However, +L+ the use of the site for social investigation, viewing and +L+ posting photographs and viewing status updates predicted +L+ the frequency of visits. It would seem from the present data +L+ that ‘keeping in touch’ may in actuality refer to ‘checking +L+ up on regularly’, while the ‘stickiness’ of the site (in terms +L+ of time spent on it) depends on use of the content and +L+ applications. This insight is clearly important for designers +L+ of social networking sites and associated content. If repeat +L+ visits are motivated by different uses and gratifications than +L+ the amount of time spent on the site, it is important to +L+ design content gratification alongside the ability to build +L+ and maintain social connections. It also suggests that the +L+ furor caused by the introduction of the newsfeed [2] has +L+ subsided, and been replaced by its new role as a ‘killer +L+ app’, at least in terms of repeat visits to the site. In many +L+ ways, this use of Facebook reflects the desire for ‘perpetual +L+ contact’ [15], and previously supplied by standalone +L+ services like Twitter [24]. While the social implications of +L+ this interest in perpetual contact and updates on ‘friends’ +L+ are beyond the remit of the present paper, it is worth noting +L+ that an increased awareness of others’ actions has +L+ potentially important implications for how we relate to +L+ others, and understand ourselves. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Design Implications +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The designers of social networking sites should consider the +L+ varied uses and gratifications reported by users, and need to +L+ recognize that not all users have the same uses of a social +L+ networking site, nor derive the same gratifications from +L+ their use. For instance, there are clear distinctions between +L+ the use of Facebook to maintain and re-create connections +L+ with friends, its use as a surveillance tool and for content +L+ delivery. There were also differences in reported uses by +L+ age, gender and occupational status. It may be that different +L+ demographic groups are motivated to use social networking +L+ sites for different purposes, with social connectivity and +L+ perpetual contact motivating younger (and female) users +L+ more than older (and male) users. +L+ The differing goals for the use of Facebook are reflected not +L+ only in usage patterns, but also in users’ privacy settings. +L+ People who have made their privacy settings more +L+ permissive are more likely to want to meet new people +L+ (they also score higher on the content gratifications scale). +L+ This is a designed aspect of the system – in both cases, to +L+ fulfill one’s goal often requires a more permissive approach +L+ to profile privacy. Many of the applications are social in +L+ nature (e.g. comparing oneself with others, asking questions +L+ to ‘friends’, viewing people from one’s neighborhood), and +L+ often circumvent elements of the default privacy settings. +L+ Similarly, if the goal is to meet new people, making one’s +L+ profile more open than by default allows others pursuing +L+ the same gratification to view your profile, and presumably +L+ increases the chances of an interaction. For these users, the +L+ profile within Facebook is likely to become a key self- +L+ presentation tool, rather than simply a way to ‘keep in +L+ touch’ with others [6, 26]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Limitations and Further research +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The present research is a ‘snap shot’ of Facebook users, and +L+ further work should consider the possibility of researching +L+ the development of use over time. In particular, it would be +L+ of interest to see how people’s uses and gratifications of +L+ Facebook develop, and if the frequency of visit is motivated +L+ by ‘perpetual contact’ over time. There is, for instance, +L+ considerable research in the field of habit formation that +L+ could inform the study of social network site use. HCI +L+ research should also consider ways in which the desire to +L+ meet new people, and to allow oneself to be viewed by +L+ strangers, can be accommodated in a privacy-protecting +L+ manner [14]. At present, Facebook has reasonably nuanced +L+ privacy controls. From the results of the present research, it +L+ would seem that users are changing the default privacy +L+ settings in a motivated manner. However, the present study +L+ only collected reported privacy settings. It would be +L+ prudent to complete research that actually examined +L+ settings via automated querying of the site [e.g. 13], or by +L+ studying a corpus of actual interactions [e.g. 11]. +L+ It should also be noted that the nature of the sampling +L+ method, and the self-selection of respondents, may have +L+ influenced the pattern of responses and overall levels of +L+ activity. Future research may wish to study a wider group +L+ of participants, or attempt to identify patterns of usage +L+ amongst non-respondents compared to respondents +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Users derive a variety of uses and gratifications from social +L+ networking sites, including traditional content gratification +L+ alongside building social capital, communication, +L+ surveillance and social networking surfing. The different +L+ uses and gratifications relate differentially to patterns of +L+ usage, with social connection gratifications tending to lead +L+ to increased frequency of use, and content gratifications to +L+ increased time spent on the site. The variety of uses to +L+ which Facebook is put by its users identifies particular +L+ challenges for the designers of such sites. For instance, a +L+ default privacy setting may be too restrictive for users +L+ seeking to meet new people, or who wish to allow new +L+ people to discover them. +L+ Since user’s desire to engage in surveillance of their peers +L+ also motivates the frequency of site visit, this also poses a +L+ unique challenge in balancing user’s privacy concerns and +L+ controls with a key raison d’être of social networking sites +L+ like Facebook. At present, Facebook allows users to +L+ manage their ‘feed’, removing ‘stories’ as they wish. This +L+ solution not only provides a degree of privacy control to +L+ users, but it also enables users to engage with the site as a +L+ self-presentation tool [26] at numerous levels – not only via +L+ their profile and network, but also through their activity +L+ (and the removal of specific ‘stories’). As perpetual contact +L+ continues to develop, designers will need to face the +L+ </SectLabel_bodyText> <SectLabel_page> 1035 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Online Social Networks	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> challenges of providing continual feeds between users, and +L+ the desire of users to control their self-representation via +L+ such sites. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGEMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Jeff Hancock, Mina Vasalou, Pam Briggs and Martin +L+ Weller are thanked for their advice and comments on an +L+ earlier draft of this paper, as are three anonymous reviewers +L+ and the associate chair. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. BBC News. Web Networkers at risk of fraud. 27 July +L+ 2007. http://news.bbc.co.uk/1/hi/uk/6910826.stm +L+ 2. Boyd, D. Facebook's Privacy Trainwreck: Exposure, +L+ Invasion, and Social Convergence. Convergence 14, 1 +L+ (2008). +L+ 3. Churchill, G. A paradigm for development of better +L+ measures of marketing constructs. Journal of Marketing +L+ Research, 16 (1979), 64-73. +L+ 4. Cashmore, P. MySpace hits 100 Million Accounts. +L+ Mashable Social Networking News, (2006). Available +L+ at: http://mashable.com/2006/08/09/myspace-hits-100- +L+ million-accounts/ +L+ 5. Donath, J. and Boyd, D. Public displays of connection". +L+ BT Technology Journal, 2, 4 (2004), 71-82. +L+ 6. Ellison, N., Heino, R. and Gibbs, J. Managing +L+ Impressions Online: Self-Presentation Processes in the +L+ Online Dating Environment. Journal of Computer- +L+ Mediated Communication, 11, 2 (2006). +L+ 7. Ellison, N., Steinfield, C. and Lampe, C. Spatially +L+ Bounded Online Social Networks and Social Capital: +L+ The Role of Facebook. Paper presented at the annual +L+ meeting of the International Communication +L+ Association, Dresden, June 2006. +L+ 8. Facebook Addiction. +L+ http://www.facebookaddiction.com/ +L+ 9. Flavelle, D. Worries follow rise of Facebook: +L+ Employers not happy with time spent on site. Toronto +L+ Star, May 04, 2007. Available at: +L+ http://www.thestar.com/Business/article/210313 +L+ 10. ForeverGeek. Debunking the MySpace Myth of 100 +L+ Million Users. +L+ http://forevergeek.com/articles/debunking_the_myspace +L+ _myth_of_100_million_users.php +L+ 11. Golder, S. A., Wilkinson, D. and Huberman, B.A. +L+ Rhythms of Social Interaction: Messaging within a +L+ Massive Online Network 3rd International Conference +L+ on Communities and Technologies, (2007). +L+ 12. Gonzalez, N. Facebook users up 89% over last year; +L+ Demographic shift (2007). +L+ http://www.techcrunch.com/2007/07/06/facebook-users- +L+ up-89-over-last-year-demographic-shift/ +L+ 13. Gross, R. and Acquisti, A. Information Revelation and +L+ Privacy in Online Social Networks. In Workshop on +L+ Privacy in the Electronic Society, ACM Press (2005). +L+ 14. Joinson, A.N. & Paine, C.B. Self-Disclosure, Privacy +L+ and the Internet. In A.N Joinson, K.Y.A McKenna, T. +L+ Postmes and U-D. Reips (Eds). Oxford Handbook of +L+ Internet Psychology (pp. 237-252). Oxford University +L+ Press (2007). +L+ 15. Katz, J.E., and Aakhus, M.A. (Eds.). Perpetual contact: +L+ mobile communication, private talk, public +L+ performance. New York: Cambridge University Press +L+ (2002). +L+ 16. Lampe, C., Ellison, N. and Steinfield, C. A Face(book) +L+ in the Crowd: Social Searching vs. Social Browsing. In +L+ proceedings of ACM Special Interest Group on +L+ Computer-Supported Cooperative Work, ACM Press +L+ (2006), 167 – 170. +L+ 17. Lampe, C., Ellison, N. and Steinfield, C. A Familiar +L+ Face(book): Profile Elements as Signals in an Online +L+ Social Network. In Proc. CHI 2007, ACM Press (2007), +L+ 435-444. +L+ 18. McGuire, W.J. Psychological motives and +L+ communication gratification (pp. 167-196). In J.Blunder +L+ & E. Katz (Eds.), The uses of mass communications: +L+ Current perspectives on gratifications research. Beverly +L+ Hills, CA: Sage (1974). +L+ 19. Nielsen//NetRatings. Facebook and Bebo: The assault +L+ on MySpace. Available from: http://www.nielsen- +L+ netratings.com/pr/pr_070628_UK.pdf +L+ 20. Rawstorne, T. How paedophiles prey on MySpace +L+ children. Daily Mail (UK), 21 July 2006. Available at: +L+ http://www.dailymail.co.uk/pages/live/femail/article.ht +L+ ml?in_article_id=397026&in_page_id=1879 +L+ 21. http://sambrook.typepad.com/sacredfacts/2007/06/faceb +L+ ook.html +L+ 22. Saucier, G. Mini-markers: A brief version of Goldberg’s +L+ unipolar big-five markers. Journal of Personality +L+ Assessment, 63, (1994), 506–516. +L+ 23. Stafford, T.F., Stafford, M.R., & Schkade, L.L. +L+ Determining uses and gratifications for the internet. +L+ Decision Sciences, 35, (2004), 259–288. +L+ 24. Twitter. http://www.twitter.com +L+ 25. Wellman B and Gulia M. The network basis of social +L+ support: A network is more than the sum of its ties, in +L+ Wellman B (Ed): ‘Networks in the Global Village’, +L+ Boulder, CO, Westview Press (1999). +L+ 26. Walther, J.B., Van Der Heide, B., Kim, S-Y., +L+ Westerman, D., Tong, S.T. The role of friends’ +L+ appearance and behavior on evaluations of individuals +L+ on Facebook: Are we known by the company we keep? +L+ Human Communication Research, (in press). +L+ </SectLabel_reference> <SectLabel_page> 1036 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> Sesame: Informing User Security Decisions +L+ with System Visualization +L+ </SectLabel_title> <SectLabel_author> Jennifer Stoll, Craig S Tashman, W. Keith Edwards, Kyle Spafford +L+ </SectLabel_author> <SectLabel_affiliation> Georgia Institute of Technology, School of Interactive Computing +L+ </SectLabel_affiliation> <SectLabel_address> Atlanta, Georgia 30332 +L+ </SectLabel_address> <SectLabel_email> jstoll@gatech.edu, {craig, keith, kyle}@cc.gatech.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Non-expert users face a dilemma when making security +L+ decisions. Their security often cannot be fully automated +L+ for them, yet they generally lack both the motivation and +L+ technical knowledge to make informed security decisions +L+ on their own. To help users with this dilemma, we present a +L+ novel security user interface called Sesame. Sesame uses a +L+ concrete, spatial extension of the desktop metaphor to +L+ provide users with the security-related, visualized system- +L+ level information they need to make more informed deci- +L+ sions. It also provides users with actionable controls to +L+ affect a system's security state. Sesame graphically facili- +L+ tates users' comprehension in making these decisions, and +L+ in doing so helps to lower the bar for motivating them to +L+ participate in the security of their system. In a controlled +L+ study, users with Sesame were found to make fewer errors +L+ than a control group which suggests that our novel security +L+ interface is a viable alternative approach to helping users +L+ with their dilemma. +L+ </SectLabel_bodyText> <SectLabel_keyword> Author Keywords: Security usability, security interface +L+ design, system visualization +L+ </SectLabel_keyword> <SectLabel_category> ACM Classification: H.5.2 User Interfaces, User-centered +L+ design; K.6.5 Management of Computing and Information +L+ Systems: Security and Protection +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> “AVG Update downloader is trying to access the Internet” +L+ “The firewall has blocked Internet access to your computer +L+ [FTP] from 192.168.0.105 [TCP Port 57796, Flags: S]” +L+ “[Your] AntiSpyware has detected that the Windows Net- +L+ BIOS Messenger Service is currently running. (This service +L+ should not be confused with the peer-to-peer Windows +L+ Messenger service, or MSN Messenger service which are +L+ used for Internet Chat). Beginning with Windows XP +L+ Service Pack 2, the Windows NetBIOS Messenger service... +L+ ...What would you like to do?” +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> The above are examples of actual alerts [15] that users are +L+ given from their anti-virus, anti-spyware and firewall tools. +L+ While some alerts are purely informational, most require +L+ users to make a decision. The choices they face are often +L+ “Always”, “This one time” or “Never,” posing the quandary +L+ of whether to suffer through even more messages or perform +L+ an action that may be irreversible. Further, the information +L+ given to help users make these decisions is often highly +L+ technical or vague (e.g., “Destination IP: 192.168.0.1: +L+ DNS” or “This program has changed since the last time it +L+ ran!”). Even when tools have a “More Info” button to +L+ provide access to more detailed information, that information +L+ is often confusing as well. +L+ How do users cope with such security decisions? Some turn +L+ to online research in an attempt to comprehend the alerts. +L+ This strategy is evident in the multitude of online forums +L+ where users ask questions, sharing their collective wisdom +L+ about such decisions (e.g., antionline.com or fo- +L+ rumz.tomshardware.com). In contrast, some cope by simply +L+ ignoring pop-ups or warnings from their security tools [20]. +L+ In fact some security books even advise users to turn off the +L+ annoying alerts; for example, one self-help security book +L+ quips, “the [stop alerts] button should say Shut Up, You are +L+ Driving Me Crazy” [15]. +L+ Simply put, users are asked to make decisions about things +L+ they do not understand, based on information that is difficult +L+ to comprehend. The poor decision making that (expectedly) +L+ is an outcome of this can result in dire consequences [18], +L+ including phishing attacks, bot infestations, and various +L+ forms of malware +L+ End-user security decisions present a troubling dilemma. On +L+ the one hand, because users must be involved in deciding +L+ how to balance security risks against the work they want to +L+ accomplish, many of these decisions are impossible to +L+ effectively automate [2, 3, 4, 5, 6] (e.g., as in the case of +L+ personal firewalls). On the other hand, the users who must +L+ make these decisions are generally uninterested in security as +L+ an end in itself [18] and, as noted, often have little useful +L+ information to help them make good decisions [4, 16]. +L+ Further, most of these decisions require a level of technical +L+ knowledge not possessed by most end-users. The key ques- +L+ tion this paper then explores is: since users must make +L+ security decisions (in particular, ones requiring system-level +L+ knowledge), how can we help them understand their system +L+ well enough to make better-informed security choices? +L+ </SectLabel_bodyText> <SectLabel_page> 1045 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 1: The Sesame ‘Behind-the-Scenes’ View running on a live system +L+ </SectLabel_figureCaption> <SectLabel_bodyText> We explore this issue of informed security decision making +L+ through Sesame, an interactive, visual, firewall-like tool +L+ designed to assist non-experts in making better informed +L+ security decisions. Sesame addresses threats similar to those +L+ of consumer firewalls, but uses a visual, direct manipulation +L+ interface that exposes system-level information in a meaning- +L+ ful, comprehensible way. To this end, Sesame provides users +L+ a ‘behind-the-scenes’ view of their computer, integrating +L+ existing elements of the desktop UI (such as windows) with +L+ previously hidden systems-level components (such as the +L+ processes that own those windows, the network connections +L+ those processes are making, and so forth). Essentially, the +L+ desktop metaphor is extended to convey system-level con- +L+ cepts in terms of their relationship to familiar desktop-level +L+ abstractions. We conjecture that this view can help to inform +L+ user security decisions and, by lowering barriers to this +L+ information, may help to motivate them as well [18]. +L+ Our initial study results suggest that this behind-the-scenes +L+ view of their system’s underlying architecture seems to help +L+ users make better security decisions involving system-level +L+ knowledge. In the sections that follow, we briefly survey +L+ current research and commercial security tools. We then +L+ describe the design of the Sesame UI and present the results +L+ of our user study. We conclude with a discussion of our +L+ study results and implications for future work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> While there are many security tools available, few are +L+ designed specifically to support end-users in their security +L+ decision making process. However, of the tools available for +L+ security decision making, many can be grouped into two +L+ categories: those for expert users and those for non-experts. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Experts +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Visualization tools: The majority of prior research focuses on +L+ visualization-based approaches intended for the expert user. +L+ While inappropriate for non-experts, they do illustrate types +L+ of information that experts find useful in order to detect +L+ security problems and make informed decisions with regard +L+ to security. For example, many of these tools support moni- +L+ toring of network connections; these include Rumint, IDS +L+ Rainstorm, VisAlert [7], and others that provide experts with +L+ a variety of useful network data representations [1]. How- +L+ ever, these tools are highly technical and complex +L+ (presenting data at the level of individual packets), and do +L+ not integrate with, or build on, existing metaphors of the +L+ desktop GUI. +L+ Text-based tools: Another set of very common tools provides +L+ extensive system behavior and status information in a text- +L+ based approach. Although these systems often ship on +L+ consumer computing platforms, they are generally intended +L+ for knowledgeable users or even system- or network- +L+ administrators. These include tools like ProcessExplorer, +L+ tcpview, and Windows Task Manager [22]. Again, these +L+ tools present system-level information useful in making +L+ informed security decisions. However, they convey informa- +L+ tion in a piecemeal fashion (different tools for different +L+ information), leaving users to assimilate and make sense of +L+ it; and the textual presentation neither supports visual meta- +L+ phors nor integrates with the existing and well-known +L+ features of the desktop UI. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Non-Experts +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In contrast to the wealth of tools available for network and +L+ security professionals, research in the area of supporting +L+ </SectLabel_bodyText> <SectLabel_page> 1046 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> security decision making for end users remains nascent [4, +L+ 18]. Discussed below are two classes of such tools. +L+ Tools for specific activities: Some tools for non-experts +L+ provide support only for security decisions that relate to a +L+ specific user activity, e.g., browsing, searching, making +L+ online transactions, sharing files, and so forth [2, 3, 21]. For +L+ example, Web Wallet [19] can be used to ensure that a user’s +L+ information will be sent to the desired site rather than to a +L+ spoof. While these tools help support security decisions in +L+ the context of a specific activity, they do not address the +L+ range of ‘system level’ security situations that are independ- +L+ ent of what the user is doing. Consequently, security +L+ decisions which do not map to a specific activity are largely +L+ overlooked by these tools. For example, a spyware or bot +L+ infection can severely endanger a user’s privacy and security +L+ regardless of what actions the user is performing [13,9]. +L+ Further, since different tools are employed for different +L+ activities, the level of security, as well as the user interface, +L+ can easily be inconsistent. This is not to say that security +L+ tools are only effective if they support all security decisions +L+ across all activities, but rather that activity-specific tools are +L+ not a complete answer to end-user security. +L+ Tools for specific threats: Other tools provide support for +L+ user decisions based on specific types of threats such as anti- +L+ phishing toolbars, anti-virus, -adware, and –spyware sys- +L+ tems. These tools tend to be based on heuristics or black lists +L+ and will only provide protection for against attacks matching +L+ their heuristics or that are on their lists. Users must con- +L+ stantly update these tools in order for them to be effective; +L+ and the way in which they alert users to problems is also +L+ problematic, as evidence shows that users often find the +L+ information in these alerts to be difficult to understand and so +L+ ignore them [20, 4]. +L+ Consumer firewalls are perhaps the most common types of +L+ security software that involves explicit user decision-making. +L+ These systems protect against a range of network-based +L+ attacks, such as worms that exploit software vulnerabilities. +L+ These systems are not task-specific, but present system-level +L+ information in a way that is often undecipherable to users +L+ (e.g., describing connection attempts in terms of process +L+ names, IP addresses, and port numbers). Users therefore +L+ have little actionable information on which to base security +L+ decisions when faced with firewall popups. +L+ Thus, there are few systems that attempt to provide users +L+ with a framework for making security decisions that are not +L+ tied directly to a specific activity, but instead help with +L+ overall system security. Our goal with Sesame is to fill this +L+ void; to provide general, firewall-like security (not, for +L+ example, detecting of malware at the point of installation), +L+ while helping users to make informed decisions. We focused +L+ specifically on handling common classes of attacks where +L+ user action is necessary to determine the correct course of +L+ action. These include situations where there is no universally +L+ correct action, but rather a tradeoff between security and +L+ convenience that the user must consider, or situations where +L+ the correct action is dependent on context outside of the +L+ system. Thus, unlike current firewalls—which convey +L+ systems-related information textually, using low-level +L+ technical details—Sesame provides a holistic graphical +L+ representation of security-related system information in a +L+ way designed to be understandable by end-users. Sesame +L+ also provides a means for exploration of the underlying +L+ system, allowing it be used for a greater range of purposes +L+ than traditional consumer firewalls. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DESIGN PROCESS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we discuss our iterative design process and +L+ choice of the representational paradigm for Sesame. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Representational Paradigm +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our overarching goal with Sesame is to convey a visual +L+ model that allows security-related system information to be +L+ meaningfully interpreted by the user. To do so, we chose a +L+ direct manipulation, model-world paradigm, as this is known +L+ to have significant benefits for learnability [8]. Within this +L+ interface we sought to 1) show how important but unfamiliar +L+ abstractions (e.g., processes and networks) relate to abstrac- +L+ tions that are familiar and meaningful to users (e.g., windows +L+ and real-world places); and 2) provide users with actionable +L+ controls that enable them to affect their security state using +L+ the information given. +L+ We believed a spatial, direct-manipulation interface would +L+ yield a number of important benefits, the first of which is to +L+ leverage existing knowledge. Our target, non-expert user +L+ already typically interacts with the computer through the +L+ spatial metaphor of the desktop GUI, which allows us to +L+ leverage that experience to convey other, more complex +L+ relationships via a similar spatial GUI. More subtly though, +L+ desktop objects are familiar and meaningful to users; by +L+ employing an interface that is metaphorically compatible +L+ with the desktop, it should be easier to represent relationships +L+ between esoteric security abstractions and the familiar +L+ windows and things found on the desktop. The second +L+ reason to use a visual representation is speed. The human +L+ visual system has an enormous facility for the rapid assess- +L+ ment of visual scenes [ 14]. By presenting data in this way, as +L+ opposed to a more verbose text-based approach, we hope to +L+ minimize the time users need to assess their security situation +L+ in any particular instance—thereby reducing one of the +L+ known barriers to user motivation for understanding security +L+ [18]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Information Content +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Portraying all low-level information about the state of the +L+ user’s system would not only potentially be overwhelming, +L+ but also likely unnecessary for protection against the classes +L+ of threats we are targeting. Thus, we do not attempt to depict +L+ all possible system information, but focus instead on a +L+ smaller subset selected through an analysis of whether the +L+ information is both practically accessible and relevant in +L+ addressing the common types of security threats we are +L+ targeting (spyware, phishing, and bot infections). +L+ </SectLabel_bodyText> <SectLabel_page> 1047 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> Thus, the information we depict includes: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Process characteristics, including depictions of which +L+ windows are associated with a given process, average and +L+ current CPU usage, putative vendor, whether that vendor +L+ can be confirmed, and process installation date; +L+ •	Network characteristics, including incoming and outgoing +L+ connections; +L+ •	Remote systems, including domain, putative owning +L+ organization, and putative geographical location. +L+ </SectLabel_listItem> <SectLabel_bodyText> Although far from comprehensive, the above data set pro- +L+ vides enough information to identify and potentially mitigate +L+ many variants of the three types of attacks on which we +L+ focus. Spyware, for example, could be identified by routine +L+ attempts at establishing connections from suspicious proc- +L+ esses targeted at suspicious remote servers. A bot infection +L+ could be detected similarly, except in some cases the remote +L+ systems would be initiating the connections. In this sense, +L+ Sesame goes further than current firewalls, enabling users to +L+ proactively explore security risks. For example, some phish- +L+ ing scams could be identified by observing one’s web +L+ browser connecting to servers that appear unaffiliated with +L+ the nominal proprietor of the website. We used situations +L+ like these to evaluate Sesame as described below in our User +L+ Study section. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Formative Study +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To get feedback on potential approaches for visual presenta- +L+ tion and terminology, we conducted a small formative study +L+ early in our design process. This formative study was in- +L+ tended to provide feedback on (1) user preferences with +L+ respect to concrete versus abstract representations of system +L+ information, (2) ability of users to decipher relationships +L+ among system information, and (3) preferred terminology for +L+ technical concepts. We showed three users paper prototypes +L+ of two designs, one using a concrete, spatial visualization +L+ similar to that of figure 1, and another using more a more +L+ abstract presentation. Participants overwhelmingly preferred +L+ the more concrete representation, and understood many of +L+ the relationships being conveyed (such as relationships +L+ between processes and windows) without explanation by the +L+ experimenters. +L+ This brief, early study was also helpful in revealing what +L+ participants did not understand. For example, participants +L+ had difficulty understanding that a part of the visualization +L+ represented physically remote computers on the Internet. Our +L+ discussions with participants also informed a number of +L+ elements of our final design, such as the use of geographic +L+ maps to suggest remoteness. Other feedback led to signifi- +L+ cant changes in the arrangement of processes used in the +L+ final version of Sesame. Finally, an additional discovery we +L+ made during the formative study was that the term ‘process’ +L+ caused significant confusion for our non-technical users. +L+ Based on participant feedback, we chose the term ‘engine’ as +L+ one that made more sense to them. (We acknowledge, +L+ however, that using such non-standard terms may confuse +L+ the more ‘technical’ users.) +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> SESAME: EXTENDING THE DESKTOP +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we describe how the system level information +L+ is presented in Sesame, and the controls given to users for +L+ interacting with the tool. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Invocation - Getting ‘behind-the-scenes’ +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Among the first design choices we faced was deciding where +L+ to place Sesame in relation to the desktop. Our requirement +L+ to connect existing elements (such as windows) to Sesame- +L+ provided elements (such as processes) meant that the tool +L+ somehow had to be integrated into the desktop. However it +L+ could not appear to be ‘just another application’ running on +L+ the desktop, because we needed to convey that Sesame is a +L+ level of abstraction ‘below’ the conventional GUI. +L+ To resolve this challenge, users can access Sesame either by +L+ invoking the always-present “Open Sesame” button on the +L+ desktop, or through a dialog box that appears when Sesame +L+ needs to interact with the user—for example, when a process +L+ requests a connection (the user may ignore the dialog box if +L+ they do not wish to respond). When Sesame is invoked, the +L+ user’s desktop GUI rotates about the vertical axis to reveal +L+ the processes, networks, and other system-level elements +L+ behind it (Figure 1). To exit from the view, users click the +L+ exit button at the top of the screen. A benefit of this smooth +L+ rotation effect is that it provides a continuous transition from +L+ the desktop interface to Sesame to keep users from becoming +L+ disoriented. +L+ Rotating the entire desktop to display the Sesame visualiza- +L+ tion raises several problems. The first is that a full-screen +L+ visualization is heavyweight, and may dissuade users from +L+ invoking it when they do not have to. It further means that +L+ the user cannot have the Sesame visualization visible while +L+ performing other tasks. These issues of motivation and +L+ efficiency may be important to explore in the future, but at +L+ this stage we are principally concerned with effective infor- +L+ mation representation. A full screen visualization helps us +L+ with this by offering conceptual advantages in showing the +L+ desktop/system-level separation, as well as the practical +L+ advantages of providing abundant space for the system-level +L+ components we wish to show. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Division of Space +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Once behind the desktop, we faced another design challenge. +L+ The effect of rotating the desktop is intended to suggest that +L+ the revealed objects are part of the user’s PC. Yet to convey +L+ abstractions such as remote computers, we also needed to +L+ represent areas that lie outside the user’s PC. Thus, Sesame +L+ provides a clean division where one side of the visualization +L+ (furthest from the rotated desktop) contains all external +L+ elements such as remote computers, and the other side +L+ (containing the rotated desktop) represents the internal +L+ elements of the user’s PC; both regions are labeled and are +L+ colored differently to emphasize the distinction. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Visual Elements +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In addition to the rotated desktop, the Sesame visualization +L+ contains several distinct types of visual elements: proc- +L+ esses, remote computers and connection requests. As +L+ </SectLabel_bodyText> <SectLabel_page> 1048 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> shown in Figure 1, blue cubes (representing window- +L+ owning processes) are connected via arrows to each of the +L+ windows on the rotated desktop. Directly beneath them are +L+ the non-window-owning processes, referred to as ‘back- +L+ ground’ processes. On the right, in the ‘external’ region, +L+ are representations of remote computers that are connected +L+ via arrows to the process with which they are communicat- +L+ ing. However, before the remote computers can connect to +L+ the user’s system, a connection request is given to the user +L+ with options to ‘Allow’ or ‘Forbid’ the connection. Also, in +L+ the lower right corner is a ‘More Info’ space providing +L+ brief descriptions when the user hovers over the various +L+ visual elements. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Processes +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Processes are arranged either floating parallel to the rotated +L+ desktop or lying on the ground, depending on whether they +L+ own windows (‘foreground’ processes) or not (‘back- +L+ ground’). To highlight the difference between them, the +L+ foreground processes are larger and in blue; the back- +L+ ground processes are smaller and in green. This difference +L+ in representation was the result of our earlier formative +L+ user studies: we found that users preferred significant +L+ visual differences between the process types. +L+ To avoid deluging users with processes, only certain ones +L+ are shown: 1) we show all processes that own windows as +L+ we want to suggest to users that all of their interaction with +L+ the computer is mediated by processes; 2) we show all +L+ processes that have ever connected to the network; and 3) +L+ we cull any process that is a known-safe component of the +L+ Windows OS, unless it is being controlled by another, +L+ untrusted process. +L+ Much of the security-related, system information provided +L+ is actually conveyed with the process cube. On its face is +L+ the name of the executable from which it was started. +L+ There is also a small square, a rectangular bar graph, and a +L+ gauge (Figure 2a). The colored square indicates whether +L+ the vendor of the executable could be verified (green for +L+ yes, yellow for no); the bar graph indicates how long the +L+ executable has been installed on the user’s system; and the +L+ gauge indicates both current and average CPU usage. +L+ These three indicators provide users with a fast way of +L+ judging if a process is behaving abnormally. Part of Ses- +L+ ame’s design intent is to help users identify abnormal +L+ behavior by developing a sense of what normal behavior +L+ looks like via the three indicators. To augment their ability +L+ to have a sense of ‘normal’ and in keeping with the safe- +L+ staging principle [17], we also use in-place semantic +L+ zooming, allowing users to learn more information when +L+ they want. When users hover over a process cube with the +L+ mouse, it expands slightly to show small explanatory text +L+ labels next to each of the glyphs on the cube’s face (Figure +L+ 2b). To obtain an even more complete explanation, users +L+ can click the cube, expanding it to a full-sized representa- +L+ tion, which also contains an editable list of security policies +L+ applied to the process (Figure 2c). Consequently, users can +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2a: unexpanded	Figure 2b: view of process +L+ process	when hovered over +L+ Figure 2c: process when clicked upon +L+ Figure 3b: Process-to-remote system arrow +L+ </SectLabel_figureCaption> <SectLabel_bodyText> visually determine whether a given process’ characteristics +L+ are unusual, then zoom in to learn what they actually mean. +L+ Coming out from the edges of processes are wide arrows +L+ joining them to the windows they own and the remote +L+ computers with which they are communicating (Figures 3a, +L+ 3b). The intention here is to connect—both conceptually +L+ and diagrammatically—concepts in the Sesame interface +L+ with familiar related concepts on the user’s desktop and in +L+ the real world. The arrows are rendered in bright colors +L+ with an extruded, gradated appearance to draw attention +L+ and make them feel concrete. We chose double-headed +L+ arrows to imply that information flows both ways. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Remote Computers +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The remote computers that act as either servers or, less +L+ likely, clients to the user’s computer are represented as +L+ stacked rectangular tiles on the right of the screen. In +L+ earlier versions of Sesame, we tried to depict the remote +L+ computers as actual concrete renderings of computers; but +L+ as it was not clear to users that these were separate from +L+ their PCs. Users tended to feel that a better way to suggest +L+ distantness would be to use maps, which prompted us to +L+ render remote computers as abstract map images overlaid +L+ with pointers to the putative location of the remote system. +L+ The arrows leading from the processes to the remote +L+ system tiles terminate over the geography of the remote +L+ system, and the panels include a textual description (e.g., +L+ “Remote computer owned by XYZ Corp”). +L+ Figure 3a: Process-to-window arrow +L+ </SectLabel_bodyText> <SectLabel_page> 1049 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 4: expanded remote system tile +L+ </SectLabel_figureCaption> <SectLabel_bodyText> As with processes, remote system tiles also allow semantic +L+ zooming. When the mouse hovers over a tile, it expands to +L+ reveal the domain, owner, and more precise geographic +L+ location associated with the remote system; the information +L+ is obtained via a reverse DNS lookup and query of a WHOIS +L+ database of domain registrations (Figure 4). +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Connection Requests +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> When an attempt is made to establish a new incoming or +L+ outgoing connection, the firewall component of the Sesame +L+ backend intercepts the request and Sesame asks the user for +L+ permission. Sesame represents the connection attempt by +L+ showing a double headed arrow between the local process +L+ and remote system tile, but the arrow is solid only on the side +L+ initiating the request, with the other end of the arrow dotted +L+ (Figure 5). There is a gap between the two ends of the arrow +L+ to suggest that they could be, but are not yet, connected. To +L+ make the user’s options more clear, the buttons where the +L+ user can accept or forbid the request appear at this +L+ solid/dotted junction. In order to draw the user’s attention to +L+ the choice, the red border around the buttons flashes for +L+ several seconds after Sesame is invoked. To ensure that users +L+ see the process associated with the connection request, a +L+ border flashes around it as well. +L+ Like current firewalls, Sesame allows the user to set policies +L+ for allowing or denying connections; but unlike most fire- +L+ walls, we try to make changes affecting future policy +L+ obvious by representing policies concretely, and visually +L+ associating them with the objects to which they apply. When +L+ the user selects the ‘accept’ or ‘forbid’ buttons, a policy +L+ dialog box is shown animating out of the relevant process +L+ cube and expanding to full size on screen. The user can +L+ select from four policy choices, allowing or denying connec- +L+ tions on a one time basis or indefinitely, and with respect to +L+ any remote computer or just those associated with a particu- +L+ lar domain. Once the user confirms their policy choice, the +L+ selected policy (represented as a card) is animated to disap- +L+ pear directly into the associated process, conveying that the +L+ policy is ‘inside’ that cube and is later accessible from it. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Usage Scenarios +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To demonstrate how Sesame might be used, we include two +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5: Remote computer requesting connection to local +L+ process. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> usage scenarios in which we show similar situations, one +L+ where the appropriate action is to allow a connection so that +L+ the user can continue with her task; and one where the +L+ appropriate action is to deny it in order to fend off an attack. +L+ In the first, the user is asked to answer an explicit question +L+ by Sesame about whether to allow access to a remote system. +L+ In the second, the user invokes Sesame herself to try to +L+ decide whether a website is fraudulent. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Scenario 1: Should I allow this connection? +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In the first scenario, the user is browsing a web page contain- +L+ ing an embedded video. Upon selecting the ‘Play’ button in +L+ the embedded video player, Sesame brings up a dialog box +L+ indicating a remote computer is trying to initiate communica- +L+ tion with a local process, and the user is directed to click a +L+ button in the dialog box to open Sesame. When clicked, the +L+ user’s screen rotates to reveal that the process, to which the +L+ remote system is trying to connect, is the same process that is +L+ running her web browser. Noting the temporal coincidence +L+ between selecting the ‘Play’ button and the appearance of the +L+ dialog box, she suspects that the new connection is needed to +L+ allow the video to play. Unsure if watching the video is +L+ worth the risk of letting a remote system connect to her +L+ computer, she hovers over the remote system and observes +L+ that it is owned by the same company associated with the +L+ website—a company she is familiar with and feels reasona- +L+ bly comfortable trusting. She clicks the ‘Allow’ button to +L+ permit the connection to proceed. However, as she does not +L+ feel comfortable giving the company free reign to connect to +L+ her computer, she assigns a permission card allowing the +L+ connection just once. The yellow connection arrow now +L+ becomes completely solid and the user clicks the ‘Exit and +L+ Return to Windows’ button which rotates her desktop back +L+ to its normal position and the video plays as expected. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Scenario 2: Is this a phishing site? +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In the second scenario, we begin with a user who is directed +L+ to a website claiming to be affiliated with their bank. When +L+ the site asks for personal information, the user becomes +L+ suspicious and invokes Sesame. Sesame shows the rotated +L+ desktop allowing the user to see which process is connected +L+ to their web browser window. They notice that the same +L+ process is connected to one remote computer. The small map +L+ shows that the remote computer is located in an unfamiliar +L+ geography and the name of the owner of the remote com- +L+ puter seems unrelated to the user’s bank. Still uncertain, the +L+ user moves their mouse to the remote computer to find out +L+ more information. When the mouse hovers over the remote +L+ system, it zooms to show it is associated with an unknown +L+ domain, and located at an address in a distant country. The +L+ user becomes concerned that they may not really have visited +L+ their bank’s website, and therefore closes their browser and +L+ calls the bank. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> IMPLEMENTATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Sesame is implemented in C++ and runs on unmodified +L+ Microsoft Windows XP. All screenshots in this paper were +L+ taken from live execution of the system. The front-end +L+ </SectLabel_bodyText> <SectLabel_page> 1050 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> interface of the system uses the standard windows GDI to +L+ render most graphics, with GDI+ used for more complex +L+ effects, such as the gradient fills on the extruded arrows. The +L+ front-end also includes a series of hooks allowing it to be +L+ driven from synthetic data sets, as this capability was neces- +L+ sary for our user study. In order to modify the datasets in real +L+ time, Sesame registers several global hotkeys allowing us to +L+ use a second keyboard to control the data Sesame shows to +L+ study participants. The Sesame front-end gathers process list +L+ data; the back-end is used to retrieve network status. It also +L+ includes a simple open-source firewall that is used to inter- +L+ cept attempts at initiating network connections, and performs +L+ reverse-DNS and WHOIS lookups to obtain information +L+ about remote systems. As we did not need complete func- +L+ tionality for our evaluations, certain backend features related +L+ to firewall operations, process culling, CPU usage, and +L+ vender verification were not fully implemented, but were +L+ mocked up in the front end. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> USER STUDY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We conducted a study to evaluate how well users could +L+ judge potential security threats using the visualization +L+ provided by Sesame. Our primary interest was in how well +L+ users could cumulatively leverage the concepts presented in +L+ Sesame to make security-related decisions. We chose to +L+ evaluate these concepts together rather than piecemeal, as the +L+ experience of our formative study suggested that the differ- +L+ ent concepts underlying Sesame are heavily interdependent. +L+ To perform this cumulative evaluation, we asked users to +L+ make security decisions in an environment with Sesame, as +L+ well as a more conventional environment using the +L+ ZoneAlarm firewall instead. To assess users’ understanding +L+ of the different ideas in Sesame, we asked various questions +L+ about different parts of the visualization and about why users +L+ made particular choices. Note that our focus in this study was +L+ to understand how the quality of decision-making was +L+ affected by our visual interface, not on evaluating Sesame’s +L+ efficiency as a UI (whether it allows faster decision-making), +L+ or its impact on user motivation (whether it incents people to +L+ be more active in security management); we leave these as +L+ future work. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Participants +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our study included a total of 20 participants recruited from a +L+ university campus: 45% female (9 subjects) and 55% male +L+ (11 subjects). All were undergraduate students; none were +L+ computer science or computer engineering majors, and none +L+ considered themselves to be experts in computer operation. +L+ While those pursuing undergraduate degrees may not ideally +L+ represent typical users, we found that our participants were +L+ indeed unfamiliar with basic concepts of computer operation +L+ such as processes, and security threats such as phishing. We +L+ therefore felt they were adequately representative. We used +L+ between-subjects testing, so participants were divided evenly +L+ between control and experimental groups. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Security Tasks +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To determine how well participants could identify security +L+ threats, we gave them several tasks. For each task, they +L+ effectively had to judge whether a given situation posed a +L+ security threat. The tasks were based on the types of deci- +L+ sions users must make in real world use; the first four were +L+ common personal firewall configuration decisions, the latter +L+ two required judging the authenticity of websites. The tasks +L+ included: T1) allow or forbid an incoming connection from +L+ Microsoft.com after clicking on a video player link; T2) +L+ allow or forbid an outgoing connection from a spyware +L+ process named loadsys.exe; T3) allow or forbid an incoming +L+ connection from a bot server requesting connection with a +L+ bot process named intmonp.exe; T4) allow or forbid an +L+ outgoing connection from the process named outlook.exe; +L+ T5) determine if the website claiming to be Mid America +L+ Bank is a phishing site; T6) determine if the website claiming +L+ to be CitiBank is a phishing site. T5 and T6 are intentionally +L+ similar but whereas T5 represents a threat, T6 does not—this +L+ allows us to test for false positives as well as false negatives. +L+ We balanced the tasks between situations that are triggered +L+ by user activity (T1, T5, and T6) and those with no direct +L+ relationship to the user’s actions (T2, T3, and T4). +L+ Participants performed the security tasks in either the control +L+ or the experimental environment. The control environment +L+ was made to reflect the most common configuration used by +L+ non-experts [15], a typical personal computer with the +L+ ZoneAlarm firewall. In addition to its ubiquity, we used +L+ ZoneAlarm in the control because, like Sesame, it provides +L+ firewall functions and therefore offers many of the same +L+ actionable controls, and asks many of the same questions, as +L+ Sesame. It also embodies the textual, indirect interaction +L+ paradigm employed in most end-user security tools. Further, +L+ the firewall tasks (T1-T4) were derived from actual prompts +L+ ZoneAlarm presents to users. Also, rather than use a live +L+ installation of ZoneAlarm, we made an interactive mockup +L+ using actual screenshots from ZoneAlarm alerts—this was to +L+ ensure that we could control the timing and exact content of +L+ the dialog boxes. We integrated interactive widgets into our +L+ mockup as well so that it would behave, as well as appear, +L+ virtually identical to the actual tool. +L+ Participants in the experimental group used the same system +L+ as those in the control, except that Sesame was provided in +L+ place of ZoneAlarm. Because Sesame offers an unusual user +L+ interface, we gave participants about 90 seconds to explore it +L+ before beginning the tasks. We provided no explanation +L+ about the meaning of Sesame’s behind-the-desktop visual +L+ elements; we only demonstrated how one could hover over +L+ and click on some objects to semantically zoom in on them. +L+ Similarly to ZoneAlarm and other firewalls, for tasks T1-T4, +L+ Sesame alerted the user to the security situation by bringing +L+ up a dialog box. From there the user would click a button to +L+ enter the Sesame UI. For T5-T6, the experimenter explained +L+ the situation and asked the user to click the always-present +L+ “Open Sesame” button to enter the Sesame interface. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Study Details +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> As each participant began the study, they were asked a series +L+ </SectLabel_bodyText> <SectLabel_page> 1051 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> of background questions to assess their familiarity with +L+ computers and security tools. Before any tasks were pre- +L+ sented to them, participants were instructed to think aloud +L+ about what information they would be using in making their +L+ decisions. All participants used the same Dell laptop with a +L+ 15” monitor running Windows XP Home edition. An exter- +L+ nal keyboard was used by the experimenters to bring up the +L+ alerts for each security task. The participants were asked to +L+ make security decisions as if the laptop being used was their +L+ own. +L+ As discussed above, for the first security task participants +L+ were asked to download a video from the Microsoft website +L+ which caused an alert dialog box from the firewall to pop-up. +L+ After completing the first task, participants were instructed to +L+ browse to any website of their choosing. They were then +L+ presented with the next three tasks while they browsed +L+ online. Upon the completion of each task, the participant +L+ continued browsing until the next one was presented to them. +L+ We were careful during these tasks not to cause a popup to +L+ appear immediately after the user took an action (e.g., +L+ clicking a link) so that they would not erroneously think they +L+ caused the popup in those cases where they did not. For the +L+ final two tasks, none of the participants were familiar with +L+ the term ‘phishing site’ so a brief explanation was given. +L+ Since we were assessing the intelligibility of the Sesame +L+ visualization, we declined to answer participant questions +L+ about the meanings of Sesame’s visual elements. We asked +L+ them to infer as best they could with the information given. +L+ Participants were asked to think aloud during their decision +L+ making process for the six tasks, in order to aid us in under- +L+ standing their reasoning processes. Upon completion of all +L+ tasks participants were asked follow-up questions in a semi- +L+ structured interview regarding the choices they made. We +L+ asked about the clarity of the decisions they had to make, the +L+ choices presented to them and the information provided by +L+ the security tool they used (i.e., Sesame or ZoneAlarm). We +L+ also asked for suggestions for improving the security tools. +L+ Each subject participated in the study individually and was +L+ voice recorded to capture responses to the interview ques- +L+ tions and the think aloud. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> For each participant, we gathered the following data: 1) +L+ background information, 2) the participant’s ‘miss-rate’ for +L+ the six tasks, and 3) responses to follow-up questions regard- +L+ ing comprehension of the system information provided. We +L+ define miss-rate to be the number of potential threats the +L+ participant evaluated erroneously (i.e., judging a threatening +L+ situation to be safe or vice versa). +L+ Participant security background: Based on the background +L+ information gathered, both the control and experimental +L+ groups were similar in terms of experience with computers +L+ and security tools. Participants in the control group had an +L+ average of 10.4 years of experience using the computer and +L+ only 2 out of 10 participants did not use any security tool +L+ (e.g. Norton Anti-virus, firewalls or anti-spyware). Similarly, +L+ participants in the experimental group had an average of 9.2 +L+ years of experience using the computer and 2 out of 10 did +L+ not use any security tools. Subjects in both groups were +L+ asked to briefly describe their security practices besides +L+ using security software. +L+ Based on the responses in both groups, the majority of the +L+ participants did not have or could not recall additional +L+ security practices they performed besides making occasional +L+ updates to their security software when alerted to do so by +L+ their tools. Further, it seems that the amount of time spent +L+ using a computer does not necessarily translate into familiar- +L+ ity with the system knowledge needed to make security +L+ decisions. Although participants’ computer usage averaged +L+ 9-10 years, all were unfamiliar with basic system-level +L+ concepts, e.g., processes, network connections. +L+ Security task miss-rate: Since our data were non-parametric, +L+ discrete, and did not appear to fit a known distribution, we +L+ used a two-tailed Mann-Whitney test to determine if the +L+ difference in the miss-rate between the ZoneAlarm (control) +L+ and Sesame (experimental) groups was statistically signifi- +L+ cant. In the table below, we summarize the miss rate for both +L+ groups of participants: +L+ </SectLabel_bodyText> <SectLabel_table> 	ZoneAlarm group miss- rate (6 males, 4 females)	Sesame group miss-rate (5 males, 5 females) +L+ Females	45.8%	25% (with outlier* 30%) +L+ Males	36%	20% +L+ Miss-rate	40%	22.2% (with outlier* 26.6%) +L+ *Outlier miss-rate: 66.7% +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1. Participant miss-rate for security tasks +L+ </SectLabel_tableCaption> <SectLabel_bodyText> Of the 20 participants, we identified one outlier whose miss- +L+ rate was 66.7%. During the study, this participant expressed +L+ concern about completing the tasks because s/he was very +L+ unfamiliar with the Windows environment given that the +L+ participant’s primary computer was a Macintosh. +L+ Cumulatively over all of the tasks, the experimental group +L+ performed significantly better than the control group. Includ- +L+ ing the outlier, the miss-rate for the group using Sesame is +L+ 26.6% while the miss-rate is 40% for the control group +L+ (Z=1.97, P=0.05). (When the outlier is excluded, the Sesame +L+ group miss-rate is reduced further to 22.2%, Z=2.53, +L+ P=0.05). Over just the firewall-like tasks (T1-T4), the +L+ experimental group using Sesame performed 41 % better than +L+ the control group; but if the outlier is included, the experi- +L+ mental group performs just 20% better. However, when tasks +L+ were considered individually or in smaller sets, we did not +L+ have statistically significant differences between groups due +L+ to our relatively small sample size. Figure 6 shows the +L+ success rates by task, including the outlier. +L+ Information Comprehension: After the completion of the six +L+ security tasks, we interviewed participants to understand +L+ their reactions to the tasks they performed and, for the +L+ experimental group, to assess how much of Sesame they +L+ understood. Most of the participants in the control group +L+ stated that they were uncertain how to use the information +L+ </SectLabel_bodyText> <SectLabel_page> 1052 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figure> T1	T2	T3	T4	T5	T6 +L+ Task (T) +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6: Participant hit-rates for security tasks +L+ </SectLabel_figureCaption> <SectLabel_bodyText> provided by ZoneAlarm in making their security decisions. +L+ Also, many of the participants (seven out of ten) said they +L+ were following a specific strategy in making allow/deny +L+ decisions. Five of the seven were either allowing or denying +L+ every connection request. For example, one participant said +L+ “I don’t want to read all this stuff so I’m just going to deny +L+ it. If I allow it, I may have to do something else.” The +L+ remaining two of the seven allowed or denied based on +L+ whether they could recognize the process name. Even though +L+ all the participants had access to a lot of information pro- +L+ vided by Zone Alarm, only five participants in total looked at +L+ the process name given to help with their decision. The more +L+ info button was consistently ignored. +L+ Cumulatively, as discussed above, the experimental group +L+ performed significantly better than the control group. One +L+ contributing factor for this may have been that none of the +L+ participants reported using a predetermined strategy in +L+ making decisions. But perhaps more importantly, Sesame +L+ users appeared to employ more of the information provided +L+ than the control group in making their allow/deny decisions. +L+ Sesame users reported using information about the processes +L+ themselves as well as the remote systems to which they were +L+ connecting. In contrast, control users tended to rely on +L+ process names only. We suspect that this may be because +L+ Sesame provided a more accessible explanation of the +L+ general significance and specific facts about remote systems. +L+ For example, five participants found the geography informa- +L+ tion provided on remote systems to be helpful. Participants +L+ also reported that the type of language used in Sesame was +L+ helpful, as well as the permission cards in the allow/deny +L+ dialog box—though it is not clear that the latter specifically +L+ aided them in performing the tasks. +L+ To assess the extent to which participants understood the +L+ conceptual model Sesame provides, we asked them to +L+ describe the visualization. We specifically inquired about the +L+ nature of the visual elements such as processes, and about the +L+ distinction between areas representing things within the +L+ computer versus outside of the computer. We found that 8 of +L+ the 10 users understood the basic significance of the cubes +L+ representing foreground processes. Background processes +L+ posed a challenge, with only 2 users understanding their +L+ purpose. We were surprised by this finding, as we explicitly +L+ included a textual description of the distinction between the +L+ two types of processes within the UI. The representations of +L+ the remote computers were understood well, with 8 partici- +L+ pants recognizing their meaning. Eight participants also +L+ understood the basic purpose of the arrows connecting +L+ processes to the remote computers. Finally, 8 of the 10 +L+ participants were also able to identify the areas of the Sesame +L+ UI that represented things considered to be within the com- +L+ puter, versus the areas representing things that were ‘outside’ +L+ the computer. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DISCUSSION AND FUTURE WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The results of our study are encouraging, suggesting that on +L+ the whole, Sesame’s novel UI helps users make better +L+ security decisions than with typical security environments +L+ with a traditional firewall. We were especially pleased that +L+ our representation could be reasonably well interpreted +L+ without explanation. E.g., nearly all users understood the +L+ division between the internal and external regions of the +L+ design—this was a particular challenge we faced in earlier +L+ prototypes. The results suggest our basic representational +L+ approach to be a viable alternative to conventional textual +L+ approaches; and that novice users seem to rapidly learn a +L+ system-level structure when it is framed visually and in terms +L+ of more familiar concepts as was done in Sesame. While +L+ there is evidence that Sesame’s visual presentation is more +L+ effective than traditional firewalls, additional, larger studies +L+ are needed to confirm this. +L+ Besides the seemingly successful aspects of Sesame, the +L+ shortcomings of the UI were also informative. A common +L+ difficulty for participants was inferring causal relationships. +L+ Many participants felt that actions they took caused Sesame +L+ to bring up dialog boxes, even in cases where the Sesame +L+ visualization itself gave indications to the contrary. Addi- +L+ tionally, participant comments suggested they often had +L+ difficulty grasping the idea of their computer’s software +L+ environment as a collection of quasi-independent causal +L+ agents, instead inferring strong relationships among the +L+ different processes. These difficulties suggest that future +L+ versions of Sesame might use metaphors that better suggest +L+ the agent-causal nature of processes—such as depicting them +L+ with animated, anthropomorphic figures. +L+ While we evaluated Sesame as a holistic combination of its +L+ features, we do believe that there are generalizable design +L+ principles from our experiences that could be applied to other +L+ systems. +L+ First, given our desire to provide a direct manipulation +L+ interface, swiveling the desktop to show the underlying +L+ system seemed to be an accessible way to provide context to +L+ help users understand otherwise hidden features. That there +L+ are other possible approaches is certain; however, we believe +L+ that the generalizable principle here is that contextualization +L+ of new information with familiar, known concepts is a key +L+ for non-expert use. Our swivel metaphor is one (but not the +L+ only) way one might accomplish this. +L+ Second, Sesame’s policy cards provide a persistent visual +L+ indication of policy settings, allowing them to be easily seen +L+ and accessed by users in the future. The utility of persistent +L+ visual indicators —to support awareness of system state, to +L+ </SectLabel_bodyText> <SectLabel_figure> 120 +L+ 100 +L+ 80 +L+ 60 +L+ 40 +L+ 20 +L+ 0 +L+ Success Rate By Task +L+ Sesame +L+ Control +L+ </SectLabel_figure> <SectLabel_page> 1053 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> serve as an affordance for reversibility, and so forth—seems +L+ particularly important for conveying security state, where re- +L+ accessibility and intelligibility are especially difficult. We +L+ believe this to be a severe failing of current firewalls, in +L+ which settings—once configured—are often difficult to re- +L+ access. +L+ Third, we believe that metaphorical 3D models may aid in +L+ helping non-experts understand system information where +L+ necessary; particularly when it involves concepts underlying +L+ the desktop metaphor. Such modeling need not be limited to +L+ a firewall UI. Like file management models (e.g. two file +L+ folders with pages transferring from one to the other to show +L+ the status of copying), other system models can be embedded +L+ in applications, perhaps with individual components rotating +L+ aside to reveal relevant underlying system information. Our +L+ study suggests that security information embedded in such a +L+ model can be effectively leveraged by end-users. +L+ Finally, Sesame shows the viability of direct manipulation +L+ (DM) in low level, security and configuration interfaces, +L+ whereas DM is traditionally only used for windowing and +L+ within applications. We believe that this area is ripe for the +L+ use of other specific design techniques from the HCI com- +L+ munity, such as the use of 2.5D UI’s to extend the desktop +L+ metaphor. Our work suggests that users can understand +L+ relationships between different levels of abstraction through +L+ such well-proved techniques, even when dealing with the +L+ complex information necessary for security and system +L+ decision-making. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Sesame brings a direct manipulation graphical interface to +L+ end-user security to help non-experts make better informed +L+ security choices. Most prior work in end-user security tended +L+ to be highly task or threat-specific; or largely text-based +L+ and/or designed for experts. There are few tools if any which +L+ focus explicitly on helping non-expert users to better under- +L+ stand the technical, system-level concepts needed to make +L+ security decisions. With Sesame, we investigate an approach +L+ to providing non-experts with a general, firewall-like tool +L+ that addresses a wider range of threats independent of +L+ specific tasks or applications. +L+ Further, we move toward making system-level concepts +L+ accessible to non-experts by representing them as concrete +L+ objects and relating them to more familiar concepts such as +L+ desktop-level objects and real world abstractions like geo- +L+ graphic locations. In a controlled study, Sesame users were +L+ more likely to identify security threats accurately than users +L+ with more typical software environments. Sesame users were +L+ further able to understand many of the otherwise unfamiliar +L+ system-level concepts conveyed, suggesting viability to our +L+ fundamental interface approach. In the future, we plan to +L+ conduct further studies and explore how better to convey +L+ those concepts with which users struggled. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGEMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We thank our colleagues at Georgia Tech for their helpful +L+ feedback, in particular, John Stasko and Pixi Lab members; +L+ and we thank Symantec for their support for this work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Conti, G., Abdullah, K., Grizzard, J., Stasko, J., Copeland, J., +L+ Ahamad, M., Owen, H., Lee, C. Countering Security Informa- +L+ tion Overload through Alert and Pack Visualization. IEEE +L+ Computer Graphics (2006). +L+ 2. Dhamija, Rachna, Tygar, J.Doug. The Battle Against Phishing: +L+ Dynamic Security Skins. Symposium On Usable Privacy and +L+ Security, (2005). +L+ 3. DiGioia, P., Dourish P. Social Navigation as a Model for +L+ Usable Security. Symposium On Usable Privacy and Security +L+ (2005). +L+ 4. Downs, J. S., Holbrook, M. B., Cranor, L. F. Decision Strate- +L+ gies and Susceptibility to Phishing. Symposium On Usable +L+ Privacy and Security, (2005). +L+ 5. Edwards, W. K., Shehan, E., Stoll, J. Security Automation +L+ Considered Harmful? NSPW (2007) +L+ 6. Flinn, S.A., Flock of Birds, Safely Staged. DIMACS Workshop +L+ on Usable Privacy & Security Software (2005). +L+ 7. Foresti, S., Agutter, J. Visual Correlation of Network Alerts. +L+ IEEE Computer Graphics (2006). +L+ 8. Hutchins, E., Hollan, J., Norman, D. Direct Manipulation +L+ Interfaces. Human Computer Interaction, 1985. 1: p. 311-338. +L+ 9. Know Your Enemy: Tracking Botnets. Honeynet Project and +L+ Research Alliance. honeynet.org/papers/bots (2005). +L+ 10. Nielsen, J., Landauer, T. K., A mathematical model of the +L+ finding of usability problems. Proceedings of the ACM +L+ INTERCHI’93 Conference (1993). +L+ 11. Shukla, S., Nah, F., Web Browsing and Spyware Intrusion. +L+ Communications of the ACM.Vol. 48, No. 8 (2005). +L+ 12. Smetters, D., Grinter, R. Moving from the Design of Usable +L+ Security Technologies to the Design of Useful Secure Applica- +L+ tions. NSPW (2002). +L+ 13. Spyware. NISCC Technical Note. National Infrastructure +L+ Security Coordination Centre. (2006). +L+ 14. Thorpe, S., Fize, D. & Marlot, C. (1996).Speed of processing +L+ in the human visual system. Nature, 381, 520-522. +L+ 15. Walker, A. Absolute Beginner’s Guide to Security, Spam, +L+ Spyware & Viruses. Que Publishing, © 2006. +L+ 16. Whalen, T., Inkpen, K. Techniques for Visual Feedback of +L+ Security State. DIMACS Workshop on Usable Privacy and Se- +L+ curity Software (2004). +L+ 17. Whitten, A., Tygar, J. Safe Security Staging. CHI 2003 +L+ Workshop on Human-Computer Interaction and Security Sys- +L+ tems (2003). +L+ 18. Whitten, A., Tygar, J., Why Johnny Can’t Encrypt. Proc. of the +L+ 8th USENIX Security Symposium (1999). +L+ 19. Wu, M., Miller, R. C., Little, G. Web Wallet: Preventing +L+ Phishing Attacks by Revealing User Intentions. Symposium On +L+ Usable Privacy and Security, (2006). +L+ 20. Wu, M., Miller, R. C., Garfinkel, S., Do Security Toolbars +L+ Actually Prevent Phishing Attacks? CHI (2006). +L+ 21. Yee, K., Sitaker, K. Passpet: Convenient Password Manage- +L+ ment and Phishing Protection. Symposium On Usable Privacy +L+ and Security, (2006). +L+ 22. www.sysinternals.com/Utilities/ +L+ </SectLabel_reference> <SectLabel_page> 1054 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> TALC: Using Desktop Graffiti to Fight +L+ Software Vulnerability +L+ </SectLabel_title> <SectLabel_author> Kandha Sankarapandian, Travis Little, W. Keith Edwards +L+ </SectLabel_author> <SectLabel_affiliation> Georgia Institute of Technology +L+ </SectLabel_affiliation> <SectLabel_address> 85 Fifth Street NW, Atlanta, GA 30308, USA. +L+ </SectLabel_address> <SectLabel_email> {kandha, tlittle , keith}@cc.gatech.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> With the proliferation of computer security threats on the +L+ Internet, especially threats such as worms that +L+ automatically exploit software flaws, it is becoming more +L+ and more important that home users keep their computers +L+ secure from known software vulnerabilities. Unfortunately, +L+ keeping software up-to-date is notoriously difficult for +L+ home users. This paper introduces TALC, a system to +L+ encourage and help home users patch vulnerable software. +L+ TALC increases home users’ awareness of software +L+ vulnerabilities and their motivation to patch their software; +L+ it does so by detecting unpatched software and then +L+ drawing graffiti on their computer’s background wallpaper +L+ image to denote potential vulnerabilities. Users can “clean +L+ up” the graffiti by applying necessary patches, which +L+ TALC makes possible by assisting in the software patching +L+ process +L+ </SectLabel_bodyText> <SectLabel_category> ACM Classification: H.5.m Information interfaces and +L+ presentation, H.5.2 User Interfaces, K.6.5 Management of +L+ Computer and Information Systems: Security and +L+ Protection, D.4.6 Operating Systems: Security and +L+ Protection +L+ </SectLabel_category> <SectLabel_keyword> General terms: Human factors, security, management +L+ Keywords: Usable security, Internet security, home users, +L+ patch management, software vulnerabilities, security +L+ framework, graffiti +L+ </SectLabel_keyword> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> One of the most significant computer security threats faced +L+ by users today results from vulnerabilities in the operating +L+ system and application software installed on users’ +L+ computers. Software defects—bugs such as susceptibility +L+ to buffer overflow attacks [7], cross site scripting [26], and +L+ so forth—represent vectors through which malware can +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00 +L+ </SectLabel_copyright> <SectLabel_bodyText> infect and compromise users’ machines. Once machines +L+ have been compromised, malicious parties can extract +L+ personal information from them, or enlist them into botnets +L+ to serve in further attacks on network resources. The latter +L+ threat, in particular, has a significant impact on the entire +L+ Internet community as botnets are the means to Distributed +L+ Denial of Service (DDoS), spam and phishing attacks [14]; +L+ the exponential increase in size and number of botnets [6] +L+ is a stark reflection on the number of vulnerable machines +L+ that exist in the Internet. Ironically, in many cases, patches +L+ exist to repair these vulnerabilities; however, users are +L+ often unaware that such patches exist, or are unmotivated to +L+ install them, or may not know how to install them. +L+ Numerous reports from both government and industry +L+ sources highlight the magnitude of the threat posed by +L+ unpatched software vulnerabilities. For example, statistics +L+ from the Computer Emergency Response Team +L+ (CERT/CC) show the rapid increase in reported software +L+ vulnerabilities since 1995 [5]. NIST's report on the +L+ economic impacts of inadequate software testing estimates +L+ damage from attacks exploiting software vulnerabilities at +L+ US$60 billion/year [25]. Furthermore, testimony from the +L+ US General Accounting Office notes the importance of +L+ effective and continual patch management in addressing the +L+ “staggering” increase in software vulnerabilities [29]. +L+ Industry sources echo these same concerns. The importance +L+ of routine patching is highlighted in Symantec’s security +L+ report [28], which notes that after having a firewall and +L+ antivirus software, the single most important practice for +L+ consumers to maintain their computer’s security is to stay +L+ current on software patches. The SAGE report [17] from +L+ McAfee Avert Labs estimates that known software +L+ vulnerabilities are increasing at a rate of about 30% +L+ annually. Microsoft’s LaMacchia [16] also notes that the +L+ window of time between when new software is released +L+ and when an exploit has been created has decreased +L+ considerably (leading to so-called zero day attacks, in +L+ which exploits are ready to be employed the day new +L+ software is released). +L+ Unfortunately, just as the necessity of maintaining up-to- +L+ date patches is increasing, the complexity of doing so is +L+ also increasing: users must now be responsible for patching +L+ not only their operating system software, but also the +L+ </SectLabel_bodyText> <SectLabel_page> 1055 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> multiplicity of application software on their systems. While +L+ operating systems have built-in facilities (such as Windows +L+ Update) to download patches and encourage users to install +L+ them, other applications use a diverse range of update +L+ mechanisms, including requiring that users explicitly visit +L+ vendors’ web sites for newer versions. Worryingly, the +L+ SAGE report indicates that in the period between +L+ December 2005 and May 2006, the vulnerabilities targeted +L+ were moving away from OS attacks, to attacks on other +L+ software, such as Internet Explorer and Firefox. Thus users +L+ must now contend with a host of disparate and confusing +L+ patch systems in order to ensure that all of the software on +L+ their machines is protected. +L+ In order to patch vulnerable software, users (1) must know +L+ that such software exists in the first place, (2) know how to +L+ go about patching it, and (3) be motivated to do so in a +L+ timely manner.1 Although there are a number of existing +L+ systems that address patching in some capacity (described +L+ below), none of these systems specifically address making +L+ home users aware of the threats that vulnerable software +L+ poses to their computer’s security and their privacy, nor do +L+ they provide a holistic approach to patch management +L+ across multiple vendors’ applications. +L+ To address these challenges, we have developed a system +L+ called TALC (for Threat Awareness, Learning, and +L+ Control) that aims to augment users’ awareness of +L+ vulnerabilities posed by unpatched software through +L+ unobtrusive yet persistent visual reminders, persuade them +L+ to remedy those vulnerabilities, and provide easier +L+ mechanisms for patch install ation across a range of +L+ applications. +L+ In this paper, we describe TALC, its architecture, and a +L+ deployment-based user study that we performed to +L+ determine its overall utility. We conclude with a discussion +L+ of our approach and directions for future research. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Most of the current work on easier patch management is +L+ either vendor-specific, or has focused on managed solutions +L+ for the enterprise environment. +L+ In the vendor-specific category, tools such as Windows +L+ Update and Mac OS X Software Update perform automatic +L+ detection and download of new patches, and single-click +L+ installation. However, these tools only work for operating +L+ system components and, as noted above, unpatched +L+ application software now represents the major source of +L+ vulnerabilities. Of course, many application vendors +L+ provide mechanisms for their own products (such as Adobe +L+ Online’s tools for update of their Creative Suite products). +L+ However, there is no unified vendor-supported mechanism +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 Even with systems that include an auto-update mechanism, the response window between +L+ the public disclosure of an exploit and the availability of a software patch is sufficient for a +L+ worm to exploit the vulnerability and achieve significant spread. Usually an advisory on +L+ working around the vulnerable software is released before the actual patch and educating +L+ users with these advisories can be effective in stopping exploits. +L+ </SectLabel_footnote> <SectLabel_bodyText> for simple updates of all software on a user’s system, +L+ requiring users to deal with these on a piecemeal basis, +L+ when such systems exist at all. +L+ In the enterprise space, a number of companies have begun +L+ to focus specifically on patch installation in managed +L+ networks, as a way for centralized IT organizations to +L+ protect the corporate network. Enterprise management +L+ solutions like Marimba Patch Management from BMC +L+ Software [3], for example, enable deployment of security +L+ patches on all devices across the enterprise. While +L+ powerful, these systems are not designed for use by home +L+ users; they require, for example, a centralized administrator +L+ who manages patch releases to the corporate network, and +L+ rely on homogeneous software installations on client +L+ devices. +L+ There is a tension between tools like Marimba, which are +L+ proactive and aim to shield end-users from direct +L+ involvement with patching, and other tools such as +L+ Windows Update that take a more interactive approach, +L+ involving the user in the patch decision process and +L+ demanding their attention [15]. +L+ While proactive tools are, on the surface, easier to use since +L+ they do not require direct user involvement, they also do +L+ not contribute to the user’s learning process: awareness of +L+ threats is a critical component in managing software +L+ vulnerabilities given the diversity each user’s individual +L+ software usage patterns. Further—and perhaps more +L+ importantly—unless potential software version conflicts +L+ can be reliably determined in advance, there is a risk that an +L+ automatically installed patch will break other software on +L+ the user’s computer. Such a hypothetical, fully-automated +L+ tool for managing software updates across applications is +L+ difficult to achieve outside the homogeneity of the +L+ managed corporate network, meaning that users will likely +L+ have to be involved in at least some aspects of patch +L+ decision making for the foreseeable future [10]. +L+ Given these practical realities of automated patch +L+ management, it is imperative that users be kept informed +L+ about the potential dangers of an unpatched system, as well +L+ as the benefits and risks of installing a given patch, if they +L+ are going to be involved in making patch decisions. +L+ A challenge, of course, is that highly interactive tools can +L+ potentially annoy users to the point that they turn off such +L+ tools completely. This problem has been especially evident +L+ in security software; most common firewalls, for instance, +L+ display pop up messages about threats such as port scans. +L+ Bailey, Konstan, and Carlis [1] report that such +L+ interruptions increase task completion times, as well as user +L+ anxiety and frustration. However the suggestion from [1] of +L+ an “attention manager” that predicts opportunities for +L+ engaging with the user may not be an optimal solution for a +L+ security task like patch management that does not require +L+ an instantaneous allow/deny decision in the way that +L+ antivirus or firewall alerts do. This suggests that different, +L+ more subtle and less intrusive approaches than interrupting +L+ the user may be employed, which allow the user to interact +L+ </SectLabel_bodyText> <SectLabel_page> 1056 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 1: TALC showing graffiti on the user's desktop along with a popup description of the threat. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> with the patch management system as a secondary task, but +L+ with sufficient persuasion that users do not ignore it +L+ completely. +L+ There have also been a number of research efforts intended +L+ to address the problem of excessive dependence on user +L+ interaction for security. For example, systems such as the +L+ Chameleon System for Desktop Security [23] attempt to +L+ categorize software into activity roles in an effort to reduce +L+ impinging on the user’s attention. However, most such +L+ tools are incomplete, or focus on a narrow range of threats. +L+ For example, Chameleon is a low-fi, paper-based prototype +L+ intended to address only the threat of malware. +L+ TALC is designed in response to the need for better patch +L+ management on end-user systems. It aims to strike a +L+ balance between proactive and interactive support, in order +L+ to provide users with awareness and control over security +L+ risks without excessive attention costs or disruption to their +L+ workflow. TALC uses “calm” notifications, rather than +L+ intrusive techniques such as popups, to motivate specific +L+ user behaviors, and to provide awareness of overall system +L+ risk from software vulnerabilities. TALC also provides a +L+ holistic approach to patch management, by assisting with +L+ patch management across the heterogeneous variety of +L+ applications and software components that may be installed +L+ on a user’s machine. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DESCRIPTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section, we describe how the user sees TALC, as +L+ well as how TALC detects and assists in repairing software +L+ vulnerabilities. +L+ TALC paints graffiti on the user’s desktop to indicate the +L+ presence of unpatched software on the user’s system (see +L+ Figure 1). Unlike intrusive techniques such as popups, this +L+ is meant to be a “low-distraction” technique, designed to +L+ make users aware of potential problems, while allowing +L+ them to act on them in their own time. In contrast to +L+ warning dialogs that interrupt users’ activities (“Your +L+ patches are out of date!”), this awareness function is +L+ intended to serve as a constant but gentle reminder, +L+ allowing users to finish their primary tasks without letting +L+ them forget about the security maintenance tasks that need +L+ their attention. +L+ </SectLabel_bodyText> <SectLabel_page> 1057 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> For each threat found on the user’s machine by TALC a +L+ single graffiti image is chosen out of a corpus of images, +L+ and composited into a randomly selected area of the screen. +L+ TALC uses the size of the graffiti image to convey the +L+ relative severity of the threat: the graffiti is shown larger +L+ for severe threats and smaller for more mild threats. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Why Graffiti? +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We chose the graffiti visualization of software +L+ vulnerabilities to convey a general sense of “decay” or +L+ “threat” to the user, suggesting that their machine has +L+ entered a state of risk. Such notions appear to be broadly +L+ associated with graffiti in physical environments for many +L+ people. Numerous studies have confirmed this association +L+ across a number of cultures and communities; see, for +L+ example Morin et al.’s study of US public health nursing +L+ students’ perception of threat in their communities [22], +L+ Bowling et al.’s study of risk perception in Britain [4] as +L+ well as others [2, 13]. +L+ We realize that this association may not hold across all +L+ cultures, or even across individuals within a given culture. +L+ (See, for instance, sources that reflect the artistic value in +L+ graffiti such as Susan Farrell’s Art Crimes site, +L+ http://www.graffiti.org, as well as academic work exploring +L+ the appropriation of graffiti by various subcultures [11].) +L+ However, even for those users that may not have negative +L+ associations with graffiti, we hoped that their personal +L+ inclinations would be outweighed by the minor annoyance +L+ of having part of their desktop covered by the graffiti +L+ (covering a personally selected photo for instance), and +L+ therefore would still provide motivation to deal with the +L+ software vulnerabilities. +L+ Our choice of a real world metaphor for visualizing +L+ security threats stems from the observations made by +L+ Redstrom, Skog and Hallnas in their work on informative +L+ art [27]. We explored a number of other, non-graffiti +L+ visualization approaches during prototyping, which we also +L+ believed might convey a sense of risk to the user. These +L+ included one that used bullet holes in the user’s background +L+ image (deemed both to be too violent, and to +L+ inappropriately convey a sense of active attack rather than +L+ simple decay), and one that rendered increasingly large +L+ piles of garbage and other debris along the bottom of the +L+ user’s screen (deemed to appropriately convey a sense of +L+ decay but perhaps be too easy to ignore). We believe using +L+ graffiti walks the line between the ambient media and +L+ diversion categories as described by McCrickard, et.al. in +L+ their model for notification systems [18]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Determining and Presenting Vulnerabilities +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> TALC determines potential vulnerabilities through a multi- +L+ step process. First, we perform a periodic system-wide +L+ audit to identify software versions installed on the user’s +L+ machine. Next, this data is compared against the online +L+ NIST National Vulnerability Database (NVD) [24], +L+ resulting in an up-to-date list of installed software for +L+ which patches exist. +L+ When the user’s cursor hovers over a graffiti area, a tooltip +L+ displays the name of the software that is vulnerable, as well +L+ as the threats posed by this vulnerability (see Figure 1). +L+ We parse the NVD at connection time to retrieve patch +L+ information, as well as the descriptions presented to users, +L+ as shown in Figure 1 above. The language used in the +L+ descriptions in the National Vulnerability Database is often +L+ highly technical, and may be confusing to home users. To +L+ make the descriptions more palatable, we use a set of +L+ heuristics to simplify the explanations. For example, an +L+ NVD threat description such as the following: +L+ </SectLabel_bodyText> <SectLabel_construct> The do_change_cipher_specfunction in OpenSSL +L+ 0.9.6c to 0.9.6k, and 0.9.7a to 0.9.7c, allows remote +L+ attackers to cause a denial of service (crash) via a +L+ crafted SSL/TLS handshake that triggers a null +L+ dereference +L+ </SectLabel_construct> <SectLabel_bodyText> would be presented by TALC as: +L+ </SectLabel_bodyText> <SectLabel_construct> Denial of service vulnerability that lets a remote +L+ attacker slow down/crash your computer. +L+ </SectLabel_construct> <SectLabel_bodyText> The descriptions are scanned for a small set of keywords, +L+ and predefined descriptions of the problems are presented +L+ to the user. This provides a more readable description for a +L+ large number of common classes of vulnerabilities; other +L+ descriptions that do not match our heuristics are explained +L+ with a generic message: “Other vulnerability.” +L+ We did not completely eliminate all information about the +L+ vulnerability to allow users to learn about common types of +L+ threats, and—if necessary—communicate such information +L+ to any people or organizations they trust to help them keep +L+ their computer safe. Thus, following Zurko [30], TALC +L+ places emphasis on helping users understand these security +L+ concepts through its use. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Repairing Vulnerabilities +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In addition to supporting threat awareness, TALC also +L+ allows users to take actions that mitigate threats. When the +L+ user clicks the right mouse button on graffiti, a popup +L+ context menu appears that allows them to repair the threats +L+ posed by a vulnerable program. When the user chooses to +L+ fix the program, TALC downloads and displays the +L+ webpage that contain patches or workarounds for +L+ vulnerabilities, and displays the control window shown in +L+ Figure 2. TALC also shows system information and the +L+ name and version number of the program with the +L+ vulnerability. +L+ Unfortunately, different vendors require different processes +L+ to acquire patches: some may require that users log in, +L+ while others require a click-through license agreement, and +L+ others may simply provide direct access to the patch itself. +L+ Thus, while TALC automates the process offinding a patch +L+ for the detected vulnerabilities, it leaves the task of actually +L+ installing patches to individual users. This is not only +L+ because of the difficulty involved in automatically dealing +L+ with multiple vendors’ web sites, but also because users +L+ must often be involved in the process of deciding whether a +L+ particular patch is appropriate for them. Security is not +L+ </SectLabel_bodyText> <SectLabel_page> 1058 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> users’ only concern; they must make security related +L+ decisions in context, such as knowing whether a new +L+ software version will break compatibility with other tools. +L+ For example, Windows XP SP2—while providing +L+ important security features—broke the functionality of a +L+ number of network-based tools [21]. Simply installing such +L+ updates automatically without considering the context of +L+ other software in use can often lead to such problems. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 2: TALC Control window displaying the +L+ website with patch information. +L+ </SectLabel_figureCaption> <SectLabel_sectionHeader> IMPLEMENTATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> TALC uses a modular implementation, with well-defined +L+ communication interfaces between modules to facilitate +L+ easy addition and replacement of components. This is +L+ exposed in the form of an API that allows developers to +L+ write pluggable modules for TALC. For example, these +L+ APIs have been used to create the software vulnerability +L+ detection system described in this paper, but can be +L+ extended to provide functionality beyond software patch +L+ management. The extensible nature of TALC is intended to +L+ be used to visualize and control a large set of security tools +L+ through a framework similar to the one described by +L+ Dourish and Redmiles in [9]. For example, information +L+ sources, sensors and aggregators can be created and +L+ plugged into TALC, allowing it to be extended to new +L+ visualizations and to detect new security threats. Our long- +L+ term goal is for TALC to ultimately serve as an integrated +L+ security suite along the lines of Internet Security suites +L+ from Symantec, McAfee and an advanced form of the +L+ Windows Security Center [20]. +L+ The TALC system is composed of four modules, +L+ Information Source Module, Correlation Module, +L+ Visualization Module and Control Module linked together +L+ by a Communication Manager that allows modules to pass +L+ messages to each other. Each module exposes hooks and +L+ registers callback functions with the other modules for +L+ communication. The Information Source module detects +L+ events from the host and the network and processes them +L+ into XML data that can be exchanged with other modules. +L+ For example, the software vulne rability detection features +L+ described in this paper are implemented as a custom +L+ Information Source module, which generates data by using +L+ Hijackthis [19] (a tool that scans the registry for references +L+ to installed programs) and a series of other system scans +L+ (such as programs on the Start Menu, or on the user’s +L+ desktop) to identify installed software. +L+ The Correlation module interprets this information by +L+ aggregating the data from the different Information +L+ Sources. For the software vulnerability detection +L+ incarnation of TALC, the Correlation module correlates the +L+ information from the system scans with data pulled from +L+ the NVD. The Correlator performs a version match with the +L+ list of vulnerable software from the NVD database, +L+ ascertains the severity of the threat (Mild, Medium or +L+ Severe), and records the website indicated by the NVD to +L+ contain information necessary to resolve the vulnerability. +L+ This is fed to the Visualization and Control modules. +L+ The Visualization module is responsible for information +L+ presentation. As described earlier, our current visualization +L+ module presents software vulnerabilities as graffiti +L+ rendered onto the user’s desktop. Other visualizations are +L+ possible; for example, one visualization we have explored +L+ renders vulnerabilities as pieces of garbage piling up along +L+ the bottom of the user’s screen; one could also create +L+ visualization modules that use standard pop-up dialog +L+ boxes to notify the user of threats. +L+ Finally, the Control module provides the means by which +L+ the user can act upon the information presented by the +L+ Visualization module. In our current system the Control +L+ module opens up websites that lets the user download a +L+ patch to fix the software vulnerability. The Control module +L+ can be easily extended to give a user control of different +L+ security software such as their firewall. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> EVALUATION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We performed a deployment-based study of TALC to +L+ determine its efficacy in providing better awareness of +L+ software vulnerabilities, and in incenting users to rectify +L+ those vulnerabilities. Our study structure consisted of a +L+ two-week deployment period during which TALC was in +L+ operation on users’ primary machines. Logging features in +L+ TALC reported on users’ use of the system so that we +L+ could collect quantitative data on their actions. We also +L+ collected data from pre- and post-study questionnaires to +L+ get qualitative data about users’ perceptions of the +L+ software. +L+ Participants were recruited from a non-university context. +L+ After consent was obtained (but before any other +L+ participation in the study), users were sent a link to an +L+ online questionnaire that tested them on their awareness of +L+ computer security concepts and threats, as well as their +L+ expertise in general computer usage. +L+ Once users completed the pre-test questionnaire they were +L+ emailed a link to the installation file and they were asked to +L+ download and install TALC. Our participants ran a +L+ specially instrumented version of TALC on their personal +L+ </SectLabel_bodyText> <SectLabel_page> 1059 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> computers for two weeks. Every week, TALC would +L+ upload the data it had collected on how users interacted +L+ with it. +L+ When the participants had been running the program for +L+ two weeks, they were sent a link via email to a post-study +L+ questionnaire to allow us to get data about their subjective +L+ experiences of using the software. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> The Participants +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Ten participants finished the study successfully. Seven +L+ more started the study, but dropped out for a variety of +L+ reasons, including changing their mind about participating +L+ in the study before downloading the software, and because +L+ of installation problems. One participant uninstalled the +L+ TALC software before the two weeks were completed; we +L+ include data from this subject in the results presented here: +L+ one of the things we hoped to discover was whether we had +L+ correctly adjusted TALC to motivate users without being +L+ annoying, and so results from users who ceased using the +L+ software had the potential to be especially illuminating. +L+ The ages of the participants ranged from twenty-three to +L+ thirty-five, with an average age of twenty-six. Of those who +L+ completed the study, four were women and six were men. +L+ Although the absolute number of participants is smaller +L+ than would be typical in a controlled lab study, the intent +L+ with our evaluation was specifically to engage users in a +L+ real-world deployment of the system over a sustained (half +L+ month) period of time, a style of evaluation that we believe +L+ is necessary for ecological validity. While lab-based studies +L+ can easily engage substantially larger numbers of users, +L+ these studies have problems in the security context, +L+ particularly around artificial experimental scenarios that are +L+ removed from users' day-to-day experiences, and also may +L+ overly prime subjects’ orientation toward security. We +L+ therefore believed that a deployment study, on users’ own +L+ computers, confronting unknown usage contexts and +L+ uncontrolled software vulnerabilities, was the only +L+ appropriate way to measure use. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> The Pre-test Questionnaire +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The pre-test questionnaire was administered online, using a +L+ common online survey vendor. Participants were emailed +L+ requests to fill out the survey, and provided links to the +L+ survey site. The survey consisted of three demographic +L+ questions, eleven questions to determine how comfortable +L+ the participants felt using computers, and how confident +L+ they were in their computer’s security. Finally there were +L+ eight questions in which the user was asked to define +L+ simple computer security related terms, to gauge their +L+ general knowledge of computer security concepts. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> The Study +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> There were two conditions in the study, to separate +L+ patching actions incented directly by TALC from other +L+ patches downloaded merely because users had an increased +L+ awareness of the security issues as a result of participating +L+ in the study itself. In all conditions TALC was downloaded +L+ and installed by the participants. In one condition, however, +L+ the tool was instrumented to not identify any vulnerabilities +L+ (and, hence, to not show any graffiti) during the first week; +L+ in the second condition the tool was instrumented so that it +L+ did not detect any vulnerabilities (nor show any graffiti) in +L+ the second week. Through these two conditions we hoped +L+ to isolate any potentially biasing novelty effects caused +L+ simply because subjects were participating in the study. In +L+ both cases, for the week it was active, TALC detected +L+ vulnerable software on all participants’ computers, and thus +L+ presented graffiti to all users during the week it was +L+ activated. During both weeks, for both conditions, TALC +L+ continued to scan the users’ systems and record +L+ vulnerabilities as well as how the participants interacted +L+ with it. +L+ Participants were randomly assigned to a condition when +L+ they consented to be a part of the study. They were not +L+ given a link to the TALC installation file until they had +L+ completed the pre-test questionnaire. Participants were +L+ instructed to inform researchers if they had any problems +L+ installing the software packages; despite this, a number of +L+ participants did have trouble installing our prototype and +L+ yet did not contact us, which contributed significantly to +L+ the drop-out rate. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> The Post-test Questionnaire +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The post-test questionnaire was administered online, again +L+ using a common online survey vendor, in the same way as +L+ the pretest questionnaire. Participants were emailed +L+ requests to fill out the survey, and provided links to the +L+ survey site, when they had run the TALC software for two +L+ weeks and their usage data had been uploaded. The survey +L+ had the same ques tions as the pretest questionnaire, along +L+ with the addition of thirteen questions to dete rmine the +L+ participants’ perceptions of using the TALC system over +L+ the deployment period. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RESULTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This section describes the results from our deployment +L+ study, and from our pre - and post-test questionnaires. +L+ Events Tracked +L+ TALC kept logs of the users’ interaction with it, over the +L+ two week period, and the data was uploaded twice to our +L+ server: once at the end of each week. +L+ From the logs we categorized five types of event s: +L+ Awareness events, Learning events, Control (Fixed) events, +L+ Control (Ignore) events, and Reappear events. An +L+ Awareness event was recorded when graffiti for a particular +L+ threat was shown to the user for the first time. Whenever +L+ our simplified description of the threat was shown to the +L+ user or when the user clicked on a graffiti, and the vendor +L+ website was displayed, it was recorded as a Learning event. +L+ Whenever users would indicate to the system that a +L+ vulnerability had been repaired and should be dismissed +L+ (through clicking the “Already Fixed” button in the TALC +L+ interface), Control (Fixed) events were recorded. If the +L+ vulnerability hadn’t actually been fixed, a Reappear event +L+ would be recorded when the vulnerability was re-detected. +L+ Finally, if a user chose to not fix a vulnerability by clicking +L+ </SectLabel_bodyText> <SectLabel_page> 1060 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 3: Awareness of new threats reported by +L+ TALC to the test participants. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the Ignore button while viewing a patch website, a Control +L+ (Ignore) event was written to the log. +L+ Each line on these graphs represents a single test +L+ participant. Figure 3 records the distribution of Awareness +L+ events—each representing a newly detected vulnerability— +L+ over the TALC active week period. The smaller spike in +L+ awareness at later stages of test period is a composite of +L+ two factors—new vulnerabilities released by NVD and new +L+ program installs by users of the software. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Usage Patterns +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The majority of user actions were taken within two days +L+ after graffiti for a particular vulnerability first appeared on +L+ the desktop: 60% of all vulnerabilities were fixed within a +L+ two-day period. However, 39% of the remaining +L+ vulnerabilities that were fixed were patched in the last two +L+ days of the test, indicating that users were patching, +L+ ignoring graffiti for a couple of days, and then coming back +L+ and patching at a later time. This is illustrated in Figure 4 +L+ below. +L+ Recall that Learning events represent visits by users to a +L+ patch website through TALC; this figure shows the +L+ distribution of such visits. Beyond simply exploring +L+ vulnerabilities through TALC, we believe that the effects of +L+ making users aware of software vulnerabilities on their +L+ systems may have resulted in greater sensitivity to patching +L+ in general: A number of the respondents to our post-test +L+ survey reported using regular web search engines to find +L+ out more details about the detected vulnerabilities. While +L+ we were encouraged by these findings, such events are +L+ beyond the scope of the instrumentation we had in place for +L+ TALC and so we only have self-reports of such activities. +L+ The Fixed and Ignore types of Control events are a good +L+ reflection of the effectiveness of TALC. When a participant +L+ applied a software patch, TALC did not immediately +L+ remove the graffiti; rather, the graffiti was removed during +L+ the next periodic scan of the user’s system. The TALC user +L+ interface, however, provided an option to manually invoke +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4: Learning events logged by TALC. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the scan to remove graffiti for threats that had been fixed. +L+ Another option, which incidentally most users adopted, was +L+ to use the Control window to mark a threat as “Fixed” so +L+ that TALC hides the graffiti. Figure 5 shows users’ usage +L+ patterns of marking threats as fixed. +L+ A common pattern across all participants is that they tried +L+ to fix a number of vulnerabilities initially, following which +L+ there was a lull period with little or no activity; finally, +L+ several days later, there were more attempts to fix +L+ vulnerabilities. We believe this pattern indicates favorable +L+ acceptance on the part of users: rather than becoming +L+ infuriated with the notifications provided by TALC, users +L+ were “living with” the notifications for a period of several +L+ days, and then fixing them at convenient intervals. Users +L+ were able to put off patching anything for a couple of days, +L+ but were not allowed to forget about the security task to +L+ which they needed to attend. This indicates that the graffiti +L+ notification system worked well in allowing users to push +L+ back their lower priority (but necessary) security tasks until +L+ they were convenient, but while still retaining awareness of +L+ the need to perform these tasks. Furthermore, we believe +L+ the sudden flurry of activity near the end of the active week +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 5: Participants marking threats as “Fixed”. +L+ </SectLabel_figureCaption> <SectLabel_figure> Awareness +L+ 1	2	3	4	5	6	7 +L+ 80 +L+ 70 +L+ 60 +L+ 50 +L+ 40 +L+ 30 +L+ 20 +L+ 10 +L+ 0 +L+ Days +L+ Learning +L+ 1	2	3	4	5	6	7 +L+ 40 +L+ 20 +L+ 90 +L+ 80 +L+ 70 +L+ 60 +L+ 50 +L+ 30 +L+ 10 +L+ 0 +L+ Days Graffiti was shown +L+ Control (Fixed) +L+ 70 +L+ 60 +L+ 50 +L+ 40 +L+ 30 +L+ 20 +L+ 10 +L+ 0 +L+ 1	2	3	4	5	6	7 +L+ Days Graffiti was shown +L+ </SectLabel_figure> <SectLabel_page> 1061 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 6: Threats that reappeared after being +L+ </SectLabel_figureCaption> <SectLabel_bodyText> marked as “Fixed” by the participants. +L+ represents a periodic turn of attention toward patching, +L+ rather than a desire by participants to “wrap up” patching +L+ before the end of the study. Much of this activity, for +L+ example, came from the condition two participants, who +L+ still had another week to participate in the study. +L+ In the subsequent scan cycles, TALC logged any of these +L+ fixed threats that still match the vulnerability description +L+ from NVD and logs them as a reappearance of a threat, +L+ shown in Figure 6. +L+ We should note here that, in our current implementation, +L+ threats for which the NVD has only an advisory (meaning: +L+ for which no patch is available) are never detected as fixed +L+ by TALC. To handle such a scenario, TALC allows the +L+ user to optionally ignore these threats that have reappeared. +L+ Threats that have been ignored will not reappear in the +L+ subsequent scans unless the user explicitly asks TALC to +L+ include them in the scan. Occurrences of this event are +L+ plotted in Figure 7. The spike near the start of the test may +L+ be due to the large number of vulnerabilities detected by +L+ TALC (see below for details on the number of raw +L+ vulnerabilities found on users’ systems). These Ignore +L+ events were also recorded when participants found the +L+ information provided by the patch website too daunting for +L+ them, and chose to leave the vulnerability unpatched. We +L+ discuss some suggestions for how to overcome both these +L+ shortcomings in the Future Work section. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> There are multiple useful metrics for determining what +L+ constitutes efficacy in a tool such as TALC. One such +L+ metric is whether the tool increases the perceived safety of +L+ users; the second is whether it increases their actual safety. +L+ While we evaluated for both metrics, we believe that the +L+ latter is actually the more important, since perceptions of +L+ increased safety are of little value without actually +L+ increased safety. +L+ In the sections below we first report on TALC’s efficacy in +L+ actually repairing system vulnerabilities, and then on users’ +L+ perceptions of its efficacy. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 7: Participants asking TALC to Ignore a +L+ threat from subsequent scans +L+ </SectLabel_figureCaption> <SectLabel_subsectionHeader> TALC Effectiveness +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We were pleased to find that TALC provided a dramatic +L+ increase in the safety of users’ systems during its +L+ deployment, and that TALC’s notifications made a large +L+ and statistically significant difference in users’ awareness +L+ and motivation to install patches. +L+ In the weeks where TALC was dormant—meaning it was +L+ not drawing graffiti on the users’ desktops—none of the +L+ users patched any vulnerabilities whatsoever. However, in +L+ the week when the graffiti was placed on the desktop, +L+ seventy percent of the users fixed at least one vulnerability, +L+ with an average of 24.3 vulnerabilities patched per user +L+ (averaged over all users), and an average of 34.7 +L+ vulnerabilities patched over users who patched at least one +L+ vulnerability. The number of vulnerabilities patched during +L+ the active period was found to be statistically significant +L+ (t(9) = 2.78, p = 0.0216) when compared to the dormant +L+ state when no vulnerabilities were patched. +L+ The absolute number of vulnerabilities patched may seem +L+ artificially high, because it does not necessarily indicate the +L+ number of fixes downloaded, but rather the number of +L+ vulnerabilities patched: A single fix downloaded by the +L+ user may or may not patch multiple vulnerabilities. +L+ At the beginning of the test, TALC found an average of +L+ 47.6 vulnerabilities on our participants’ machines. All +L+ participants’ computers had at least five vulnerabilities, +L+ with the most having 64 vulnerabilities. These numbers not +L+ only confirm the threat posed by unpatched software, but +L+ also users’ lack of awareness about vulnerabilities on their +L+ systems for which patches exist. +L+ Although TALC was effective in getting users to fix some +L+ of the vulnerabilities in their systems, none of the users +L+ patched their machines completely. Of the 482 +L+ vulnerabilities left unpatched, 208 (43.15%) were +L+ considered serious threats by the NVD, 88 (18.25%) were +L+ considered moderate threats by the NVD, and 186 +L+ (38.59%) were considered mild threats. The total number of +L+ vulnerabilities increased over the test period in part due to +L+ </SectLabel_bodyText> <SectLabel_figure> Reappearances +L+ 2.5 +L+ 2 +L+ 1.5 +L+ 1 +L+ 0.5 +L+ 0 +L+ 1	2	3	4	5	6	7 +L+ Days Grafitti was shown +L+ Control (Ignore) +L+ 70 +L+ 1	2	3	4	5	6	7 +L+ 40 +L+ 20 +L+ 60 +L+ 50 +L+ 30 +L+ 10 +L+ 0 +L+ Days Graffiti was shown +L+ </SectLabel_figure> <SectLabel_page> 1062 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> new software being installed by our test subjects, but +L+ mostly due to updates in the vulnerability listing from the +L+ NVD. 50% of the users responded that it was very difficult +L+ to correct the security vulnerabilities reported by TALC, +L+ which we attribute to the poor usability of many of the web +L+ pages supplied by the NVD. In addition, sometimes the +L+ links provided in the NVD data were not valid. Further +L+ discussion can be found in the Future Work section below. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> User Perce ptions +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> One of the goals of TALC was to experiment with gentle +L+ reminder functions to motivate users to take security +L+ actions. In the post-test questionnaire, when asked to +L+ suggest improvements to the program, thirty-three percent +L+ of the participants suggested various solutions for making +L+ the graffiti less invasive. However, our goal was to make +L+ TALC as motivating as possible, without being overly +L+ annoying (which could have caused users to disable the +L+ program entirely). A majority (67%) of users raised no +L+ issue with intrusiveness; further, we find it to be telling that +L+ the only user who reported disabling the TALC software +L+ during the test did so because he felt it was slowing his +L+ system down too much. The implementation of our +L+ Visualization module uses the default.NET transparency +L+ effects, which run on the CPU on systems without modern +L+ graphics cards, so this may have contributed to the +L+ problem. +L+ Finally, with regard to the effect of TALC on perceived +L+ user safety, we found that four of the seven users who +L+ responded to the post-study questionnaire felt that using +L+ TALC had improved their ability to protect their computer, +L+ and that their computer was safer as a result. Although this +L+ figure does not represent universal success in increasing +L+ perceived safety, we were delighted to demonstrate any +L+ increase, since our pre-test questionnaire showed users +L+ were generally unaware of any vulnerabilities. We believe +L+ that this sort of awareness of risk is essential for self- +L+ managed computers (at least given today’s state-of-the-art +L+ in automated security systems), and points to necessary +L+ further work in the area of conveying an accurate sense of +L+ risk or safety to users. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Future Work +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our near-term goals concern both expanding the +L+ capabilities of the TALC framework, as well as evaluating +L+ with a larger user base on how well our visualization and +L+ control features motivate and support users to mitigate +L+ threats. +L+ One of the common problems encountered by users was the +L+ complexity of vendors’ update websites. To address this +L+ issue, we intend to add another level of proactivity in the +L+ system, to allow TALC to automatically download and +L+ install patches from a set of “common” websites, the patch +L+ processes of which can be built into the tool itself. With +L+ this addition, TALC would only present instructional web +L+ pages for late-breaking advisories that do not include a +L+ direct download link to a software patch. This feature can +L+ be accompanied by an additional layer of abstraction over +L+ the patch websites that simply asks the user to update their +L+ software to the latest version without loading the entire +L+ website containing specific details of the threat. +L+ To prevent display of “unfixable” problems (such as +L+ advisories that do not have valid URLs), we intend to filter +L+ the advisories for common problems, and never show +L+ graffiti for the vulnerabilities reported. +L+ We also intend to add several more information sources to +L+ the program, to exercise the extensibility features described +L+ in the Implementation section. Most important among these +L+ are the creation of modules that integrate a number of +L+ existing security tools, such as NMAP [12] and Nessus [8], +L+ to extract data about other sorts of system vulnerabilities. +L+ Specifically, we plan to use NMAP (a port scanning tool) +L+ to independently audit the user’s firewall, since properly +L+ configured firewalls are very effective at blocking many +L+ automated attacks. Nessus is a tool that performs security +L+ audits by running exploit code against a user’s computer. It +L+ has an active development base that releases new exploits +L+ to be used while auditing. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We have presented TALC, a software system that assists +L+ users in protecting their computers from some of the most +L+ serious threats on the Internet, software with known +L+ vulnerabilities. TALC uses a low-intrusion notification +L+ mechanism for presenting users with information about +L+ vulnerabilities. Through the use of automated system audits +L+ and correlation against online databases, we can detect +L+ potential software vulnerabilities and give users easier +L+ mechanisms to act to repair those vulnerabilities. The +L+ extensible architecture of TALC allows it to be used to +L+ detect, visualize, and mitigate against a wide range of +L+ threats. +L+ More generally, we believe that the approach taken by +L+ TALC may be useful in cases where 1) user motivation for +L+ a task may be low (as is often the case with security tasks), +L+ 2) intrusive or disruptive notifications may actually incent +L+ users to disable the system, and 3) there is not the need for +L+ immediate action. This combination of factors makes this +L+ class of tasks somewhat different from others (such as +L+ firewalls or background email notification) that have been +L+ widely studied in our community. We believe that the +L+ strategy of background notifications that strike a balance +L+ between awareness and annoyance to gently incent the user +L+ can be profitably applied to this class of problems. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Bailey, B.P., Konstan, J.A. and Carlis, J. V. (2001) The +L+ effects of interruptions on task performance, +L+ annoyance, and anxiety in the user interface. +L+ Proceedings ofINTERACT ’01, pp. 593-601. +L+ 2. Bennett, R. and Flavin, J. “Determinants of Fear of +L+ Crime: The Effect of Cultural Setting.” Justice +L+ Quarterly, 11:3, September 1994, pp. 357-381. +L+ 3. BMC Software. Marimba Patch Management +L+ Software, http://www.marimba.com/ +L+ 1063 +L+ CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ 4. Bowling, A., Barber, J., Morris, R., and Ebrahim, S. +L+ ÒDo Perceptions of Neighbourhood Environment +L+ Influence Health? Baseline Findings from a British +L+ Survey of Aging.” Journal of Epidemioogy and +L+ Community Health, 60:476-483.2006. +L+ 5. Computer Emergency Response Team (CERT), 2006. +L+ CERT/CC	Statistics	1988-2006. +L+ http://www.cert.org/stats +L+ 6. Cooke, E., Jahanian, F., and McPherson, D. The +L+ Zombie Roundup: Understanding, Detecting and +L+ Disrupting Botnets, in First Workshop on Steps to +L+ Reducing Unwanted Traffic on the Internet (SRUTI), +L+ 2005. +L+ 7. Cowan, C., Wagle, P., and Pu, C. Buffer Overflows: +L+ Attacks and Defenses for the Vulnerability of the +L+ Decade,	DARPA	Information	Survivability +L+ Conference and Expo, 1999. +L+ 8. Deraison, R. Nessus - A Comprehensive Vulnerability +L+ scanning program, http://www.nessus.org/, 1998. +L+ 9. Dourish, P., Redmiles, D. , An approach to usable +L+ security based on event monitoring and visualization. +L+ In Proceedings of the New Security Paradigms +L+ Workshop (NSPW), 2002. pp. 75-81. +L+ 10. Edwards, W.K., Poole, E.S., and Stoll, J. Security +L+ Automation Considered Harmful? In Proceedings of +L+ the New Security Paradigms Workshop (NSPW), White +L+ Mountain, New Hampshire. September 18-21, 2007. +L+ 11. Ferrell, J. Crimes of Style: Urban Graffiti and the +L+ Politics of Criminality. New York: Garland. 1993. +L+ 12. Fyodor. Nmap - Free Security Scanner for Network +L+ Exploration and Security Audits, Insecure.org, 1997. +L+ 13. Geason, S. “Preventing Graffiti and Vandalism.” +L+ Proceedings of Designing Out Crime: Crime +L+ Prevention through Environmental Design, Sydney, +L+ Australia. June 16, 1989. +L+ 14. Ianelli, N., and Hackworth, A. Botnets as a Vehicle for +L+ Online Crime, CERT, Request for Comments (RFC) +L+ 1700, December 2005. +L+ 15. Isbell, C. and Pierce, J. An IP Continuum for Adaptive +L+ Interface Design. In Proceedings of HCI International, +L+ 2005. +L+ 16. LaMacchia, B.A. Security Attacks and Defenses, in +L+ 47th Meeting of IFIP WG 10.4.2005. +L+ 17. McAfee AVERT Labs. SAGE. Security Vision from +L+ McAfee AVERT Labs, July 2006. +L+ 18. McCrickard, D. S., Chewar, C. M., Somervell, J. P., & +L+ Ndiwalana, A. A Model for Notification Systems +L+ Evaluation—Assessing User Goals for Multitasking +L+ Activity. ACM Transactions on Computer-Human +L+ Interaction (TOCHI), 10 (4), 2003. +L+ 19. Merijn. HijackThis . +L+ http://www.spywareinfo.com/~merijn/programs.php. +L+ 20. Microsoft. Manage Your Computer's Security Settings +L+ in	One	Place	with	Security	Center, +L+ http://www.microsoft.com/windowsxp/using/security/i +L+ nternet/sp2_wscintro.mspx. +L+ 21. Microsoft. Programs that are known to experience a +L+ loss of functionality when they run on a Windows XP +L+ Service	Pack	2-based	computer, +L+ http://support.microsoft.com/?id=884130. +L+ 22. Morin, K., Hayes, E., Carroll, M., and Chamberlain, B. +L+ “Selected Factors Associated with Students’ +L+ Perceptions of Threat in the Community.” Public +L+ Health Nursing, 19:6, pp. 451-459, Nov. 2002 +L+ 23. Moskowitz, C.L.a.C. Simple Desktop Security with +L+ Chameleon. in Lorrie Faith Cranor, S.G. ed. Security +L+ and Usability, O'Reilly, August 2005. +L+ 24. National Institute of Standards and Technology +L+ (NIST).	National	Vulnerability	Database, +L+ http://nvd.nist.gov. +L+ 25. National Institute of Standards and Technology +L+ (NIST), 2002. The economic impacts of inadequate +L+ infrastructure for software testing. Technical Report +L+ 02-3, May 2002. This report estimates damage from +L+ attacks exploiting software vulnerabilities at $60 +L+ billion/year. +L+ 26. Rafail, J. Cross-Site Scripting Vulnerabilities, CERT +L+ Coordination Center, 2001. +L+ 27. Redstrom, J., Skog, T. and Hallnas, L. Informative Art: +L+ Using Amplified Artworks as Information Displays, in +L+ Proceedings of the Designing Augmented Reality +L+ Environments Conference ‘00, (Elsinore Denmark, +L+ 2000), 103 Ð114. +L+ 28. Symantec Internet Security Threat Report, Volume IX. +L+ www.symantec.com/enterprise/threatreport/index.jsp. +L+ 29. US General Accounting Office (GAO), 2003. +L+ "Effective Patch Management is Critical to Mitigating +L+ Software Vulnerabilities." Testimony before the +L+ Subcommittee on Technology, Information Policy, +L+ Intergovernmental Relations, and the Census. +L+ 30. Zurko, M.E. User-Centered Security: Stepping Up to +L+ the Grand Challenge ACSAC, 2005. +L+ </SectLabel_reference> <SectLabel_page> 1064 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> You’ve Been Warned: An Empirical Study of the +L+ Effectiveness of Web Browser Phishing Warnings +L+ </SectLabel_title> <SectLabel_author> Serge Egelman +L+ </SectLabel_author> <SectLabel_affiliation> Carnegie Mellon University +L+ </SectLabel_affiliation> <SectLabel_email> egelman@cs.cmu.edu +L+ </SectLabel_email> <SectLabel_author> Lorrie Faith Cranor +L+ </SectLabel_author> <SectLabel_affiliation> Carnegie Mellon University +L+ </SectLabel_affiliation> <SectLabel_email> lorrie@cs.cmu.edu +L+ </SectLabel_email> <SectLabel_author> Jason Hong +L+ </SectLabel_author> <SectLabel_affiliation> Carnegie Mellon University +L+ </SectLabel_affiliation> <SectLabel_email> jasonh@cs.cmu.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Many popular web browsers now include active phishing +L+ warnings since research has shown that passive warnings +L+ are often ignored. In this laboratory study we examine the +L+ effectiveness of these warnings and examine if, how, and +L+ why they fail users. We simulated a spear phishing attack +L+ to expose users to browser warnings. We found that 97% +L+ of our sixty participants fell for at least one of the phishing +L+ messages that we sent them. However, we also found that +L+ when presented with the active warnings, 79% of partici- +L+ pants heeded them, which was not the case for the passive +L+ warning that we tested—where only one participant heeded +L+ the warnings. Using a model from the warning sciences we +L+ analyzed how users perceive warning messages and offer +L+ suggestions for creating more effective phishing warnings. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Phishing, warning messages, mental models, usable privacy +L+ and security +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> H.1.2 User/Machine Systems, H.5.2 User Interfaces, D.4.6 +L+ Security and Protection +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Online security indicators have historically failed users be- +L+ cause users do not understand or believe them. The preva- +L+ lence of phishing, a scam to collect personal information +L+ by mimicking trusted websites, has prompted the design of +L+ many new online security indicators. Because phishing is a +L+ semantic attack that relies on confusing people, it is difficult +L+ to automatically detect these attacks with complete accuracy. +L+ Thus, anti-phishing tools use warnings to alert users to po- +L+ tential phishing sites, rather than outright blocking them. +L+ The question remains, do anti-phishing warnings actually +L+ help users? Up until recently these tools have relied on pas- +L+ sive indicators to alert users. A passive indicator indicates +L+ a potential danger by changing colors, providing textual in- +L+ formation, or by other means without interrupting the user’s +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, or +L+ republish, to post on servers or to redistribute to lists, requires prior specific +L+ permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5 - 10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00. +L+ </SectLabel_copyright> <SectLabel_figureCaption> Figure 1. The active Internet Explorer 7.0 phishing warning. +L+ Figure 2. The passive Internet Explorer 7.0 phishing warning. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> task. However, research has shown that passive indicators +L+ are failing users because users often fail to notice them or do +L+ not trust them [23]. +L+ The newest web browsers now include active warnings, which +L+ force users to notice the warnings by interrupting them. Mi- +L+ crosoft’s Internet Explorer 7 includes both active and passive +L+ phishing warnings (Figures 1 and 2, respectively). When IE7 +L+ encounters a confirmed phishing website, the browser will +L+ display an active warning message giving the user the op- +L+ tion of closing the window (recommended) or displaying the +L+ website (not recommended). This warning is a full screen +L+ error, which turns the URL bar red if the user chooses to dis- +L+ play the website (Figure 1). The passive indicator, a popup +L+ </SectLabel_bodyText> <SectLabel_page> 1065 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 3. The active Firefox 2.0 phishing warning. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> dialog box, is displayed to the user when the browser be- +L+ lieves a website is suspicious (Figure 2), but that website +L+ has not been verified as being a phishing website (i.e. it does +L+ not appear on a blacklist). We consider this warning to be +L+ more passive because it does not give the user any choices, +L+ nor does it make any recommendations. +L+ Firefox 2.0 also includes an active phishing warning, which +L+ was part of the Google Toolbar extension for previous ver- +L+ sions of Firefox. When a user encounters a confirmed phish- +L+ ing website, a non-interactive dimmed version of the web- +L+ site is displayed with an overlayed dialog box. The user is +L+ given a choice between continuing to the site or leaving. The +L+ user may also click the red ‘X’ in the corner of the warning, +L+ which has the same effect as continuing to the website (Fig- +L+ ure 3). +L+ In this study we compared the effectiveness of active and +L+ passive phishing warnings by analyzing them using a warn- +L+ ing analysis methodology used by researchers in the warning +L+ sciences field, called the Communication-Human Informa- +L+ tion Processing Model (C-HIP) model [21]. +L+ This paper makes three contributions. First, it presents the +L+ results of a study evaluating the effectiveness of active secu- +L+ rity indicators in current web browsers. Second, it presents +L+ an analysis of the results using a model from the warning +L+ sciences. Third, it presents recommendations for improving +L+ these security indicators such that fewer users fall victim to +L+ online fraud. +L+ We first frame our study within the context of previous phish- +L+ ing and warning research, and then describe the methodol- +L+ ogy behind our study. We then discuss the results of our +L+ user study and how effective we determined each warning +L+ message to be. Finally, we make recommendations based on +L+ these results for designing more effective security indicators. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> BACKGROUND +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we describe previous work related to users’ +L+ susceptibility to phishing, warning indicators used in web +L+ browsers, and user perceptions of warning messages. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Phishing Susceptibility +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Despite growing efforts to educate users and create better +L+ detection tools, users are still very susceptible to phishing +L+ attacks. Unfortunately, due to the nature of the attacks, it is +L+ very difficult to estimate the number of people who actually +L+ fall victim. A 2006 report by Gartner estimated the costs at +L+ $1,244 per victim, an increase over the $257 they cited in +L+ a 2004 report [11]. In 2007 Moore and Clayton estimated +L+ the number of phishing victims by examining web server +L+ logs. They estimated that 311,449 people fall for phishing +L+ scams annually, costing around 350 million dollars [15]. An- +L+ other study in 2007 by Florencio and Herley estimated that +L+ roughly 0.4% of the population falls for phishing attacks an- +L+ nually [9]. +L+ Phishing works because users are willing to trust websites +L+ that appear to be designed well. In a 2001 study on web- +L+ site credibility, Fogg et al. found that the “look and feel” +L+ of a website is often most important for gaining a user’s +L+ trust [10]. A 2006 phishing study by Dhamija et al. found +L+ that 90% of the participants were fooled by phishing web- +L+ sites. The researchers concluded that current security indi- +L+ cators (i.e. the lock icon, status bar, and address bar) are inef- +L+ fective because 23% of the participants failed to notice them +L+ or because they did not understand what they meant [7]. +L+ In a similar study, Downs et al. showed participants eight +L+ emails, three of which were phishing. They found that the +L+ number of participants who expressed suspicion varied for +L+ each email; 47% expressed suspicion over a phishing mes- +L+ sage from Amazon, whereas 74% expressed suspicion over +L+ a phishing message from Citibank. Those who had inter- +L+ acted with certain companies in the past were significantly +L+ more likely to fall for phishing messages claiming to be from +L+ these companies. Participants were also likely to ignore or +L+ misunderstand web browser security cues [8]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Phishing Indicators +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> New research has focused on creating new anti-phishing in- +L+ dicators because existing security indicators have failed. The +L+ Passpet system, created by Yee et al. in 2006, uses indicators +L+ so that users know they are at a previously-trusted website. +L+ Users can store an animal icon within the web browser for +L+ each trusted site with which they interact. The system will +L+ only send a password when the user recognizes that the ani- +L+ mal icons match. Preliminary user testing suggests that this +L+ system is easy for users to use [25]. Other proposals have +L+ also been put forth to modify browser chrome to help users +L+ detect phishing websites. In one system, “synchronized ran- +L+ dom dynamic boundaries,” by Ye and Smith, the browser +L+ chrome is modified to blink at a random rate. If the blink +L+ rate matches a trusted window’s blink rate, the user knows +L+ that the window in question has not been spoofed [24]. A +L+ similar solution using a trusted window was also proposed +L+ by Dhamija and Tygar in 2005. In their system the chrome +L+ of the browser window contains a colored pattern that must +L+ be matched with the trusted window. The user knows to rec- +L+ ognize the trusted window because it contains a personal im- +L+ age that the user selected during the initial configuration [6]. +L+ Since all of these proposals require the use of complicated +L+ third-party tools, it’s unclear how many users will actually +L+ benefit from them. These proposals have only undergone +L+ minimal user testing in unrealistic environments. User test- +L+ ing should be performed under real world conditions before +L+ any new security indicator is recommended. +L+ </SectLabel_bodyText> <SectLabel_page> 1066 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> The SiteKey system was introduced in 2005 to simplify au- +L+ thentication by not forcing the user to install additional soft- +L+ ware. SiteKey uses a system of visual authentication images +L+ that are selected by the user at the time of enrollment. When +L+ the user enters his or her username, the image is displayed. +L+ If the user recognizes the image as the original shared secret, +L+ it is safe to enter the password [2]. However, a recent study +L+ found that 92% of participants still logged in to the website +L+ using their own credentials when the correct image was not +L+ present [19]. However, this sample may have been drawn +L+ from a biased population since others refused to participate, +L+ citing privacy and security concerns. +L+ Some argue that the use of extended validation (EV) certifi- +L+ cates may help users detect phishing websites. An EV cer- +L+ tificate differs from a standard SSL certificate because the +L+ website owner must undergo background checks. A regular +L+ certificate only tells a user that the certificate was granted +L+ by a particular issuing authority, whereas an EV certificate +L+ also says that it belongs to a legally recognized company [4]. +L+ The newest version of Microsoft’s Internet Explorer sup- +L+ ports EV certificates, coloring the URL bar green and dis- +L+ playing the name of the company. However, a recent study +L+ found that EV certificates did not make users less likely to +L+ fall for phishing attacks. The study also found that after +L+ reading a help file, users were less suspicious of fraudulent +L+ websites that did not yield warning indicators [13]. +L+ Many web browser extensions for phishing detection cur- +L+ rently exist. Unfortunately, a recent study on anti-phishing +L+ toolbar accuracy found that these tools fail to identify a sub- +L+ stantial proportion of phishing websites [26]. A 2006 study +L+ by Wu et al. found that the usability of these tools is also +L+ lacking because many of them use passive indicators. Many +L+ users fail to notice the indicators, while others often do not +L+ trust them because they think the sites look trustworthy [23]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> A MODEL FOR WARNINGS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this paper we will analyze our user study results using +L+ a model from the warnings sciences. Computer scientists +L+ can benefit from studies in this field. Many studies have ex- +L+ amined “hazard matching” and “arousal strength.” Hazard +L+ matching is defined as accurately using warning messages to +L+ convey risks—if a warning does not adequately convey risk, +L+ the user may not take heed of the warning. Arousal strength +L+ is defined as the perceived urgency of the warning [12]. +L+ To date, few studies have been conducted to evaluate the +L+ arousal strength of software warnings. In one study of warn- +L+ ing messages used in Microsoft Windows, researchers found +L+ that using different combinations of icons and text greatly af- +L+ fected participants’ risk perceptions. Participants were shown +L+ a series of dialog boxes with differing text and icons, and +L+ were instructed to estimate the severity of the warnings us- +L+ ing a 10-point Likert scale. The choice of icons and words +L+ greatly affected how each participant ranked the severity. +L+ The researchers also examined the extent to which individu- +L+ als will continue to pay attention to a warning after seeing it +L+ multiple times (“habituation”). They found that users dis- +L+ missed the warnings without reading them after they had +L+ seen them multiple times. This behavior continued even +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4. Diagram of the different phases of the C-HIP model [21]. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> when using a similar but different warning in a different sit- +L+ uation. The only way of recapturing the user’s attention was +L+ to increase the arousal strength of the warning [1]. +L+ Wogalter proposed the Communication-Human Information +L+ Processing Model (C-HIP) for structuring warning research, +L+ as shown in Figure 4. He suggests that C-HIP be used to +L+ identify reasons that a particular warning is ineffective [21]. +L+ The C-HIP model begins with a source delivering a warning +L+ through a channel to a receiver, who receives it along with +L+ other environmental stimuli that may distract from the mes- +L+ sage. The receiver goes through five information processing +L+ steps, which ultimately determine whether the warning re- +L+ sults in any change in behavior. +L+ We can ask the following questions to examine the different +L+ steps in Wogalter’s model [5]: +L+ </SectLabel_bodyText> <SectLabel_listItem> 1. Attention Switch and Maintenance — Do users notice the +L+ indicators? +L+ 2. Comprehension/Memory — Do users know what the indi- +L+ cators mean? +L+ 3. Comprehension/Memory — Do users know what they are +L+ supposed to do when they see the indicators? +L+ 4. Attitudes/Beliefs — Do they believe the indicators? +L+ 5. Motivation — Are they motivated to take the recommended +L+ actions? +L+ 6. Behavior — Will they actually perform those actions? +L+ 7. Environmental Stimuli — How do the indicators interact +L+ with other indicators and other stimuli? +L+ </SectLabel_listItem> <SectLabel_bodyText> Observing users as they complete a task while thinking aloud +L+ provides insights into most of the above questions. Alterna- +L+ tively, users can complete tasks and then fill out post-task +L+ questionnaires or participate in interviews, although these +L+ require users to remember why they did something and re- +L+ port it afterwards, and users sometimes say what they think +L+ </SectLabel_bodyText> <SectLabel_figure> Environmental +L+ Stimuli +L+ Comprehension +L+ Memory +L+ Attention +L+ Maintenance +L+ Motivation +L+ Attention +L+ Switch +L+ Attitudes +L+ Beliefs +L+ Behavior +L+ Channel +L+ Delivery +L+ Source +L+ </SectLabel_figure> <SectLabel_page> 1067 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> the researcher wants to hear. In our study of active phishing +L+ indicators we performed a think-alound experiment followed +L+ by a post-task questionnaire. We then used the C-HIP model +L+ to analyze our data. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> METHODOLOGY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this study participants made online purchases and then +L+ were told to check their email, whereupon they encountered +L+ phishing messages we had sent them. We observed partic- +L+ ipants visit the URLs in these phishing messages, at which +L+ point the participants were exposed to the web browser warn- +L+ ing messages. We took note of whether participants read +L+ these warnings and how they chose to proceed. Finally, par- +L+ ticipants were given an exit survey. +L+ The primary purpose of this study was to examine the effec- +L+ tiveness of phishing warnings found in current web browsers. +L+ These warnings serve as the last line of defense against a user +L+ divulging his or her sensitive information to a con artist. In +L+ other words, prior to these warnings being displayed, it is +L+ likely that users believe they are visiting legitimate websites. +L+ Thus, we needed users to fall for the phishing messages we +L+ sent them during our study so that they would be in a similar +L+ state of mind when they encountered the warnings. At the +L+ same time, we needed our attack to be plausible. Thus, we +L+ simulated a spear phishing attack. Spear phishing “involves +L+ personalized emails or emails sent to a specifically targeted +L+ group, such as employees of a particular organization” [8]. +L+ For instance, a phisher might send a message to email ad- +L+ dresses at aol.com announcing account changes impacting +L+ AOL users. Since all the recipients are AOL users, this scam +L+ may have increased credibility because users believe it to be +L+ relevant to them. In our study, if participants did not believe +L+ our phishing messages to be credible, they would be less +L+ likely to follow the links and thus would not see the browser +L+ warning messages. +L+ We framed our study as an “online shopping study”—items +L+ were purchased online, and then we sent the participants +L+ phishing messages claiming to be from those shopping web- +L+ sites. Participants were told that we were examining how +L+ they interact with shopping websites and that they needed to +L+ think aloud during their purchases. After the first purchase +L+ was made, participants checked their email to confirm that +L+ the order was going to be shipped, thereby encountering the +L+ first phishing message. Once the participants were confi- +L+ dent that the first purchase had been completed, instructions +L+ were provided for the second purchase. This purchase was +L+ then made using a different website, and a different phishing +L+ message was sent. Participants in the experimental condi- +L+ tions were given an exit survey before leaving. In this sec- +L+ tion we will provide the details of our recruitment process +L+ and the study design. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Recruitment +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> This study was designed as a between-subjects study, with +L+ four different conditions using the Internet Explorer 7.0 and +L+ Firefox 2.0 web browsers: participants were shown either +L+ the Firefox warning (Figure 3), the active IE warning (Fig- +L+ ure 1), the passive IE warning (Figure 2), or no warning at +L+ all. As of June 2007, users of Internet Explorer and Firefox +L+ comprised 58.5% and 34.0% of all Internet users, respec- +L+ tively [18]. Additionally, both browsers have automatic up- +L+ date features. Thus, it is only a matter of time before most +L+ users will be using the newest versions of these browsers +L+ which contain these phishing warnings. We began recruiting +L+ participants in May of 2007. +L+ We did not tell participants that we were studying online se- +L+ curity because we wanted to simulate a natural environment +L+ by not priming them to security concerns. We recruited par- +L+ ticipants from all over Pittsburgh in order to make our re- +L+ sults generalizable. We attached flyers to telephone posts, +L+ bus stops, and community bulletin boards. We also posted +L+ online to Craigslist and a CMU website for recruiting study +L+ participants. We constructed a screening survey to screen out +L+ technically savvy individuals, users of certain web browsers, +L+ participants in previous phishing studies, and users of certain +L+ email providers. We also used this survey to glean some ba- +L+ sic demographic information from participants, such as age, +L+ gender, occupation, prior online shopping experience, etc. +L+ Participants who contacted us after seeing a recruitment flyer +L+ were directed to our online screening survey. Since we were +L+ examining the newest versions of Firefox (2.0) and IE (7.0) +L+ to include the active warnings, we made sure that all par- +L+ ticipants in the experimental conditions already used one of +L+ these browser versions. Thus the screening survey included +L+ a question about current browser version (with graphics de- +L+ picting how to determine the version) to screen out users of +L+ other web browsers. +L+ Since our lab has conducted previous studies on phishing, +L+ we were concerned about the potential for priming of prior +L+ participants. Thus we disqualified anyone who had previ- +L+ ously participated in a phishing-related study. We were also +L+ concerned that savvy users would not believe the emails, and +L+ thus not be exposed to the warnings. We asked four ques- +L+ tions to gauge each participant’s experience: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Have you ever designed a website? +L+ •	Have you ever registered a domain name? +L+ •	Have you ever used SSH? +L+ •	Have you ever configured a firewall? +L+ </SectLabel_listItem> <SectLabel_bodyText> In our pilot we discovered that participants who answered +L+ yes to all four questions were just as likely to believe the +L+ phishing emails as all other participants. Thus, we decided +L+ not to disqualify participants based on these questions. +L+ We tried to make our scenarios as realistic as possible by +L+ requiring participants to use their own email accounts and +L+ financial information for the purchases. The screening sur- +L+ vey explicitly asked whether or not they could check their +L+ email using a web browser on a foreign computer. We also +L+ asked them to enter their email addresses so that we could +L+ contact them as well as to determine which email provider +L+ they were using. We initially found that some of the larger +L+ free email providers were detecting our phishing messages +L+ and filtering them out. We minimized this problem by im- +L+ plementing DKIM and SPF on our outgoing mail server to +L+ help recipient mail servers verify the message sender. 1,2 +L+ </SectLabel_bodyText> <SectLabel_footnote> 1http://www.dkim.org/ +L+ 2http://www.openspf.org/ +L+ </SectLabel_footnote> <SectLabel_page> 1068 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> Of the 282 individuals who completed our screening sur- +L+ vey, only 70 qualified and showed up. Despite our efforts +L+ to screen out individuals who used email providers that were +L+ likely to filter out our messages, we still found that we could +L+ not collect data from ten participants because they did not re- +L+ ceive either of our phishing messages. These ten participants +L+ were not included in our results. +L+ Based on the browser versions that they indicated in the +L+ screening survey, participants were placed in one of the four +L+ conditions. The average age of participants was 28 (σ = +L+ 10.58), and there was no significant difference between the +L+ groups in terms of age or gender. The Firefox condition con- +L+ sisted of 20 users of Firefox 2.0, while the other two exper- +L+ imental conditions consisted of users of Internet Explorer +L+ 7 (20 participants in the active IE condition and 10 partic- +L+ ipants in the passive IE condition). The ten participants in +L+ the control group all used an older version of one of the two +L+ browsers. The control group was used to determine whether +L+ or not participants were willing to enter information into our +L+ phishing websites in the absence of any warning messages. +L+ This told us whether the warning was affecting phishing sus- +L+ ceptibility or if it could be attributed to some other factor. +L+ The group sizes were chosen based on a power analysis per- +L+ formed prior to recruitment. +L+ We were initially concerned that the self-selected nature of +L+ the groups (based on web browser preference) may have bi- +L+ ased our study. However, we found no statistical differences +L+ between the average number of hours participants in each +L+ group claimed to use the Internet, nor with regard to the av- +L+ erage number of email messages participants claimed to re- +L+ ceive. In each of the active warning groups, exactly seven +L+ participants answered “no” to all of the questions used to +L+ gauge technical prowess. Thus, there were equal numbers of +L+ novices in each group. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Scenarios +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We decided to spoof Amazon and eBay since they were the +L+ most commonly phished non-bank websites [17]. Thus, re- +L+ gardless of familiarity with the real websites, it is likely that +L+ participants have previously encountered phishing messages +L+ claiming to be from these websites. Our spoofed websites +L+ consisted of login forms for usernames and passwords. To +L+ make these websites look authentic, we registered two do- +L+ main names: ebay-login.net and amazonaccounts.net. The +L+ websites were designed to mimic the login pages of the orig- +L+ inal websites. We created two spoof URLs at each domain +L+ name. +L+ We took steps to ensure our phishing websites triggered the +L+ warnings in each web browser. Firefox downloads its lo- +L+ cally stored blacklist from Google, so we modified it locally +L+ to include our URLs [16]. Microsoft agreed to add our spoof +L+ URLs to their remote blacklists, causing those URLs to trig- +L+ ger the IE phishing warnings. +L+ We copied two common phishing emails spoofing Amazon +L+ and eBay and changed the content to fit our study. The +L+ message claiming to be from Amazon was sent out in plain +L+ text and informed the recipient that the order was delayed +L+ and would be cancelled unless the recipient clicked the in- +L+ cluded URL. The message claiming to be from eBay was in +L+ HTML and informed the recipient that all international or- +L+ ders needed to be confirmed by visiting a URL contained +L+ within the message. Both messages contained random or- +L+ der numbers to help convince the recipients of their legit- +L+ imacy, though no information specific to the recipients was +L+ included in these messages in order to make our attacks real- +L+ istic. The scenario was such that it would have been entirely +L+ possible for a person to have just completed a purchase from +L+ one of these websites and then received a generic phishing +L+ message spoofing that same website. It is also possible for +L+ a phisher to monitor wireless Internet traffic and conduct a +L+ similar phishing attack after detecting a purchase. We be- +L+ lieve that the coincidental nature of this attack was the reason +L+ why many more participants fell for our attacks than what +L+ has been found in similar studies [8, 7, 19, 14, 20]. Previous +L+ phishing studies have spoofed companies with whom vic- +L+ tims had relationships. However we are unaware of any user +L+ studies that have used phishing messages timed to coincide +L+ with a transaction with the spoofed brand. +L+ Participants arrived at our laboratory and were told that they +L+ would be purchasing two items online from Amazon and +L+ eBay. We randomized the order in which the purchases were +L+ made. We also informed participants that we were recording +L+ them, so they needed to think aloud about everything they +L+ were doing. Participants did the study individually with the +L+ experimenter sitting behind them in the laboratory. +L+ We were concerned that if we allowed participants to pur- +L+ chase whatever they wanted, they might take too long to de- +L+ cide, and that other factors might confound our results. We +L+ also wanted participants to focus on buying cheap items so +L+ that we could reimburse them for both purchases while still +L+ giving them enough additional money for their time. We lim- +L+ ited the scope of the purchases by asking them to purchase a +L+ box of paper clips from Amazon, which cost roughly $0.50, +L+ plus around $6 in shipping (the exact prices changed with +L+ each order since all participants did not purchase the same +L+ paperclips). We asked participants to make their eBay pur- +L+ chases from a cheap electronics store based in Hong Kong +L+ that sold a variety of items for around $5-$10, including +L+ shipping. Participants were compensated $35 for their time +L+ and the purchases, which were made using their personal +L+ credit cards. +L+ After each purchase, participants received a sheet of five +L+ questions relating to shopping. These questions were part +L+ of an unrelated study on shopping behaviors, but helped our +L+ study by convincing participants that this was indeed a shop- +L+ ping study. While the participant answered these questions, +L+ the experimenter sent them a phishing message. We con- +L+ structed a web interface for the study, so that the experi- +L+ menter only needed to enter an email address, the brand to +L+ spoof, and the experimental condition. +L+ After the written questions were completed, the experimenter +L+ told the participant to “check your email to make sure that +L+ the order is confirmed and ready to ship so we can move +L+ on.” When participants checked their email, they encoun- +L+ tered legitimate messages relating to their orders as well as +L+ a phishing message. After examining (and reacting) to all of +L+ </SectLabel_bodyText> <SectLabel_page> 1069 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_table> Condition Name	Size	Clicked	Phished +L+ Firefox	20	20(100%)	0(0%) +L+ Active IE	20	19(95%)	9(45%) +L+ Passive IE	10	10(100%)	9(90%) +L+ Control	10	9(90%)	9(90%) +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1. An overview depicting the number of participants in each +L+ condition, the number who clicked at least one phishing URL, and +L+ the number who entered personal information on at least one phishing +L+ website. For instance, nine of the control group participants clicked at +L+ least one phishing URL. Of these, all nine participants entered personal +L+ information on at least one of the phishing websites. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> the messages, participants received a set of instructions for +L+ the second purchase. After participants checked their email +L+ after the second purchase (thereby encountering the second +L+ phishing message), an exit survey was administered. This +L+ online exit survey contained questions about participants’ re- +L+ actions to the warning messages. The experimenter observed +L+ participants fill this out and asked followup questions if any +L+ of the responses were too terse or did not seem to follow +L+ the behaviors exhibited during the experiment. Those in the +L+ control group were not asked to complete an exit survey as +L+ they had not seen any warnings. Participants took an aver- +L+ age of forty minutes to complete all the tasks and were given +L+ $35 in cash before leaving. +L+ We were initially concerned that since participants did not +L+ explicitly want the items, the results might be skewed in fa- +L+ vor of participants acting more cautious. However, we be- +L+ lieve their desire to complete the study negated this. Thus, +L+ the desire to buy the items to complete the study was likely +L+ just as strong as if the participant were at home purchasing +L+ a desired item. Additionally, we do not believe that the cost +L+ of the items played any role since an attacker could use the +L+ stolen account credentials to make any number of larger pur- +L+ chases. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RESULTS AND ANALYSIS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Overall we found that participants were highly susceptible +L+ to our spear phishing attack. However, users of the active +L+ phishing warnings were largely protected, since 79% chose +L+ to heed them. We found a significant difference between +L+ the active IE and Firefox warnings (p < 0.0004 for Fisher’s +L+ exact test) as well as no significant difference between the +L+ passive IE warning and the control group (i.e. significantly +L+ more users were helped by the active Firefox warning than +L+ the active IE warning, while the passive IE warning is no dif- +L+ ferent than not displaying any warning). We also found sig- +L+ nificant differences between the active IE warning and the +L+ control group (p < 0.01) demonstrating that the active IE +L+ warning is still significantly better than not displaying any +L+ warning. Table 1 depicts these results. In this section we ex- +L+ amine how participants reacted to the initial phishing mes- +L+ sages, and then we use the C-HIP model to analyze why +L+ certain warnings performed better than others. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Phishing Susceptibility +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Our simulated spear phishing attack was highly effective: +L+ of the 106 phishing messages that reached participants’ in- +L+ boxes, participants clicked the URLs of 94 of them (89%). +L+ While all participants made purchases from both Amazon +L+ and eBay, not every participant received both of our phish- +L+ ing messages due to email filtering. Only two participants +L+ (3%) did not attempt to visit any of the phishing URLs. Of +L+ the 46 participants who received both phishing messages, 43 +L+ clicked the Amazon link and 37 clicked the eBay link. How- +L+ ever this difference was not statistically significant, nor were +L+ there any significant correlations based on which phishing +L+ message was viewed first. It should also be noted that every +L+ participant in the control group who followed a link from an +L+ email message also submitted information to the phishing +L+ websites (Table 1). Thus, in the absence of security indica- +L+ tors, it is likely that this type of phishing attack could have a +L+ success rate of at least 89%. +L+ With regard to the technical questions mentioned in the Re- +L+ cruitment section, we noticed a negative trend between tech- +L+ nical experience and obeying the warnings among Internet +L+ Explorer users (i.e. users with more technical experience +L+ were more likely to ignore the warnings). With Firefox, +L+ technical experience played no role: all users obeyed the +L+ warnings regardless of their technical experience. +L+ We did not actually collect any information entered into the +L+ phishing websites. Instead the experimenter observed each +L+ participant and noted when they submitted information. Thus +L+ we cannot conclusively say whether all participants entered +L+ their correct information. However, the experimenter did +L+ note that all usernames were entered correctly, and no partic- +L+ ipants denied entering their correct information when asked +L+ in the exit survey. +L+ We found that participants had very inaccurate mental mod- +L+ els of phishing. Both of our phishing messages contained +L+ language that said the orders would be cancelled if they did +L+ not visit the URLs. Thirty-two percent of the participants +L+ who heeded the warnings and left the phishing websites be- +L+ lieved that their orders would be cancelled as a result—they +L+ believed that the emails were really sent from eBay and Ama- +L+ zon. We asked 25 of the participants how they believed +L+ the fraudulent URLs came to them, and only three recog- +L+ nized that the emails had been sent by someone not affiliated +L+ with either eBay or Amazon (we added this question halfway +L+ through the study). Thus, there seems to be some cognitive +L+ dissonance between recognizing a fraudulent website and +L+ the fraudulent email that spread it. This raises grave con- +L+ cerns about Internet users’ susceptibility to phishing. Highly +L+ targeted phishing attacks will continue to be very effective as +L+ long as users do not understand how easy it is to forge email. +L+ At the same time, effective browser warnings may mitigate +L+ the need for user education, as we will now show. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Attention Switch and Maintenance +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> The first stage in the C-HIP model is “attention switch.” If +L+ a warning is unable to capture the user’s attention, the warn- +L+ ing will not be noticed and thus be rendered useless. Unlike +L+ the passive indicators examined by Wu et al. [23], the ac- +L+ tive warnings in Firefox and Internet Explorer get the user’s +L+ attention by interrupting their task—the user is forced to +L+ choose one of the options presented by the warning. +L+ </SectLabel_bodyText> <SectLabel_page> 1070 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_table> Firefox	20	20	13	4	17	19 +L+ Active IE	20	19	10	10	10	12 +L+ Passive IE	10	8	3	5	3	5 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 2. This table depicts the number of participants in each experi- +L+ </SectLabel_tableCaption> <SectLabel_bodyText> mental condition, the number who saw at least one warning, the num- +L+ ber who completely read at least one warning, the number who recog- +L+ nized the warnings, the number who correctly understood the warn- +L+ ings, and the number who understood the choices that the warnings +L+ presented. +L+ This was not the case with the passive warning in IE (Figure +L+ 2). This warning is a single dialog box with only the op- +L+ tion to dismiss it. We observed that it could take up to five +L+ seconds for this warning to appear. If a user starts typing +L+ during this period, the user’s keystrokes will inadvertently +L+ dismiss the warning. Six of the ten participants in this con- +L+ dition never noticed the warning because their focus was on +L+ either the keyboard or the input box. Two of these partic- +L+ ipants had this happen on both phishing websites, so they +L+ had no idea they were ever exposed to any warnings. We +L+ found no statistical significance between this condition and +L+ the control group. Thus, this type of warning is effectively +L+ useless. +L+ Effective warnings must also cause attention maintenance— +L+ they must grab the users’ attention long enough for them to +L+ attempt comprehension. We examined the number of partici- +L+ pants who read the warnings (as determined by self-reporting +L+ and confirmed by the observations of the experimenter) in +L+ order to determine their effectiveness at attention mainte- +L+ nance. Table 2 shows the number of warnings read and the +L+ number of participants who claimed to have seen the warn- +L+ ings prior to this study, for each experimental condition. +L+ Not counting the two participants who failed to notice the +L+ warnings entirely, and the participant in the active IE condi- +L+ tion who did not click on the URLs, we found that twenty-six +L+ of the remaining forty-seven (55%) claimed to have com- +L+ pletely read at least one of the warnings that were displayed. +L+ When asked, twenty-two of these twenty-six (85%) said they +L+ decided to read the warning because it appeared to warn +L+ about some sort of negative consequences. +L+ Upon seeing the warnings, two participants in the active IE +L+ condition immediately closed the window. They went back +L+ to the emails and clicked the links, were presented with the +L+ same warnings, and then closed the windows again. They +L+ repeated this process four or five times before giving up, +L+ though never bothered to read the warnings. Both said that +L+ the websites were not working. Despite not reading or un- +L+ derstanding the warnings, both were protected because the +L+ warnings “failed safely.” Thus, if users do not read or under- +L+ stand the warnings, the warnings can still be designed such +L+ that the user is likely to take the recommended action. +L+ Nineteen participants claimed to have previously seen these +L+ particular warnings. A significantly higher proportion of +L+ participants in the active IE condition (50%) claimed to have +L+ recognized the warnings as compared to participants in the +L+ Firefox condition (20%; p < 0.048 for Fisher’s exact test). +L+ Many of the participants who encountered the active IE warn- +L+ ing said that they had previously seen the same warning on +L+ websites which they trusted, and thus they ignored it. It is +L+ likely that they did not read this phishing warning because +L+ IE uses a similar warning when it encounters an expired or +L+ self-signed SSL certificate. Therefore they did not notice +L+ that this was a slightly different and more serious warning. +L+ We found a significant negative Pearson correlation between +L+ participants recognizing a warning message and their will- +L+ ingness to completely read it (r = —0.309, p < 0.03). +L+ This implies that if a warning is recognized, a user is sig- +L+ nificantly less likely to bother to read it completely (i.e. ha- +L+ bituation). Thus, very serious warnings should be designed +L+ differently than less serious warnings in order to increase the +L+ likelihood that users will read them. This was also the basis +L+ for Brustoloni and Villamar´ın-Salom´on’s work on dynamic +L+ warnings [3]. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Warning Comprehension +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A well-designed warning must convey a sense of danger and +L+ present suggested actions. In this study we asked partici- +L+ pants what they believed each warning meant. Twenty-seven +L+ of the 47 participants (57%) who saw at least one of the +L+ warnings correctly said they believed that they had some- +L+ thing to do with giving information to fraudulent websites +L+ (Table 2). Of the 20 participants who did not understand the +L+ meaning of the warnings, one said that she did not see it long +L+ enough to have any idea, while the others had widely varying +L+ answers. Examples include: “someone got my password,” +L+ “[it] was not very serious like most window[s] warning[s],” +L+ and “there was a lot of security because the items were cheap +L+ and because they were international.” +L+ Using Fisher’s exact test, we found that those using Firefox +L+ understood the meaning of the warnings significantly more +L+ than those exposed to the active IE warnings (p < 0.041) +L+ and the passive IE warnings (p < 0.005), though we found +L+ no significant difference between the active and passive IE +L+ warnings. We found a significant Pearson correlation be- +L+ tween completely reading a warning and understanding its +L+ meaning for the active IE warning (r = 0.478, p < 0.039), +L+ but not for Firefox. Since all but one Firefox user correctly +L+ understood what the warning wanted them to do, this im- +L+ plies that users did not need to completely read it to know +L+ the appropriate actions to take. +L+ Overall, 31 of the 47 participants who noticed the warnings +L+ mentioned that they thought they were supposed to leave +L+ the website or refrain from entering personal information. +L+ Those who did not understand the warnings provided re- +L+ sponses such as “panic and cancel my accounts,” “confirm +L+ information about the orders,” and “put in my account infor- +L+ mation so that they could track it and use it for themselves.” +L+ </SectLabel_bodyText> <SectLabel_page> 1071 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_subsectionHeader> Attitudes and Beliefs +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> We asked participants how their attitudes and beliefs influ- +L+ enced their perceptions and found a highly significant corre- +L+ lation between trusting and obeying the warnings (i.e. users +L+ who did not trust the warnings were likely to ignore them; +L+ r = 0.76, p < 0.0001). More telling, all but three partic- +L+ ipants who ignored a warning said it was because they did +L+ not trust the warning. Two of the participants who ignored +L+ the warnings in the active IE group said they did so because +L+ they trusted them but thought the warnings were not very se- +L+ vere (“since it gave me the option of still proceeding to the +L+ website, I figured it couldn’t be that bad”). The other par- +L+ ticipant who trusted the warning yet ignored it was in the +L+ passive IE group and blamed habituation (“my own PC con- +L+ stantly bombards me with similar messages”). All three of +L+ these participants questioned the likelihood of the risks, and +L+ thus were more interested in completing the primary task. +L+ We found a significant correlation between recognizing and +L+ ignoring a warning (r = 0.506, p < 0.0003). This fur- +L+ ther implies that habituation was to blame when participants +L+ ignored warnings: they confused them with similar look- +L+ ing, but less serious warnings, and thus did not understand +L+ the level of risk that these warnings were trying to convey. +L+ This was only a problem for the warnings used by IE, as +L+ all the Firefox users obeyed the warnings (though only 20% +L+ claimed to have seen them before, compared to the 50% with +L+ IE). The IE users who ignored the warnings made comments +L+ such as: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	“Oh, I always ignore those” +L+ •	“Looked like warnings I see at work which I know to ig- +L+ nore” +L+ •	“Have seen this warning before and [it] was in all cases +L+ [a] false positive” +L+ •	“I’ve already seen such warnings pop up for some other +L+ CMU web pages as well” +L+ •	“I see them daily” +L+ •	“I thought that the warnings were some usual ones dis- +L+ played by IE” +L+ </SectLabel_listItem> <SectLabel_bodyText> A warning should not require domain knowledge for a user +L+ to understand it. In order to examine whether prior knowl- +L+ edge of phishing impacted user attitudes towards the warn- +L+ ings, we asked them to define the term “phishing.” Twenty- +L+ six of the forty-seven participants who noticed the warn- +L+ ings were able to correctly say they had something to do +L+ with using fraudulent websites to steal personal information. +L+ We calculated Pearson’s correlation coefficient and found +L+ a significant correlation between knowing what phishing is +L+ and both reading (r = 0.487, p < 0.0005) and heeding +L+ (r = 0.406, p < 0.005) the warnings. Thus, if a user does +L+ not understand what phishing is, they are less likely to be +L+ concerned with the consequences, and thus less likely to pay +L+ attention to the warning. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Motivation and Warning Behaviors +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Table 1 depicts the number of participants from each condi- +L+ tion who fell for at least one phishing message. Some partic- +L+ ipants only clicked on one of the two phishing messages, and +L+ in other cases some participants only received one phishing +L+ message due to email filtering. +L+ Overall we found that active phishing warnings were signifi- +L+ cantly more effective than passive warnings (p < 0.0002 for +L+ Fisher’s exact test). We showed the passive Internet Explorer +L+ warning to ten different participants, but only one participant +L+ heeded it and closed the website, whereas the other times +L+ participants dismissed it and submitted personal information +L+ to the phishing websites (in two of these cases participants +L+ failed to notice the warnings altogether). We found that this +L+ passive warning did not perform significantly different than +L+ the control group (p < 1.0 for Fisher’s exact test). The ac- +L+ tive IE warning was ignored by nine participants, while in +L+ the Firefox condition every participant heeded the warning +L+ and navigated away from the phishing websites. This was a +L+ highly significant difference (p < 0.0004, for Fisher’s exact +L+ test), however the active IE warning still performed signifi- +L+ cantly better than the control condition (p < 0.01) and the +L+ passive IE warning (p < 0.044). +L+ Qualitatively, we examined why participants were motivated +L+ to heed or ignore the warnings. A total of thirty-one partic- +L+ ipants chose to heed the warnings, and in twenty-three of +L+ these cases participants said that the warnings made them +L+ think about risks: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	“I didn’t want to get burned” +L+ •	“...it is not necessary to run the risk of letting other poten- +L+ tially dangerous sites to get my information” +L+ •	“I chose to heed the warning since I don’t like to gamble +L+ with the little money I have” +L+ •	“I felt it better to be safe than sorry” +L+ •	“I heeded the warning because it seemed less risky than +L+ ignoring it” +L+ </SectLabel_listItem> <SectLabel_bodyText> Participants who chose to submit information said that they +L+ did so because they were unaware of the risks (i.e. they did +L+ not read the warnings), were used to ignoring similarly de- +L+ signed warnings (i.e. habituation), or they did not under- +L+ stand the choices that the warnings presented. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Environmental Stimuli +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the passive IE condition, three of the participants who ig- +L+ nored the warnings said they did so because they incorrectly +L+ placed some degree of trust in the phishing website because +L+ of stimuli other than the warning messages. When asked +L+ why they chose to ignore the warnings, one participant said +L+ she had “confidence in the website.” Another participant +L+ ignored the warning “because I trust the website that I am +L+ doing the online purchase at.” These answers corroborate +L+ Fogg’s work, showing that the look and feel of a website +L+ is often the biggest trust factor [10]. Participants who ig- +L+ nored the active IE warning provided similar answers, and +L+ also said that they ignored the warnings because they trusted +L+ the brands that the emails had spoofed. +L+ We also found that when some participants saw the warn- +L+ ings, they examined other security context information be- +L+ fore making a decision. One Firefox user reexamined the +L+ original phishing email and noticed the lack of any person- +L+ alized information. She then decided to “back out and log +L+ in from the root domain to check.” After seeing the warn- +L+ ings, ten other Firefox users also examined either the URL +L+ bar or the email headers. Some observations included: “The +L+ </SectLabel_bodyText> <SectLabel_page> 1072 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> URL did not match the usual eBay URL and so it could be +L+ fraudulent;” “I did look at the URL that I opened from the +L+ email, and the sender of the email, to confirm that they did +L+ look suspicious;” and “it made me look at the web address +L+ which was wrong.” One participant in the passive IE con- +L+ dition and three in the active IE condition incorrectly used +L+ this information to fall for the phishing attacks. Some of +L+ the comments included: “The address in the browser was +L+ of amazonaccounts.com which is a genuine address” and “I +L+ looked at the URL and it looked okay.” +L+ Finally, at least four participants claimed that the timing of +L+ the phishing emails with the purchases contributed to them +L+ ignoring the warnings. It is unclear how susceptible these +L+ participants would have been to a broader phishing attack, +L+ rather than the targeted attack that we examined. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In this section we provide some recommendations for im- +L+ proving the design of phishing indicators based on the results +L+ of our study. +L+ Interrupting the primary task — Phishing indicators need +L+ to be designed to interrupt the user’s task. We found that the +L+ passive indicator, which did not interrupt the user’s task, was +L+ not significantly different than not providing any warning. +L+ The active warnings were effective because they facilitated +L+ attention switch and maintenance. +L+ Providing clear choices — Phishing indicators need to pro- +L+ vide the user with clear options on how to proceed, rather +L+ than simply displaying a block of text. The users that no- +L+ ticed the passive Internet Explorer warning, read it but ig- +L+ nored it because they did not understand what they were sup- +L+ posed to do. They understood it had something to do with +L+ security, but they did not know how to proceed. In contrast, +L+ the active warnings presented choices and recommendations +L+ which were largely heeded. Wu found similar results with +L+ regard to providing users with clear choices [22]. +L+ Failing safely — Phishing indicators must be designed such +L+ that one can only proceed to the phishing website after read- +L+ ing the warning message. Users of the active Internet Ex- +L+ plorer warning who did not read the warning or choices could +L+ only close the window to get rid of the message. This pre- +L+ vented them from accessing the page without reviewing the +L+ warning’s recommendations. However, users of the passive +L+ Internet Explorer warning had the option of clicking the fa- +L+ miliar ‘X’ in the corner to dismiss it without reading it, and +L+ accessing the page anyway. +L+ Preventing habituation — Phishing indicators need to be +L+ distinguishable from less serious warnings and used only +L+ when there is a clear danger. Users ignored the passive in- +L+ dicators because they looked like many other warnings that +L+ users have ignored without consequences, thus they appear +L+ to be “crying wolf.” Even the active Internet Explorer warn- +L+ ing was not read in a few cases because users mistook it for +L+ other IE warnings. More people read the Firefox warnings +L+ because they are designed unlike any other warnings. Dy- +L+ namic warning messages may help prevent habituation [3]. +L+ Altering the phishing website — Phishing indicators need to +L+ distort the look and feel of the website such that the user does +L+ not place trust in it. This can be accomplished by altering its +L+ look or simply not displaying it at all. The overall look and +L+ feel of a website is usually the primary factor when users +L+ make trust decisions [10]. When the website was displayed +L+ alongside the passive indicators, users ignored the warnings +L+ because they said that they trusted the look of the website. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This study has given us insights into creating effective se- +L+ curity indicators within the context of phishing. Such in- +L+ dicators are clearly needed as 97% of participants believed +L+ the phishing emails enough to visit the URLs. Of the par- +L+ ticipants who saw the active warnings, 79% chose to heed +L+ them and close the phishing websites, whereas only 13% of +L+ those who saw the passive warnings obeyed them. Without +L+ the active warning indicators, it is likely that most partici- +L+ pants would have entered personal information. However, +L+ the active indicators did not perform equally: the indica- +L+ tors used by Firefox performed significantly better than the +L+ active warnings used by IE, though both performed signif- +L+ icantly better than the passive IE warnings (which was not +L+ significantly different from not showing any warnings in the +L+ control group). +L+ As phishing attacks continue to evolve, it is likely that highly +L+ targeted attacks will become more prevalent. Future indica- +L+ tors within the phishing context need to be designed such +L+ that they interrupt the user’s primary task, clearly convey the +L+ recommended actions to take, fail in a secure manner if the +L+ user does not understand or ignores them, draw trust away +L+ from the suspected phishing website, and prevent the user +L+ from becoming habituated. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Thanks to the members of the Supporting Trust Decisions +L+ project for their feedback, and Matthew Williams for his as- +L+ sistance. This work was supported in part by the National +L+ Science Foundation under grant CCF-0524189. The views +L+ and conclusions contained in this document are those of the +L+ authors and should not be interpreted as representing the of- +L+ ficial policies, either expressed or implied, of the National +L+ Science Foundation or the U.S. government. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. AMER, T. S., AND MARIS, J. B. Signal words and +L+ signal icons in application control and information +L+ technology exception messages – hazard matching and +L+ habituation effects. Tech. Rep. Working Paper +L+ Series–06-05, Northern Arizona University, Flagstaff, +L+ AZ, October 2006. +L+ 2. BANK OF AMERICA. How Bank of America SiteKey +L+ Works for Online Banking Security. +L+ http://www.bankofamerica.com/privacy/sitekey/, 2007. +L+ 3. BRUSTOLONI, J. C., AND VILLAMAR´IN-SALOM´ON, +L+ R. Improving security decisions with polymorphic and +L+ audited dialogs. In SOUPS ’07: Proceedings of the 3rd +L+ symposium on Usable privacy and security (New York, +L+ NY, USA, 2007), ACM Press, pp. 76–85. +L+ </SectLabel_reference> <SectLabel_page> 1073 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Am I Safe	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_reference> 4. CERTIFICATION AUTHORITY/BROWSER FORUM. +L+ Extended validation ssl certificates, Accessed: July 27, +L+ 2007. http://cabforum.org/. +L+ 5. CRANOR, L. F. What do they “indicate?”: Evaluating +L+ security and privacy indicators. Interactions 13, 3 +L+ (2006),45–47. +L+ 6. DHAMIJA, R., AND TYGAR, J. D. The battle against +L+ phishing: Dynamic security skins. In Proceedings of +L+ the 2005 Symposium on Usable Privacy and Security +L+ (New York, NY, USA, July 6-8 2005), ACM Press. +L+ 7. DHAMIJA, R., TYGAR, J. D., AND HEARST, M. Why +L+ phishing works. In CHI ’06: Proceedings of the +L+ SIGCHI conference on Human Factors in computing +L+ systems (New York, NY, USA, 2006), ACM Press, +L+ pp. 581–590. +L+ 8. DOWNS, J. S., HOLBROOK, M., AND CRANOR, L. +L+ Decision Strategies and Susceptibility to Phishing. In +L+ Proceedings of The 2006 Symposium on Usable +L+ Privacy and Security (Pittsburgh, PA, July 12-14, +L+ 2006). +L+ 9. FLORENCIO, D., AND HERLEY, C. A large-scale study +L+ of web password habits. In WWW ’07: Proceedings of +L+ the 16th international conference on World Wide Web +L+ (New York, NY, USA, 2007), ACM Press, pp. 657–666. +L+ 10. FOGG, B., MARSHALL, J., LARAKI, O., OSIPOVICH, +L+ A., VARMA, C., FANG, N., PAUL, J., RANGEKAR, +L+ A., SHON, J., SWANI, P., AND TREINEN, M. What +L+ Makes Web Sites Credible? A Report on a Large +L+ Quantitative Study. In Proceedings of the ACM +L+ Computer-Human Interaction Conference (Seattle, +L+ WA, March 31 - April 4, 2001), ACM. +L+ 11. GARTNER, INC. Gartner Says Number of Phishing +L+ E-Mails Sent to U.S. Adults Nearly Doubles in Just +L+ Two Years. +L+ http://www.gartner.com/it/page.jsp?id--498245, +L+ November 9 2006. +L+ 12. HELLIER, E., WRIGHT, D. B., EDWORTHY, J., AND +L+ NEWSTEAD, S. On the stability of the arousal strength +L+ of warning signal words. Applied Cognitive Psychology +L+ 14 (2000), 577–592. +L+ 13. JACKSON, C., SIMON, D., TAN, D., AND BARTH, A. +L+ An evaluation of extended validation and +L+ picture-in-picture phishing atttacks. In Proceedings of +L+ the 2007 Usable Security (USEC’07) Workshop +L+ (February 2007). +L+ http://www.usablesecurity.org/papers/jackson.pdf. +L+ 14. KUMARAGURU, P., RHEE, Y., ACQUISTI, A., +L+ CRANOR, L. F., HONG, J., AND NUNGE, E. +L+ Protecting people from phishing: the design and +L+ evaluation of an embedded training email system. In +L+ CHI ’07: Proceedings of the SIGCHI conference on +L+ Human factors in computing systems (New York, NY, +L+ USA, 2007), ACM Press, pp. 905–914. +L+ 15. MOORE, T., AND CLAYTON, R. An empirical analysis +L+ of the current state of phishing attack and defence. In +L+ Proceedings of the 2007 Workshop on The Economics +L+ of Information Security (WEIS2007) (May 2007). +L+ http://www.cl.cam.ac.uk/ twm29/weis07-phishing.pdf. +L+ 16. OBERHEIDE, J. Google safe browsing, November 6 +L+ 2006. http://jon.oberheide.org/blog/2006/11/13/google- +L+ safe-browsing/. +L+ 17. OPENDNS. PhishTank Annual Report. +L+ http://www.phishtank.com/, October 2007. +L+ 18. REFSNES DATA. Browser statistics, Accessed: April 4, +L+ 2007. +L+ http://www.w3schools.com/browsers/browsers stats.asp. +L+ 19. SCHECHTER, S. E., DHAMIJA, R., OZMENT, A., +L+ AND FISCHER, I. The emperor’s new security +L+ indicators. In Proceedings of the 2007 IEEE +L+ Symposium on Security and Privacy (May 2007). +L+ 20. SHENG, S., MAGNIEN, B., KUMARAGURU, P., +L+ ACQUISTI, A., CRANOR, L., HONG, J., AND NUNGE, +L+ E. Anti-phishing phil: The design and evaluation of a +L+ game that teaches people not to fall for phish. In +L+ Proceedings of the 2007 Symposium On Usable +L+ Privacy and Security (Pittsburgh, PA, July 18-20, +L+ 2007), ACM Press. +L+ 21. WOGALTER, M. S. Communication-Human +L+ Information Processing (C-HIP) Model. In Handbook +L+ of Warnings, M. S. Wogalter, Ed. Lawrence Erlbaum +L+ Associates, 2006, pp. 51–61. +L+ 22. WU, M. Fighting Phishing at the User Interface. PhD +L+ thesis, Massachusetts Institute of Technology, August +L+ 2006. +L+ 23. WU, M., MILLER, R. C., AND GARFINKEL, S. L. Do +L+ Security Toolbars Actually Prevent Phishing Attacks? +L+ In Proceedings of the SIGCHI Conference on Human +L+ Factors in Computing Systems Held in Montreal +L+ (2006), ACM Press, pp. 601–610. +L+ 24. YE, Z. E., AND SMITH, S. Trusted paths for browsers. +L+ In Proceedings of the 11 th USENIX Security +L+ Symposium (2002), pp. 263–279. +L+ 25. YEE, K.-P., AND SITAKER, K. Passpet: Convenient +L+ password management and phishing protection. In +L+ SOUPS ’06: Proceedings of the Second Symposium on +L+ Usable Privacy and Security (New York, NY, USA, +L+ 2006), ACM Press, pp. 32–43. +L+ 26. ZHANG, Y., EGELMAN, S., CRANOR, L. F., AND +L+ HONG, J. Phinding phish: Evaluating anti-phishing +L+ tools. In Proceedings of the 14th Annual Network & +L+ Distributed System Security Symposium (NDSS 2007) +L+ (28th February - 2nd March, 2007). +L+ http://lorrie.cranor.org/pubs/toolbars.html. +L+ </SectLabel_reference> <SectLabel_page> 1074 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> The Adaptation of Visual Search Strategy to Expected +L+ Information Gain +L+ </SectLabel_title> <SectLabel_author> Yuan-Chi Tseng +L+ </SectLabel_author> <SectLabel_affiliation> The University of Manchester +L+ </SectLabel_affiliation> <SectLabel_address> MBS Crawford House 2.30, Booth Street West, +L+ Manchester, M15 6PB, UK +L+ </SectLabel_address> <SectLabel_email> yuan-chi.tseng@postgrad.manchester.ac.uk +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> An important question for Human-Computer Interaction is +L+ to understand how and why visual search strategy is +L+ adapted to the demands imposed by the task of searching +L+ the results of a search engine. There is emerging evidence +L+ that a key part of the answer concerns the expected +L+ information gain of each of the set of available information +L+ gathering actions. We build on previous research to show +L+ that people are acutely sensitive to differences in the density +L+ and in the number of items returned by the search engine. +L+ These factors cause shifts in the efficiency of the available +L+ information gathering actions. We focus on an image +L+ browsing task, and show that, as a consequence of changes +L+ to the efficiency of available actions, people make small but +L+ significant changes to eye-movement strategy. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Visual Search, Visual Exploration, Strategy, Cognitive +L+ Models, Decision Making, Image Search Results, +L+ Thumbnails, Eye Movement, Density, Spacing +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> H.5.2 Information Interfaces and Presentation: User +L+ Interfaces – Theory and methods; H.1.2 Models and +L+ Principles: User/Machine Systems – Human Information +L+ Processing; H.3.3 Information Storage and Retrieval: +L+ Information Search and Retrieval – Selection process +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> An important question for Human-Computer Interaction +L+ (HCI) is how to design the presentation of search results to +L+ facilitate visual search behaviour [9,20,35]. Researchers +L+ have proposed many alternatives to the standard list of +L+ results. They include Space-filling thumbnails of text [7], +L+ Tabular interface [36], Faceted category interface [42], and +L+ Textually-enhanced thumbnails [41]. +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00. +L+ </SectLabel_copyright> <SectLabel_author> Andrew Howes +L+ </SectLabel_author> <SectLabel_affiliation> The University of Manchester +L+ </SectLabel_affiliation> <SectLabel_address> MBS East F31, Booth Street West, Manchester, +L+ M15 6PB, UK +L+ </SectLabel_address> <SectLabel_email> howesa@manchester.ac.uk +L+ </SectLabel_email> <SectLabel_bodyText> The number of proposals is, in part, a reflection of the scale +L+ of the design space. Identifying which of the potentially +L+ hundreds of interesting points in this space is best might be +L+ informed by a theory of how people choose to search +L+ through results, e.g. an ACT-R or EPIC theory of the +L+ cognitive, motor, and perceptual processing required to +L+ achieve the task [2,18,22,23]. Recent efforts to understand, +L+ what Payne, Howes and Reader [28] call interactive search +L+ tasks, have included [5,6,8,13,31]. However, it is +L+ sometimes difficult to ascertain which interface will be best +L+ when the details of the strategy that people will use given a +L+ particular task environment are unclear [10,14,17,19]. In +L+ this paper we are interested in the strategy choices that +L+ people make about visual search, i.e. how they choose to +L+ look at search results. Our focus is on the assumption that +L+ people attempt to maximise expected utility given the +L+ constraints of the human visual system [8,39] and that +L+ design changes which change the efficiency of information +L+ gathering actions will lead to changes in observed +L+ behaviour. +L+ There is some evidence in the visual perception and +L+ cognition literature that visual search strategy is adapted to +L+ the demands imposed by particular task environments +L+ [4,26,40]. It appears that people seek to maximise the +L+ efficiency of visual search in the context of the particular +L+ display layout. For example, longer fixations enable more +L+ information to be gathered from fovea and peripheral +L+ vision, although longer fixations can only be effective if the +L+ information is available within the perceptual span [26]. +L+ Moreover, people need to manage the trade-off between the +L+ increased information gain of longer fixations and the effort +L+ and time cost of holding a fixation. +L+ Some evidence that people adapt their strategy to the task +L+ can also be found in the HCI literature. For example, trade- +L+ offs in the use of wide versus narrow spacing between icons +L+ [11], in more but smaller text versus fewer but bigger text +L+ in the same space [15] and in the use of Hyperbolic browser +L+ versus standard browser [30]. Even in a standard results +L+ layout, such as a simple list of links, there is evidence that +L+ search strategy is affected by the context in which the +L+ relevance of items is assessed [5,6,8,13]. The position of a +L+ target item in a list and relationship between the relevance +L+ of this target and distractor items affects search behaviour +L+ </SectLabel_bodyText> <SectLabel_page> 1075 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> [5,6,13]. These finding support the view that search and +L+ selection is guided by maximising the expected information +L+ gain [8]. Strikingly there is evidence that maximising +L+ expected gain can involve fixating on every other item in a +L+ list, rather than fixating on contiguous items [5,6], +L+ particularly in contexts where good results have already +L+ been encountered. +L+ In a number of respects people appear to be sensitive to the +L+ expected information gain of information gathering actions. +L+ In the following section we review in more detail the +L+ literature on the trade-offs that contribute toward the +L+ efficiency of information gathering actions during visual +L+ search. We are interested in the broad class of interactive +L+ search tasks [28] but our particular focus is on the visual +L+ search strategies and decision-making processes that people +L+ use when looking through items returned by an image +L+ search engine such as Flickr or Google. Further, we are +L+ interested in everyday tasks with somewhat underspecified +L+ target criteria, e.g. look for attractive images of the city +L+ Florence. In what follows we consider the relevance of the +L+ visual search and HCI literature to understanding these +L+ kinds of task. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> Visual Search in HCI +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Researchers in HCI have suggested that the details of +L+ interface design affect visual search strategy (e.g., +L+ [11,15,30]). Everett and Byrne [11], for example, suggested +L+ a small difference of 1.6 degrees of visual angle between +L+ items can result in participants either fixating on an icon or +L+ not. Similarly, Halverson and Hornof [15] provided +L+ evidence that low density, task-meaningless large words +L+ could lead participants to use fewer and shorter fixations +L+ and so shorter overall search time than when given high +L+ density, small, task-meaningless, words. Similarly, Pirolli, +L+ Card, and Van Der Wege [30] found that participants used +L+ more but shorter fixations when using a Hyperbolic browser +L+ than when using a standard browser, especially in areas of +L+ the Hyperbolic browser in which small, low information +L+ scent items, were grouped closely together. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Visual Search in Experimental Psychology +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> In the visual search literature, there is compelling evidence +L+ that the density of items on the display has consequences +L+ for search strategy. Ojanpää, Näsänen and Kojo [26] found +L+ that decreased spacing in a vertical list of words (common +L+ Finnish verbs, nouns and adjectives) resulted in longer but +L+ fewer numerous fixations. Vlaskamp, Over and Hooge [40] +L+ found that the fixation duration, number of fixation and +L+ search time increased dramatically with decreasing item +L+ spacing, as the range of spacing smaller than 1.5° visual +L+ angle. On the other hand, their data showed that at wide +L+ spacing range between 1.5° to 7.1° fixation duration, +L+ number of fixations and search time increased slightly as +L+ the spacing increased. Bertera and Rayner [4] found that as +L+ the item spacing increased the number of fixations and +L+ fixation duration increased. +L+ Although allowing a high degree of experimental control +L+ the tasks [4,26,40] lack ecological validity. For example, +L+ Vlaskamp, Over and Hooge [40] used abstract shapes (e.g., +L+ squares) in their search task, and, Bertera and Rayner [4] +L+ used an unstructured alphanumeric array. Ojanpää, Näsänen +L+ and Kojo [26]) used common-words, which reduced the +L+ task to a simple visual pattern match, rather than a match of +L+ information scent but it is known that search behaviour is +L+ contingent on label relevance [5,6]. The different materials +L+ may account for the different effects. Both tasks are far +L+ from the real HCI task in which the stimuli are more +L+ heterogeneous and complicated. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Strategy Change during Search +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Visual search strategies are also known to change during +L+ the course of a search [27,33]. Over, Hooge, Vlaskamp and +L+ Erkelens [27] found that fixation duration increased and the +L+ amplitude of saccade decreased gradually as search +L+ progressed. They called this is a coarse-to-fine strategy. +L+ Rao, Zelinsky, Hayhoe and Ballard [33] used a coarse-to- +L+ fine matching mechanism to model the skipping saccades +L+ because it could increase the probability of an early match. +L+ In contrast, Brumby and Howes [6] found a fine-to-coarse +L+ search strategy. People increased saccade amplitude once +L+ they had found, but not committed to, a highly relevant +L+ target suggesting that although it is known that strategy can +L+ change, the reasons for change are not always clear. +L+ Also, it is known that people spend more time examining +L+ items that are presented nearer to the top of the returned +L+ search results [9,13], presumably, and in part, because +L+ search engines tend to rank order results. However, what is +L+ not clear is whether people change visual search strategy as +L+ they examine links further from the top of the results list. +L+ For example, they may be more likely to skip i.e. fixate +L+ alternate, non-adjacent, items once a potential hit has been +L+ found, as was observed in [6]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> THEORY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We assume that people make strategic adaptations in order +L+ to improve the efficiency of visual search, that is that they +L+ rationally adapt the strategy, given cognitive, perceptual, +L+ and motor constraints, to maximise benefits while +L+ minimising costs [1,8,17,28,29]. More specifically, people +L+ adapt their eye movement strategy to maximise the +L+ expected gain of task-relevant information and minimise the +L+ neural resources devoted to memory [25] and cognitive and +L+ attentional load [3]. In this paper, we take an “active vision” +L+ approach [12] to understanding the complex interactive +L+ behaviours that people exhibit when searching for +L+ information. Understanding these behaviours requires +L+ bridging between theories of visual search, visual +L+ exploration, and decision-making. +L+ Following Cox and Young [8] we assumed that people are +L+ sensitive to the prior probabilities that any particular item +L+ will be the target item. Cox and Young operationalised this +L+ assumption by assuming that prior probabilities were +L+ </SectLabel_bodyText> <SectLabel_page> 1076 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> normalized over the set of potential targets. More search +L+ items implies lower initial priors. +L+ Following Ojanpää, Näsänen and Kojo [26] we assumed +L+ that the information that can be gathered given the +L+ perceptual span is (1) contingent on the item density, (2) +L+ contingent on the duration of fixation. +L+ Combining the theories of Cox and Young [8] and Ojanpää, +L+ Näsänen and Kojo [26] we assume that the visual search +L+ strategy is rationally adapted to the information that can be +L+ gained from the perceptual span. On this basis we made +L+ two predictions: +L+ (1) that packing items together more closely (high density) +L+ would allow more information to be harvested from the +L+ perceptual span given sufficient time. We predicate that +L+ people would therefore increase the duration spent looking +L+ at any one item, and reduce the number of items directly +L+ fixated. That is, they would take advantage of the fact that +L+ more information was available for each fixation. +L+ (2) if there are more search items then the gaze durations +L+ and the rate of number of items directly fixated in a trial +L+ should decrease because the prior probability of any one +L+ item being the target item is decreased [8]. In other words, +L+ when there are more results returned we expect people to +L+ give each result less time, presumably reducing the quality +L+ of each evaluation in order to conduct more evaluations. +L+ The eye movement strategy should switch to a lower cost +L+ strategy, involving more item skipping [6]. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> EXPERIMENT +L+ </SectLabel_sectionHeader> <SectLabel_subsectionHeader> Method +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> This study investigates the consequences of expected +L+ information gain for the visual search strategy that people +L+ use given pages of thumbnails returned by a search engine. +L+ Participants were asked to imagine that they were choosing +L+ where to go on holiday, and that they were using the web to +L+ gather information about various places that they could visit. +L+ In particular, their goal was to use a particular image search +L+ engine site to find an image of a visually attractive +L+ destination. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Stimuli and Design +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> There were 180 unique sets of images. Each set included +L+ image thumbnails about a place, such as a resort, city or +L+ country. These image thumbnails were selected from the +L+ search results responding to keywords or search phrases +L+ input in an image search engine of Flickr, a photo sharing +L+ website. Moreover, each set at least contains 1,000 photo +L+ results matching the name of the place. For example, Flickr +L+ found 423,166 up-to-date results for photos matching +L+ Florence. We filtered out the tourists’ poor quality self- +L+ portrait, maps and pictures with special effect, such as High +L+ Dynamic Range (HDR) images which is not clear in the +L+ task thumbnail size. Finally, we selected 180 photo +L+ thumbnails for each set. So, totally, we had unique 32,400 +L+ thumbnails (180 sets × 180 thumbnails). At the eye-to- +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1: an example of search task with high density +L+ layout and one number of pages condition. +L+ Figure 2: A sample trial of low density layout and 5 pages +L+ set size. This shows the thumbnails on page 3. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> screen distance of 60 centimeters used in the experiment, +L+ the size of thumbnails (75 × 75 pixels) subtended a visual +L+ angle of 2.15°. +L+ The experiment was a within-subjects design and had two +L+ independent variables. These were thumbnail density and +L+ number of pages (set size). Density had two levels with +L+ narrow or wide spacing between items displayed in the +L+ search task. Figure 1 shows a sample display from one high +L+ density trial. The edge-to-edge item spacings were 3 pixels +L+ (visual angle = 0.085°) in high density layout. Figure 2 +L+ </SectLabel_bodyText> <SectLabel_page> 1077 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> shows one low density trial. The edge-to-edge item +L+ spacings were 30 pixels (visual angle = 0.85°) in low +L+ density display. Unlike in Halverson and Hornof [15] and +L+ Pirolli, Card, and Van Der Wege [30] the size of thumbnails +L+ was held constant, so that density was manipulated by +L+ changing proximity only. +L+ Number of pages had three levels: 1-page, 2-pages and 5- +L+ pages. Each page consisted of exactly 36 thumbnails (6 × 6 +L+ square layout). In the 1-page condition, there were 36 +L+ thumbnails in total and the first 36 of 180 thumbnails of +L+ sets were used. In 2-pages condition, 1st to 36th thumbnails +L+ were used in the first page; 37th to 72nd thumbnails were +L+ used in the second page and there were 72 thumbnails in +L+ total. In the 5-pages condition, all 180 thumbnails were +L+ used sequentially. There were page links below thumbnails +L+ in any page. It appeared all the time (see figure 1 and 2). +L+ Participants could click these links to switch between pages +L+ during a trial. So, when Participants searched in the first +L+ page, they also gained the information about how many +L+ pages in this task. +L+ In total, there were 6 conditions (2 densities × 3 number of +L+ pages). Each condition had 30 trials. The total number of +L+ trials was 180. Each set of thumbnails could be arranged +L+ into one of six conditions. The presentations of each set of +L+ thumbnails were counterbalanced across subjects. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Apparatus +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> The experiment was performed with a Tobii 1750 eye +L+ tracker. The eyetracker is embodied in a 17” monitor. The +L+ eye tracker has a tracking rate or the frame rate of 50 Hz. +L+ The temporal resolution of 20 ms is sufficient for +L+ monitoring long fixation and eye movements in our task. +L+ Although low temporal resolution could cause noise in +L+ signal sampling, noise was reduced by averaging several +L+ gazes per page and by testing 30 trials per condition. The +L+ screen resolutions were set at 1024×768 pixels. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Participants +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Participants were twenty four students from University of +L+ Manchester who received 5-10 pounds depending on their +L+ time for their volunteer. All participants were between 18 +L+ and 28 years old and have normal vision or corrected-to- +L+ normal vision. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Procedure +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Participants were presented with instructions and then a +L+ practice block allowed them to become familiar with the +L+ task. After the practice block, the eye tracker system was +L+ calibrated. And then participants completed all 180 trials. +L+ The whole Experiment took between 40 minutes and 70 +L+ minutes depending on different participants. The variation +L+ in time was because we didn’t give our participants any +L+ time limitation. Some participants were faster than others. +L+ The participant looked at a place label then clicked on the +L+ search button to make thumbnail search results appear. The +L+ participant was instructed to move the cursor to a thumbnail +L+ of a visually attractive destination and click on it to finish a +L+ trial. The trial process and interface are shown in Figure 1. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Results +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Normally, eye movement studies analysed participants’ +L+ search time, number of fixations and fixation duration. +L+ When we observed the participants’ search behaviour, we +L+ also found the gaze transitions were not always from one +L+ item to the next adjacent item; instead, participants skipped +L+ one or more items during their saccade. We calculated how +L+ many gaze transitions were not between spatially adjacent +L+ thumbnails. Therefore, in this study, we were interested in +L+ visual search performance, particularly in how (1) search +L+ time and (2) number of item gazes (all contiguous fixations +L+ on an item were combined to be a single gaze), (3) gaze +L+ duration (the sum of all fixation durations on a thumbnail +L+ prior to moving to another thumbnail), and (4) the +L+ proportion of skipping gaze transitions (the total number of +L+ gaze transitions divided by the number of skipping gaze +L+ transitions), are affected by the thumbnails density and +L+ number of pages. The data in the revisited page were not +L+ included. These four measures might reveal the adaptation +L+ of search strategy while participants interact with different +L+ item densities and number of items. +L+ We predicted that when the thumbnails were displayed +L+ close together, participants would spend longer in a gaze, +L+ and be more likely to skip thumbnails during search in the +L+ display, than when thumbnails were displayed further apart. +L+ In addition, page links below the thumbnails indicated that +L+ the number of pages and total items in the task. We +L+ predicted that when there are more pages and thumbnails, +L+ people would adopt a different strategy which would be +L+ revealed by these four measures. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Overall Performance +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Figure 3 illustrates the overall performance. There were +L+ significant main density effect on total search time (F(1,23) +L+ = 23.43, p < 0.001), number of gazes (F(1,23) = 15.87, +L+ p=0.001), gaze duration (F(1,23) = 17.34, p < 0.001), and +L+ proportion of skipping gaze transitions (F(1,23) = 35.04, +L+ p<0.001). The results showed that participant spent less +L+ time (M = 12,709 ms vs 14,044 ms, figure 3a), fewer gazes +L+ (M = 8.16 vs 9.28, figure 3b), longer gaze (M = 407 ms vs +L+ 387 ms, figure 3c) and skipped more often (M = 72.4% vs +L+ 67.7%, figure 3d) in high versus low density. +L+ There were significant main number of pages effects on +L+ total search time (F(2,46) = 136.54, p < 0.001), number of +L+ gazes (F(2,46) = 73.18, p < 0.001), gaze duration (F(2,46) = +L+ 10.16, p < 0.001), and proportion of skipping gaze +L+ transitions (F(2,46) = 8.37, p=0.001). A Bonferroni test +L+ revealed that every pairwise comparison was significantly +L+ different (p < 0.05). According to the pairwise comparison, +L+ participants spent significantly less time (1-page: M = 7,514 +L+ ms, 2-pages: M = 11,482 ms and 5-pages: M = 21,132 ms), +L+ gazed less (1-page: M = 10.96, 2-pages: M = 15.8 and 5- +L+ pages: M = 28.02), and skipped less often (1-page: M = +L+ 66.5%, 2-pages: M = 70.7% and 5-pages: M = 72.9%), in +L+ </SectLabel_bodyText> <SectLabel_page> 1078 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 3: four measures of overall search performance per +L+ density and number of page condition. (a) mean response +L+ times; (b) mean number of gazes; (c) mean gaze duration +L+ and (d) proportion of skipping gaze transitions for high +L+ density (white bars) and low density (grey bars) in each +L+ page set size. Error bars indicate 95% confidence intervals +L+ and also in all figures below. +L+ </SectLabel_figureCaption> <SectLabel_bodyText> the fewer page condition. Also, pairwise comparison +L+ showed that they spent longer gaze duration in the 1-page +L+ condition (M = 401 ms) than 5-page condition (M = 369 +L+ ms), although 2-pages is not significant different to 1-page +L+ and 5-pages condition. There were no significant +L+ interactions on total search time, number of gazes and gaze +L+ duration, except proportion of skipping gaze transitions, +L+ F(2,46) = 3.73, p < 0.05. +L+ Moreover, following Reiman, Young and Howes [34], we +L+ analysed the revisiting. Figure 4a shows the number of +L+ items that were visited at least once and revisited (i.e. +L+ visited at least twice) for each experimental condition. +L+ Participant revisited around 22% to 27% of visited items for +L+ each condition. Density and number of pages had similar +L+ effects on the number of visited and the number of revisited +L+ items. Participants visited and revisited more items in low +L+ density (all p’s < 0.01) and when there were more pages (a +L+ Bonferroni test for analysing three page levels showed +L+ every pair was significantly different, p < 0.001). In +L+ number of pages +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 4: (a) mean number of items visited at least once +L+ (bars with solid pattern) and items visited at least twice +L+ (bars with line pattern) and (b) mean duration of an item +L+ visited at the first time visit (bars with solid pattern) and +L+ at the second time visit (bars with line pattern). +L+ </SectLabel_figureCaption> <SectLabel_bodyText> addition, figure 4b shows that our participants spent more +L+ time on an item when they revisited it than when they +L+ visited it the fist time, F(1,20) = 99.788, P < 0.001. +L+ Performance in the First Page +L+ We analysed data in more detail. In this section we contrast +L+ performance in the first page of each condition. (Not all +L+ conditions had subsequent pages so the first page provides +L+ the critical test. In addition, it is the first page behaviour +L+ that should be affected by information about the number of +L+ pages, rather than by the quality of the additional items.) +L+ We analysed our four measures (search time, number of +L+ gazes, gaze duration and skipping rate) taken from the first +L+ page of search results. We conducted a 2 × 3 (density × +L+ number of pages) repeated-measure ANOVA for each +L+ measure separately. +L+ Duration on a page was defined as the total time from the +L+ display onset to the time leaving the first page by click a +L+ page number link, or from the onset to the start of +L+ participants’ last fixation on the clicked target. +L+ As expected, there was a main effect of density on mean +L+ search time in the first page, F(1,23) = 28.27, p < 0.001. +L+ The search performance in the fist page was quicker in high +L+ density (M = 6082.24 ms) than low density (M = 6742.9 +L+ ms). There was also a main effect of number of pages on +L+ search time of the first page, F(2,46) = 15.82,p < 0.001. +L+ </SectLabel_bodyText> <SectLabel_figure> 1	2	5	(a) +L+ 25 +L+ 20 +L+ 35 +L+ 30 +L+ 15 +L+ 10 +L+ 5 +L+ 0 +L+ 1	2	5	(b) +L+ 450 +L+ 430 +L+ 410 +L+ 390 +L+ 370 +L+ 350 +L+ 1	2	5	(c) +L+ 100% +L+ 90% +L+ 80% +L+ 70% +L+ 60% +L+ 50% +L+ 1	2	5	(d) +L+ number of pages +L+ 30000 +L+ 25000 +L+ 20000 +L+ 15000 +L+ 10000 +L+ 5000 +L+ high density +L+ low density +L+ 1	2	5	(a +L+ 1	2	5	(b) +L+ 35 +L+ 30 +L+ 25 +L+ 20 +L+ 15 +L+ 10 +L+ 5 +L+ 0 +L+ at least once, high density +L+ at least tw ice, high density +L+ at least once, low density +L+ at least twice, low density +L+ first time visit, high density +L+ second time visit, high density +L+ first time visit, low density +L+ second time visit, low density +L+ 700 +L+ 600 +L+ 500 +L+ 400 +L+ 300 +L+ 200 +L+ </SectLabel_figure> <SectLabel_page> 1079 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> Figure 5c illustrates mean first page gaze duration. +L+ Participants’ mean gaze duration in the first page was +L+ longer in the high density (M = 407 ms) than low density +L+ layout (M = 387.259 ms), F(1,23) = 22.414, p < 0.001. The +L+ main effect of number of pages was not significant. There +L+ was no interaction between density and number of pages. +L+ Figure 5d shows the data of mean proportion of first page +L+ skipping gaze transitions. The density had significant effect, +L+ F (1, 23) = 46.72, p < 0.001. It was found that that +L+ participants were more likely to skip thumbnails in the high +L+ density (M = 72%) than in the low density layout (M = +L+ 67.2%). The main effect of number of pages was also +L+ significant, F (2, 46) = 15.00, p < 0.001. Every pairwise +L+ comparison of number of pages condition was significantly +L+ different (p < 0.01). It showed that our participants skipped +L+ more often in 5-pages condition (M = 66.5%), than they did +L+ in 2-pages condition (M = 70.3%), which in turn was more +L+ often than in the 1-page condition (M = 71.8%). There was +L+ no interaction between density and number of pages. +L+ </SectLabel_bodyText> <SectLabel_figure> 15 +L+ 12 +L+ 9 +L+ 6 +L+ 3 +L+ 1	2	5	(b +L+ high density +L+ low density +L+ 1	2	5 +L+ (a +L+ 12500 +L+ 10500 +L+ 8500 +L+ 6500 +L+ 4500 +L+ 2500 +L+ 1	2	5 +L+ (c +L+ 450 +L+ 430 +L+ 410 +L+ 390 +L+ 370 +L+ 350 +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 5: the four measures of mean search performance in +L+ the first page per density condition and number of page +L+ condition. (a) mean duration of first page; (b) mean number +L+ of first page gazes; (c) mean first page gaze duration and (d) +L+ mean proportion of first page skipping gaze transitions. +L+ </SectLabel_figureCaption> <SectLabel_figure> 100% +L+ 90% +L+ 80% +L+ 70% +L+ 60% +L+ 50% +L+ 1	2	5 +L+ number of pages +L+ (d) +L+ 10000 +L+ 8000 +L+ 6000 +L+ 4000 +L+ 2000 +L+ 15 +L+ 12 +L+ 9 +L+ 6 +L+ 3 +L+ 1 page: p.1	2 pages: pp.1-2	5 pages: pp.1-5 +L+ p.1	p.2	p.3	p.4	p.5 +L+ (a) +L+ </SectLabel_figure> <SectLabel_bodyText> Every pair comparison of number of pages condition with +L+ Bonferroni method was significantly different (p < 0.01). It +L+ was found that time spent in the first page was more in the +L+ fewer page condition (1-page: M = 7514.54 ms, 2-pages: M +L+ = 6117.07 ms and 5-pages: M = 5606.10 ms). The +L+ interaction between density and number of pages was not +L+ significant. Results are summarized in Figure 5a. +L+ Figure 5b shows the data of mean number of gazes in the +L+ first page. Number of gazes was approximate one fewer in +L+ the high density (M = 8.16) than low density (M = 9.29), +L+ F(1,23) = 24.65, p < 0.001. There was also a main effect of +L+ number of pages, F(2,46) = 24.9, p < 0.001. Every pair +L+ comparison of number of pages condition with Bonferroni +L+ method was significantly different (p < 0.001). It showed +L+ that participants spent greater number of gazes in fewer +L+ page conditions (1-page: M = 10.96, 2-pages: M = 8.07 and +L+ 5-pages condition: M = 7.11). In addition, the interaction of +L+ density and number of pages was also significant, F(2,46) = +L+ 4.48, p = 0.017. The fewest number of gazes were made in +L+ the first page of the high density display when there were 5- +L+ pages of items. +L+ </SectLabel_bodyText> <SectLabel_figure> 1 page: p.1	2 pages: pp.1-2	5 pages: pp.1-5 (b) +L+ 1 page: p.1	2 pages: pp.1-2	5 pages: pp.1-5 (d) +L+ number of pages +L+ </SectLabel_figure> <SectLabel_figureCaption> Figure 6: the four measures of mean search performance +L+ per page. Page 1, page 1 to 2 and page 1 to 5 of 1-page, 2- +L+ pages and 5-pages condition, respectively in high density +L+ condition. (a) mean search time per page; (b) mean +L+ number of gazes per page; (c) mean gaze duration per +L+ page and (d) mean proportion of skipping gaze +L+ transitions per page. +L+ </SectLabel_figureCaption> <SectLabel_figure> 	500 450 400 350 300 +L+ 1 page: p.1	2 pages: pp.1-2	5 pages: pp.1-5 (c) +L+ 	100% 90% 80% 70% 60% 50% +L+ </SectLabel_figure> <SectLabel_page> 1080 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_subsubsectionHeader> Performance per Page +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Figure 6 summarises the mean performance of our eighteen +L+ participants’ eye movement behaviour in the high density +L+ condition. (All results of the low density condition has the +L+ same trend as in the high density condition) Figure 6a +L+ shows mean search duration per page, figure 6b shows +L+ mean number of gazes per page, figure 6c shows mean gaze +L+ duration per page and figure 6d shows the proportion of +L+ skipping gaze transitions in number of pages condition. +L+ Each bar within a page condition shows the performance on +L+ each page of that condition. For example, the only bar in the +L+ 1-page condition shows performance in the 1-page +L+ condition, the two bars in 2-pages condition show each +L+ performance in page 1 and 2 and the five bars in 5-page +L+ condition show each performance from page 1 to page 5, +L+ from left to right respectively. +L+ When we examined whether participants adopted different +L+ strategies in different pages in a search course, the values of +L+ four measures of search performance were taken from each +L+ page. +L+ Because there were four participants who didn’t have data +L+ in all five pages in the 5-page condition, we took out these +L+ four participants. Then, we arbitrarily selected and removed +L+ another two participants from the analysis to keep the +L+ counterbalanced design. Therefore, in this section, the +L+ results were from analysing eighteen participants’ data. The +L+ following analyses were conducted in 2-pages and 5-pages +L+ condition separately. +L+ In the 2-pages condition, every measure of four search +L+ performance of participants was not significantly different +L+ in page 1 and page2. it indicated that the search strategy did +L+ not vary in the first and second page, when participants +L+ search in a 2-pages display (see Figure 6a, b, c, d, 2-pages +L+ condition). +L+ In 5-pages condition, the results of a 5×2 repeated-measure +L+ ANOVA (page number: page 1, 2, 3, 4 and 5 × density: +L+ high and low) showed there were main effect of page +L+ number and density on every measure of search +L+ performance our participants’ performance (see Figure 6 a, +L+ b, c, d, 5-pages condition). The results of search +L+ performance per page in the 5-pages condition are +L+ summarized in Table 1. In general, search performance was +L+ in page number 1 is different to other pages. +L+ pairwise comparisons of page number with Bonferroni +L+ method in search duration, number of gazes, and proportion +L+ of skipping gaze transitions showed that the measures in +L+ page 1 were different from those in page 2 to page 5 (pair +L+ between page 1 and any other page was significant, p<0.01 +L+ and pair between page 2 to page 5 were not significant +L+ different). There was a significant difference between page +L+ 1 and page 3 in gaze duration. Although any other pair does +L+ not reach significant, there was a trend for people to take +L+ longer gaze on page 1 than on pages 2, 3, 4, or 5. This +L+ indicated that participants search in the first page more +L+ carefully than other pages. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The results support the claim that visual search strategy is +L+ guided by expected information gain when people search +L+ results returned by an image search engine: +L+ 1. Participants were observed to adjust the duration that +L+ they attended to each item to the item density. When more +L+ </SectLabel_bodyText> <SectLabel_table> 		search duration (ms)	number of gazes		gaze duration (ms)	proportion of skipping gaze +L+ 						transitions +L+ 		mean	SD	mean	SD	mean	SD	mean	SD +L+ page number +L+ 	p. 1	5,238.64a	2,048.75	6.83 a	3.49	406.8 1b	55.64	74.64%a	0.08 +L+ 	p. 2	4,004.51	1,494.82	5.35	2.72	388.18	37.58	79.33%	0.08 +L+ high density	p. 3	3,796.32	1,295.97	5.27	2.30	377.32 c	39.58	79.36%	0.06 +L+ 	p. 4	3,781.66	1,359.42	5.13	2.41	380.30	34.42	77.55%	0.08 +L+ 	p. 5	3,999.54	1,653.50	5.50	2.88	381.00	36.14	77.86%	0.06 +L+ 	p. 1	5,810.45a	2,240.65	7.95 a	4.01	399.47 b	57.47	69.53%a	0.08 +L+ 	p. 2	4,176.94	1,853.84	5.64	3.20	365.01	37.52	77.33%	0.07 +L+ low density	p. 3	4,164.26	1,624.12	5.82	2.97	365.20 c	38.85	76.84%	0.09 +L+ 	p. 4	4,282.31	1,828.08	5.98	3.14	378.27	55.09	74.17%	0.08 +L+ 	p. 5	4,346.20	1,540.77	5.91	2.53	381.86	36.53	74.66%	0.06 +L+ repeated-measures	spacing	F	MSE	p	F	MSE	p	F	MSE	p	F	MSE	p +L+ ANOVA +L+ 		14.32	4.83E+05 0.001	9.54	1.942	0.007	4.56	757.26	0.048	13.73	0.003	0.002 +L+ 	page	29.63	5.21E+05 <0.001	17.06	1.397	<0.001	5.4	998.24	0.001	9.70	0.002	<0.001 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1. Search duration, number of gazes, gaze duration and proportion of skipping gaze transitions across per page in the 5- +L+ pages condition and per density condition. (Note. p. = page number. asearch performance is significant different to all other pages in the same +L+ density condition. b,csearch performance is significant different to each other in the same density condition) +L+ </SectLabel_tableCaption> <SectLabel_page> 1081 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> items were available within the perceptual span (higher +L+ density), longer item visits were used, and when fewer +L+ items were available (lower density), shorter item visits +L+ were used. Longer visits were only used when they were +L+ efficient, i.e. when expected information gain was high. +L+ 2. Longer visits to items were combined with skipping. +L+ Participants in the high density conditions skipped items +L+ more often, visited fewer items, and spent less time +L+ searching overall.	In other words, when the potential +L+ information gain for longer visits was high, people made +L+ more use of longer visits and reduced the number of items +L+ that they directly fixated. They made more use of the +L+ perceptual span when it was rational given the constraints +L+ of the visual system to do so, and gained an overall +L+ reduction in the required search time. +L+ 3. Participants were observed to reduce the number of items +L+ that they visited, i.e. they skipped, when there was a larger +L+ number of items (Figure 5d) but without increasing the +L+ duration spent attending to each item (Figure 5c). These +L+ findings suggest that skipping was sensitive to the prior +L+ probability that any one item would be attractive enough to +L+ be selected but that gaze duration was not. +L+ 4. Participants spent more time on an item, when they +L+ revisited it than when they visited it for the first time. this +L+ findings indicates that people may use an iterative +L+ deepening of attention [34]. This strategy may help +L+ participants to parse large amounts of information quickly +L+ using lower tolerance to rule out poor quality items and +L+ then focus effort on a smaller set. The difference between +L+ [34] and our study is that our participants didn’t sample all +L+ items before they started to revisit, especially in conditions +L+ with more pages (Figure 4b). This finding suggests that the +L+ set size of potential items may be constrained by the +L+ capacity limitation in memory. We also observed +L+ participants select an item at the first time visit before +L+ sampling all items. This indicated that people also picked +L+ an item immediately when the value of the item currently +L+ visited is beyond their satisficing criteria [38]. Adopting the +L+ coarse-to-fine strategy [27,33] and employing satisficing +L+ [38] could save lots of visual processing cost for sampling +L+ out poor items quickly and gain an item with satisficing +L+ value or gain a revisited item with acceptable value. +L+ In sum, the results support the view that people adjusted +L+ their visual search strategy to their expectations of +L+ information gain, and that these expectations were +L+ contingent on (a) the density of items, and (b) the prior +L+ likelihood that an item is the one that they will want to +L+ select. In the remainder of the Discussion we first offer +L+ further explanations for the details of the findings, we then +L+ discuss some design implications. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Further Explanations +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Why, for some designs, is it more efficient to use fewer +L+ longer gazes than more, shorter, ones? Essentially, because +L+ planning and executing gaze transitions, saccades, is +L+ expensive. Planning and executing a new gaze so as to +L+ bring a foveal fixation to a new place requires effort, both +L+ mental and physical effort. Neurophysiological evidence +L+ indicates that making a saccade involves high cost neural +L+ processes to keep the visual scene stable (see [37] review). +L+ Therefore, to prolong an old gaze to gain information about +L+ an adjacent item can cost less than to plan and execute a +L+ new gaze. For example, in our experiment a new item visit +L+ took an extra 400 ms, but a longer gaze took only about 20 +L+ ms more. Although each longer gaze takes more time, fewer +L+ gazes are required, reducing the overall visual search time. +L+ Why was it not possible for participants to increase +L+ perceptual span to cover adjacent items in lower density +L+ displays? Findings reported by Hooge, Vlaskamp and Over +L+ [16] suggest that information can be gathered from a wider +L+ area of a display as gaze durations increase, i.e. it is +L+ possible that perceptual span is itself contingent on duration +L+ of fixation. However, there are clearly limits [4], and it +L+ appears as though the reduction in the rate that information +L+ can be acquired caused by the lower densities in our +L+ experiment pushed participants beyond this limit. +L+ Will the findings generalise to other types of search result +L+ items? There is some evidence that the finding would +L+ generalise to word items. For example, Pollatsek, Perea +L+ and Binder [32] found that fixation duration is longer when +L+ a word with more neighbours than few neighbours. Longer +L+ fixation duration in a more dense display suggests that more +L+ information within the perceptual span is required to be +L+ processed serially. In contrast, Motter and Belky [24] +L+ showed that fixation duration didn’t change as the number +L+ of items within a 4° constant area and assumed that this +L+ result is because items surrounding a fixation are processed +L+ in parallel (number of items surrounded the fixation will not +L+ affect the fixation duration). These contrary results could be +L+ because of the relative complexities of stimuli in these two +L+ tasks are so different. Simple items, e.g. symbols, adjacent +L+ to a fixation can be processed in parallel, but complex items +L+ such photographic thumbnails in our task have to be +L+ processed in series and result in longer durations. +L+ Why do the findings differ from previous research? The +L+ importance of ecological validity. The observed density +L+ effect on gaze duration and number of gazes are consistent +L+ with some previous studies [26], but contrary to others +L+ [4,15]. Unlike most of the previous studies, our target was +L+ not defined by only one or two visual features (color, shape, +L+ and orientation et al.) or non-words (non-words without +L+ semantic meaning can be searched by their shape or the +L+ order of characters), but was, instead, informed by the +L+ semantic description of a place. In contrast to these +L+ previous studies, participants in our study had to search a +L+ target based on aesthetic judgment. They spent longer gaze +L+ duration in our task than normal visual search task. These +L+ differences in the task environment may lead to the +L+ different results of the effect on search performance. +L+ Ecological validity is therefore crucial to understanding the +L+ constraints on visual search strategy. +L+ </SectLabel_bodyText> <SectLabel_page> 1082 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> On the other hand, we have chosen to control some +L+ variables that are confounded in studies of potential new +L+ designs. For example, in studies of hyperbolic browsers +L+ [30] item size and item density are confounded, i.e. items +L+ that are close together are also smaller than items that are +L+ further apart. Although this is perhaps, a natural confound +L+ for systems like hyperbolic browsers, failing to isolate what +L+ are otherwise independent factors will limit the generality +L+ of conclusions drawn from the data. +L+ A downside of our effort to engage our participants in a +L+ meaningful task -- looking for attractive images of potential +L+ destinations -- was that there was a weak criterion for +L+ successful trial completion. The trial was over when +L+ participants indicated that they had found an attractive +L+ thumbnail but some participants may have used radically +L+ different thresholds for ‘attractive’ than others. In contrast, +L+ experiments in which participants were asked to find a +L+ particular item have a strong and measurable criterion for +L+ trial completion and moreover participants can make errors. +L+ Although we claim that our design reflects some part of the +L+ natural task ecology, it also reduces the strength of the +L+ conclusions that we can draw from the fact that participants +L+ took less time, overall, when items were presented with a +L+ higher density. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Design Implications +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> A persistent problem for HCI is that the exploration of new +L+ designs in the absence of adequate empirically grounded +L+ theory might lead to the rejection of the design on the basis +L+ of false negatives. Hyperbolic or other fish-eye view +L+ projections have, for example, had no impact on the way +L+ that most people use computer systems, despite the huge +L+ research investment over many years. It is tempting to take +L+ this fact as evidence that hyperbolic browsers are not fit for +L+ purpose. However, an alternative reason for their failure to +L+ find a role might be that the particular design instances are +L+ not tuned to the detailed constraints and adaptive capability +L+ of the human visual system. Hyperbolic browsers that fail +L+ to let people adapt to the constraints and capabilities +L+ imposed by their visual system are unlikely to succeed. +L+ Careful empirical investigation in response to well-formed +L+ theory and motivated by the ecology of user’s real task +L+ environments may have the potential to address this +L+ problem. +L+ For example, although users might adapt their search +L+ strategy to achieve the highest payoff given a particular +L+ thumbnail density, some layout designs are better for +L+ decreasing of total search time and manual response time. +L+ The results reported above suggest the high density (3 +L+ pixels gap between thumbnails) can facilitate search. +L+ Interestingly, even higher densities than we have explored +L+ in this paper are unlikely to produce further gains. Previous +L+ work [21] has suggested that when the spacing between +L+ icons is zero pixels search becomes more difficult. +L+ Thumbnails require at least some space for visual search, +L+ which perhaps reflects common intuitions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This research is supported by an Overseas Scholarship +L+ Scheme (OSS) Award from the University of Manchester +L+ and a Manchester Business School Award to Yuan-Chi +L+ Tseng and Office of Naval Research grant (N00014-03-1- +L+ 0087) to Andrew Howes. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Anderson, J.R. The Adaptive Character of Thought. +L+ Lawrence Erlbaum Associates, 1990. +L+ 2. Anderson, J.R., Bothell, D., Byrne, M.D., Douglass, S., +L+ Lebiere, C., and Qin, Y. An integrated theory of the +L+ mind. Psychological Review 111, 4 (2004), 1036-1060. +L+ 3. Araujo, C.C., Kowler, E.E., and Pavel, M.M. Eye +L+ movements during visual search: the costs of choosing +L+ the optimal path. Vision Research 41, 25-26 (2001), +L+ 3613-3625. +L+ 4. Bertera, J.H. and Rayner, K. Eye movements and the +L+ span of the effective visual stimulus in visual search. +L+ Perception & Psychophysics 62 (2000), 576-585. +L+ 5. Brumby, D.P. and Howes, A. Good enough but I’ll just +L+ check: Web-page search as attentional refocusing. In +L+ Proc. ICCM 2004, Lawrence Erlbaum Associates +L+ (2004). +L+ 6. Brumby, D.P. and Howes, A. Strategies for guiding +L+ interactive search: An empirical investigation into the +L+ consequences of label relevance for assessment and +L+ selection. Human-Computer Interaction (in press). +L+ 7. Cockburn, A., Gutwin, C., and Alexander, J. Faster +L+ document navigation with space-filling thumbnails. In +L+ Proc. CHI 2006, ACM Press (2006), 1-10. +L+ 8. Cox, A.L. and Young, R.M. A Rational Model of the +L+ Effect of Information Scent on the Exploration of +L+ Menus. In Proc. ICCM 2004, Lawrence Erlbaum +L+ Associates (2004). +L+ 9. Cutrell, E. and Guan, Z. What are you looking for? An +L+ eye-tracking study of information usage in Web search. +L+ In Proc. CHI 2007, ACM Press (2007). +L+ 10.Eng, K., Lewis, R.L., Tollinger, I., Chu, A., Howes, A., +L+ and Vera, A. Generating automated predictions of +L+ behavior strategically adapted to specific performance +L+ objectives. In Proc. CHI 2006, ACM Press (2006), 621- +L+ 630. +L+ 11.Everett, S.P. and Byrne, M.D. Unintended effects: +L+ Varying icon spacing changes users' visual search +L+ strategy. In Proc. CHI 2004, ACM Press (2004), 695- +L+ 702. +L+ 12.Findlay, J.M. and Gilchrist, I.D. Active vision-the +L+ psychology of looking and seeing. Oxford University +L+ Press, 2003. +L+ 13.Fu, W.-T. and Pirolli, P. SNIF-ACT: A Cognitive +L+ Model of User Navigation on the World Wide Web. +L+ Human-Computer Interaction 22 (2007). +L+ </SectLabel_reference> <SectLabel_page> 1083 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Search	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_reference> 14.Gray, W.D. and Boehm-Davis, D.A. Milliseconds +L+ Matter: An introduction to microstrategies and to their +L+ use in describing and predicting interactive behavior. +L+ Journal of Experiment Psychology: Applied 6, 4 (2000), +L+ 322-335. +L+ 15.Halverson, T. and Hornof, A.J. Explaining eye +L+ movements in the visual search of varying density +L+ layouts. In Proc. ICCM 2004, Lawrence Erlbaum +L+ Associates (2004), 124-129. +L+ 16.Hooge, I.T.C., Vlaskamp, B.N.S., and Over, E.A.B. +L+ Saccadic search: the relation between fixation duration +L+ and saccade amplitude. Perception 31, ECVP Abstract +L+ Supplement (2002). +L+ 17.Howes, A., Vera, A., and Lewis, R.L. Bounding rational +L+ analysis: Constraints on asymptotic performance. In +L+ W.D. Gray (Ed.), Integrated Models of Cognitive +L+ Systems, Oxford University Press (2006). +L+ 18.Kieras, D.E. and Meyer, D.E. An overview of the EPIC +L+ architecture for cognition and performance with +L+ application to human-computer interaction. Human- +L+ Computer Interaction 12 (1997), 391-438. +L+ 19.Kieras, D.E. and Meyer, D.E. The role of cognitive task +L+ analysis in the application of predictive models of +L+ human performance. In J.M. Schraagen and S.F. +L+ Chipman (Eds.), Cognitive task analysis (2000), 237- +L+ 260. +L+ 20.Klöckner, K., Wirschum, N., and Jameson, A. Depth +L+ and breadth-first processing of search result lists. In Ext. +L+ Abstracts CHI 2004, ACM Press (2004), 1539-1539. +L+ 21.Lindberg, T. and Nasanen, R. The effect of icon spacing +L+ and size on the speed of icon processing in the human +L+ visual system. Displays 24, 3 (2003), 111-120. +L+ 22.Meyer, D.E. and Kieras, D.E. A computational theory of +L+ executive control processes and human multiple-task +L+ performance: Part 1. Basic Mechanisms. Psychological +L+ Review 104 (1997), 3-65. +L+ 23.Meyer, D.E. and Kieras, D.E. A computational theory of +L+ executive control processes and human multiple-task +L+ performance: Part 2. Accounts of Psychological +L+ Refractory-Period Phenomena. Psychological Review +L+ 104 (1997), 749-791. +L+ 24.Motter, B.C. and Belky, E.J. The zone of focal attention +L+ during active visual search. Vision Research 38 (1998), +L+ 1007-1022. +L+ 25.Najemnik, J. and Geisler, W.S. Optimal eye movement +L+ strategy in visual search. Nature 434 (2005), 387-391. +L+ 26.Ojanpää, H., Näsänen, R., and Kojo, I. Eye movements +L+ in the visual search of word lists. Vision Research 42, 12 +L+ (2002), 1499-1512. +L+ 27.Over, E.A.B., Hooge, I.T.C., Vlaskamp, B.N.S., and +L+ Erkelens, C.J. Coarse-to-fine eye movement strategy in +L+ visual search. Vision Research 47 (2007), 2272-2280. +L+ 28.Payne, S.J., Howes, A., and Reader, W.R. Adaptively +L+ distributing cognition: a decision-making perspective on +L+ human-computer interaction. Behaviour and Information +L+ Technology 20, 5 (2001), 339-346. +L+ 29.Pirolli, P. and Card, S.K. Information foraging. +L+ Psychological Review 106, 4 (1999), 643-675. +L+ 30.Pirolli, P., Card, S.K., and Van Der Wege, M. The +L+ effects of information scent on visual search in the +L+ hyperbolic tree browser. ACM Transactions on +L+ Computer Human Interaction 10, 1 (2003), 20-53. +L+ 31.Pirolli, P. and Fu, W.-T. SNIF-ACT: a model of +L+ information foraging on the world wide web. In Proc. +L+ Ninth International Conference on User Modeling, +L+ Springer (2003). +L+ 32.Pollatsek, A., Perea, M., and Binder, K.S. The Effecs of +L+ Neighborhood Size in Reading and Lexical Decision. +L+ Journal of Experimental Psychology: Human Perception +L+ and Performance 25, 4 (1999), 1142-1158. +L+ 33.Rao, R.P.N., Zelinsky, G., Hayhoe, M.M., and Ballard, +L+ D.H. Eye movements in iconic visual search. Vision +L+ Research 42, 11 (2002), 1447-1463. +L+ 34.Reiman, J., Young, M., and Howes, A. A dual-space +L+ model of interatively deepening exploratory learning. +L+ International Journal of Human-Computer Studies 44 +L+ (1996), 743-775. +L+ 35.Rele, R.S. and Duchowski, A.T. Using eye tracking to +L+ evaluate alternate search results interfaces. In Proc. +L+ HFES 49th Annual Conference (2005). +L+ 36.Resnick, M.L., Maldonado, C.A., Santos, J.M., and +L+ Lergier, R. Modeling On-line Search Behavior Using +L+ Alternative Output Structures. In Proc. HFES 45th +L+ Annual Conference (2001), 1166-1171. +L+ 37.Ross, J., Morrone, M.C., Goldberg, M.E., and Burr, +L+ D.C. Changes in visual perception at the time of +L+ saccades. Trends in Neurosciences 24, 2 (2001), 113- +L+ 121. +L+ 38.Simon, H.A. A behavioral model of rational choice. +L+ Quarterly Journal of Economics 69 (1955), 99-118. +L+ 39.Sperling, G. and Dosher, B.A. Strategy and optimization +L+ in human information processing. In K.R. Boff, L. +L+ Kaufman, and J.P. Thomas (Eds.), Handbook of +L+ perception and human performance, Vol. I, Sensory +L+ processes and perception, Wiley (1986). +L+ 40.Vlaskamp, B.N.S., Over, E.A.B., and Hooge, I.T.C. +L+ Saccadic search performance: the effect of element +L+ spacing. Experimental Brain Research 3 (2005), 1-14. +L+ 41.Woodruff, A., Faulring, A., Rosenholtz, R., Morrison, +L+ J., and Pirolli, P. Using Thumbnails to Search the Web. +L+ In Proc. CHI 2001, ACM Press (2001), 198-205. +L+ 42.Yee, K.P., Swearingen, K., Li, K., and Hearst, M. +L+ Faceted metadata for image search and browsing. In +L+ Proc. CHI 2003, ACM Press (2003), 401-408. +L+ </SectLabel_reference> <SectLabel_page> 1084 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> Don’t Look Now, But We’ve Created a Bureaucracy: +L+ The Nature and Roles of Policies and Rules +L+ in Wikipedia +L+ </SectLabel_title> <SectLabel_author> Brian Butler +L+ </SectLabel_author> <SectLabel_affiliation> Katz Graduate School of +L+ Business, University of +L+ Pittsburgh +L+ </SectLabel_affiliation> <SectLabel_email> bbutler@katz.pitt.edu +L+ </SectLabel_email> <SectLabel_author> Elisabeth Joyce +L+ </SectLabel_author> <SectLabel_affiliation> Edinboro University of +L+ Pennsylvania +L+ </SectLabel_affiliation> <SectLabel_email> ejoyce@edinboro.edu +L+ </SectLabel_email> <SectLabel_author> Jacqueline Pike +L+ </SectLabel_author> <SectLabel_affiliation> Katz Graduate School of +L+ Business, University of +L+ Pittsburgh +L+ </SectLabel_affiliation> <SectLabel_email> jpike@katz.pitt.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Wikis are sites that support the development of emergent, +L+ collective infrastructures that are highly flexible and open, +L+ suggesting that the systems that use them will be +L+ egalitarian, free, and unstructured. Yet it is apparent that +L+ the flexible infrastructure of wikis allows the development +L+ and deployment of a wide range of structures. However, we +L+ find that the policies in Wikipedia and the systems and +L+ mechanisms that operate around them are multi-faceted. In +L+ this descriptive study, we draw on prior work on rules and +L+ policies in organizations to propose and apply a conceptual +L+ framework for understanding the natures and roles of +L+ policies in wikis. We conclude that wikis are capable of +L+ supporting a broader range of structures and activities than +L+ other collaborative platforms. Wikis allow for and, in fact, +L+ facilitate the creation of policies that serve a wide variety of +L+ functions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Wikis, Wikipedia, community, collaboration, policy, +L+ policies, rules, dynamics. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> K.4.3. [Computers and Society]: Organizational Impact – +L+ Computer-supported collaborative work, H.5.3 +L+ [Information Interfaces]: Group and Organization Interfaces +L+ - Collaborative computing, Web-based interaction, +L+ Computer-supported cooperative work, H.3.5 [Information +L+ Storage and Retrieval]: Online Information Systems. +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_construct> “The Wikipedia online encyclopedia — written by +L+ thousands of individuals working without a boss – shows +L+ the way... ” [28] +L+ </SectLabel_construct> <SectLabel_bodyText> Wikipedia is characterized by many as emergent, complex, +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00 +L+ </SectLabel_copyright> <SectLabel_bodyText> messy, informal, popularly uncontrolled, non- +L+ organizational, and radically different from traditional +L+ organizations [5, 11, 22, 42, 52, 53]. Consistent with this +L+ characterization, one of the founding principles of +L+ Wikipedia is “Ignore all rules,” which states that if a rule +L+ inhibits developing Wikipedia, the contributor should +L+ ignore it [45]. +L+ Yet examination of the administrative structures of +L+ Wikipedia reveals a complex structure of rules, processes, +L+ policies, and roles. There are 44 wiki pages in the +L+ “Wikipedia Official Policy” category as of September +L+ 20071. There are 248 wiki pages categorized as “Wikipedia +L+ guidelines” which are organized into at least eight +L+ subcategories. In addition, these do not seem to be +L+ sufficient, since there are 45 pending proposals for +L+ guidelines and policies, not to mention the 200 rejected +L+ proposals for guidelines and policies. +L+ Even the principle of “Ignore all rules,” labeled as one of +L+ the official Wikipedia policies, is not immune from such +L+ “development” [45]. While the “Ignore all rules” policy +L+ itself is only sixteen words long, the page explaining what +L+ the policy means contains over 500 words, refers readers to +L+ seven other documents, has generated over 8,000 words of +L+ discussion, and has been changed over 100 times in less +L+ than a year. +L+ Studies of Wikipedia activities [5, 11, 42, 43] and +L+ anecdotal discussions among participants [49] suggest that +L+ these policies, rules, and guidelines play an important part +L+ in both the day-to-day operations and overall success of +L+ Wikipedia. These arguments are consistent with findings +L+ and arguments made with regard to other types of online +L+ collective action, such as online communities [19, 24, 32], +L+ open source development [15, 16, 26, 41, 44], and virtual +L+ organizations [1]. +L+ The purpose of this study is to propose a conceptual +L+ framework for understanding the nature and role of policies +L+ and rules within wikis. Drawing from prior studies of rules +L+ </SectLabel_bodyText> <SectLabel_footnote> 1 All references to Wikipedia content are based on data +L+ exported from the site in September 2007. Since the +L+ policies undergo perpetual re-editing and reconfiguring, the +L+ data presented here represents a snap-shot. +L+ </SectLabel_footnote> <SectLabel_page> 1101 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> and policies in a variety of contexts, including teams, +L+ traditional organizations, and legal systems [29], different +L+ images of rules and policies are considered. In each case, +L+ examples and evidence are drawn from Wikipedia to +L+ illustrate that view of rules and policies. Following this we +L+ discuss the implications of the framework for understanding +L+ both the potential and likely outcomes of wiki efforts and +L+ design implications of the different perspectives for both +L+ wiki implementation and development of infrastructures for +L+ supporting wiki-like initiatives. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RULES, POLICIES AND GUIDELINES +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In the broadest scope, terms like rules, policies, and +L+ guidelines all refer to the “explicit and implicit norms, +L+ regulations, and expectations that regulate the behavior of +L+ individuals and interactions between them” [29 p. 5]. +L+ Taken in its most general sense, this definition can also be +L+ seen as including informal or implicit norms and constraints +L+ that can significantly affect behavior and interaction, even +L+ though they are not formally recognized or recorded. +L+ Given the widely discussed importance of informal norms +L+ in contexts such as open source projects [35, 41] and other +L+ online social settings [3, 4], it may be beneficial to equate +L+ formal and informal rules. In particular this approach +L+ suggests that insights and results from the study of explicit, +L+ formal rules and policies may be useful for understanding +L+ the development, application, and impact of informal rules +L+ and norms. However, in this study we focus on formal, or +L+ written, policies, rules, and guidelines. +L+ We use the terms rules, policies, and guidelines +L+ interchangeably. While there are some contexts in which +L+ these are clearly conceptually distinguishable, the +L+ difference between and application of the terms vary from +L+ context to context. This suggests that while some aspects +L+ require distinguishing them from each other, there is not +L+ ready agreement on what they are or how the terms should +L+ be used. Furthermore, and for our purposes perhaps more +L+ importantly, this conceptual equivalence is reflected in the +L+ Wikipedia definitions which state: +L+ </SectLabel_bodyText> <SectLabel_construct> “A guideline is any page that is: (1) actionable (i.e. it +L+ recommends, or recommends against, an action to be taken +L+ by editors) and (2) authorized by consensus. Guidelines are +L+ not set in stone and should be treated with common sense +L+ and the occasional exception. +L+ A policy is similar to a guideline, only more official and +L+ less likely to have exceptions.” [48] +L+ </SectLabel_construct> <SectLabel_bodyText> This study focuses on formal written policies for both +L+ practical and conceptual reasons. Practically, formal rules +L+ are ideal for study because of the relative ease of +L+ determining what the policy is, when it was put in place, +L+ who participated in creating it, when it was referenced, and, +L+ in some cases, when it was removed. This is particularly +L+ true in the context of a wiki because of its facilities for +L+ archiving and managing collaborative documents. +L+ However, beyond the practical issues, formal written rules +L+ and policies are significant because of their role as +L+ boundary objects [33, 34], or as specifications of how the +L+ content will be used and communication will occur. They +L+ can serve a variety of purposes by virtue of the fact that +L+ they are explicit and external. Because they are explicit and +L+ visible, though, written policies and rules are often sites of +L+ conflict [29 p. 18]. These same characteristics also mean +L+ that written policies have greater potential as levers for +L+ developers, designers, and managers to affect a community +L+ or collaborative effort [10, 19]. +L+ Hence, while it may be the case that informal norms are +L+ important, it makes sense to focus on the nature and role of +L+ formal written rules and policies in the operation of a +L+ distributed collaborative effort like Wikipedia. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> IMAGES AND ROLES OF RULES AND POLICIES +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Because of their centrality in so many aspects of society +L+ and organizational and individual behaviors, rules and +L+ policies have been studied by scholars in a wide variety of +L+ fields, including law, sociology, political science, +L+ economics, management science, anthropology, linguistics, +L+ and organizational studies. While these scholars typically +L+ adopt definitions similar to those described above, the +L+ assumptions they make about the source, nature, and +L+ implications of rules and policies can vary significantly. +L+ While this lack of consensus can present challenges, it also +L+ provides a basis for characterizing the multifaceted nature +L+ of rules, policies, and guidelines. +L+ In particular, prior work provides several perspectives +L+ which can be used to view rules and policies, including +L+ rules and policies as: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Rational efforts to organize and coordinate +L+ •	Evolving, competing entities +L+ •	Constructions of meaning & identity +L+ •	External signals +L+ •	Internal signals +L+ •	Negotiated settlements and trophies +L+ •	Control mechanisms +L+ </SectLabel_listItem> <SectLabel_bodyText> In the following sections, we consider each of these +L+ perspectives. For each one we begin with a discussion of +L+ the core assumptions that are made about the nature and +L+ implications of policies playing this role with reference to +L+ principles and examples drawn from studies of rules in +L+ traditional organizational and social contexts. We then +L+ consider examples from Wikipedia that illustrate how the +L+ policies and guidelines there are consistent with the +L+ perspective. +L+ The purpose of these discussions and examples is to +L+ illustrate how the guidelines and policies in Wikipedia, and +L+ the systems and mechanisms that operate around them, are +L+ multi-faceted. As such, these perspectives should not be +L+ </SectLabel_bodyText> <SectLabel_page> 1102 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_figureCaption> Figure 2. Wikipedia +L+ Guideline Box +L+ </SectLabel_figureCaption> <SectLabel_bodyText> treated as a set of mutually exclusive categories, but rather +L+ as a set of potentially overlapping lenses, each of which +L+ highlights different aspects of the policies and guidelines +L+ that exist within Wikipedia. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Rules and Policies as Rational Efforts to Organize or +L+ Coordinate +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> All groups are faced with challenges created by +L+ communication and coordination problems which must be +L+ solved if the shared objectives are to be achieved [7, 13, 14, +L+ 21, 38, 41]. In this view rules are conscious, intentional +L+ actions put in place for the purpose of improving collective +L+ performance. Rules and policies address, or at least +L+ substantially increase the chance of addressing, the +L+ problems of communication and coordination by creating a +L+ context in which distributed actions are taken in reliable +L+ and consistent ways [27]. In other words, rules and policies +L+ are means of solving communication and coordination +L+ problems by increasing the reliability and consistency of +L+ action (i.e., eliminating the need to explicitly communicate +L+ and coordinate) [29]. Rules in this role assume that all +L+ parties have the same motivations and goals. +L+ Rules and policies for coordination and communication +L+ have been identified as particularly important in contexts +L+ where there is high turnover (people coming and going on a +L+ regular basis), where there is substantial autonomy of +L+ action, and where explicit coordination is costly and yet +L+ important to success of the activity [9, 25]. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Rational Efforts to Organize or Coordinate in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Wikipedia has high turnover in that a large majority of the +L+ editors make only a few changes once, the editors are +L+ distributed globally, and coordination is necessary in order +L+ to continuously provide a functioning product (i.e., an +L+ encyclopedia) on demand. +L+ For Wikipedia, this perspective +L+ on rules and policies suggests +L+ that policies and guidelines are +L+ being put in place to achieve +L+ consistency and reliability in +L+ terms of how things are +L+ handled and coordinate efforts +L+ [23], but also to confirm the +L+ authority of those most likely +L+ to implement the policies: the +L+ administrators. For example, a +L+ policy was created which +L+ outlines the process which +L+ should be taken to block a user +L+ or delete an entry in +L+ Wikipedia. The 48 policies +L+ under consideration on the +L+ Wikipedia Policy proposals +L+ indicate the administrators’ continual need to reinforce their +L+ limited power over the dispersed population of this +L+ community. +L+ Because of the decentralized +L+ nature of the role of user (i.e., +L+ editor) in Wikipedia and the high +L+ turnover, more written policies +L+ and guidelines are needed to +L+ facilitate the transfer of +L+ knowledge [40] from one user to +L+ the next and maintain +L+ consistency during the editing. +L+ Alternatively, fewer policies will +L+ be specified for administrators +L+ than users. There are more +L+ rules, therefore, for editors, since +L+ their population experiences +L+ greater turnover and their +L+ activities are more dispersed +L+ than those of the administrators +L+ [13, 14]. Also, since this +L+ population gains members more +L+ frequently than the +L+ administrators, written policies +L+ assist new people by lending +L+ them direction with their +L+ contributions, in a similar +L+ fashion as Frequently asked +L+ questions pages in online +L+ communities [6]. +L+ Most policy and guideline pages +L+ provide a box with general +L+ information about Wikipedia +L+ policy or guidelines, +L+ respectively, in the prominent +L+ upper right hand corner, as +L+ shown in Figures 1 and 2. The +L+ policies selected for emphasis +L+ are divided into two sections: +L+ one discussing procedures for +L+ editing articles and the other +L+ reminding users about +L+ behavioral standards. +L+ These boxes serve to introduce first time participants to the +L+ norms of the hybrid community/document paradigm and +L+ remind more experienced and committed members about +L+ the essential rules. After all, according to the Wikipedia +L+ contributors, these policies and guidelines help make +L+ Wikipedia successful [48]. Also, since the items in these +L+ lists are links, they point readily to the written document for +L+ each of these policies. +L+ Wikipedia editors suggest that policy is often enacted after +L+ it has been used in practice and recognized as important +L+ [48], such as to increase the speed, efficiency, or reduce the +L+ cost of administering the encyclopedia. These enactments +L+ embody the rational efforts of this role of a policy or +L+ guideline. +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 1. Wikipedia +L+ Policy Box +L+ </SectLabel_figureCaption> <SectLabel_page> 1103 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> A case where a process was “policy-fied” to accomplish +L+ these goals is the policy outlining the Criteria for Speedy +L+ Deletion [50]. The deletion policy is written exclusively for +L+ administrators, for they are the only community members +L+ empowered to determine what pages should remain in +L+ Wikipedia and what pages should be removed. This policy +L+ is written in extensive form and with multiple categorized +L+ subsections to accommodate the autonomy of the +L+ administrators and to support consistency in their decision +L+ making. Written policies of this sort ensure coordination +L+ among administrators and prevent arbitrary decisions. +L+ Additionally, written policies establish guidelines for +L+ behavior so that each decision does not require the endless +L+ time expense of lengthy discussion and consensus +L+ development. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Implications for Rational Efforts to Organize or Coordinate +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Wikipedia policies and guidelines can be seen as intentional +L+ efforts to solve coordination and communication problems +L+ by either eliminating the need for direct coordination or +L+ communication or significantly reducing the costs of such +L+ communication. Reduction of costs is often achieved by +L+ codifying the interaction, and hence, reducing the effort +L+ needed to engage in the exchanges. These mechanisms +L+ remain important even though communication and +L+ coordination costs are lower (at least theoretically) because +L+ of the technology infrastructure due to issues such as +L+ information overload [17]. At least for large scale wikis, +L+ such as Wikipedia, the need for organizational mechanisms +L+ to reduce communication and coordination cost remains. +L+ Rules and Policies as Evolving, Competing, Self- +L+ propagating Entities +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Rules can be seen as self-propagating entities that are the +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> result of an evolving, competitive process. This perspective +L+ rejects the idea of intention, design, and agency as the +L+ primary drivers of policy development, largely because of +L+ the bounded rationality of individuals and high levels of +L+ complexity in the organizational system. Instead it is +L+ argued that rules are the result of competition for shifting +L+ attention. This results in systems of policies in which +L+ dynamics of rule development have the following features: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Rules generate more rules (either exponentially, linearly, +L+ or at a declining rate) +L+ •	Areas or problems can be saturated, so modification or +L+ adaptation of rules will drop off as the space gets +L+ “populated” +L+ •	Developing manner rules in one area will draw attention +L+ and people from other areas [29] +L+ </SectLabel_listItem> <SectLabel_subsubsectionHeader> Evolving, Competing, Self-propagating Entities in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> For Wikipedia, the basic conditions of this perspective +L+ definitely apply. Wikipedia is an extremely complex system +L+ of documents, people, roles, policies and guidelines, yet +L+ individuals possess bounded rationality [39]. This suggests +L+ that we should see these kinds of dynamics of rules in the +L+ formation and modifications of the policies and guidelines +L+ in Wikipedia, and the archives bear this out. +L+ One useful measure of increased complexity is the change +L+ in lengths in terms of word count alone of the policies from +L+ the first version to most current. All policies studied grew +L+ enormously. +L+ </SectLabel_bodyText> <SectLabel_listItem> •	Copyrights: 341 words 4 3200 words: 938% +L+ •	What Wikipedia is not: 541 words 4 5031 words: 929% +L+ •	Civility: 1741 words 4 2131 words: 124% +L+ •	Consensus: 132 words 4 2054 words: 1557% +L+ •	Deletion: 405 words 4 2349 words: 580% +L+ •	Ignore all rules: exceptional case +L+ </SectLabel_listItem> <SectLabel_bodyText> The first version of the Ignore all rules policy is only 23 +L+ words long, stating, “If rules make you nervous and +L+ depressed, and not desirous of participating in the Wiki, +L+ then ignore them and go about your business” [45]. The +L+ current version is actually shorter, only 16 words, and says, +L+ “If a rule prevents you from working with others to improve +L+ or maintain Wikipedia, ignore it” [45]. However, as +L+ suggested earlier in this paper, while the actual wording of +L+ this policy declined 69% and it appears on the surface to be +L+ the least bureaucratic of the policies, the supplemental page +L+ directly linked to this policy contains 579 words, indicating +L+ that the policy swelled over 3600% [45]. +L+ The Deletion policy appears to grow less than most of the +L+ other policies, but this statistic is misleading as the deletion +L+ policy is continually broken down into smaller +L+ subcategories in order to prevent discussion of particular +L+ instances of deletion decisions from appearing on the +L+ general policy page. The Deletion policy, therefore, is a +L+ policy of tremendous proliferation and complication. +L+ Increased complexity is apparent in the Copyrights policy, +L+ among others, where the diction and syntax emulate that of +L+ legal documents. The first version of the policy, for +L+ instance, starts with: “The goal of Wikipedia is to create +L+ information that is available to everyone.” The current +L+ version, starting after a disclaimer note, begins: “The +L+ license Wikipedia uses grants free access to our content in +L+ the same sense as free software is licensed freely.” The +L+ earlier version uses simple sentence construction and +L+ vernacular diction. The current one relies on words from the +L+ legal profession, such as license, grants, access, and later in +L+ the policy, permission, obligation, rights. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Implications for Evolving, Competing, Self-propagating +L+ Entities +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> This role of rules and policies is necessary because it +L+ attempts to take the complex system that is Wikipedia and +L+ make it manageable. The evolutionary aspect of this role +L+ also promotes the continuous updating and modification of +L+ rules, which is needed in this type of dynamic environment. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Rules and Policies as Constructions of Meaning and +L+ Identity +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Rules and policies answer questions about “who we are” +L+ [29]. They also indicate the way things “should be” (i.e., +L+ ideals). Lastly they serve to define and exemplify the “talk” +L+ </SectLabel_bodyText> <SectLabel_page> 1104 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> of the place. Previous work has looked at the role of shared +L+ group and community identity [2, 12, 14] and individual +L+ identity [18, 30, 37] construction. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Constructions of Meaning and Identity in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> For Wikipedia this includes policies that serve the purpose +L+ of defining what Wikipedia is and is not, either explicitly +L+ (e.g., What Wikipedia is not page) or through the +L+ articulation of policy pages on community principles (e.g., +L+ Ignore all rules page). +L+ Meaning and identity are also likely to be reflected in +L+ discussions of policy changes that center on whether or not +L+ something is consistent with the “core principles” of +L+ Wikipedia. An example of these discussions is that which +L+ centers around the Neutral Point of View policy. This +L+ policy contains detailed definitions of bias, carefully +L+ organized guidelines for how to maintain neutrality in an +L+ article, and specifically outlined procedures for handling +L+ conflicts [51]. The archived discussion pages on this policy +L+ are so voluminous that many of them contain the +L+ discussions held within only one or two days [51]. +L+ The What Wikipedia is not policy sets up a clear sense of +L+ identity, but less through a description or definition of it +L+ (i.e., what we are) than through a negative approach. This +L+ policy is divided into three sections: Style and Format, +L+ Content, and Community. The first section differentiates +L+ Wikipedia from printed and published encyclopedias that +L+ are written by paid experts. The second section defines the +L+ term encyclopedia. The third one is perhaps the most +L+ illustrative of the identity of the group, however, for it lays +L+ out explicitly norms for community behavior, and for the +L+ purposes of this paper, “Wikipedia is not a bureaucracy” is +L+ the section most directed towards policy. Interestingly, it +L+ denies the bureaucratic nature of this organization: +L+ “Wikipedia is not a moot court, and rules are not the +L+ purpose of the community. Instruction creep should be +L+ avoided. A perceived procedural error made in posting +L+ anything, such as an idea or nomination, is not grounds for +L+ invalidating that post. Follow the spirit, not the letter, of +L+ any rules, policies and guidelines if you feel they conflict. If +L+ the rules prevent you from improving the encyclopedia, you +L+ should ignore them. Disagreements should be resolved +L+ through consensus-based discussion, rather than through +L+ tightly sticking to rules and procedures.” +L+ The irony of this statement is that discussion surrounding +L+ policy development and modification turns to policy for +L+ support on a regular basis. The Three-revert rule policy, for +L+ instance, refers to the Copyrights, Spamming, Non-free +L+ content, Biographies of living persons, Blocking, and +L+ Consensus policies in its statement alone, with reference to +L+ those and other policies and guidelines proliferating +L+ through the discussion pages. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Implications for Constructions of Meaning and Identity +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Rules in this role allow the wiki and its users to develop a +L+ sense of identity and meaning, which can be viewed, +L+ literally and figuratively, by new and current editors. These +L+ editors can then measure their fit with the community and +L+ decide on their intended level of participation. For +L+ example, someone looking for a wiki focused on social +L+ interaction may not be satisfied with one which focuses on +L+ purely identity formation [36]. In a volitional environment, +L+ such statement of meaning, values, and identity can become +L+ highly influential and rallying. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Rules and Policies as External Signals +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Sometimes rules are ways of indicating to outside +L+ stakeholders or concerned parties that things that they care +L+ about are being attended to. The rules can be symbolic; can +L+ reflect action, or both. It is possible to discern these +L+ responses through media coverage of Wikipedia that +L+ provoked changes in policies or the creation of new ones. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> External Signals in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> For Wikipedia, the Copyrights policy illustrates a rule +L+ acting as an external signal. Based on an analysis of the +L+ Copyrights guidelines, it appears they were developed in +L+ response to external complaints or concerns about the +L+ unpermitted use of protected material [47]. It is perhaps +L+ because of this external stimulant that this policy’s +L+ discussion pages, while devoted extensively to copyright +L+ rules in general, often devolve into discussions of particular +L+ cases, especially those concerning images [47]. Also, +L+ perhaps because of the need to signal recognition of an +L+ issue by Wikipedia, this page developed from a simply +L+ stated list of rules into a more extensively organized but +L+ also more linguistically complicated treatise, as mentioned +L+ above. The language evolved from simple sentence +L+ structures and vernacular style into the more complicated +L+ grammars and dictions of the legal profession, possibly due +L+ to the hiring of general counsel [8], but also as the need for +L+ greater credibility and as a reflection of the community’s +L+ pressures to protect itself and its reputation from outside +L+ attacks or influences. +L+ Another example of a policy acting as an external signal, or +L+ at least being heavily oriented toward external stakeholders, +L+ is the Biographies of living persons policy [46]. Notable +L+ characteristics of this policy include: +L+ </SectLabel_bodyText> <SectLabel_listItem> •	It includes full contact information for Jimmy Wales as +L+ the “Designated Agent” (which references specific +L+ requirement of US Law) unlike the other policies. It also +L+ includes a link to the Wikimedia Foundations Board of +L+ Trustees in a related readings section. +L+ •	It explicitly references external legal structures +L+ requirements (“Such material requires a high degree of +L+ sensitivity, and must adhere strictly to the law in Florida, +L+ United States and to our content policies”). The other +L+ </SectLabel_listItem> <SectLabel_page> 1105 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_listItem> policy that turns to this type of language is the Copyrights +L+ policy. +L+ •	It uses very insistent language (“Editors must take +L+ particular care...”, “...must adhere strictly to...” +L+ (emphasis in original), or “We must get the article right” +L+ (emphasis in original)). +L+ •	In internal discussions, explanations for the policy and +L+ changes to the policy are described in the third person +L+ (they, them, he, her) not first person (we, us, I, me). +L+ •	External attention triggers changes to the policy and +L+ concerns of the external stakeholders are one of the +L+ factors referenced in discussions of the policy, such as a +L+ comment on the Biographies of living person page +L+ stating, “I started this due to the Daniel Brandt situation.” +L+ •	The policy is referenced in statements to external +L+ stakeholders and media. +L+ </SectLabel_listItem> <SectLabel_bodyText> While the Biography of living persons policy is an extreme +L+ case, which came about because of a very high level of +L+ external attention to a particular aspect of Wikipedia, other +L+ examples do exist, such as the sock-puppeting of external +L+ organizations that edit the content of the entries on them or +L+ the revelations that editors were not as qualified as they +L+ claimed. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Implications for External Signals +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> External signals serve the role of demonstrating that the +L+ wiki and its members recognize an issue as important or +L+ significant. However, doing this too often can weaken the +L+ impact or message that an external signal sends to the +L+ external audience. It is important to note that rules which +L+ act as external signals speak to external audiences and are +L+ not necessarily meant for internal audiences, or members. +L+ Theoretically, a rule which serves only as an external signal +L+ could be removes and not change the internal dynamic of +L+ the community. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Rules and Policies as Internal Signals +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Policies and rules can be used to signal to the community +L+ what the community finds important, such as creating a +L+ policy about a particular issue or behavior which is +L+ significant to the community. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Internal Signals in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> An example of a policy developed as an internal signal is +L+ the Civility policy. Its function is to promote polite +L+ interactions between members of the community, rather +L+ than to recognize or call significance to an issue for an +L+ external audience. Perhaps as a result of its role, this +L+ policy’s current form differs from its original form the least +L+ of all the policies and generates the fewest discussions or +L+ dissensions. Policies fitting into this category are not +L+ threatened by outside forces, so their language can remain +L+ less formal in contrast to rules acting as external signals. As +L+ such, they also will not serve as the foci of editorial +L+ pressures faced by those responding to external signals. +L+ This policy also acts as an internal signal via its prominent +L+ placement on the list of policies for first-time users [48]. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Implications for Internal Signals +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Internal signals demonstrate values and identity to internal +L+ stakeholders. It is significant to note that while internal +L+ signals speak to the internal stakeholders, they can be +L+ triggered by either internal members (e.g. via complaints or +L+ problematic events) or external viewers. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Rules and Policies as Negotiated Settlements and +L+ Trophies +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> People have different interests or perspectives. Rules and +L+ policies are negotiated settlements or trophies. Settlements +L+ are creating to avoid the cost of continued conflict, while +L+ trophies are created to give credibility and influence to the +L+ “winner” in future discussions. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Negotiated Settlements and Trophies in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> In Wikipedia, references to “that has already been decided” +L+ or references to policy changes as having been determined +L+ in one side’s favor or another reflect this perspective. An +L+ example of this “trophy” type situation occurs in the +L+ Consensus policy. As mentioned previously, extensive +L+ </SectLabel_bodyText> <SectLabel_figureCaption> Figure 3. Wikipedia consensus flowchart +L+ </SectLabel_figureCaption> <SectLabel_page> 1106 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> discussion surrounds the development of written guidelines +L+ to assist smooth negotiations and to achieve acceptable +L+ compromises. For the Consensus policy, this type of +L+ discussion focuses on whether to use specific numbers for +L+ confirmation of consensus or not. That is, should the group +L+ need 80% or 75% of the population’s approval? Or should +L+ the policy simply state that general agreement is all that is +L+ required for consensus? An active member wants polling +L+ results to determine consensus, but he “loses” this effort in +L+ 2005; consensus is explained through the organizational +L+ structure of the flowchart in Figure 3. In July 2007, +L+ someone starts adding the numbers requirement back into +L+ the Consensus policy, and the administrators talk about it +L+ and remove it each time. In August, there is more +L+ discussion of consensus versus the supermajority, and then +L+ it appears that it might be the original participant who lost +L+ the battle who is adding the numbers back to the policy, and +L+ even if it is not, he is actively participating in discussions to +L+ bring back the “numbers” [49]. +L+ As an example of the discussion leading away from +L+ decisions through polling, one administrator says, “for all +L+ but uncontroversial trivial propositions, it is unusual for +L+ decisions on Wikipedia talk pages to operate on a true +L+ consensus. Instead they operate on a rough consensus +L+ where it is recognised that a minority are in opposition. The +L+ question then arises is how large must the majority be to +L+ ignore the opinions of a minority?” [49]. +L+ When the policy is revised to respect the majority of +L+ decision makers, the discussion is titled, “The numbers +L+ came back again.” The first comment reads, “So I removed +L+ them.” The response to it, by the “losing” contestant says, +L+ “Did you not read the section above? There is not +L+ consensus to remove them,” in an assertion that previous +L+ decisions support this, but the response to this remark is: “I +L+ have read the section above. There is clearly no consensus +L+ to include them. Stop adding them,” reinforcing the +L+ community’s adherence to “rough consensus” as opposed to +L+ numerically-based decision-making. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Implications for Negotiated Settlements and Trophies +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Policies, therefore, reflect a continuous process of battles +L+ engaged in and won or lost, so that no conclusion is +L+ achieved. Much discussion of these policies includes a +L+ continuous recycling of old fights and unresolved +L+ contentions. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Rules and Policies as Control Mechanisms +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> To complete tasks and meet goals, rules and policies are +L+ often written to act as control mechanisms. Control is +L+ defined as any effort made to ensure appropriate action +L+ [31]. In the systems development literature, control has +L+ been defined more specifically from the behavioral sense as +L+ attempts to ensure that individuals on a project team act in +L+ accordance with a previously agreed-upon strategy to +L+ accomplish desired goals and objectives [21]. Control +L+ mechanisms are “devices used by controllers to ensure +L+ proper controllee behavior” [20]. Both formal and informal +L+ control modes rely on control mechanisms to influence +L+ behavior, but formal modes control via performance +L+ evaluation and rewards while informal modes control via +L+ socialization to reduce goal differences [20]. Wikis draw +L+ upon informal modes of control by writing policies which +L+ describe ideal work output or behavior. +L+ Explicitly highlighting this role of rules and policies also +L+ allows for consideration of ways that rules are used to +L+ manage divergence of individual and organizational goals, a +L+ phenomenon that is an important element of organizational +L+ evolution. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Control Mechanisms in Wikipedia +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Wikipedia’s hierarchy of roles creates a class of people who +L+ apply the control mechanisms for the group: the +L+ administrators. Though, it is sometimes claimed that this +L+ hierarchy does not exist. Administrators are the only ones +L+ who can, as the Wikipedia site suggests, “protect and delete +L+ pages, block other editors, and undo these actions as well.” +L+ Also, in its categories of policies, Wikipedia devotes an +L+ entire section to what it calls, “enforcement,” a term for +L+ controlling the behavior of others. Two in this section that +L+ require administrator participation are Deletion and +L+ Blocking. Since control mechanisms ensure consistency +L+ between the goals and actions of the individual and those of +L+ the community, and since the goals of individuals are +L+ sometimes destructive, such as vandalism, a policy like +L+ Blocking prevents chronic disrupters from damaging the +L+ group or its efforts. Blocking is the term for the prevention +L+ of editing rights for those participants who refuse to support +L+ the goals of the organization. An interesting feature of the +L+ Blocking policy ameliorates its punitive approach: it is not +L+ meant for “retaliation,” but instead for “encouraging” +L+ appropriate behavior. A control mechanism of this sort, +L+ therefore, guides normative behavior rather than punishes +L+ deviance. +L+ The Three-Revert Rule (called a “rule” but considered a +L+ “policy” by Wikipedia) is a community-specific control +L+ mechanism. It states that an editor may not make more than +L+ three changes to an encyclopedia page within a twenty-four +L+ hour period. The stated purpose for this policy is to prevent +L+ what the group calls “edit wars," or conflicts between two +L+ or more editors over an entry that result in the constant +L+ effort to assert the validity of one version of it over another. +L+ Administrators established this rule because, as Wikipedia +L+ founder Jimbo Wales says, “revert warring has become an +L+ absurd drain on us.” This control mechanism, therefore, +L+ protects the administrators from overload created by editors +L+ who refuse to negotiate about the contents of an entry. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Implications for Control Mechanisms +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Where coordination assumes that all participants in a +L+ community have the same motives and merely need to +L+ understand how to get something done in terms of sequence +L+ or procedure, control suggests to the community what not to +L+ do and establishes rules of prevention of behaviors that will +L+ disrupt the process of the organization. These mechanisms +L+ </SectLabel_bodyText> <SectLabel_page> 1107 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> are exceptional, for they can largely be applied only by +L+ administrators, by those with power acceded to them by the +L+ group through their demonstrated degree of participation in +L+ editing and through the acknowledgement of those efforts +L+ by the other power holders and wielders in the system. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> IMPLICATIONS FOR CREATION OF WIKI-BASED +L+ SYSTEMS AND DESIGN OF PLATFORMS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> We suggest, therefore, that pursuing the “policyless” ideal +L+ that wikis represent is a pipedream. Policy creation and +L+ maintenance is an important aspect of the work that must be +L+ done to keep the community running +L+ Providing tools and infrastructure mechanisms that support +L+ the development and management of policies is an +L+ important part of creating social computing systems that +L+ work. For example, work has been done on policy +L+ extraction that focuses on identifying rules which can be +L+ embedded in the infrastructure to support coordination and +L+ organization. This study suggests that policies vary in their +L+ intention to support coordination and organization, thus the +L+ application of the work on policy extraction is narrowed. +L+ For example, a rule acting as an external signal has less +L+ intention for coordination and organization. As a result, it +L+ may not be appropriate for it to be embedded in the +L+ infrastructure to support coordination and organization. +L+ Looking at the language used in writing the policy may +L+ signal its intention to support coordination and organization +L+ and, subsequently, whether it is a candidate for embedding +L+ in the infrastructure. More detailed systematic studies can +L+ provide insight into a policies candidacy for embedding in +L+ the infrastructure. +L+ While there is something to be said about treating policies +L+ as coordination mechanisms that are automated or at least +L+ embedded directly in the technology, that approach is not +L+ without its pitfalls. Since policies can also be highly +L+ symbolic or meaning-filled, embedding them or automating +L+ them may not work because it could remove this function or +L+ make it less effective for this purpose. Furthermore, it may +L+ be possible that different rules playing different roles have +L+ varying importance for the success of the wiki at specific +L+ phases in the wiki lifecycle. +L+ This work raises important questions for organizations +L+ implementing wikis and collaborative technologies for +L+ internal use. When organizations invest in these +L+ technologies, such as Lotus Notes and Microsoft +L+ Sharepoint, their first step is often to put in place a +L+ collection of policies and guidelines regarding their use. +L+ However, less attention is given to the policies and +L+ guidelines created by the groups that use these systems – +L+ which are often left to “emerge” spontaneously. The +L+ examples and concepts described in this paper highlight the +L+ complexity of rule formation and suggest that support +L+ should be provided to help collaborating groups create and +L+ maintain effective rulespaces. +L+ Lastly, the Wikipedia archives suggest that facilitating, +L+ supporting, and managing this system of rules may not be +L+ simply a matter of data collection. Rather, serving this +L+ system well is a matter of promoting situational awareness +L+ and strategic intervention in a complex, evolving system. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Wikis have captured the imagination of many because as a +L+ technology they support unencumbered, highly flexible, +L+ very visible, and accessible collaboration [5, 11, 22, 42, 52, +L+ 53]. These features have led many commentators and +L+ authors to wax eloquently about the possibility of new types +L+ of work and organization which are peer-based, non- +L+ hierarchical, non-bureaucratic, emergent, complex, and +L+ communal. +L+ While it may be the case that wikis do in fact provide a +L+ basis for this type of work arrangement, the study reported +L+ here suggests that the true power of wikis lies in the fact +L+ that they are a platform that provides affordances which +L+ allow for a wide variety of rich, multifaceted organizational +L+ structures. Rather than assuming that rules, policies, and +L+ guidelines are operating in only one fashion, wikis allow +L+ for, and in fact facilitate, the creation of policies and +L+ procedures that serve a wide variety of functions – and as a +L+ result they are capable of truly supporting a much broader +L+ range of structures and activities than many of the other +L+ more structured, collaborative platforms. +L+ This suggests that not only are wikis a platform that has +L+ greater potential in organizational and public use, but also +L+ that, from a design perspective, they provide a valuable +L+ opportunity for using the “sidewalk design strategy” of +L+ providing a field of grass and watching where and how the +L+ users walk, or so-called desire paths. This study provides a +L+ basis for describing these paths. Future studies in particular +L+ applications would do well to ask how these issues are +L+ addressed, capabilities are used, and how the activities and +L+ mechanisms that come into play can be helpfully reinforced +L+ or supported through the interface and infrastructure. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGEMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The authors would like to thank the members of the Online +L+ Community Research Group at Carnegie Mellon University +L+ and especially Bob Kraut and John Levine. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. Ahuja, M.K. and K.M. Carley, Network structure in +L+ virtual organizations. Organization Science, 1999. +L+ 10(6): p. 741-757. +L+ 2. Barreto, M. and N. Ellemers, The impact of anonymity +L+ and group identification on progroup behavior in +L+ computer-mediated groups. Small Group Research, +L+ 2002. 33(5): p. 590-610. +L+ 3. Baym, N., Interpreting soap operas and creating +L+ community: Inside a computer-mediate fan culture. +L+ Journal of Folklore Research, 1993. 30: p. 143-176. +L+ 4. Baym, N.K., Tune in, log on: Soaps, fandom, and +L+ online community. 2000, Thousand Oaks, CA: Sage +L+ Publications. +L+ 5. Bryant, S.L., A. Forte, and A. Bruckman, Becoming +L+ wikipedia: Transformation of participation in a +L+ </SectLabel_reference> <SectLabel_page> 1108 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_reference> collaborative online encyclopedia, in GROUP. 2005: +L+ Sanibel Island, FL. +L+ 6. Burnett, G. and L. Bonnici, Beyond the faq: Explicit +L+ and implicit norms in usenet newsgroups. Library & +L+ Information Science Research, 2003. 25(3): p. 333. +L+ 7. Chidambaram, L. and L.L. Tung, Is out of sight, out of +L+ mind? An empirical study of social loafing in +L+ technology-supported groups. Information Systems +L+ Research, 2005. 16(2): p. 149-168. +L+ 8. Cohen, N., Defending wikipedia's impolite side, in The +L+ New York TImes. 2007, The New York Times +L+ Company: New York, NY. +L+ 9. Connelly, T. and B.K. Thorn, Discretionary databases: +L+ Theory, data, and implications, in Organizations and +L+ communication technology, J. Fulk and C. Steinfield, +L+ Editors. 1990, Sage Publications: Newbury Park, CA. +L+ p. 219-233. +L+ 10. Fogel, K., Producing open source software: How to +L+ run a successful free software project 2005, +L+ Sebastopol, CA O'Reilly Media, Inc. +L+ 11. Forte, A. and A. Bruckman, Why do people write for +L+ wikipedia? Incentives to contribute to open-content +L+ publishing, in Group 2005 Workshop - Sustaining +L+ Community: The role and design of incentive +L+ mechanisms in online systems. 2005: Sanibel Island, +L+ FL. +L+ 12. Galegher, J., L. Sproull, and S. Kiesler, Legitimacy, +L+ authority, and community in electronic support groups. +L+ Written Communication, 1998. 15(4): p. 493-530. +L+ 13. Hinds, P.J. and D.E. Bailey, Out of sight, out of sync: +L+ Understanding conflict in distributed teams. +L+ Organization Science, 2003. 14(6): p. 515-632. +L+ 14. Hinds, P.J. and M. Mortensen, Understanding conflict +L+ in geographically distributed teams: The moderating +L+ effects of shared identity, shared context, and +L+ spontaneous communication. Organization Science, +L+ 2005. 16(3): p. 290-307. +L+ 15. Jensen, C. and W. Scacchi. Collaboration, leadership, +L+ control, and conflict negotiation in the netbeans.Org +L+ software development community. in Proceedings of +L+ the 38th Hawaii International Conference on System +L+ Sciences. 2005. Waikola Village, HI. +L+ 16. Jensen, C. and W. Scacchi. Role migration and +L+ advancement processes in ossd projects: A +L+ comparative case study. in Proceedings of the 27th +L+ International Conference on Software Engineerring. +L+ 2007. Minneapolis, MN. +L+ 17. Jones, Q., G. Ravid, and S. Rafaeli, Information +L+ overload and the message dynamics of online +L+ interaction spaces: A theoretical model and empirical +L+ exploration. Information Systems Research, 2004. +L+ 15(2): p. 194-210. +L+ 18. Kennedy, H., Beyond anonymity, or future directions +L+ for internet identity research. New Media & Society, +L+ 2006. 8(6): p. 859-876. +L+ 19. Kim, A.J., Community building on the web: Secret +L+ strategies for successful online communities. 1 ed. +L+ 2000, Berkeley, CA Peachpit Press. +L+ 20. Kirsch, L.J., Portfolios of control modes and is project +L+ management. Information Systems Research, 1997. +L+ 8(3): p. 215-239. +L+ 21. Kirsch, L.J., et al., Controlling information systems +L+ development projects: The view from the client. +L+ Management Science, 2002. 48(4): p. 484-498. +L+ 22. Kittur, A., et al., Power of the few vs. Wisdom of the +L+ crowd: Wikipedia and the rise of the bourgeoisie, in +L+ Conference on Human Factors in Computing Systems +L+ (CHI). 2007: San Jose, CA. +L+ 23. Kittur, A., et al., He says, she says: Conflict and +L+ coordination in wikipedia, in Conference on Human +L+ Factors in Computing Systems (CHI). 2007: San Jose, +L+ CA. +L+ 24. Kollock, P. and M. Smith, Managing the virtual +L+ commons: Cooperation and conflict in computer +L+ communities, in Computer-mediated communication: +L+ Linguistic, social and cross-cultural perspectives, S. +L+ Herring, Editor. 1996, John Benjamins: Amsterdam. p. +L+ 109-128. +L+ 25. Kuechler, W.L. and C. Vaishnavi, So, talk to me: The +L+ effect of explicit goals on the comprehension of +L+ business process narratives. MIS Quarterly, 2006. +L+ 30(4): p. 961-A16. +L+ 26. Lakhani, K.R. and E. von Hippel, How open source +L+ software works: "Free" User-to-user assistance. +L+ Research Policy, 2003. 32(6): p. 923-943. +L+ 27. Malone, T., W. and K. Crowston, The interdisciplinary +L+ study of coordination. ACM Computing Surveys, 1994. +L+ 26(1): p. 87-119. +L+ 28. Maney, K., Mass collaboration could change way +L+ companies operate, in USA Today. 2006. +L+ 29. March, J.G., M. Schulz, and X. Zhou, The dynamics of +L+ rules: Change in written organizational codes. 2000, +L+ Stanford, CA: Stanford University Press. +L+ 30. McKenna, K.Y.A. and J.A. Bargh, Coming out in the +L+ age of the internet: Identity "Demarginalization" +L+ Through virtual group participation. Journal of +L+ Personality & Social Psychology. 75(3): p. 681. +L+ 31. Nidumolu, S.R. and M.R. Subramani, The matrix of +L+ control: Combining process and structure approaches +L+ to managing software development. Journal of +L+ Management Information Systems, 2003. 20(Issue 3): +L+ p. 159. +L+ 32. Ostrom, E., Collective action and the evolution of +L+ social norms. Journal of Economic Perspectives, 2000. +L+ 14(3): p. 137-158. +L+ 33. Petronio, S., The boundaries of privacy: Praxis of +L+ everyday life, in Balancing the secrets of private +L+ disclosures, S. Petronio, Editor. 2000, Lawrence +L+ Erlbaum Associates: Mahwah, N. J. p. 37-50. +L+ 34. Petronio, S., et al., (mis)communicating across +L+ boundaries. Communication Research, 1998: p. 571- +L+ 595. +L+ </SectLabel_reference> <SectLabel_page> 1109 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_reference> 35. Raymond, E., The cathedral and the bazaar. +L+ Knowledge and Policy, 1999. 12(3). +L+ 36. Ren, Y., R.E. Kraut, and S. Kiesler, Applying common +L+ identity and bond theory to the design of online +L+ communities. In press, 2006. +L+ 37. Riva, G. and C. Galimberti, Computer-mediated +L+ communication: Identity and social interaction in an +L+ electronic environment. Genetic, Social & General +L+ Psychology Monographs, 1998. 124(4): p. 434-463. +L+ 38. Roberts, J.A., I.-H. Hann, and S.A. Slaughter, +L+ Understanding the motivations, participation, and +L+ performance of open source software developers: A +L+ longitudinal study of the apache projects. Management +L+ Science, 2006. 52(7): p. 984-999. +L+ 39. Simon, H., Bounded rationality and organizational +L+ learning. Organization Science, 1991. 2(1): p. 125-134. +L+ 40. Slaughter, S.A. and L.J. Kirsch, The effectiveness of +L+ knowledge transfer portfolios on software process +L+ improvement: A field study. Information Systems +L+ Research, 2006. 17(3): p. 301-320. +L+ 41. Stewart, K.J. and S. Gosain, The impact of ideology on +L+ effectiveness in open source software development +L+ teams. MIS Quarterly, 2006. 30(2): p. 291-314. +L+ 42. Viegas, F., M. Wattenberg, and K. Dave, Studying +L+ cooperation and conflict between authors with history +L+ flow visualizations, in Conference on Human Factors +L+ in Computing Systems (CHI). 2004: Vienna, Austria. +L+ 43. Viegas, F.B., et al. Talk before you type: Coordination +L+ in wikipedia. in Proceedings of the 40th Hawaii +L+ International Conference on System Sciences. 2007. +L+ Big Island, HI. +L+ 44. von Krogh, G., S. Spaeth, and K.R. Lakhani, +L+ Community, joining, and specialization in open source +L+ software innovation: A case study. Research Policy, +L+ 2003. 32(7): p. 1217-1241. +L+ 45. Wikipedia contributors, Ignore all rules. 2006, +L+ Wikipedia, The Free Encyclopedia. +L+ 46. Wikipedia contributors, Wikipedia talk:Biographies of +L+ living persons 2007, Wikipedia, The Free +L+ Encyclopedia. +L+ 47. Wikipedia contributors, Wikipedia talk: Copyrights +L+ 2007, Wikipedia, The Free Encyclopedia. +L+ 48. Wikipedia contributors, Wikipedia: Policies and +L+ guidelines. 2007, Wikipedia, The Free Encyclopedia. +L+ 49. Wikipedia contributors, Wikipedia: Consensus. 2007, +L+ Wikipedia, The Free Encyclopedia. +L+ 50. Wikipedia contributors, Wikipedia: Criteria for speedy +L+ deletion. 2007, Wikipedia, The Free Encyclopedia. +L+ 51. Wikipedia contributors, Wikipedia:Neutral point of +L+ view. 2007, Wikipedia, The Free Encyclopedia. +L+ 52. Zhang, X.M. and F. Zhu, Intrinsic motivation of open +L+ content contributors: The case of wikipedia, in +L+ Workshop on Information Systems and Economics. +L+ 2006: Evanston, IL. +L+ 53. Zlatić, V., et al., Wikipedias: Collaborative web-based +L+ encyclopedias as complex networks. Physical Review, +L+ 2006. 74(016115). +L+ </SectLabel_reference> <SectLabel_page> 1110 +L+ </SectLabel_page>
<SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_title> Exploring the Role of the Reader in the Activity of +L+ Blogging +L+ </SectLabel_title> <SectLabel_author> Eric Baumer	Mark Sueyoshi	Bill Tomlinson +L+ </SectLabel_author> <SectLabel_affiliation> Department of Informatics	Int’l Studies / East Asian Cultures	Department of Informatics +L+ U of California, Irvine, USA	U of California, Irvine, USA	U of California, Irvine, USA +L+ </SectLabel_affiliation> <SectLabel_email> ebaumer@ics.uci.edu	msueyosh@uci.edu	wmt@uci.edu +L+ </SectLabel_email> <SectLabel_sectionHeader> ABSTRACT +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Within the last decade, blogs have become an important +L+ element of popular culture, mass media, and the daily lives +L+ of countless Internet users. Despite the medium’s +L+ interactive nature, most research on blogs focuses on either +L+ the blog itself or the blogger, rarely if at all focusing on the +L+ reader’s impact. In order to gain a better understanding of +L+ the social practice of blogging, we must take into account +L+ the role, contributions, and significance of the reader. This +L+ paper presents the findings of a qualitative study of blog +L+ readers, including common blog reading practices, some of +L+ the dimensions along which reading practices vary, +L+ relationships between identity presentation and perception, +L+ the interpretation of temporality, and the ways in which +L+ readers feel that they are a part of the blogs they read. It +L+ also describes similarities to, and discrepancies with, +L+ previous work, and suggests a number of directions and +L+ implications for future work on blogging. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> Author Keywords +L+ </SectLabel_sectionHeader> <SectLabel_keyword> Blogging, blog readers. +L+ </SectLabel_keyword> <SectLabel_sectionHeader> ACM Classification Keywords +L+ </SectLabel_sectionHeader> <SectLabel_category> H.5.m. Information interfaces and presentation (e.g., HCI): +L+ Miscellaneous; K.4.m. Computers and Society: +L+ Miscellaneous. +L+ </SectLabel_category> <SectLabel_sectionHeader> INTRODUCTION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> By most indications, blogs are proliferating at an ever- +L+ increasing rate. Although specific figures vary among +L+ different sources [16,25] there is consensus that blogs have +L+ become an important, active, and influential part of online +L+ media. Research on blogging, e.g., [11,15,21], has revealed +L+ important insights about the activity of blogging, the +L+ attitudes of bloggers, and the practices surrounding blogs. +L+ However, blogging is not a solo activity. While work has +L+ been done in areas such as analyzing conversations between +L+ </SectLabel_bodyText> <SectLabel_copyright> Permission to make digital or hard copies of all or part of this work for +L+ personal or classroom use is granted without fee provided that copies are +L+ not made or distributed for profit or commercial advantage and that copies +L+ bear this notice and the full citation on the first page. To copy otherwise, +L+ or republish, to post on servers or to redistribute to lists, requires prior +L+ specific permission and/or a fee. +L+ </SectLabel_copyright> <SectLabel_note> CHI 2008, April 5–10, 2008, Florence, Italy. +L+ </SectLabel_note> <SectLabel_copyright> Copyright 2008 ACM 978-1-60558-011-1/08/04...$5.00. +L+ </SectLabel_copyright> <SectLabel_bodyText> blogs, e.g., [7,10], and applying social network analysis to +L+ blogs, e.g., [8,19], little work has been done examining the +L+ role of the reader in the blogging process. This gap is +L+ surprising, considering Nardi et al.’s prediction that “future +L+ research is sure to pay attention to blog readers” [21:231]. +L+ Furthermore, according to Lenhart and Fox [16], as of July +L+ 2006, 57 million American adults read blogs, over a third of +L+ the 147 million who use the Internet. Sifry [25] puts the +L+ number of unique Technorati visitors at over 9 million in +L+ March 2007, up by over 50% from February 2007. Clearly, +L+ not only is the number of blogs increasing, but also the +L+ number of blog readers. The role of this ever increasing +L+ population of blog readers presents a promising and +L+ important, yet little-explored, area of research. +L+ This paper is not the first call for a focus on readers. In the +L+ 1960’s and 1970’s, a shift occurred in literary theory from +L+ focusing primarily on the literary object itself to including +L+ the reader’s response to the literature. Reader-response +L+ theory, or reader-response criticism, cf. [3,17], focuses not +L+ on the literature itself but rather on the audience’s response +L+ to, and interpretation of, the text. The reader is not a passive +L+ recipient of content, this critique argues, but rather engages +L+ in an active process of interpretation. Reality and meaning +L+ exist neither solely in the text nor solely in the reader, but +L+ are constructed through the dialectic interactions between +L+ the two. Similarly, the reality and meaning of a blog exists +L+ neither solely in the blog itself nor solely in the reader, but +L+ rather in the reader’s active interpretation of, and +L+ interaction with, the blog. Furthermore, technologies and +L+ practices such as commenting, linking, tagging, and +L+ trackbacks enable a level of explicit interaction with both +L+ the text and the author not available in previous textual +L+ media. This paper argues for a shift in the study of blogging +L+ similar to that in literary criticism represented by reader- +L+ response theory. This shift to emphasize the interactional +L+ aspects of blogging also fits into a larger trend in HCI +L+ research of moving from the user as information processor, +L+ to human actor, to embodied experiencer [5]. In order to +L+ understand the myriad contexts in which human-computer +L+ interaction takes place, researchers have adopted different +L+ stances toward users and taken different perspectives on +L+ HCI systems. Similarly, in order to understand fully the +L+ activity of blogging, we must study not only bloggers and +L+ the blogs they produce, but also the readers of those blogs +L+ and their interactions with the blog and the blogger. +L+ </SectLabel_bodyText> <SectLabel_page> 1111 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> This paper reports on the results of a qualitative study into +L+ the practices and culture of blog readers. How do readers +L+ contribute to and help shape the various blogs they read? +L+ When, why, and how do readers choose to comment? How +L+ do readers perceive the identity of the blogger? Do readers +L+ feel overwhelmed by the amount of information available +L+ through blogs? What habits do readers follow? While this +L+ paper cannot address these questions in their entirety, it is a +L+ first step toward understanding the role, contributions, and +L+ significance of the reader in the activity of blogging. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> RELATED WORK +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Recent work on blogging covers a wide range of topics (see +L+ [24] for an overview). This section highlights work that +L+ helps inform this study. Nardi [21] examined the social +L+ nature of blogging activity, illustrating that blogs are quite +L+ unlike a personal diary. Lenhart’s [15] work pointed to the +L+ slow development of norms given the decentralized and +L+ non-standardized activities that occur on and off blogs. +L+ Herring et al. [11] provide a genre analysis of weblogs. +L+ Within the emerging medium of blogs, boyd [1] addressed +L+ the need for self-awareness tools to manage the fluidity of +L+ identity presentation in blogs. Some have applied existing +L+ analytic tools, such as social network analysis, e.g., [8, 19], +L+ to show that blogs are not highly interconnected in a +L+ decentralized fashion, but rather grouped in numerous +L+ clusters of blogs with limited links between clusters. +L+ Studies using conversation analysis have suggested that +L+ conversations across blogs and amongst bloggers are +L+ limited to a small number of “A-list” blogs [10]. +L+ Here, our focus is the audience. In Lenhart’s examination of +L+ norm formation in blogging [15], she discusses the +L+ anonymity of the blog audience and their occasional +L+ terrifying effect on bloggers. Similarly, Reed [23] illustrates +L+ blogger self-censorship due to an audience made up of +L+ certain friends and family. boyd [2] describes the +L+ expectations that bloggers feel are placed on them by their +L+ audience and how bloggers negotiate the formation and +L+ fulfillment of these expectations. Nardi suggests that +L+ “readers create blogs as much as writers” [21:225], giving +L+ them an equal role in the activity of blogging. Despite +L+ acknowledging the presence and impact of an audience, no +L+ previous blogging research has made blog readers the +L+ primary focus. This paper aims to fill that gap. +L+ The position of the blog reader is often an ambiguous one. +L+ Most research on blogs adopts the view that readers, +L+ commenters, and participants are also bloggers themselves, +L+ e.g., [8]. However, according to the statistics cited above, +L+ there is obviously a large discrepancy between bloggers and +L+ people who read blogs – not every reader is a blogger. The +L+ question becomes, when does one move from being a blog +L+ reader to a blogger? Is simply owning a blog enough, or +L+ must there be regular updates? What is the requisite update +L+ frequency? Despite indications to the contrary, are there +L+ any bloggers who do not read other blogs? It is be argued +L+ below that, rather than attempting to split individuals into +L+ either the category of blogger or the category of reader, it +L+ might be more useful to consider the question in terms of +L+ degree of membership, where an individual may be both a +L+ blogger and a reader to varying and independent degrees. +L+ For the purposes of this study, we focus on those who have +L+ a high enough degree of readership to self-identify as a blog +L+ reader, regardless of their degree of bloggership. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> THEORY +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In examining the activities that surround blog reading, this +L+ study is partially informed by ideas from reader-response +L+ theory [3,17], which help provide a general framework with +L+ which to analyze the act of reading blogs. This section +L+ provides a brief introduction to reader-response theory, +L+ situating it in the context of literary criticism, and describes +L+ how the theory is applied in this paper. +L+ Despite the general inclination to situate reader-response +L+ theory in opposition to formalism, which posits that only +L+ the materiality of the text is significant, it actually +L+ developed from within formalism itself (Tompkins, cited in +L+ [3]). In the 1950’s, reader-response theory branched out +L+ from formalist discourse under the auspices of Gibson’s +L+ “mock reader” – the persona a reader should adopt to +L+ understand the text [3]. Thus a slight variation within +L+ formalism became the seed from which the reader and her +L+ or his interpretation gained significance. +L+ Later reader-response theorists, such as Crosman [3], +L+ argued specifically that the “construction of meaning +L+ ultimately resides in the auspices of readers, who approach +L+ literary texts... from their own subjective perspectives” +L+ [3:66]. This view is reminiscent of Nardi et al.’s [21] +L+ assertion that the reader and writer both participate in co- +L+ creating the blog, as well as Dourish’s [5] emphasis on +L+ viewing the user as a situated, embodied actor that actively +L+ engages with a system in context. As an extension of +L+ Crosman’s approach, Lewis [17] presents an alternate +L+ method of performing a literary critique. He suggests rather +L+ than judging books as good or bad and making assertions +L+ about someone’s tastes based on the books he or she reads, +L+ “let us try to discover how far it might be plausible to +L+ define a good book as a book which is read in one way, and +L+ a bad book as a book which is read in another way” [16:1]. +L+ He argues that “good literature [is] that which permits, +L+ invites, or even compels good reading” [16:104], and that +L+ examining the type of reading that a given work permits, +L+ invites, or compels can tell you about the merits of that +L+ work. While there may be questions as to what constitutes +L+ good reading, the purpose at hand is not to separate good +L+ blogs from bad. Rather, it is to explore the extent to which +L+ we may understand a blog not by features of its content, +L+ structure, or technological aspects, but rather by the type of +L+ reading practices in which readers of the blog engage. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> METHODS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> The authors chose to employ qualitative and ethnographic +L+ methods in order to gain an understanding of the subjective +L+ </SectLabel_bodyText> <SectLabel_page> 1112 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> experience of reading blogs. Participants were recruited +L+ through physical fliers posted on community bulletin +L+ boards and in public posting areas, as well as through +L+ online bulletin boards for the local community. The posted +L+ criteria were that respondents read at least 5 different blogs +L+ at least 2 to 3 times per week. In total, 23 individuals +L+ replied to these advertisements: 21 responded to the +L+ physical fliers, and 2 replied to the online postings. Of +L+ those, 19 fit our criteria (18 from the physical fliers and 1 +L+ from the online postings). Some potential subjects decided +L+ not to participate or stopped responding, resulting in a total +L+ of 15 respondents. All participants were compensated up to +L+ $20 US depending on the extent of their participation. +L+ Participant names used in this paper are pseudonyms. +L+ Three main data-gathering techniques were used: two semi- +L+ structured interviews with each participant, logging +L+ software to track reading patterns, and a survey to gather +L+ some basic data, such as demographics. Since there is little +L+ focus on blog readers in the existing literature, the first set +L+ of interviews were largely exploratory and generative; +L+ although there were specific themes on which this study +L+ was to focus, the first interview was also used to find other +L+ interesting themes or issues in our respondents’ blog +L+ reading practices. During the second interviews, +L+ participants were asked to discuss further some of the +L+ specific themes that emerged during the first round of +L+ interviews. Participants were also solicited to complete their +L+ second interview as a group interview. Four participants +L+ expressed interest in group interviews, but due to logistic +L+ constraints only one pair of participants completed the +L+ second interview as a group. One participant, Connie was +L+ unable to complete the second interview, and another, Jill, +L+ had to answer questions for the second interview via email. +L+ During both interviews, all participants spent time reading +L+ blogs as they normally would, showing and describing +L+ items of interest and parts of their blog-reading routines to +L+ the interviewer. All interviews and notes were transcribed +L+ and coded, initially using open coding and then +L+ transitioning to axial coding (see [18]). Coding was an +L+ iterative process during which two of the authors +L+ independently coded each interview transcript and then +L+ exchanged the transcripts to confer on the codes used and +L+ the themes they represented. The initial coding began after +L+ the completion of the first interviews, so that results from +L+ analysis of the first set of interviews helped inform and +L+ direct the second set. The axial codes form the basis for the +L+ findings reported below. +L+ Participants were also asked to install logging software on +L+ their computers to track their blog reading. The logger was +L+ implemented as a plugin for IBM’s Web Intermediaries +L+ infrastructure [28], which recorded a series of time-stamped +L+ URLs. Unfortunately, most participants either elected not to +L+ install the logger or ran into technical difficulties. Since +L+ only five participants successfully ran the logger, an +L+ analysis of those logs is not presented here, but it was used +L+ to generate questions for some of the second interviews. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> FINDINGS AND DISCUSSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Table 1 presents a profile of our respondents. Because +L+ subjects were recruited from the geographical area around a +L+ university, many are students or recent graduates. That said, +L+ they represent a diverse set of blog reading habits and +L+ practices. The data presented in this table were collected +L+ through an online survey completed by all but one of the +L+ participants (Connie). “Regular blogs” is the number of +L+ blogs the participant reads on a regular basis as determined +L+ by the participant, “example blogs” are a selection of +L+ representative examples from the blogs she or he reads, and +L+ “tools” describes the technology the participant uses to find +L+ and to read blogs. Since the purpose of these data is not to +L+ make statistical inferences about blog readers but rather to +L+ help create a picture of the various participants, and since +L+ the sample is not sufficiently large to generate statistically +L+ significant results, no quantitative analysis is performed. +L+ For statistics about blog readers, see [16]. +L+ This section includes a description of blog reading practices +L+ that were common among most of our participants, along +L+ with some of the factors that influence the myriad +L+ differences in approaches to reading blogs. Drawing on this +L+ diversity in blog reading practices, the section then +L+ addresses the question “what is a blog?” from readers’ +L+ perspectives; discusses the presentation and perception of +L+ online identity, noting important similarities and differences +L+ with previous work; and describes ways in which readers +L+ can feel that they are “a part” of the blogs they read. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Common Blog Reading Practices +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> While reader-response theory helps make sense of the +L+ significant variations of the data, some aspects of our +L+ participants’ reading practices are fairly consistent. Thirteen +L+ explicitly stated that blog reading is a form of “chilling +L+ out”, “wasting time”, “brain candy”, or “doing nothing”, +L+ similar to the pottering activities described by Wyche et al. +L+ [27]. The other two later indicated on the survey that blog +L+ reading was “sometimes” an activity during periods of +L+ boredom. Similar to some instances of pottering, blog +L+ reading can also have a habitual nature. When Fern reads +L+ blogs, she adheres to a self-prescribed system, despite her +L+ lack of interest in the content of some posts she reads. +L+ Lillian indicated that reading blogs became part of her +L+ morning routine. When we asked Charles if he looked +L+ forward to reading blogs everyday he responded: +L+ </SectLabel_bodyText> <SectLabel_construct> I don’t know if I look forward to [reading blogs]... I don’t +L+ really look forward to cigarettes anymore, but it’s something +L+ that happens through the course of the day that I feel like I +L+ might need to do. It just becomes habit, I guess. +L+ </SectLabel_construct> <SectLabel_bodyText> Though in all likelihood most blog readers do not share +L+ Charles’s outlook on the intensity of blog reading’s +L+ addictiveness, blog reading often becomes habitual. For +L+ Krish, who has only been reading blogs for eight months, +L+ “checking blogs is like checking one’s email,” which is +L+ similar to the habitualness described by nine other +L+ participants. For many, checking email is a routine, almost +L+ </SectLabel_bodyText> <SectLabel_page> 1113 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_table> Pseudonym	Age	Gender	Occupation	Regular Blogs	Frequency	Example Blogs	Years Reading	Tools +L+ Connie	22	F	--	--	Every Day	--	--	-- +L+ Fern	19	F	Student	1-2	Every Other Day	xanga.com, blogspot.com, livejournal.com	5-6 Years	4, 5, AIM Profiles +L+ Selena	18	F	Student	6-10	2-3 Times a Week	greatestjournal.com, myspace.com, xanga.com, asianave.com	6-7 Years	1, 4, 5 +L+ Charles	24	M	Admin. Assistant	6-10	Several Times a Day	dailykos.com, boingboing.net, blogspot.com, slashdot.org, poplicks.com	6-7 Years	1, 4 +L+ Lillian	33	F	Graduate Student	20+	Every Day	blogspot.com, indigirl.com/blog, carrieoke.net, doggedknits.com	4.5 Years	2 +L+ Judith	20	F	Student	3-5	Every Other Day	myspace.com, xanga.com, facebook.com	3 Years	4 +L+ Jill	20	F	Student	6-10	Several Times a Day	livejournal.com, flickfilosopher.com/blog, ingliseast.typepad.com/ingliseast	5-6 Years	1 +L+ Cindy	19	F	Student	1-2	Several Times a Day	xanga.com, livejournal.com	5 Years	4 +L+ Patricia	20	F	Student	1-2	2-3 Times a Week	sibol.in, mochix.com	4 Years	1, 2, 5 +L+ Natalie	25	F	Legal Assistant	11-20	Every Other Day	perezhilton.com, blogspot.com, myspace.com, livejournal.com	10 Years	1, 4, 5 +L+ Tony	31	M	Graduate Student	3-5	Every Day	slashdot.org, fark.com, treehugger.com, somethingawful.com	6 Years	1, 3, iGoogle +L+ Matthew	26	M	Graduate Student	11-20	Several Times a Day	blogspot.com, firejoemorgan.com, kugelmass.wordpress.com, sadlyno.com	6 Years	1, 2 +L+ Laura	27	F	Admin. Assistant	3-5	2-3 Times a Week	mypapercrane.com, blogspot.com, livejournal.com, bloesem.blogs.com	2 Years	1, 4 +L+ Cheryl	24	F	Graduate Student	3-5	2-3 Times a Week	fourfour.typepad.com,	2-3 Years	1 +L+ 						2manadvantage.com, nydailynews.com/blogs/mets +L+ Krish	22	M	Student	3-5	Every Day	metblogs.com, kiruba.com, blogspot.com, aparnasblog.wordpress.com	8 Months	1 +L+ </SectLabel_table> <SectLabel_tableCaption> Table 1 – Profile of participants. For tools, 1 is web browser, 2 is RSS aggregator, 3 is email client, 4 is blogging website, 5 is +L+ links from reader’s blog. Participants listed specific regular blogs, from which the authors generalized and chose examples. +L+ </SectLabel_tableCaption> <SectLabel_bodyText> quintessential part of going online. Whether one expects an +L+ email or not is unimportant, because one will check her or +L+ his email account not with the expectation of receiving +L+ email but rather as part of an Internet ritual. +L+ Much work in information retrieval, search technologies, +L+ and related fields is based on the premise that the sheer +L+ volume of information available is simply overwhelming, +L+ often referred to as “information overload,” and that users +L+ feel compelled to try and stay on top of the ever increasing +L+ amount of available information. This attitude dates at least +L+ as far back as Barnaby Rich’s assertion, in 1613, that “one +L+ of the diseases of this age is the multiplicity of books; they +L+ doth so overcharge the world that it is not able to digest the +L+ abundance of idle matter that is every day hatched and +L+ brought forth into the world” (quoted in [4:63]). However, +L+ such a sense of information overload with respect to blogs +L+ was not common among our respondents. Only two of the +L+ fifteen, Charles and Lillian, expressed feeling overwhelmed +L+ by the potential information available through blogs. The +L+ other participants indicated that they are not bothered when +L+ they cannot stay current with the newest posts for the blogs +L+ they frequent. Some would eventually catch up on old posts +L+ when the time suited them, while others simply choose the +L+ most recent or most interesting posts to read, skipping the +L+ rest. Laura reveals, “I don’t kill myself over it, because it’s +L+ not like I can’t always go back and see, ‘okay what +L+ happened two weeks ago’ ... I know what’s there and I +L+ know where to find it when I need it.” This attitude +L+ challenges the commonly accepted notion that users feel +L+ overwhelmed with staying constantly up to date. +L+ It also raises interesting issues of synchronicity. Computer +L+ mediated communication is often considered either +L+ synchronous, e.g., live video or audio chat; near +L+ synchronous, e.g., instant messaging; or asynchronous, e.g., +L+ email. Clearly, there are not fine distinctions but rather a +L+ gradient from synchronous to asynchronous, and blogs are +L+ generally placed closer to the asynchronous end of the +L+ spectrum [21]. However, based on our participants’ +L+ descriptions, they do not read blogs in a temporally situated +L+ manner. When returning to a blog that has not been visited +L+ recently, it does not matter if the most recent three posts +L+ occurred in the past week, in the past day, or in the past +L+ hour. What matters is the order in which posts appear on the +L+ blog. The most recent post on one blog, even if it is several +L+ </SectLabel_bodyText> <SectLabel_page> 1114 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> days old, is more likely to be read than the fourth post down +L+ on another blog, even if that post is from the previous day. +L+ This is somewhat similar to instant messaging +L+ conversations where time lapses between turns do not +L+ necessarily have an impact on the conversation [22]. Here, +L+ we introduce the term non-chronous to describe practices +L+ where individual events in one context, here a single blog, +L+ are considered in the temporal order in which they +L+ occurred, but not with regard to the specific time at which +L+ they occurred. This non-chronous approach does not mean +L+ that time-date stamps are utterly ineffectual, but they +L+ become much less important, especially with the advent of +L+ RSS aggregators, email clients, blog-host subscription lists, +L+ etc. For example, when Matthew falls behind on his regular +L+ blogs, he reads the five or so most recent posts in his RSS +L+ reader and his friends’ blogs. Patricia notes the time- +L+ stamp’s existence in passing, but does not take it into +L+ account while reading. Generally, participants in this study +L+ do not see themselves as struggling to handle a deluge of +L+ information streaming through blogs–a missed post is not +L+ usually a missed opportunity. +L+ Stepping back from the details of common blog reading +L+ practices, there are dramatic differences in how blog +L+ readers understand the visible object of their activity. When +L+ asked about motivations for reading blogs, participants said +L+ they visit blogs for information, inspiration, entertainment, +L+ and to a certain extent because it is just what they have +L+ always done. However, when asked the deceptively simple +L+ question, “what is a blog?” the responses were far more +L+ vague and varied. Patricia responded canonically, “well +L+ there’s the technical term and my own definition.” How +L+ does she determine which definition to use at what time? +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> “It Depends” +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Among our respondents, the manner of reading and +L+ interacting with a blog depends on myriad factors +L+ including, among others, the content of the blog, the intent +L+ of the reader, the perceived intent of the blogger, and the +L+ relationship of the reader to the blogger. We argue that part +L+ of the reason for the great diversity in approaches to blog +L+ reading is the great diversity of blogs. Previous work, e.g., +L+ [11,21], has tried to classify blogs as a genre with certain +L+ structural and content-based divisions into sub-genres. +L+ However, our findings align more closely with boyd’s +L+ argument [2] that blogs are a medium, and that a variety of +L+ different activities and interactions can occur in and through +L+ that medium. Furthermore, drawing on reader-response +L+ theory [17], we argue that, in order to distinguish between +L+ different types of blogs, it may be less useful to look at the +L+ structure or content of the blog and more informative to +L+ follow the ways that readers read and interact with the blog. +L+ The analysis presented here focuses on the following +L+ themes as dimensions along which approaches to blog +L+ reading may vary: the concept of a blog, perception and +L+ presentation of blogs, and “being a part” of blogs. From an +L+ analytic standpoint, uncovering data based on a consistent +L+ definition of blogs seems to make intuitive sense, but given +L+ the fluid character of blogs it may be misleading to do so. +L+ Rather than trying to impose a definition of what counts or +L+ does not count as a blog, the authors strove for a more +L+ authentic, emic perspective by allowing our blog reader +L+ participants to decide what constitutes a blog. The styles of +L+ blogs that our participants read varied as much as the +L+ specific reading practices. These practices depend in large +L+ part on the reader’s approach towards, and perception of, a +L+ blog, which shape and reshape the activity of blogging +L+ itself. An example of this iterative process is Krish’s +L+ approach toward blogs; he generally views blogs as just +L+ another thing to do on the Internet when he’s bored. He +L+ calls himself a passive reader of blogs, unlikely to search +L+ out a new set of blogs despite his disappointment in the lack +L+ of content in the blogs he reads. However, during his blog +L+ reading activity Krish began to note points of interest in his +L+ hometown that were described in a blog. Now, when Krish +L+ returns home, he applies the knowledge he acquired online +L+ to his experience offline. Although Krish’s initial +L+ motivation for reading blogs shaped his self-labeled +L+ “passive reading” of blogs, his Internet-only experience +L+ reshaped itself into an activity with offline implications. +L+ Reader-response theory directs us to note the ways that +L+ individual readers read different blogs differently. While a +L+ blog reader may feel fine lurking on popular blogs, she or +L+ he may feel obligated to interact on the blogs of friends. +L+ Although examining format and content in order to +L+ categorize a blog may reveal a general understanding of a +L+ blog, this approach is likely to neglect the audience for +L+ whom the blog is, at least in part, intended. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> What is a Blog? +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Definitions of the term “blog” cited in the academic +L+ literature often resemble Herring et al.’s, “frequently +L+ modified web pages in which dated entries are listed in +L+ reverse chronological order” [10:1]. boyd [2] provides a +L+ survey of various definitions from dictionaries, researchers, +L+ mass media, and bloggers themselves. When we asked our +L+ participants, “what is a blog?” the responses were a mixture +L+ that pointed to updates, commenting capabilities, +L+ authorship, RSS feeds, personal content, etc. Unlike the +L+ bloggers boyd describes, there is little or no uniformity of +L+ definition among readers. For example Judith considered +L+ the notes on facebook.com and the blog option on +L+ myspace.com examples of blogs while many others did not +L+ agree. When asked to define a blog some participants did +L+ refer to the frequency of modifications, but there was no +L+ mention of dated entries or reverse chronological order. +L+ Rather than structural features, thirteen participants +L+ discussed interactional attributes. For many bloggers, a blog +L+ is not something you have, blogging is something you do +L+ [2]. However, among our participants, there was not such a +L+ clear distinction. For example, Patricia emphasizes the +L+ conversational nature of blogging: +L+ </SectLabel_bodyText> <SectLabel_construct> A blog is something that’s still going on, that still has a +L+ conversation going on, that has people commenting, [it] +L+ </SectLabel_construct> <SectLabel_page> 1115 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_construct> doesn’t have to be all the time, but it does have this dialogue +L+ between the person who’s posting and the people who are +L+ reading, yeah that’s a blog.... [When the conversation stops], +L+ by my definition, yeah it’s a dead site. +L+ </SectLabel_construct> <SectLabel_bodyText> However, not all participants stressed conversational +L+ interaction. Providing another perspective, Natalie suggests +L+ that “a blog is a journal, like an electronic journal where +L+ people can express whatever they want, you know, and let +L+ everyone read it I guess.... it could be anything I guess.” +L+ Many respondents referred to “getting” a Xanga or +L+ “having” a blog, which foregrounds the blog as a +L+ possession and backgrounds the interactivity and process of +L+ blogging. Eight participants varied in their usage of the +L+ term “blog”: sometimes it would refer to an individual blog, +L+ an individual post, e.g., “I write a lot of blogs,” or even an +L+ entire blog-hosting site, such as when participants include +L+ LiveJournal in the blogs they frequently read. +L+ In Patricia’s definition, the interaction that occurs makes it +L+ a blog, while in Natalie’s definition the content makes it a +L+ blog. “It could be anything” demonstrates just how fluid the +L+ notion of blog can be. Another respondent, Tony, listed a +L+ series of technical requirements, including commenting and +L+ RSS, when asked if a particular website was a blog or not: +L+ </SectLabel_bodyText> <SectLabel_construct> That website is [a blog], yeah, but it doesn’t have live +L+ comments from people who read it. It has message boards that +L+ are associated with it, but they’re not as directly linked with +L+ different page articles, I don’t know. It’s not a static page, I +L+ mean every week you go to it, it will have different articles, +L+ but it’s not exactly the same format as a blog, it does have an +L+ RSS feed though so you can see what’s new on it. +L+ </SectLabel_construct> <SectLabel_bodyText> If readers and writers are both involved in the co- +L+ construction of the blog [21], how do differences in +L+ definitions impact this process? +L+ As with boyd’s [2] respondents, many readers used +L+ metaphors to define the term blog, and the metaphors with +L+ which they attempt to make sense of blogs in turn affect +L+ their understanding of, perception of, and interaction with +L+ blogs. Seven of the fifteen participants referred to blogs as a +L+ newspaper or magazine, and ten of the fifteen used the term +L+ diary or journal to describe at least one blog they read. +L+ These data point to the problematic nature of basing +L+ research on blogging activity upon the traditional format- +L+ oriented definition of blogs. Although a blog’s format may +L+ invite a certain reading, reader-response theory helps us +L+ understand the actual interaction or lack of interaction that +L+ occurs between the blog reader and the blogger. Although +L+ definitions found in the research literature [11,21] can be +L+ useful from an analytic standpoint, they may be less useful +L+ or even misleading when trying to understand how the +L+ bloggers and readers themselves approach blogging. When +L+ seeking to understand blogging from the blogger’s or the +L+ reader’s perspective, the authors found it more useful and +L+ informative to consider blogs not in terms of academic +L+ definitions, but rather in the terms of those involved in the +L+ activity of blogging. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> Presentation and Perception +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> Previous work [1,2,15,23] has explored how bloggers use +L+ blogs as a means of presentation of self (see [9]) online. +L+ This section explores the other half of that phenomenon, +L+ that is, how readers perceive the self that bloggers present. +L+ In some respects, these results align with previous findings. +L+ However, findings about our respondents also differ in a +L+ number of important ways from previous assertions about +L+ audience and perception in blogging. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Agreement with Previous Findings +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Past work on authenticity, one aspect of bloggers’ +L+ presentation of self, illustrates that audiences of blogs hope +L+ and expect authenticity, and that without it readership will +L+ be lost (McNeil in [15]). For blogs, authenticity does not +L+ hinge upon the accuracy of information they present, but +L+ rather upon their interpretability. (Langellier and Peterson +L+ in [15]). Lenhart bases her conclusion on Langellier and +L+ Peterson’s examination of the persistent interpretability of +L+ narratives. Arguing that blogs are a form of narrative, she +L+ posits that the blog is perceived “as one person’s ‘take’ on +L+ an issue, one person’s perspective on a story, left open to +L+ the interpretation of, and evaluation by, the reader, rather +L+ than as an unbiased source of information” [15:58-59]. +L+ Among our participants, eleven described the blogs they +L+ read regularly as feeling authentic. Connie “definitely [gets] +L+ an inside look at their lives”, while Natalie feels like she is +L+ traveling alongside the bloggers who write about their +L+ travels. All thirteen of our participants who read single- +L+ authored blogs recognize that posts of the blogs they read +L+ regularly were either opinion or personal narratives, which +L+ are important components of the perception of authenticity. +L+ Blogs are generally considered a one-to-many medium, but +L+ are often experienced by bloggers as one-to-one [15]. In +L+ this study, eight participants have experienced blogs as one- +L+ to-one communication between them and the blogger. +L+ Selena says, “for like some people..., I guess sometimes I +L+ feel like they’re writing to me.” +L+ This study also shows that negotiations between online and +L+ offline identity for blog readers are similar to those of +L+ bloggers. Early research into online identity, e.g., [26], +L+ argued that people used online worlds to create alternate +L+ identities or to explore certain facets of their personality +L+ that were not as prominent. However, more recent work, +L+ e.g., [1,20], has pointed to the ways in which a person’s +L+ online identity is a part or an extension of their offline +L+ identity, such as the way that Trinidadians use the Internet +L+ as just another way of being “Trini” [20], and that +L+ attempting to sever the two can be misleading and +L+ confusing. Similarly, while blogs and “real life” +L+ experiences are still distinct realms for readers, there is a +L+ relatively tight coupling between readers’ online and offline +L+ identities. Describing one of her friends, Fern says that “the +L+ way he types is the way he talks and thinks,” and Lillian +L+ hesitates to refer to only her offline friends as her “real” +L+ friends. Were there a connection with Patricia’s online and +L+ offline life, she says, “I [would be] a little bit surprised and +L+ </SectLabel_bodyText> <SectLabel_page> 1116 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> a little bit happy that there was this link between my online +L+ life that I publish online with just a typical school day that it +L+ could be considered that I’m still the same... my screen +L+ name versus me.” However, she is also wary of creating too +L+ strong a link between her online and offline identities. “I +L+ don’t want my dad to find me because there was this whole +L+ breakup thing [between my mom and dad] and he wasn’t +L+ such a good person.” While Lillian was initially cautious +L+ about linking her knit-blogging with her life as a graduate +L+ student, and she still does not give out her full name online, +L+ she also feels that her blogging activities are “a part of [her] +L+ and it’s not worth hiding it.” This sort of ambivalence was +L+ common among participants; they do not view the set of +L+ identities they construct through blog reading as identical to +L+ their set of offline identities, but they also do not view the +L+ two sets as totally disjoint; most of them continuously +L+ negotiate and redefine the relationship between the two. +L+ </SectLabel_bodyText> <SectLabel_subsubsectionHeader> Differences from Previous Findings +L+ </SectLabel_subsubsectionHeader> <SectLabel_bodyText> Previous work on blogs has overlooked several elements of +L+ presentation and perception, possibly due to its focus on +L+ bloggers as both producers and consumers of blogs. Blog +L+ readers are often perceived by bloggers as an unnerving and +L+ anonymous group of lurkers or instigators [15], placing +L+ expectations on the blogger, creating awkward social +L+ situations, or sometimes presenting an unwanted, invading +L+ presence [2]. However, blog readers often approach +L+ different blogs differently, and may contribute differently in +L+ different contexts. Each participant shared that she or he +L+ would variously comment, lurk, or instigate, depending on +L+ the blog. As for commenting, eleven respondents stated that +L+ they would semi-regularly encounter statements or +L+ sentiments with which they disagreed, but only four +L+ participants shared instances where their views differed +L+ significantly and decided to express their disagreement +L+ through comments. However, only one of these four would +L+ make comments with the aim of instigating an awkward +L+ situation or invading the blogger’s space. Lillian is of the +L+ opinion that “it’s not worth being negative.” She strives to +L+ ensure that “whatever comes out of [her] mouth... or what +L+ comes out of [her] fingers is positive.” Kirsh, though, said +L+ that he enjoys bashing on bloggers or simply kindling +L+ “flame wars” of nearly unfounded, ruthless arguments. +L+ While previous research has described the expectations +L+ readers place on bloggers, readers feel that there are certain +L+ expectations of them, as well. According to Patricia, “a +L+ good post deserves a reply from the audience,” and Jill sees +L+ “[commenting] as a courtesy.” Furthermore, while some +L+ comments are used as simple, lo-fi communication or +L+ notification mechanisms (discussed further below), many +L+ readers spend a significant amount of time formulating their +L+ comments in order for them to be coherent and insightful. +L+ In the rare event that Charles comments, he needs “time to +L+ sit down and plot out a cogent response.” While bloggers +L+ feel pressures about the content and identity they present, +L+ readers feel pressures about ensuring that their comments +L+ make a significant contribution. Similarly, while bloggers +L+ may feel pressured to update, ten of our participants felt +L+ obligated to read or comment, particularly on friends’ blogs +L+ or blogs of which they felt that they were “a part” (see next +L+ section). Selena “admits” that there are some posts, even on +L+ blogs of close friends, that she does not read. Lillian was +L+ relieved to learn that other readers did not follow every +L+ single post and skimmed many. However, while a reader +L+ can “get away” with not reading every post without much +L+ notice, it is more obvious when there are lapses on the part +L+ of the blogger. Though expectations and obligations may +L+ not be symmetrical, the activity of blogging nevertheless +L+ exerts social pressures on both bloggers and readers. +L+ However, the situation with respect to readers’ expectations +L+ is somewhat more complex still. Thirteen respondents +L+ expressed expectations with regard to update frequency, +L+ visual style, navigability, responsiveness, appropriateness, +L+ and other aspects. However, just as readers read different +L+ blogs differently, they have different expectations of +L+ different blogs. Expectations are often more lax for friends’ +L+ blogs and greater for more popular “big” blogs. On the +L+ other hand, for example, when readers comment on these +L+ big blogs, they rarely expect a response, while a comment +L+ on a friend’s blog almost demands reciprocation. Natalie is +L+ interested in travel, and so often reads and comments on +L+ travel blogs. She does not expect the blogger to respond to +L+ her questions, but is pleasantly surprised when it happens. +L+ Differences in expectations of blogger and reader are not +L+ split only along the lines of friend blogs vs. big blogs. For +L+ example, on knit blogs, Lillian comments, answers +L+ questions, and provides positive feedback, but she is +L+ unwilling to do the same on other blogs she reads, such as a +L+ science blog that relates to her graduate studies. +L+ Many of these differences—in expectations, in +L+ commenting, in other regards—can be traced to the reader’s +L+ perception of the blogger or blog, and to the reader’s +L+ motivation for reading. Lillian views the knit and craft +L+ blogs as a community and often attributes certain +L+ characteristics of the community to its primarily female +L+ composition. Tony accounts for his commenting practices +L+ as something he enjoys doing as an engineer. Charles reads +L+ blogs as a routine that helps him obtain information. Judith +L+ reads primarily to keep in contact with friends. It is not only +L+ the way the blogger presents herself or himself that affects +L+ the readers perception of the blogger, but also the purpose +L+ for which the reader is reading. +L+ Although this paper focuses on blog readers, only three of +L+ the fifteen participants do not have their own blog. Despite +L+ the fact that many of our participants are also bloggers, at +L+ least nominally, the findings presented here are still +L+ applicable to blog readers, because, as argued above, there +L+ is no evidence in the literature that there exist bloggers who +L+ do not read blogs. However, one difference is the tendency +L+ for the non-bloggers to read only popular, highly trafficked +L+ blogs, whereas, of the twelve blog readers with blogs, ten +L+ used their blogs to keep up with friends. Ultimately, though, +L+ </SectLabel_bodyText> <SectLabel_page> 1117 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> the activity of blog reading is neither a dichotomy of +L+ blogger versus reader nor a set of transactions that are +L+ confined to the materiality of the blog. “It depends” +L+ describes how the experience of blog reading is highly +L+ contingent on the individual reader and not solely the blog. +L+ </SectLabel_bodyText> <SectLabel_subsectionHeader> “Being a Part” +L+ </SectLabel_subsectionHeader> <SectLabel_bodyText> When discussions with respondents turned to themes of +L+ participation in, and contribution to, the blogs they read, +L+ eleven of them described feeling that they were a “part” of +L+ a blog in some way. This is distinct from the feeling of +L+ membership or belonging in a community [6,12]. Some +L+ respondents felt that they were part of a blog without ever +L+ making their individual presence known to the blogger or +L+ other readers. Being part of a blog is more than consistent +L+ readership, a sense of community, or a feeling of +L+ connectedness, although it includes all those things. +L+ Readership is one component to being a part of a blog. +L+ Connie stated that, “just by reading I feel like I’m +L+ participating.” Nevertheless, a few question if they are part +L+ of any of the blogs that they read. For Charles, the idea of a +L+ community is a central component to the definition of a +L+ blog, and thus he does not feel part of a blog because he +L+ does not consistently contribute as he might expect a +L+ community-member to do. Importantly, Charles reads +L+ mostly “big” blogs—ones that are relatively popular, +L+ generate a high volume of traffic, and receive copious +L+ comments—and while other participants could be a part of +L+ a big blog without commenting, some sort of interaction +L+ was necessary for Charles. While an important component +L+ of being a part, readership alone is often not sufficient. +L+ Six of the fifteen participants said specifically that they felt +L+ “connected” to a blog or blogger. Kuwabara et al.’s +L+ examination of FaintPop [14] revealed that the ability to +L+ express things that might not be considered an important +L+ topic of conversation, such as moods, help construct a +L+ feeling of connectedness. In the blogosphere, such +L+ examples are lo-fi comments, ones that are short, do not +L+ convey much content-wise, and are relatively generic. +L+ These comments also share certain aspects with the +L+ communication afforded by the Virtual Intimate Object +L+ (VIO) [13], in that they are relatively low bandwidth +L+ communication but carry a high degree of meaning and +L+ value for both reader and blogger. However, of the +L+ participants who expressed a feeling of connectedness, only +L+ Natalie described a feeling of presence similar to FaintPop +L+ or the VIO. When reading travel blogs she feels as if she is +L+ traveling with the blogger, sharing the blogger’s +L+ experiences, supporting her or his travels. Furthermore, +L+ though lo-fi comments are not the exception, they are not +L+ the rule, either. Although Cheryl feels connected to one of +L+ the sports-fan blogs she reads, she has yet to feel the need +L+ to leave comments of any sort. On the one hand, there is an +L+ argument that the readers who do not comment are not +L+ really connected, or that those who comment are more +L+ connected. Drawing on reader response theory, this paper +L+ argues instead that connectedness is constituted differently +L+ in different contexts; being a part of a blog looks different +L+ for different readers, and connectedness, even when +L+ achieved by different means, is still connectedness. +L+ Connectedness does not always entail feeling connected to +L+ the blogger as a person. Despite the distinctive personal +L+ style and presentation of self in many blogs, not all readers +L+ visit blogs for the blogger. Instead, they are more interested +L+ in the content or information presented on the blog. Among +L+ our respondents, ten of fifteen read certain blogs because +L+ they know or are familiar with the blogger in person, while +L+ eleven of fifteen read certain blogs because they want +L+ information about a particular topic. However, motivations +L+ can change over time. During the interviews, eleven +L+ participants described situations where they began reading +L+ blogs for information purposes, but continued reading +L+ because they developed a connection with the blogger. +L+ Cheryl reads fourfour, a blog with pop culture news and +L+ commentary. Initially, she started reading for the blogger’s +L+ witty and insightful entries about hip hop and “snarky +L+ commentary” about reality TV shows. However, the +L+ blogger would also occasionally post about his cats. +L+ </SectLabel_bodyText> <SectLabel_construct> At first, when he was posting pictures about his cat, not that I +L+ thought it was a little nutty, but it was like, ‘what’s the sense +L+ in doing this?’, but then I would read the entries and they +L+ would be really cute or hilarious pictures so then I became +L+ even a fan of the cat postings then I was like, ‘oh my god, this +L+ is so petty’.... he’s a charismatic person so pretty much any +L+ topic you’ll get some sort of satisfaction or chuckle... +L+ </SectLabel_construct> <SectLabel_bodyText> Even though Cheryl was initially drawn to the blog for the +L+ content, she ended up feeling connected with the blogger +L+ due in part to the personal information with which he +L+ supplemented his posts. In contrast, there were no instances +L+ where a blog reader began reading a blog for the blogger, +L+ and despite a falling out or loss of feeling connected +L+ continued to read for the content. This pattern suggests that, +L+ contrary to previous findings [2,23], it is important not to +L+ conflate the blogger with the content of the blog when +L+ considering the perspective of the reader. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> SUMMARY AND IMPLICATIONS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> This section highlights salient themes from the above +L+ findings as well as potential implications. These include not +L+ only design implications, but also more broadly future +L+ research directions and societal implications. +L+ Routine – All of our participants mentioned in some way +L+ the habitual nature of blog reading. Charles’ statement that +L+ reading blogs is “something that happens” frames the reader +L+ as passive, neither self-aware nor reflective about their +L+ reading. While some participants were conscious of why +L+ they read blogs, few were reflective of how they read. For +L+ example, participants rarely reflected on the routine or +L+ time-consuming nature of blog reading prior to +L+ participation in this study. This finding suggests that +L+ designing tools to raise self-awareness and encourage +L+ reflection could be valuable in transforming routinized blog +L+ </SectLabel_bodyText> <SectLabel_page> 1118 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> reading into a more engaging, fulfilling experience. Future +L+ work should also compare blog reading to other routine +L+ media use, such as watching television, checking email, +L+ reading the newspaper, or listening to the radio. +L+ Not Information Overload – Research on information +L+ retrieval (IR) and related areas often asserts that the copious +L+ quantity of information available leads to information +L+ overload, wherein overwhelmed users are unable to find +L+ relevant or important information in the ever-growing +L+ deluge. However, among this study’s participants, only two +L+ feel overwhelmed by the information content available to +L+ them. These readers do not feel the need to be constantly up +L+ to date with everything posted in the blogs they read. More +L+ studies should explore information overload, or lack +L+ thereof, from users’ perspectives in other contexts, so as not +L+ to spend research time developing IR algorithms that +L+ alleviate a problem not actually experienced by users. +L+ Non-chronous – While readers experience blog posts in +L+ temporal order, the exact times of the posts does not +L+ significantly impact the reading activity. The recency of a +L+ post has more to do with the number of other posts that +L+ have occurred since the post in question than with the +L+ amount of time that has passed. Not only does this finding +L+ have implications for the design of blog reading tools with +L+ respect to determining what content users/readers will find +L+ important, but it also points to the ways in which +L+ technologies such as blogs influence perceptions of +L+ temporality. The passage of time here is marked by the +L+ occurrence of certain events, i.e., posts, such that the +L+ significance of a period of time and even the perception of +L+ how much time has elapsed are influenced by how much +L+ happens in that period, i.e., how many posts occur. Future +L+ studies should pay attention to the ways in which the design +L+ and use of other technologies and communication media, +L+ such as instant messaging, email, or Twitter, influence how +L+ users perceive and constitute the passage of time. +L+ Identity– This paper builds on previous studies of online +L+ identity by exploring readers’ perceptions of bloggers’ +L+ identities. The findings here agree with previous ones, for +L+ example, that online and offline identities are not +L+ completely separate and distinct from one another [2,20], +L+ but there are also differences. For example, while previous +L+ work has looked at the obligations the blogger feels from +L+ his or her audience, this paper looked also at the obligations +L+ readers feel toward the blogger. Also, these findings +L+ indicate disconnects between the pressures felt by bloggers +L+ and the expectations of readers. While these findings +L+ provide future directions for exploring identity perception +L+ and presentation in social media and its connection to other +L+ aspects of interaction, such as privacy, appropriateness, and +L+ authority, they also suggest a design space for tools to allow +L+ more nuanced interactions between bloggers and readers. +L+ “Being a Part” – Being a part of a blog involves regular +L+ reading, a feeling of community, and a sense of +L+ connectedness, though these aspects are neither necessary +L+ nor sufficient. What it takes to be a part of a blog varies +L+ depending on the individual reader and the specific blog, +L+ especially whether it is a “big” popular blog or a personal +L+ friend’s blog. Based on these varied interactions between +L+ reader and blog, readership is defined and constituted +L+ differently in different contexts. Future work should +L+ examine the feeling of “being a part” both in different +L+ social media, such as YouTube or Wikipedia, as well as in +L+ instances where the division between authors and readers is +L+ more nebulous or even nonexistent, such as social +L+ networking sites or Twitter. Furthermore, it will be +L+ important to examine how participation in these and other +L+ online interactions impacts our definition of what it means +L+ to be a member of a community, both online and offline. +L+ Interactional Approach – drawing on reader response +L+ theory [3,17] and recent trends in HCI research [5], the +L+ research presented here takes an interactional approach to +L+ studying blog reading, shifting the focus from structural, +L+ technical, or content-oriented aspects of blogs to the +L+ reader’s experiences with them. This approach leads to +L+ novel, reader-centered definitions of the term “blog” based +L+ on the types of reading and interaction it allows and +L+ encourages. Focusing on interaction also foregrounds the +L+ ways in which blogs are not a genre but a medium for +L+ multi-directional communication among bloggers and +L+ readers. Previous work focused predominantly on the +L+ blogger, and this paper focuses mostly on readers. Future +L+ work should take an integrative approach, looking at the +L+ same interaction from both the blogger’s and the reader’s +L+ perspectives, as well as looking at interactions between +L+ blog readers in specific contexts, such as political blogs, +L+ religious blogs, or mommy blogs. What interactional +L+ aspects of blogging, including both blogger and reader, are +L+ unique to each of these communities? What aspects +L+ transcend individual communities to characterize blogging +L+ in general? How do common practices from blogging +L+ impact social interaction in other contexts beyond blogs? +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> CONCLUSION +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> In examining the increasingly common social activity of +L+ blogging, we must consider the experiences, roles, and +L+ contributions of readers, even when less readily apparent +L+ than those of bloggers. This paper presents a qualitative +L+ study that focuses on blog readers, their reading practices, +L+ their perceptions of blogs and bloggers, and what it means +L+ to be a part of a blog. The findings presented here indicate +L+ that the activity of blogging, of which readers are an +L+ integral part, is far more heterogeneous and multifaceted +L+ than previously suggested. Even though ten of our fifteen +L+ participants are between 18-25 years old and eleven of +L+ fifteen are students, the ways in which they read blogs, and +L+ even their definitions of what constitutes a blog, are +L+ dramatically different. The analysis here draws on reader- +L+ response theory to argue that, rather than using structural or +L+ content-based features in order to classify blogs, it may be +L+ more informative to consider them in terms of interactional +L+ features and readers’ experiences. This focus resonates with +L+ </SectLabel_bodyText> <SectLabel_page> 1119 +L+ </SectLabel_page> <SectLabel_note> CHI 2008 Proceedings · Shared Authoring	April 5-10, 2008 · Florence, Italy +L+ </SectLabel_note> <SectLabel_bodyText> current trends in HCI, such as embodied interaction [5]. +L+ This paper describes various ways in which blog reading is +L+ more than just reading. It leads the reader to form complex +L+ definitions of the term “blog,” each of which differs to +L+ varying degrees from the definitions used by bloggers. The +L+ paper also offers a view into the perception of the digital +L+ presentation of self in blogs. Blog reading allows for widely +L+ varied means of “being a part,” giving rise to new notions +L+ of community and belonging. This paper outlines the role of +L+ the reader in the activity of blogging, laying a general +L+ foundation for future work. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> ACKNOWLEDGMENTS +L+ </SectLabel_sectionHeader> <SectLabel_bodyText> Thanks to the California Institute for Telecommunications +L+ and Information Technology, the Summer Undergraduate +L+ Research Fellowship in Information Technologies Program, +L+ the Emulex Corporation, and the Donald Bren School of +L+ Information and Computer Sciences for their support. +L+ Thanks also to Bonnie Nardi for comments on an earlier +L+ draft, to Peter Krapp and Nathaniel Pope for discussions +L+ about blogs and blogging, and to the anonymous reviewers +L+ for their valuable feedback and suggestions. +L+ </SectLabel_bodyText> <SectLabel_sectionHeader> REFERENCES +L+ </SectLabel_sectionHeader> <SectLabel_reference> 1. boyd, d. Faceted id/entity: Managing representation in a +L+ digital world. Masters Thesis, Massachusetts Institute of +L+ Technology, Cambridge, MA, 2002 +L+ 2. boyd, d. A blogger's blog: Exploring the definition of a +L+ medium Reconstruction 10, 4 (2006). +L+ 3. Davis, T.F. and Womack, K. Formalist Criticism and +L+ Reader-Response Theory. Palgrave, New York, 2002. +L+ 4. de Solla Price, D.J. Little Science, Big Science. +L+ Columbia University Press, New York, 1963. +L+ 5. Dourish, P. Where the action is. MIT Press, Cambridge, +L+ MA, 2001. +L+ 6. Driskell, R.B. and Lyon, L. Are virtual communities +L+ true communities? Examining the environments and +L+ elements of community. City & Community 1, 4 (2002). +L+ 373-390. +L+ 7. Efimova, L. and Moor, A.d., Beyond personal +L+ webpublishing: An exploratory study of conversational +L+ blogging practices. in HI Int’l Conf on Sys Sci, (2005), +L+ IEEE Computer Society. +L+ 8. Furukawa, T., Matsuo, Y., Ohmukai, I., Uchiyama, K. +L+ and Ishizuka, M., Social Networks and Reading +L+ Behavior in Blogosphere. in Int’l Conf on Weblogs and +L+ Social Media, (2007). +L+ 9. Goffman, E. The presentation of self in everyday life. +L+ Doubleday, New York, 1959. +L+ 10. Herring, S.C., Kouper, I., Paolillo, J.C., Scheidt, L.A., +L+ Tyworth, M., Welsch, P., Wright, E. and Yu, N., +L+ Conversations in the blogosphere: An analysis "from the +L+ bottom up." in HI Int’l Conf on Sys Sci, (2005), IEEE +L+ Computer Society. +L+ 11. Herring, S.C., Scheidt, L.A., Bonus, S. and Wright, E., +L+ Bridging the gap: A genre analysis of weblogs. in HI +L+ Int’l Conf on Sys Sci, (2004), IEEE Computer Society. +L+ 12. Hillery, G. Definitions of community. Rural Sociology +L+ 20, (1955), 779-791. +L+ 13. Kaye, J., I just clicked to say I love you: Rich +L+ evaluations of minimal communication. in CHI +L+ Extended Abstracts, (2006), ACM Press, 363-368. +L+ 14. Kuwabara, K., Watanabe, T., Ohguro, T., Itoh, Y. and +L+ Maeda, Y., Connectedness oriented communication +L+ fostering a sense of connectedness to augment social +L+ relationships. in SAINT, (2002), Computer Society. +L+ 15. Lenhart, A.B. Unstable Texts: An Ethnographic Look at +L+ How Bloggers and Their Audience Negotiate Self- +L+ Presentation, Authenticity, and Norm Formation, +L+ Masters Thesis, Georgetown University, Washington, +L+ D.C., 2005. +L+ 16. Lenhart, A.B. and Fox, S. Bloggers: A portrait of the +L+ Internet's new storytellers, Pew Internet & American +L+ Life Project, 2006. +L+ 17. Lewis, C.S. An experiment in criticism. Cambridge +L+ University Press, 1992. +L+ 18. Lofland, J. and Lofland, L. Analyzing social settings: A +L+ guide to qualitative observation and analysis. +L+ Wadsworth, Belmont, CA, 1995. +L+ 19. Marlow, C., Audience, Structure, and Authority in the +L+ Weblog Community. in Int’l Comm Assoc Conf, (2004). +L+ 20. Miller, D. and Slater, D. Chapter One - Conclusions. in +L+ The Internet: An Ethnographic Approach, Berg, Oxford, +L+ 2000. +L+ 21. Nardi, B.A., Schiano, D.J. and Gumbrecht, M., +L+ Blogging as social activity, or, would you let 900 +L+ million people read your diary? in ACM CSCW, (2004), +L+ ACM Press, 222-231. +L+ 22. Nardi, B.A., Whittaker, S. and Bradner, E., Interaction +L+ and outeraction: instant messaging in action. in ACM +L+ CSCW, (2000), ACM Press, 79--88. +L+ 23. Reed, A. 'My blog is me': Texts and person in UK +L+ online journal culture (and anthropology). Ethnos 70, 2 +L+ (2005). +L+ 24. Schmidt, J. Blogging practices: An analytical +L+ framework. Journal of CMC 12, 4, (2007). +L+ 25. Sifry, D. The State of the Live Web, Technorati, 2007, +L+ accessed September 4, 2007, from +L+ http://www.sifry.com/alerts/archives/000493.html. +L+ 26. Turkle, S. Life on the Screen. Simon and Schuster, New +L+ York, 1995. +L+ 27. www.almaden.ibm.com/cs/wbi. Accessed July 2007. +L+ 28. Wyche, S.P., Taylor, A. and Kaye, J., Pottering: A +L+ design-oriented investigation. in CHI Extended +L+ Abstracts, (2007), ACM Press, 1893-1898. +L+ </SectLabel_reference> <SectLabel_page> 1120 +L+ </SectLabel_page>
