<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="100401">
<algorithm name="SectLabel" version="100410">
<variant no="0" confidence="0.000000">
<title confidence="0.948245">
Reversibility in a Constraint and Type based Logic Grammar:
Application to Secondary Predication
</title>
<author confidence="0.603738">
Palmira Marrafa Patrick Saint-Dizier
</author>
<affiliation confidence="0.335627">
FLL - Universidade de Lisboa / 1LTEC IRIT Universite Paul Sabatier
Ava Engo Arantes e Oliveira, 40 (Lte 41) , 8 E â€¢ 118, route de Narbonne
</affiliation>
<address confidence="0.34123">
P- 1900 LISBOA F-31062 TOULOUSE cedex France
Portugal e-mail: stdizier@iritirit.fr
</address>
<sectionHeader confidence="0.982301" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.5470915">
In this document, we present a formalism for
natural language processing which associates type
</bodyText>
<figureCaption confidence="0.687631416666667">
construction principles to constraint logic
programming. We show that it provides more
uniform, expressive and efficient tools for parsing
and generating language. Next, we present two
abstract machines which enable us to design, in a
symmetric way, a parser and a generator from that
formalism. This abstract machinery is then
exemplified by a detailed study of secondary
predication within the framework of a principled-
based description of language: Government and
Binding theory.
Introduction
</figureCaption>
<bodyText confidence="0.906646692307692">
Lexical as well as grammatical and discursive
knowledge required to understand or to produce
natural language utterances is usually a description
which is independent of the sentence production or
comprehension &amp;apos;algorithms&amp;apos;. It comes naturally into
mind to have a common, shared knowledge base of
what language is, independently of its potential uses.
Besides well-known advantages of uniformity and
transparency, this point of view is very convenient
for the computer scientist who does not have to
integrate into his parsers or generators the
unavoidable updatings required by grammar
development. The linguistic knowledge is thus
specified in a declarative way in different modules
(lexical, grammatical, discursive, ...) and different
strategies are applied which refer to these data
(directly for interpreters or via the production of a
compiled code for compilers). This approach can
however be realized more or less easily depending on
the formalisms used to describe language
phenomena.
7 -
In this document we introduce new advanced
tools of the Logic Programming framework and
show that they contribute to meeting the
requirements imposed by the manipulation and the
</bodyText>
<figureCaption confidence="0.905249105263158">
control of large amounts of data required by both the
parsing and the generation procedure. We first
consider logical types which are a declarative and
easy-to-use tool and formalism which permit a
grammar writer to encode knowledge in a very
flexible and principled-based way.
In addition to types, we introduce new active
constraints of the Constraint Logic Programming
(CLP) framework which allow us to treat and to
check for consistency of constraints throughout the
whole generation procedure and not to only evaluate
them when they are given in the programme or
grammar. These active constraints are fully
declarative and can be used by any type of
parsing/generation process.
CLP introduces a greater expressive power
together with a higher efficiency since the resolution
of constraints is postponed till they can be properly
evaluated and since constraints have to be always
</figureCaption>
<bodyText confidence="0.948031076923077">
true and consistent with each other. Finally, a feature
of active constraints is that they are usually
independent of the way they are processed, they are
thus strategy independent and can equivalently be
used for parsing and for generation.
To make reversibility concrete in our system,
we develop in this document two different abstract
machines based on type construction and constraint
satisfaction which give the foundations of a parser
and a generator using the same source of declarative
linguistic knowledge. The differences between these
machines exemplifies the &amp;apos;technical&amp;apos; differences one
may have between parsing and generation processes.
</bodyText>
<page confidence="0.992907">
2
</page>
<sectionHeader confidence="0.929343" genericHeader="method">
1. A type based description language
</sectionHeader>
<bodyText confidence="0.964050533333333">
Three major types of operations are at the basis
of the typed-based language we have designed for
language processing, namely:
the expression of type construction to generate
phrase structures,
the expression of dependencies (either local or
long-distance) between types,
the expression of well-formedness constraints
on types.
Types here refers to the usual data structures in
computer science. We now informally present the
syntax of our type-based language. It is directly
derived from the syntax of Login (Ait-ICagi and Nasr
86). The syntactic representation of a structured term
is called a Nr-term. It consists of:
</bodyText>
<listItem confidence="0.986181142857143">
(1) a root symbol, which is a type constructor
and denotes a class of entities,
(2) attribute labels, which are record field
symbols. Each attribute denotes a function in
extenso, from the root to the attribute value. The
attribute value can itself be a reference to a type.
(3) coreference constraints among paths of
</listItem>
<bodyText confidence="0.946345052631579">
labels, which indicate that the corresponding
attributes denote the same function. They are
indicated by variables. Here is an example:
person( id =&amp;gt; name(first =&amp;gt; string,
last =&amp;gt; X: string),
born =&amp;gt; date(day =&amp;gt; integer,
month =&amp;gt; monthname,
year =&amp;gt; integer),
father =&amp;gt; person( id =&amp;gt;
name(last =&amp;gt; X ))).
The root symbol is person; id, born and
father are three sub-w-terms which have either
constants or types as values. X indicates a
coreference. All different type structures are tagged
by different symbols. Notice also that in the latter
field only relevant information about person is
mentioned. Infinite structures can also be specified
by coreference links. Variables are in capital letters,
constants in small letters.
</bodyText>
<sectionHeader confidence="0.851628" genericHeader="method">
2. Dealing with constraints
</sectionHeader>
<bodyText confidence="0.998121166666667">
We have extended the type description
framework with active constraints and have given
them a Constraint Logic Programming (Colmerauer
90, Jaffar and Lassez 87) interpretation, permitting
us to have a more principled-based description of
language. The general form of a type is :
</bodyText>
<equation confidence="0.625576333333333">
Type :- Constraints.
We view constraints as part of the type:
(Type :- Constraints)
</equation>
<bodyText confidence="0.998443369565218">
is itself a type, subsumed by Type.
The simplest constraint is the precedence
constraint:
precede(X,Y),
where X and Y are of type string. This
constraint imposes that the string X precedes of the
string Y. When processing a sentence, precedence
constraints on constituents are stated in the grammar
rules and possibly at the lexical level. At each stage
i of the processing, there is a partial order P1(i) on
the words and structures already processed. At the end
of the process, precedence constraints give all the
possible word orderings which meet the constraints
of the grammar. In the case of parsing, constraints
imposed by the order of words in the input string
must be coherent with the precedence results of the
parse.
The next constraint imposes the presence of an
attribute in a type:
has(Attribute, Type)
where Attribute is either an attribute label or a
full pair attribute-value and Type is a reference to a
given type. This constraint imposes that at some
stage there is an attribute in Type which is
subsumed by or equal to Attribute. Informally, (1)
when incoherence with Attribute is detected or (2)
when Type is fully constructed, the non-satisfaction
of has(Attribute,Type) will provoque
backtracking. This constraint permits us to encode
thematic role assignment and focus management, and
also to encode the inclusion of a set of values into
another.
The last class of constraint is mainly related to
the expression of long-distance relations between
sentence constituents. Within the framework of
types, the notion of long-distance is somewhat
obsolete since there is no ordering relation on
subtypes in a type (attributes may be written in any
order). Thus, the notion of long-distance dependency
will be here formulated as a sub-type co-occurence
constraint. This constraint emerged from Dislog
(Saint-Dizier 87, 89). Very briefly, the co-occurence
of two or more subtypes in a larger type is expressed
by the constraint: pending(A,B) where A is a
type specification and B is a list of type
specifications. Informally, this constraint means that
</bodyText>
<page confidence="0.998295">
3
</page>
<bodyText confidence="0.9964806">
A originates the pending of the sub-types in B, in
other terms that A can be used if, somewhere else in
the main type (corresponding for example to a full
sentence), all the sub-types in B are used with
identical substitutions applied to identical variables.
</bodyText>
<sectionHeader confidence="0.422533" genericHeader="method">
3. Processing Language with types
and constraints
</sectionHeader>
<bodyText confidence="0.9954742">
We will mainly present here simple,
motivational examples. A more abstract syntactic
description will be given in section 6 which will
more fully motivate our approach. The examples
given in this text show that our description language
can accomodate principled-based descriptions of
language like Government and Binding theory as
well as lexically and head driven descriptions like in
the HPSG framework.
In the following simple examples, we only
have two main type constructors:
x0 corresponding to lexical entries,
xp corresponding to phrase structures.
Here is the description of the lexical entry
corresponding to the verb to give:
</bodyText>
<construct confidence="0.912692">
x0( cat =&amp;gt; v, string =&amp;gt; [give] ) :-
pending(x0(cat =&amp;gt; v),
[xp( cat =&amp;gt; n, string =&amp;gt; Si, role =&amp;gt; patient ),
xp( cat =&amp;gt; p, string =&amp;gt; S2,
role =&amp;gt; recipient) ] ),
</construct>
<bodyText confidence="0.9424398125">
precede([give],S1), precede(S1, S2).
This entry says that give is a verb which
subcategorizes for an np with role patient and a pp
with role recipient; np and pp are left pending. The
string Si generated from the np has to precede the
string S2 generated from the pp. These constraints
will be treated at the level of the type describing the
structure of a vp. The whole description x0
construction and related constraints is the type of
the verb to give. Let us now consider the
construction of a vp with an np and a pp
complements. To the construction of a vp type
corresponds the generation of a (set of) string(s)
corresponding to a vp, this is stored in S. We then
have the following construction:
xp( cat .&amp;gt; v, string =&amp;gt; S,
</bodyText>
<equation confidence="0.6171018">
const1 =&amp;gt; x0(cat =&amp;gt; v ),
const2 =&amp;gt; X : xp(cat =&amp;gt; n),
const3 =&amp;gt; Y : xp( cat =&amp;gt; p) ) :-
has(role, X), has(case, X),
has(role, Y), has(case, Y).
</equation>
<bodyText confidence="0.967285428571429">
The constraints has(role,X) and has(role,Y)
impose that the constituents e,onst2 and const3 have
a role assigned at some level of the type construction
process. The same situation holds for case. This is a
simple expression, for example, of the case filter in
GB theory. Notice that most pending situations are
satisfied locally, which limits complexity.
</bodyText>
<sectionHeader confidence="0.825603" genericHeader="method">
4. An abstract machine for type
</sectionHeader>
<bodyText confidence="0.973527928571428">
construction in a parsing process
Parsing a sentence is constructing a well-formed
type describing the sentence structure. We present in
this section an abstract machine which describes how
types are constructed. This machine is based on the
procedural semantics of Prolog but it resembles a
push-down tree automaton whose stack is updated
each time a subtype is modified.
There are two kinds of type constructors: those
corresponding to non-terminal structures (such as xp
and xi in our examples) and those corresponding to
terminal structures (e.g. x0). We now present a step
in the construction of a type. It can be decomposed
into 3 levels:
</bodyText>
<listItem confidence="0.9087972">
(1) current state
c0( ai =&amp;gt; 4, a2 =&amp;gt; t2, an =&amp;gt; tn),
(2) selection in the current programme P of a
type construction specification:
c1( bi =&amp;gt; t&amp;apos;1, ...,bm =&amp;gt; enn )
</listItem>
<bodyText confidence="0.8711774">
such that t1 subsumes it or unifies with it
modulo the mgu 01.
(3) New state cri+1 : t1 is replaced by :
el( bi =&amp;gt; t&amp;apos;i, bm =&amp;gt; t&amp;apos;m ),
with, as a result, the following type:
</bodyText>
<equation confidence="0.992579">
c0(ai =&amp;gt; c1( bi =&amp;gt; bin -=&amp;gt; ,
a2 =&amp;gt; t2, ..., an =&amp;gt; tn) 0i
</equation>
<bodyText confidence="0.997808571428571">
The process goes on and processes t&amp;apos;1. The type
construction strategy is here similar to Prolog&amp;apos;s
strategy and computation rule : depth-first and from
left to right. The main difference at this level with
SLD-resolution is that only types corresponding to
non-terminal structures are expanded. Informally,
when a type ti corresponds to a terminal structure, an
attempt is made to find a terminal type description
ej in the programme which is subsumed by or
unifies with t. and, if so, a replacement occurs. C. is
. J
oesaid to be m a final state. If t&amp;apos; â€¢ d s not exist,
backtracking occurs. The next type description
immediately to the right of ei is then treated in the
</bodyText>
<page confidence="0.994214">
4
</page>
<figureCaption confidence="0.423836961538462">
same manner. The type construction process
successfully ends when all subtypes corresponding to
terminal symbols are in a final state and it fails if a
terminal type description tp cannot reach a final
state. The initial state is :
xp( cat =&amp;gt; sentence,
string =&amp;gt; [ string,to,parse] ).
4.2. Extension of the abstract
machine to constraints
The above abstract machine can be extended in a
simple way to deal with constraints. Constraint
resolution mechanisms are similar to usual
constraint logic programming systems like Prolog
III. The three above levels become:
(1) current state ai represented by the couple:
&amp;lt; c0( al =&amp;gt; t1, a2 =&amp;gt; t2, ..., an =&amp;gt; tn), S &amp;gt;
where S is the set of current constraints,
(2) selection in the current programme P of a
type construction specification:
c1( bi =&amp;gt; t&amp;apos;1, bm =&amp;gt; t&amp;apos;m ) :- R.
where R is the set of constraints associated to
cl , and t1 subsumes Or unifies with t&amp;apos;1 modulo the
mgu
(3) New state ai+ I characterized by the
following couple:
&amp;lt; c0( al =&amp;gt; c1( =&amp;gt; bm =&amp;gt; t&amp;apos;m ) ,
</figureCaption>
<equation confidence="0.728165333333333">
a2 =&amp;gt; t2, ..., an =&amp;gt; tn) 0i
S uRu subsume(t1,c1( b1 =&amp;gt; t&amp;apos;1, â€¢ ,
bm =&amp;gt; t&amp;apos;m ) ) &amp;gt;
</equation>
<bodyText confidence="0.970740083333333">
with the condition that the new set of
constraints must be satisfiable with respect to the
constraint resolution axioms defined for each type of
constraint and, if not, a backtracking occurs. At this
level constraints simplifications may also occur.
The output of the parsing process may be
simply a syntactic tree, but it may also be a logical
formula, similar to the one used and presented in
section 5. We however think that both processes,
parsing and generating, need not necessarily
respectively produce And start from the same abstract
internal representation.
</bodyText>
<sectionHeader confidence="0.6873865" genericHeader="method">
5. An Abstract Machine for
Language Generation
</sectionHeader>
<bodyText confidence="0.986886333333333">
From the above declarative descriptions of
language construction, an abstract machine for
language generation can also be defined. At the level
of type construction, generation proceeds by
monotone increasing restrictions: a phrase structure
is described by a type constructor linking a set of
subtypes. This operation introduces a restriction on
the possible left and right contexts that each of the
subtypes could potentially have if they were
independent from each other. The degree of generality
of the selected type constructor linking those
subtypes can be subject to various interpretations.
Finally, generation is guided by the semantic
representation from which a sentence is uttered. As
shall be seen, the semantic representation will
determine the computation rule and the subgoal
selection procedure. It is thus much more
deterministic than its parsing process counterpart.
Let us now briefly consider the abstract machine
for language generation. The general technique, that
we have already exemplified in (Saint-Dizier 89),
</bodyText>
<listItem confidence="0.937534777777778">
consists in:
- (1) writing a formal grammar of the semantic
representation from which the generation process
starts,
- (2) identifying the phrasal units and the lexical
units (and intermediate units if necessary) which can
be associated to the symbols of that formal
grammar,
- (3) associating generation points to these
</listItem>
<figureCaption confidence="0.965665363636364">
symbols (terminal and non-terminal) which will
generate natural language fragments based on a
consultation of the grammatical and the lexical
system (these generation points could be added
automatically).
For example, if the formal grammar of the
semantic representation of quantified noun phrases
is:
Quant_np --&amp;gt; det([Quant, Np,
Rest_of_sentence).
Np --&amp;gt; and( Noun, Modifiers ).
</figureCaption>
<bodyText confidence="0.88204075">
We then have, for example, and informally, the
following generation points, where the call
p(formula, string, corresponding syntactic category)
is used to process the semantic representation:
</bodyText>
<subsectionHeader confidence="0.37623">
p(det([Quant,VarI,Np,Rest_of_sent),
Type ) :-
</subsectionHeader>
<footnote confidence="0.7266005">
p(Quant, Typel), p(Np, Type2),
generation_point(Typel, Type2, Type3),
p(Rest_of_sentence, Type4),
generation_point(Type3, Type4, Type).
</footnote>
<page confidence="0.931304">
5
</page>
<equation confidence="0.283372666666667">
p(and(Np,Mod),Type) :-
p(Np, Typal), p(Mod,Type2),
generation_point(Typet , Type2, Type).
</equation>
<bodyText confidence="0.702496666666667">
The relation between a predicate (or an
argument) and a word is established by a call to a
lexical entry as follows:
</bodyText>
<sectionHeader confidence="0.289849" genericHeader="method">
p(Predicate, Type) :-
</sectionHeader>
<figureCaption confidence="0.876666956521739">
Type, has(Type,
sem_rept =&amp;gt; Predicate ).
Informally, Typel and Type2 are constructed
from the treatment of the quantifier and the noun
phrase, they are then combined, in the first rule
above, by means of the first call to
generation_point, resulting in Type3. This
generation point includes the treament of the string
of words being generated (including the precedence
constraints on the words generated from lexical
insertion) and the treatment of more abstract features
such as category, inflection or semantic
characteristics. Finally, the second call to
generation_point integrates Type3 with Type4, the
latter being the type associated to the remainder of
the sentence. The result is Type.
Generation points support by themselves the
generation strategy. A model of these generation
points is given below by means of an abstract
machine. As can be noticed, calls to generation
points occur after the parse of the corresponding
semantic structure. This means that calls to
generation points will be stacked (by Prolog) and
</figureCaption>
<bodyText confidence="0.900776105263158">
will be then unstacked in the reverse order they have
been stacked: the strategy is then bottom-up.
Generation points determine, by means of a call
to the grammatical system, the resulting syntactic
category and the way the partial strings of words in
Typet , Type2 and Type4 are assembled. The way
types are constructed by generation points is
modelled by the following abstract machine. At this
level, we generalize the generation points to take
into account any number of subtypes, and not only
two as shown in the examples.We claim that this
method is general and can be used from most current
semantic representations (such as, for example, DRT
or Conceptual Graphs).
The abstract machine for language generation
can be described by its initial state and a step in the
construction procedure. It has the general form of a
finite state tree automaton. The initial state is ao, it
is the empty type. Let us now consider a step a1.
</bodyText>
<listItem confidence="0.991943571428571">
1. Two cases arise: it is either
(a) a set of subtypes from which a more
general type can be constructed :
= (a) (C1, C2, ..., Cn) is an unordered
sequence of subtypes ; or
(b) it is a single type : a1 = D1
2. Type constructor selection:
</listItem>
<figureCaption confidence="0.973386583333333">
(a) let DC be such that: DC has exactly k
attributes consti, k n,
and llSC is of the form:
DC := xp(..., consti =&amp;gt; C&amp;apos;1, constk =&amp;gt; C&amp;apos;k )
and :
for all j E [Lk], subsume(C&amp;apos;i, Ci )
(notice that the C are not necessarily the jth
element of the list given in 1 above, notice also that
the type constructor DC contains the subtypes
const together with other information like category
and morphology.)
or (b) D&amp;apos; (single type)
3. ai +1 = (a) ( DC , Ck+i, Cn)
for all j E [1,k1
or (b) (D1, D&amp;apos;).
The type constructor DC contains the subtypes
const together with other information like category
and morphology. It should be noticed that the
constructor DC is selected according to a
subsumption criterion, which is more general and
powerful than standard unification. It better
corresponds to the process of incremental generation
of phrases. The process ends when a type with
category sentence is reached. This is a terminal state
</figureCaption>
<bodyText confidence="0.963072">
in the automaton, more precisely it is the root of the
tree automaton, since our generation system proceeds
in a bottom-up fashion.
Let us now consider the step 2 above devoted to
the selection of a type constructor. This selection is
mainly guided by the generation points given in the
formal grammar of the semantic representation. They
indeed select between cases (a) or (b) and in case (a)
they directly characterize which of the ci will be
included in the type construction at the current stage.
Finally, since active constraints associated to type
descriptions can be executed at any time, the
constraint resolution mechanisms which maintain
constraint coherence are independent of the
generation strategy. In other terms, these
mechanisms are independent of the way and the order
constraints are added to the set of active constraints.
</bodyText>
<page confidence="0.994383">
6
</page>
<bodyText confidence="0.99926625">
The abstract machine which handles types and
constraints is the following. It is represented by a
tuple: &amp;lt;type, set of active constraints&amp;gt;.
We then have:
</bodyText>
<listItem confidence="0.902251266666667">
1. =
(a) &amp;lt; (C1, C2, ..., Ca), S &amp;gt; sequence of
subtypes Ci and of active constraints S
(b) &amp;lt;D1, S&amp;gt;
2. Type constructor selection:
(a) &amp;lt; DC, R&amp;gt; where R is the set of
constraints associated to DC and such that:
i) same restrictions as above on DC and
ii) R is consistent with S
(b) &amp;lt; D&amp;apos; , R &amp;gt; (single type)
with R consistent with S.
3. al =
(a) &amp;lt; ( DC , Ck+i, ..., Ca), (S L.) R L.)
(subsume(C&amp;apos;i =&amp;gt; Ci )) for all j E [1,k] ) &amp;gt;
(1)) &amp;lt; (Di, D&amp;apos;), S u R&amp;gt;
</listItem>
<bodyText confidence="0.986621">
At the end of the generation process, the set of
possible admissible surface sentences can be directly
derived from the precedence constraints which may
not be a total order (some words may have different
positions).
</bodyText>
<sectionHeader confidence="0.813557" genericHeader="method">
6. An Application to Secondary
Predication
</sectionHeader>
<bodyText confidence="0.999850636363636">
We now present a more elaborate and
comprehensive example which will further motivate
our approach. Secondary predication is described at
both lexical and syntaciic levels, the intertwining of
several constraints makes it simpler to describe in a
fully declarative way. The description is thus
independent of its use: parsing or generation. This
gives a good application example of the specification
and use of our formalism and system for a real
phenomenon of much importance to natural
language system designers.
</bodyText>
<subsectionHeader confidence="0.999603">
6.1 A linguistic approach
</subsectionHeader>
<bodyText confidence="0.950512625">
Secondary Predication is a term used in the
literature to denote a very productive structural
relationship in many languages: the relationship
between a subject and a predicate, the subject being
assigned a thematic role by that predicate and by an
obligatory thematic role assigner in the sentence,
namely the verb. For instance, in (1)
(1) Mary drinks the water cold
the water, the direct object of drinks, is assigned a
thematic role by this verb and another one by the
adjective cold. Then, water is, at the same time, an
object for drinks and a subject for cold. In other
terms, water integrates as an object a primary
predication which corresponds to the whole sentence,
and as a subject a secondary predication which
corresponds to the sequence the water cold.
</bodyText>
<sectionHeader confidence="0.557248" genericHeader="method">
6.1.1 Object-oriented Predicates
</sectionHeader>
<bodyText confidence="0.948318923076923">
Secondary predication is not an uniform or an
homogeneous phenomenon, neither from the point
of view of a specific language, nor from a
crosslinguistic one. We will describe here some of
the most relevant structural properties and lexical
constraints of this type of construction in French.
Let us begin by considering the French sentence
corresponding to (1):
(2) Marie boit l&amp;apos;eau froide.
(2) is an ambiguous sentence as can be
illustrated by the paraphrases below (the English
translations of the examples are, all of them, literal
translations):
</bodyText>
<figure confidence="0.61866625">
(2) (a)( Marie boit l&amp;apos;eau qui est froide
(&amp;quot;Mary drinks the water which is cold&amp;quot;)
(b) Quand Marie boit l&amp;apos;eau, l&amp;apos;eau est froide.
(&amp;quot;When Mary drinks the water, the water is cold&amp;quot;)
</figure>
<bodyText confidence="0.965762888888889">
Considering the interpretation in (2)(a), the
adjective is part of the direct object of the verb,
which is not the case for the interpretation in (2)(b).
Then, /&amp;apos;eaufroide can have the structure
(3)(a) [NP[NP l&amp;apos;eau] [AP froide]]
or the structure:
(3)(b) [NP i l&amp;apos;eau][APi froide].
In (3)(a) froide is a modifier of eau, while in
(3)(b) it behaves as a predicate, assigning a
secondary thematic roleto the NP. The predication
relationship is expressed by coindexation.
Let us now consider the sentence (4):
(4) Marie boil l&amp;apos;eau minerale
(&amp;quot;Mary drinks the water mineral&amp;quot;)
In spite of its superficial structural resemblance
with the example above, (4) is not ambiguous, the
interpretation corresponding to the paraphrase (b)
being not available:
</bodyText>
<footnote confidence="0.708330666666667">
(4)(a) Marie boit l&amp;apos;eau qui est minerale
(&amp;quot;Mary drinks the water that is mineral&amp;quot;)
but :
(4)(b) *Quand Marie boit l&amp;apos;eau, l&amp;apos;eau est
minerale
(&amp;quot;When Mary drinks the water, the water is
mineral&amp;quot;)
This means that the possibility of having or not
having an object-oriented secondary predication
</footnote>
<page confidence="0.99739">
7
</page>
<bodyText confidence="0.901315">
depends on the semantic nature of the adjective.
Moreover, there also exist semantic co-occurrence
restrictions between the adjectival predicate and the
verb:
</bodyText>
<listItem confidence="0.8100684">
(5) *Marie boil l&amp;apos;eau congelie
(&amp;quot;Mary drinks the water frozen&amp;quot;)
(5) is excluded because something frozen cannot
be drunk. Notice that the presence of an adjective in
sentences like (2) is optional, in opposition to what
happens in sentences like (6) (for the same
interpretation of the verb):
(6) Marie considere l&amp;apos;eau froide
(&amp;quot;Mary considers the water cold&amp;quot;)
(6)(a) *Marie considÃ©re l&amp;apos;eau
</listItem>
<bodyText confidence="0.987041578947368">
&amp;quot;Mary considers the water&amp;quot;)
What we can infer from the fact that (6)(a) is
ruled out is that: (i) considirer (to consider) does not
subcategorize for an NP, then froide can not be a
modifier of l&amp;apos;eau; (ii) if froide is not a modifier of
l&amp;apos;eau it must be a predicate, but, in this case, we
dont have the structure presented in (3)(b). In fact,
l&amp;apos;eau froide behaves like a clausal phrase. It can even
be replaced by a completive sentence (the semantic
interpretation remaining the same) as exemplified in
(6)(b):
(6)(b) Marie considere que l&amp;apos;eau est froide
(&amp;quot;Mary considers that the water is cold&amp;quot;)
We have then empirical evidence to analyse
l&amp;apos;eau froide in (6) as a clause, a &amp;quot;small clause&amp;quot; using
an usual label in the literature (since the categorial
status of the small clause is irrelevant for our
purposes, we will only use the symbol &amp;quot;Sc,&amp;apos; to refer
to this constituent, assuming the small clauses
analysis proposed by Stowell (1981) and Stowell
(1983)). As a consequence, l&amp;apos;eau froide is a
predication having the structure in (7):
(7) [SC[NPi l&amp;apos;eau] [APi ficâ€¢idel]
In this case it is the whole predication, and not only
its subject, which is theta-marked by the verb.
StrictÂ° sensus, we have not a secondary predication,
nevertheless, the contrastive analysis remains
important since the two kinds of structures are
superficially very similar.
As largely assumed in the GB framework
(Chomsky (1981) and (1986)), predication is
configurationnaly constrained: subject and predicate
must be reciprocally m-commanded, that is, all
maximal projections (phrase levels) dominating one
of them must dominate the other one. Given this
condition and the facts we have examined, (8) and
(9) are appropriate representations (we use here X-bar
notation only when relevant), respectively, for (2)
</bodyText>
<listItem confidence="0.91356725">
and (6):
(8) [S [NP Marie] [V&amp;quot; [VIV boit] [NPi l&amp;apos;eau] ]
[APi froide]]]
(9) [S [NP Marie] [V&amp;quot;[VIV considere] [SC [NPi
</listItem>
<bodyText confidence="0.982877324324324">
l&amp;apos;eau] [APi firoide]]]]]
Although attached to different nodes inside V&amp;quot;
(while in (8) the subject of the secondary
predication occupies the direct object position and
its predicate is in a weak adjunction position (in the
sense of Demonte (1988)), in (9) subject and
predicate are together in direct object position), the
predications we have considered so far involve only
adjacent elements. Let us now examine sentence
(10):
(10) La lessive rend le tinge blanc
(The washing makes the clothes white&amp;quot;)
Similary to what happens in (6), the sentence is
ruled out if the adjectif is not present:
(10)(a) *La lessive rend le tinge
(&amp;quot;The washing makes the linge&amp;quot;)
With respect to these facts it seems to be
natural that sentence (10) is structurally identical to
(9). Nevertheless, (10)(b), which is equivalent to
(10), does not support this hypothesis:
(10)(b) La lessive blanchit le tinge
(&amp;quot;The washing whitens the clothes&amp;quot;)
As we can observe, blanchit, a verbal predicate,
can replace the verb rend and the adjectival
predicate blanc (for the same semantic
interpretation). Then, rend blanc behaves like a
single predicate. At the same time, blanc is a
secondary predicate for linge. Following a proposal
by Marra&amp;amp; (1983) and (1985) for similar cases in
Portuguese, we consider rend blanc as a
discontinuous complex predicate and we express the
relationship between the two elements that
constitue it by co-superscription. Therefore, (10) has
the structure (10)(c),where k indicates the
discontinuity in the predicate rend-blanc
(10)(c) [S [NP La lessive] [V&amp;quot;[V&amp;apos;[Vk rend] [NPi
le linge] [APki blanc]]]]
</bodyText>
<subsectionHeader confidence="0.460693">
6.1.2 Subject-oriented predicates
</subsectionHeader>
<bodyText confidence="0.950756333333333">
Discontinuity can also be an obligatory
property of a certain kind of secondary predication,
namely in the case of subject-oriented predicates.
</bodyText>
<listItem confidence="0.9140405">
(11) is an example:
(11) Jean dansait triste
</listItem>
<page confidence="0.995978">
8
</page>
<bodyText confidence="0.989346">
(&amp;quot;John dansed sad&amp;quot;)
Since Jean is a proper noun, it can not be
modified. Then triste is necessarily a predicate for
John. Although the subject of triste is the main
subject and not an NP in object position as in the
above sentences, there are semantic co-occurrence
constraints between the verb and the adjectival
predicate, as illustrated below:
(11)(a) *Jean dansait repenti
(&amp;quot;John dansed repented&amp;quot;)
Taking into account these constraints and not
violating the m-command condition refered to above,
we represent the adjectival predicate as an strong
adjunction (again in the terms of Demonte (1988)) to
V&amp;quot;, the syntactic representation of (11) being, then,
</bodyText>
<listItem confidence="0.4669765">
(11) (b):
(11)(b) [S[NPi Jean] [V&amp;quot; [V&amp;quot; [V&amp;apos; [V dansait]]]
</listItem>
<bodyText confidence="0.988451411764706">
[APi triste]]]
Notice that continuous and discontinuous
secondary predications can co-occurre in the same
sentence:
(12)Jean1 boit l&amp;apos;eauj froidei tristei
(&amp;quot;John drinks the water cold sad&amp;quot;)
It is also interesting to point out that, in certain
cases, sentences are ;ambiguous with respect to
continuous and discontinuous secondary predication:
(13 )Jeans laisse son amiej tristei j
(&amp;quot;John left his girl friend sad&amp;quot;)
To summarize, secondary predication can be
associated to different types of structure and to
continuous or discontinuous elements. Moreover,
there are numerous and different semantic co-
occurrence restrictions of different types affecting
the lexical items involved.
</bodyText>
<subsectionHeader confidence="0.9871855">
6.2 An Implementation in terms of
types and constraints
</subsectionHeader>
<bodyText confidence="0.9992908">
We now show how the above examples are
expressed both at syntactic and lexical levels. The
full syntactic structures are given under (8), (9),
(10c) and (1 1b). The structure in (8) says that the AP
is a sister of the V&amp;apos; (noted in the grammar as V with
bar level 1) and that the AP is co-indexed with the
object NP, the co-indexation relation is left pending
since it is preceded by the V&amp;apos; description. We have
the following construction:
xp( cat =&amp;gt; v, string =&amp;gt; SV,
</bodyText>
<equation confidence="0.525854333333333">
constl =&amp;gt; xl ( cat =&amp;gt; v, string =&amp;gt; $3),
const2 =&amp;gt; xp( cat =&amp;gt; a, string =&amp;gt; S4,
index =&amp;gt; I ) :-
</equation>
<bodyText confidence="0.977739734693877">
pending(xp(cat =&amp;gt; v),[x1( cat =&amp;gt; v,
string =&amp;gt; T, const1 =&amp;gt; x0( cat =&amp;gt; v,
string =&amp;gt; Si), const2 =&amp;gt; xp( cat =&amp;gt; n,
string =&amp;gt; S2, index =&amp;gt; I ) ) j ),
precede(S1,S2), precede(S3,S4).
Since the AP is not obligatory (it is a weak
adjunct), there is nothing said about it in the
lexicon.
Construction (9) introduces a small clause
(noted here as sc). Since it is not necessarily
contiguous to the V&amp;apos;, but only dominated by the V&amp;apos;,
we need a pending constraint. The type construction
is the following:
x1( cat =&amp;gt; v, string =&amp;gt; SV,
const1 =&amp;gt; x0( cat =&amp;gt; v, string =&amp;gt; Si ),
const2 =&amp;gt; xp( cat =&amp;gt; Sc, string =&amp;gt; S2 ) ) :-
pending(x1 (cat =&amp;gt; v),[x1 ( cat =&amp;gt; Sc,
string =&amp;gt; SV, const1 =&amp;gt; xp( cat =&amp;gt; n,
index =&amp;gt; I, string =&amp;gt; S3), const2
xp( cat =&amp;gt; a, string =&amp;gt; S4,index =&amp;gt; I ) ) ),
precede(S1,S2), precede(S3,S4).
The lexical entry of the verb (here considerer)
has a pending constraint for the small clause: the
verb subcategorizes for a small clause.
Construction (10c) introduces a double
indexation but no long-distance dependency for the
compound predicate &amp;apos;rend-blanc&amp;apos;. We represent it as
follows:
x1( cat =&amp;gt; v, string =&amp;gt; SV,
const1 =&amp;gt; x0( cat =&amp;gt; v,
compound_pred =&amp;gt; K,
string =&amp;gt; Si),
const2 =&amp;gt; xp( cat =&amp;gt; n, index =&amp;gt; I,
string =&amp;gt; S2 ),
const3 =&amp;gt; xp( cat =&amp;gt; a, index =&amp;gt; I,
string =&amp;gt; S3, compound_pred =&amp;gt; K) ) :-
precede(S1,S2), precede(S2,S3).
Finally, the construction given in (11b)
introduces a long-distance relation between an NP in
subject position and an AP which is in object
position. To handle this phenomenon, we have to go
up to the sentence level, that we will represent here
for simplicity as s (instead of, for example, COMP).
The type construction is the following:
xp( cat =&amp;gt; s, string =&amp;gt; SV,
const1 =&amp;gt; xp( cat =&amp;gt; n, string =&amp;gt; Si,
index =&amp;gt; I ),
const2 =&amp;gt; xp( cat =&amp;gt; v, string =&amp;gt; S2 ) ) :-
pending(xp(cat =&amp;gt; s),[xp( cat =&amp;gt; v,
</bodyText>
<page confidence="0.993831">
9
</page>
<bodyText confidence="0.810383777777778">
string =&amp;gt; T,
const1 =&amp;gt; xp( cat =&amp;gt; v,string =&amp;gt; S3),
const2 =&amp;gt; xp( cat =&amp;gt; a,
string =&amp;gt; S4, index =&amp;gt; I ) ) ),
precede(S1,S2), precede(S3,S4).
At the lexical level, the adjoined AP is not
mentioned, since it is not syntactically necessary
(but it might be necessary from a semantic point of
view, as also for case (8) above).
</bodyText>
<sectionHeader confidence="0.503708" genericHeader="method">
7. Specific features of our approach
</sectionHeader>
<bodyText confidence="0.999947255813954">
Our approach can be contrasted mainly with the
usual systems based on unification grammar (UG)
formalisms (Shieber, 86), (Emele and Zajac 90). The
first major difference is that the unification and
rewriting mechanisms usually associated with UG
are replaced by a more constraining operation, type
construction, which always proceeds by sucessive
restrictions (or monotone increasing specialisation)
each time a type is further expanded. From that point
of view, our approach also substantially differs from
(Alt Kaci and Nasr, 86) who propose a powerful and
semantically clear mechanism for typed unification
associated to type inheritance.
Next, we have a single operation: type
construction; we do not have on the one hand
grammar rules and on the other hand, associated to
each rule, a set of equations to deal with feature
values and constraints. The constraints we have
associated with our types are not of the same nature
and cannot be compared to the equations of UGs.
They are moreover a part of the type.
Constraints added to types are interpreted within
the CLP framework, this permits us to have a more
expressive and powerful constraint system, which is
also more efficient and simpler to write. Constraint
satisfaction is not indeed guaranteed at the level they
are given, but throughout the whole type
construction process.
Our approach is compatible with the current
principled-based approaches to describing languages.
This is exemplified in section 4 by the constraints
on role and case assignments. In a more general way,
the description language we have presented here is
particularly appropriate for highly abstract
descriptions of language, which corresponds to
several current trends in computational linguistics.
Our description language is, in the same time, well-
adapted to deal with lexical-based approaches to
language processing (those approaches like lexicon
grammars where the lexicon plays a central role) and
to describe representations developed within lexical
semantics.
Finally, a constraint like pending generalises
the notion of long-distance dependency to several
other kinds of dependencies. This generalization is in
particular a consequence of the fact that type
structures do not have any ordering on subtypes and
they cannot, thus, directly express the difference
between remote and close constituents.
Besides these general properties, our approach
has several interesting properties which are more
specific to reversibility. First, the common data
shared by the two processes is all the linguistic data
which is specified in a declarative way: lexical and
grammatical. The semantic composition rules are the
same. In the case of generation, they are translated
into a parser of the formal grammar of this semantic
representation. It should be pointed out that the
parser given in section 5 can be generated
automatically.
Both processes also have a lot of elements in
common at the procedural level: the type
construction mechanisms are identical, the major
difference at this level being the selection rule,
which is, in the case of generation, guided by the
semantic form from which the process starts. The
other difference is that parsing proceeds a priori top-
down in the case we have exemplified (it could also
proceed bottom-up). Generation proceeds bottom-up,
for reasons explained in section 5. From this
difference it results that the starting type in the case
of parsing is a general type corresponding to
sentence whereas there are no starting type in the
case of generation, the starting points being the
types corresponding to the predicates appearing in
the logical formula, which are deduced from an
operation close to lexical insertion. If the parsing
process were bottom-up, then the starting types
would be the same and the subsumption operation
would also be used instead of the standard
unification.
Finally, and most importantly, the constraint
system that we have presented is fully independent of
the strategies used and of the direction of the process:
generation of parsing. This is a consequence of the
fact that constraints are evaluted only when there is
</bodyText>
<page confidence="0.998214">
10
</page>
<bodyText confidence="0.999825">
sufficient available information to evaluate them and
also that their coherence with the other constraints is
checked throughout the whole proof construction
procedure. The variables which are used by active
constraints are thus global variables.
</bodyText>
<sectionHeader confidence="0.847354" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999912222222222">
In this document, we have first defined a
formalism based on types and active constraints of
the Logic Programming framework and have shown
that it is well-appropriate to describe language
constructions. We have in particular illustrated it by
focussing on second* predication, an important
phenomenon in language processing. Finally, we
have shown that our formalism is particularly
appropriate to be used by a parser and by a generator,
in a symmetric way, and we have defined for that
prupose two abstract machines. This work is now
fully implemented in Sicstus Prolog (which allows
the writing of constraint resolution mechanism) on a
Sun workstation. Since constraints are so far meta-
interpreted, we cannot make real comparisons with
existing NLP systems. A significant result is
however the much smaller number of bacictraking
operations that we have observed.
</bodyText>
<sectionHeader confidence="0.996382" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.897713">
We thank N. Hathout for his insightful
comments on this work, which was supported by the
PRC Communication Homme-Machine and the
French ministry of research.
</bodyText>
<sectionHeader confidence="0.997572" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998741674418605">
Aiit-Kagi, H., Nasr, R., LOGIN: A Logic
Programming Language with Built-in Inheritance,
journal of Logic Programming, vol. 3, pp 185-215,
1986.
Chomsky, N., Lectures on Government and
Binding, Foris, 1981.
Chomsky, N., Barriers, Linguistic Inquiry
monograph no 13, MIT Press, 1986.
Colmerauer, A., An Introduction to Prolog III,
CACM 33-7, 1990.
Demonte, V., Remarks on Secondary Predicates:
C-Command, Extraction and Reanalysis, The
Linguistic Review 6, pp 1-39, 1988.
Emele, M., Zajac, R., Typed Unification
Grammars, in proc. COLING&amp;apos;90, Helsinki, 1990.
Jaffar, J., Lassez, J.L., Constraint Logic
Programming, Proc. 14th ACM Symposium on
Principles of Programming Languages, 1987.
Marrafa, P., Teoria das Pequenas Oragoes vs
Teoria da Predicacao: ControlÂ°, Criterio Tematico e
Principio de Projeccao, ms, FLL-University of
Lisbon, 1983.
Marrafa, P., A Construcao Transitiva-
Predicativa em Portuguas, FLL-University of
Lisbon, 1985.
Saint-Dizier, P., Contextual Discontinuous
Grammars, 2nd NLULP, Vancouver 1987 and in:
Natural Language Understanding and Logic
Programming II, V. Dahl and P. Saint-Dizier Edts,
North Holland, 1988.
Saint-Dizier, P., A generation Strategy based on
GB Principles, proc. 2nd European workshop on
language generation, Edinburgh, 1989.
Saint-Dizier, P., Constrained Logic
Programming for Natural Language Processing,
proc. ACL-89, Manchester, 1989.
Sheiber, S., An Introduction to Unification-
Based Approaches to Grammar, CSLI lecture notes
no 4, Chicago University Press, 1986.
Stowell, T., Origins of Phrase Structure, PhD.
dissertation, MIT, 1981.
Stowell, T., Subject across Categories, The
Linguistic Review 2, pp 285-312, 1983.
</reference>
<page confidence="0.536936">
1].
</page>
</variant>
</algorithm>
</algorithms>