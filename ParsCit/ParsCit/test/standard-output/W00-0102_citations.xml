<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="100401">
<algorithm name="ParsCit" version="100401">
<citationList>
<citation valid="true">
<authors>
<author>R Boyd</author>
<author>J Driscoll</author>
<author>I Syu</author>
</authors>
<title>incorporating Semantics Within a Connectionist Model and a Vector Processing Model</title>
<date>1994</date>
<booktitle>In Proceedings of the TREC-2</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="12609" citStr="Boyd et al. (1994)">ed out in Section 1, all terms are not known in partial text retrieval. Further, our approach is based on semantic weight rather than word frequency. Therefore any frequency based measures defined by Boyd et al. (1994) and Wendlandt (1991) need to be built from the probabilities of individual semantic categories. Those modifications are described below. As a simplifying assumption, we assume SEMCATs have a uniform </context>
<context position="23521" citStr="Boyd et al. (1994)">ar structures function as 12 minor predication and as such are loci of semantic intent or coherence. In order to facilitate the use of long runs as predictors, we modified the traditional measures of Boyd et al. (1994), Wendlandt (1991) to accommodate semantic categories and partial text retrieval. The revised metrics and the computational method we propose were used in the statistical experiments presented above. </context>
</contexts>
<marker>Boyd, Driscoll, Syu, 1994</marker>
<rawString>Boyd R., Driscoll J, and Syu I. (1994) incorporating Semantics Within a Connectionist Model and a Vector Processing Model. In Proceedings of the TREC-2, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Anaysis</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science</journal>
<pages>41--6</pages>
<marker>Deerwester, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester S., Furnas G., Landauer T., and Harshman R. (1990) Indexing by Latent Semantic Anaysis. Journal of the American Society of Information Science 41-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar. Edward</title>
<date>1985</date>
<location>Arnold, London</location>
<contexts>
<context position="6268" citStr="Halliday (1985)">as a semantic predictor. We examined all the long runs of the Jang (1997) collection and discovered most of them originate from the prepositional phrase and subject complement positions. According to Halliday (1985), a preposition is explained as a minor verb. It functions as a minor Predicator having a nominal group as its complement. Thus the internal structure of &apos;across the lake&apos; is like that of &apos;crossing th</context>
<context position="23291" citStr="Halliday (1985)">hort run lengths are drawn from different populations, (2) our observation that these long runs of content words originate from the prepositional phrase and subject complement positions. According to Halliday (1985) those grammar structures function as 12 minor predication and as such are loci of semantic intent or coherence. In order to facilitate the use of long runs as predictors, we modified the traditional </context>
</contexts>
<marker>Halliday, 1985</marker>
<rawString>Halliday M.A.K. (1985) An Introduction to Functional Grammar. Edward Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jang</author>
</authors>
<title>Extracting Context from Unstructured Text Documents by Content Word Density</title>
<date>1997</date>
<tech>M.S. Thesis</tech>
<institution>University of Missouri-Kansas City</institution>
<contexts>
<context position="5299" citStr="Jang (1997)">Runs Partial Information Retrieval has to with detection of main ideas. Main ideas are topic sentences that have central meaning to the text. Our method of detecting main idea paragraphs extends from Jang (1997) who observed that after stemming and stopping a document, long runs of content words cluster. Content word runs are a sequence of content words with a function word(s) prefix and suffix. These runs c</context>
<context position="6126" citStr="Jang (1997)">erify this, we designed a methodology to incorporate semantic features into information retrieval and examined long runs of content words as a semantic predictor. We examined all the long runs of the Jang (1997) collection and discovered most of them originate from the prepositional phrase and subject complement positions. According to Halliday (1985), a preposition is explained as a minor verb. It functions</context>
<context position="7268" citStr="Jang (1997)">tions, it would suggest that the speaker is saying something important and the longer runs of content words would signal a locus of the speaker&apos;s intention. Extending from the statistical analysis of Jang (1997) and our observations of those long runs in the collection, we give a basic assumption of OUT study: Long runs of content words contain significant semantic information that a speaker wants to express</context>
<context position="10592" citStr="Jang (1997)">ogy. 3.1 Revised Probability and Vector Processing In order to understand the calculation of SEMCATs, it is helpful to look at the structure 8 of a preprocessed document. One document &amp;quot;Barbie&amp;quot; in the Jang (1997) collection has a total of 1,468 words comprised of 755 content words and 713 function words. The document has 17 paragraphs. Filtering out function words using the Brown Corpus exposed the runs of co</context>
<context position="17067" citStr="Jang (1997)">raphs with long runs, computing and summing the semantic coherence of the longest runs only, (3) ranking the eligible paragraphs for retrieval based upon their semantic weights relative to the query. Jang (1997) established that the distribution of long runs of content words and short runs of content words in a collection of paragraphs are drawn from different populations. This implies 10 that either long ru</context>
</contexts>
<marker>Jang, 1997</marker>
<rawString>Jang S. (1997) Extracting Context from Unstructured Text Documents by Content Word Density. M.S. Thesis, University of Missouri-Kansas City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moffat</author>
<author>R Davis</author>
<author>R Wilkinson</author>
<author>J Zobel</author>
</authors>
<title>Retrieval of Partial Documents</title>
<date>1994</date>
<booktitle>In Proceedings of TREC-2</booktitle>
<marker>Moffat, Davis, Wilkinson, Zobel, 1994</marker>
<rawString>Moffat A., Davis R., Wilkinson, R., and Zobel J. (1994) Retrieval of Partial Documents. In Proceedings of TREC-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shin</author>
</authors>
<title>Incorporating Semantic Categories (SEMCATs) into a Partial Information Retrieval System</title>
<date>1997</date>
<tech>M.S. Thesis</tech>
<institution>University of Missouri Kansas City</institution>
<contexts>
<context position="18701" citStr="Shin (1997">between the sum of long run SEMCAT weights and the semantic coherence of a paragraph, the total paragraph SEMCAT weight. A detailed description of these experiments and their outcome are described in Shin (1997, 1999). The results of the experiments and the implications of those results relative to the method we propose are discussed below. Table 3 gives the SEMCAT weights for seventeen paragraphs randomly </context>
</contexts>
<marker>Shin, 1997</marker>
<rawString>Shin H. (1997) Incorporating Semantic Categories (SEMCATs) into a Partial Information Retrieval System. M.S. Thesis, University of Missouri Kansas City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shin</author>
<author>J Stach</author>
</authors>
<title>Incorporating Probabilistic Semantic Categories (SEMCATs) Into Vector Space Techniques for Partial Document Retrieval</title>
<date>1999</date>
<marker>Shin, Stach, 1999</marker>
<rawString>Shin H., Stach J. (1999) Incorporating Probabilistic Semantic Categories (SEMCATs) Into Vector Space Techniques for Partial Document Retrieval.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<journal>Journal of Computer Science and Information Management</journal>
<volume>2</volume>
<note>to appear</note>
<contexts>
<context position="18708" citStr="(1997, 1999)">en the sum of long run SEMCAT weights and the semantic coherence of a paragraph, the total paragraph SEMCAT weight. A detailed description of these experiments and their outcome are described in Shin (1997, 1999). The results of the experiments and the implications of those results relative to the method we propose are discussed below. Table 3 gives the SEMCAT weights for seventeen paragraphs randomly chosen </context>
</contexts>
<marker>1999</marker>
<rawString>Journal of Computer Science and Information Management, vol. 2, No. 4, December 1999, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wendlandt</author>
<author>R Driscoll</author>
</authors>
<title>Incorporating a semantic analysis into a document retrieval strategy</title>
<date>1991</date>
<journal>CACM</journal>
<volume>31</volume>
<pages>54--48</pages>
<marker>Wendlandt, Driscoll, 1991</marker>
<rawString>Wendlandt E. and Driscoll R. (1991) Incorporating a semantic analysis into a document retrieval strategy. CACM 31, pp. 54-48.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>