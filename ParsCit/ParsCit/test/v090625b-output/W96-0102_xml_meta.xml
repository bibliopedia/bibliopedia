<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="100401">
<algorithm name="ParsHed" version="100401">
<variant no="0" confidence="0.224230">
<title confidence="0.999434">MBT: A Memory-Based Part of Speech Tagger-Generator</title>
<author confidence="0.967335">Walter Daelemans</author>
<author confidence="0.967335">Jakub Zavrel</author>
<affiliation confidence="0.9948045">Computational Linguistics and AT Tilburg University</affiliation>
<address confidence="0.997723">P.O. Box 90153, NL-5000 LE Tilburg</address>
<email confidence="0.698977">fwalter.daelemans,zavrelleaub.n1</email>
<author confidence="0.992495">Peter Berck</author>
<author confidence="0.992495">Steven Gillis</author>
<affiliation confidence="0.9990845">Center for Dutch Language and Speech University of Antwerp</affiliation>
<address confidence="0.520012">Universiteitsplein 1, B-2610 Wilrijk</address>
<email confidence="0.43325">Ipeter.berck,steven.gillislftia.ua.ac.be</email>
<abstract confidence="0.997397210526316">We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using IGTree, a tree-based formalism for indexing and searching huge case bases. The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="100401">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance-based learning algorithms</title>
<date>1991</date>
<journal>Machine Learning</journal>
<volume>7</volume>
<pages>37--66</pages>
<contexts>
<context position="5522" citStr="Aha et al. 1991">sing terminology such as similarity-based, example-based, memory-based, exemplar- based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen, 1989; Chandler, 1992; Scha, 1992</context>
<context position="6837" citStr="Aha et al., 1991">using equation (2), an overlap metric, for symbolic features (we will have no numeric features in the tagging application). S(xt, yz) = 0 if xi = yi, else 1 (2) We will refer to this approach as IB1 (Aha et al., 1991). We extended the algorithm described there in the following way: in case a pattern is associated with more than one category in the training set (i.e. the pattern is ambiguous), the distribution of p</context>
<context position="26371" citStr="Aha et al., 1991">hree memory-based learning algorithms: (i) IB1, a slight extension (to cope with symbolic values and ambiguous training items) of the well-known k-nn algorithm in statistical pattern recognition (see Aha et al., 1991), (ii) IB1-IG, an extension of IB1 which uses feature relevance weighting (described in Section 2), and (iii) IGTree, a memory- and processing time saving heuristic implementation of IB1-IG (see Secti</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>Aha, D. W., Kibler, D., &amp; Albert, M. (1991). &apos;Instance-based learning algorithms&apos;. Machine Learning, 7, 37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part-of-speech tagger</title>
<date>1992</date>
<booktitle>Proceedings Third ACL Applied</booktitle>
<pages>152--155</pages>
<location>Trento, Italy</location>
<contexts>
<context position="3247" citStr="Brill (1992)">icon. These rules can either be hand-crafted (Garside et al., 1987; Klein &amp; Simmons, 1963; Green 8.6 Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word (or a lexical representation for the word) with preceding and following context, and the corresponding cate</context>
<context position="33044" citStr="Brill (1992)">eusability bottlenecks, and to robustness and coverage problems (similar advantages motivated Markov model-based statistical approaches). Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Brill, E. (1992) &apos;A simple rule-based part-of-speech tagger&apos;. Proceedings Third ACL Applied, Trento, Italy, 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>A case-based approach to knowledge acquisition for domain-specific sentence analysis</title>
<date>1993</date>
<booktitle>In AAAI-93</booktitle>
<pages>798--803</pages>
<contexts>
<context position="29907" citStr="Cardie (1993">g on known and unknown words Accuracy	Percentage Known	96.7	94.5 Unknown	90.6	5.5 Total	96.4	100.0 6 Related Research A case-based approach, similar to our memory-based approach, was also proposed by Cardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging but also semantic tagging and structural disambiguation). We will discuss only the reported POS tagging results here. Using a fa</context>
<context position="30659" citStr="Cardie, 1993"> parsed already), and with a tag set of 18 tags (7 open-class, 11 closed class), she reports a 95% tagging accuracy. A decision-tree learning approach to feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevant features. Results are based on experiments with 120 randomly chosen sentences from the TIPSTER JV corpus (representing 2056 cases). Cardie (p.c.) reports 89.1% correct t</context>
</contexts>
<marker>Cardie, 1993</marker>
<rawString>Cardie, C. (1993a). &apos;A case-based approach to knowledge acquisition for domain-specific sentence analysis&apos;. In AAAI-93, 798-803.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>Using Decision Trees to Improve Case-Based Learning</title>
<date>1993</date>
<booktitle>In Proceedings of the Tenth International Conference on Machine Learning</booktitle>
<pages>25--32</pages>
<contexts>
<context position="29907" citStr="Cardie (1993">g on known and unknown words Accuracy	Percentage Known	96.7	94.5 Unknown	90.6	5.5 Total	96.4	100.0 6 Related Research A case-based approach, similar to our memory-based approach, was also proposed by Cardie (1993a, 1994) for sentence analysis in limited domains (not only POS tagging but also semantic tagging and structural disambiguation). We will discuss only the reported POS tagging results here. Using a fa</context>
<context position="30659" citStr="Cardie, 1993"> parsed already), and with a tag set of 18 tags (7 open-class, 11 closed class), she reports a 95% tagging accuracy. A decision-tree learning approach to feature selection is used in this experiment (Cardie, 1993b, 1994) to discard irrelevant features. Results are based on experiments with 120 randomly chosen sentences from the TIPSTER JV corpus (representing 2056 cases). Cardie (p.c.) reports 89.1% correct t</context>
</contexts>
<marker>Cardie, 1993</marker>
<rawString>Cardie, C. (1993b). &apos;Using Decision Trees to Improve Case-Based Learning&apos;. In Proceedings of the Tenth International Conference on Machine Learning, 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis</title>
<date>1994</date>
<tech>Ph.D. Thesis</tech>
<institution>University of Massachusetts</institution>
<location>Amherst, MA</location>
<contexts>
<context position="5916" citStr="Cardie (1994">, 1992; Scha, 1992). In computational linguistics (apart from incidental computational work of the linguists referred to earlier), the general approach has only recently gained some popularity: e.g., Cardie (1994, syntactic and semantic disambiguation); Daelemans (1995, an overview of work in the early nineties on memory-based computational phonology and morphology); Jones (1996, an overview of example-based </context>
<context position="33260" citStr="Cardie 1994">k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g. Cardie 1994). 24 7 Conclusion We have shown that a memory-based approach to large-scale tagging is feasible both in terms of accuracy (comparable to other statistical approaches), and also in terms of computation</context>
</contexts>
<marker>Cardie, 1994</marker>
<rawString>Cardie, C. (1994). &apos;Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis&apos;. Ph.D. Thesis, University of Massachusetts, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chandler</author>
</authors>
<title>Are rules and modules really necessary for explaining language</title>
<date>1992</date>
<journal>Journal of Psycholinguistic research</journal>
<volume>22</volume>
<pages>593--606</pages>
<contexts>
<context position="5710" citStr="Chandler, 1992">ner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen, 1989; Chandler, 1992; Scha, 1992). In computational linguistics (apart from incidental computational work of the linguists referred to earlier), the general approach has only recently gained some popularity: e.g., Cardie</context>
</contexts>
<marker>Chandler, 1992</marker>
<rawString>Chandler, S. (1992). &apos;Are rules and modules really necessary for explaining language?&apos; Journal of Psycholinguistic research, 22(6): 593-606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text</title>
<date>1988</date>
<booktitle>Proceedings Second ACL Applied NLP</booktitle>
<pages>136--143</pages>
<location>Austin, Texas</location>
<contexts>
<context position="2725" citStr="Church, 1988">un, as in they man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estima</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988). &apos;A stochastic parts program and noun phrase parser for unrestricted text&apos;. Proceedings Second ACL Applied NLP, Austin, Texas, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cost</author>
<author>S Salzberg</author>
</authors>
<title>A weighted nearest neighbour algorithm for learning with symbolic features</title>
<date>1993</date>
<booktitle>Machine Learning</booktitle>
<volume>10</volume>
<pages>57--78</pages>
<marker>Cost, Salzberg, 1993</marker>
<rawString>Cost, S. and Salzberg, S. (1993). &apos;A weighted nearest neighbour algorithm for learning with symbolic features.&apos; Machine Learning, 10, 57-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pederson</author>
<author>P Sibun</author>
</authors>
<title>A practical part of speech tagger</title>
<date>1992</date>
<booktitle>Proceedings Third ACL Applied NLP</booktitle>
<pages>133--140</pages>
<location>Trento, Italy</location>
<contexts>
<context position="2760" citStr="Cutting et al. 1992">). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-b</context>
</contexts>
<marker>Cutting, Kupiec, Pederson, Sibun, 1992</marker>
<rawString>Cutting, D., Kupiec, J., Pederson, J., Sibun, P. (1992). A practical part of speech tagger. Proceedings Third ACL Applied NLP, Trento, Italy, 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
</authors>
<title>Memory-based lexical acquisition and processing</title>
<date>1995</date>
<booktitle>Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898</booktitle>
<pages>85--98</pages>
<editor>In Steffens, P., editor</editor>
<publisher>Springer</publisher>
<location>Berlin</location>
<contexts>
<context position="5973" citStr="Daelemans (1995">rt from incidental computational work of the linguists referred to earlier), the general approach has only recently gained some popularity: e.g., Cardie (1994, syntactic and semantic disambiguation); Daelemans (1995, an overview of work in the early nineties on memory-based computational phonology and morphology); Jones (1996, an overview of example-based machine translation research); Federici and Pirrelli (199</context>
</contexts>
<marker>Daelemans, 1995</marker>
<rawString>Daelemans, W. (1995). &apos;Memory-based lexical acquisition and processing.&apos; In Steffens, P., editor, Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898. Berlin: Springer, 85-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
</authors>
<title>Generalisation performance of backpropagation learning on a syllabification task</title>
<date>1992</date>
<booktitle>In M. Drossaers &amp; A. Nijholt (Eds.), TWLT3: Connectionism and Natural Language Processing</booktitle>
<pages>27--38</pages>
<institution>Twente University</institution>
<location>Enschede</location>
<marker>Daelemans, Van den Bosch, 1992</marker>
<rawString>Daelemans, W., Van den Bosch, A. (1992). &apos;Generalisation performance of backpropagation learning on a syllabification task.&apos; In M. Drossaers &amp; A. Nijholt (Eds.), TWLT3: Connectionism and Natural Language Processing. Enschede: Twente University, 27-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>T Weijters</author>
</authors>
<title>IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms</title>
<date>1996</date>
<booktitle>Al Review Special Issue on Lazy Learning, forthcoming</booktitle>
<editor>In Aha, D. (ed</editor>
<marker>Daelemans, Van den Bosch, Weijters, 1996</marker>
<rawString>Daelemans, W., Van den Bosch, A., Weijters, T. (1996). &apos;IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms.&apos; In Aha, D. (ed.). Al Review Special Issue on Lazy Learning, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<pages>31--39</pages>
<contexts>
<context position="2739" citStr="DeRose, 1988"> man the boats). Several approaches have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tag</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>DeRose, S. (1988). &apos;Grammatical category disambiguation by statistical optimization. Computational Linguistics 14, 31-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B L Derwing</author>
<author>R Skousen</author>
</authors>
<title>Real Time Morphology: Symbolic Rules or Analogical Networks</title>
<date>1989</date>
<journal>Berkeley Linguistic Society</journal>
<volume>15</volume>
<pages>48--62</pages>
<marker>Derwing, Skousen, 1989</marker>
<rawString>Derwing, B. L. and Skousen, R. (1989). &apos;Real Time Morphology: Symbolic Rules or Analogical Networks&apos;. Berkeley Linguistic Society 15: 48-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Federici</author>
<author>V Pirelli</author>
</authors>
<title>Analogy, Computation and Linguistic Theory</title>
<date>1996</date>
<booktitle>New Methods in Language Processing</booktitle>
<pages>forthcoming.</pages>
<editor>In Jones, D. (ed</editor>
<publisher>UCL Press</publisher>
<location>London</location>
<marker>Federici, Pirelli, 1996</marker>
<rawString>Federici S. and V. Pirelli. (1996). &apos;Analogy, Computation and Linguistic Theory.&apos; In Jones, D. (ed.) New Methods in Language Processing. London: UCL Press, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The computational analysis of English: A corpus-based approach</title>
<date>1987</date>
<location>London: Longman</location>
<contexts>
<context position="3101" citStr="Garside et al., 1987">xical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein &amp; Simmons, 1963; Green 8.6 Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept i</context>
</contexts>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, R., Leech, G. and Sampson, G. (1987). The computational analysis of English: A corpus-based approach, London: Longman, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Greene</author>
<author>G M Rubin</author>
</authors>
<title>Automatic Grammatical Tagging of English. Providence RI</title>
<date>1971</date>
<institution>Department of Linguistics, Brown University</institution>
<marker>Greene, Rubin, 1971</marker>
<rawString>Greene, B.B. and Rubin, G.M. (1971). Automatic Grammatical Tagging of English. Providence RI: Department of Linguistics, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Acquiring disambiguation rules from text</title>
<date>1989</date>
<booktitle>In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics</booktitle>
<location>Vancouver, BC</location>
<contexts>
<context position="3181" citStr="Hindle (1989)">roaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein &amp; Simmons, 1963; Green 8.6 Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case consists of a word (or a lexical representation for the word</context>
</contexts>
<marker>Hindle, 1989</marker>
<rawString>Hindle, Donald. (1989). &apos;Acquiring disambiguation rules from text.&apos; In Proceedings, 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hunt</author>
<author>J Mann</author>
<author>P Stone</author>
</authors>
<title>Experiments in Induction</title>
<date>1966</date>
<publisher>Academic Press</publisher>
<location>New York</location>
<contexts>
<context position="7743" citStr="Hunt et al. 1966">ith its information gain; a number expressing the average amount of reduction of training set information entropy when knowing the value of the feature (Daelemans &amp; van de Bosch, 1992, Quinlan, 1993; Hunt et al. 1966) (Equation 3). We will call this algorithm IB-IG. ,A(X,Y) = E G(fi)5(xi,yi) (3) 3 IGTrees Memory-based learning is an expensive algorithm: of each test item, all feature values must be compared to the</context>
</contexts>
<marker>Hunt, Mann, Stone, 1966</marker>
<rawString>Hunt, E., J. Mann, P. Stone. (1966). Experiments in Induction. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jones</author>
</authors>
<title>Analogical Natural Language Processing</title>
<date>1996</date>
<publisher>UCL Press</publisher>
<location>London</location>
<contexts>
<context position="6085" citStr="Jones (1996">gained some popularity: e.g., Cardie (1994, syntactic and semantic disambiguation); Daelemans (1995, an overview of work in the early nineties on memory-based computational phonology and morphology); Jones (1996, an overview of example-based machine translation research); Federici and Pirrelli (1996). 2.1 Similarity Metric Performance of a memory-based system (accuracy on the test set) crucially depends on t</context>
</contexts>
<marker>Jones, 1996</marker>
<rawString>Jones, D. Analogical Natural Language Processing. London: UCL Press, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Klein</author>
<author>R Simmons</author>
</authors>
<title>A grammatical approach to grammatical coding of English words</title>
<date>1963</date>
<journal>JACM</journal>
<volume>10</volume>
<pages>334--347</pages>
<contexts>
<context position="3124" citStr="Klein &amp; Simmons, 1963">robabilities as estimated from a tagged corpus. In rule-based approaches, words are assigned a tag based on a set of rules and a lexicon. These rules can either be hand-crafted (Garside et al., 1987; Klein &amp; Simmons, 1963; Green 8.6 Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992). In a memory-based approach, a set of cases is kept in memory. Each case con</context>
</contexts>
<marker>Klein, Simmons, 1963</marker>
<rawString>Klein S. and Simmons, R. (1963). &apos;A grammatical approach to grammatical coding of English words.&apos; JACM 10, 334-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kolodner</author>
</authors>
<title>Case-Based Reasoning</title>
<date>1993</date>
<publisher>Morgan Kaufmann</publisher>
<location>San Mateo</location>
<contexts>
<context position="5505" citStr="Kolodner, 1993"> to robotics), using terminology such as similarity-based, example-based, memory-based, exemplar- based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen, 1989; Chandler,</context>
</contexts>
<marker>Kolodner, 1993</marker>
<rawString>Kolodner, J. (1993). Case-Based Reasoning. San Mateo: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langley</author>
<author>S Sage</author>
</authors>
<title>Oblivious decision trees and abstract cases</title>
<date>1994</date>
<booktitle>In D. W. Aha (Ed.), Case-Based Reasoning: Papers from the 1994 Workshop</booktitle>
<tech>Technical Report WS-94-01</tech>
<publisher>AAAI Press</publisher>
<location>Menlo Park, CA</location>
<contexts>
<context position="9240" citStr="Langley &amp; Sage, 1994">nce, so the maximal depth of the tree is always equal to the number of features, and at the same level of the tree, all nodes have the same test (they are an instance of oblivious decision trees; cf. Langley &amp; Sage, 1994). The reasoning behind this reorganisation (which is in fact a compression) is that when the computation of feature relevance points to one feature clearly being the most important in classification, </context>
</contexts>
<marker>Langley, Sage, 1994</marker>
<rawString>Langley, P. and Sage, S. (1994). &apos;Oblivious decision trees and abstract cases.&apos; In D. W. Aha (Ed.), Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). Menlo Park, CA: AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<volume>20</volume>
<pages>155--172</pages>
<contexts>
<context position="2776" citStr="Merialdo, 1994"> have been proposed to construct automatic taggers. Most work on statistical methods has used n-gram models or Hidden Markov Model-based taggers (e.g. Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.). In 14 these approaches, a tag sequence is chosen for a sentence that maximizes the product of lexical and contextual probabilities as estimated from a tagged corpus. In rule-based approaches,</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, B. ( 1994). &apos;Tagging English Text with a Probabilistic Model.&apos; Computational Linguistics 20 (2), 155-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>Beyond Word N-grams</title>
<date>1995</date>
<booktitle>Proceedings Third Workshop on Very Large Corpora, MIT</booktitle>
<pages>95--106</pages>
<location>Cambridge Mass</location>
<contexts>
<context position="19258" citStr="Pereira et al., 1995"> window size used by the algorithm will also dynamically change depending on the information present in the context for the disambiguation of a particular focus symbol (see Schiitze et al., 1994, and Pereira et al., 1995 1ACL Data Collection Initiative CD-ROM 1, September 1991. 2We disregarded a category associated with a word when less than 10% of the word tokens were tagged with that category. This way, noise in th</context>
</contexts>
<marker>Pereira, Singer, Tishby, 1995</marker>
<rawString>Pereira, F., Y. Singer, N. Tishby. (1995). &apos;Beyond Word N-grams.&apos; Proceedings Third Workshop on Very Large Corpora, MIT, Cambridge Mass., 95-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning</title>
<date>1993</date>
<publisher>Morgan Kaufmann</publisher>
<location>San Mateo, CA</location>
<contexts>
<context position="7724" citStr="Quinlan, 1993"> each feature with its information gain; a number expressing the average amount of reduction of training set information entropy when knowing the value of the feature (Daelemans &amp; van de Bosch, 1992, Quinlan, 1993; Hunt et al. 1966) (Equation 3). We will call this algorithm IB-IG. ,A(X,Y) = E G(fi)5(xi,yi) (3) 3 IGTrees Memory-based learning is an expensive algorithm: of each test item, all feature values must</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Salzberg</author>
</authors>
<title>A nearest hyperrectangle learning method</title>
<date>1990</date>
<journal>Machine Learning</journal>
<volume>6</volume>
<pages>251--276</pages>
<contexts>
<context position="5539" citStr="Salzberg, 1990">such as similarity-based, example-based, memory-based, exemplar- based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen, 1989; Chandler, 1992; Scha, 1992). In computation</context>
</contexts>
<marker>Salzberg, 1990</marker>
<rawString>Salzberg, S. (1990) &apos;A nearest hyperrectangle learning method&apos;. Machine Learning 6, 251-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Morphological Tagging Based Entirely on Bayesian Inference</title>
<date>1994</date>
<booktitle>In Proceedings of the 9th Nordic Conference on Computational Linguistics</booktitle>
<location>Stockholm University, Sweden</location>
<contexts>
<context position="20439" citStr="Samuelsson, 1994"> by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions). In most taggers, some form of morphological analysis is performed on unknown words, in an attempt to relate the unknown word to a known combination of known morphemes, thereby</context>
</contexts>
<marker>Samuelsson, 1994</marker>
<rawString>Samuelsson, C. (1994) &apos;Morphological Tagging Based Entirely on Bayesian Inference.&apos; In Proceedings of the 9th Nordic Conference on Computational Linguistics, Stockholm University, Sweden, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
</authors>
<title>Virtuele Grammatica&apos;s en Creatieve Algoritmen</title>
<date>1992</date>
<journal>Gramma/TTT</journal>
<volume>1</volume>
<pages>57--77</pages>
<contexts>
<context position="5723" citStr="Scha, 1992">t al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen, 1989; Chandler, 1992; Scha, 1992). In computational linguistics (apart from incidental computational work of the linguists referred to earlier), the general approach has only recently gained some popularity: e.g., Cardie (1994, synta</context>
</contexts>
<marker>Scha, 1992</marker>
<rawString>Scha, R. (1992) &apos;Virtuele Grammatica&apos;s en Creatieve Algoritmen.&apos; Gramma/TTT 1 (1), 57-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Part-of-speech tagging with neural networks</title>
<date>1994</date>
<booktitle>In Proceedings of COLING, Kyoto</booktitle>
<location>Japan</location>
<contexts>
<context position="20421" citStr="Schmid, 1994">based approach by combining these two sources of information in the case representation, and having the information gain feature relevance weighting technique figure out their relative relevance (see Schmid, 1994; Samuelsson, 1994 for similar solutions). In most taggers, some form of morphological analysis is performed on unknown words, in an attempt to relate the unknown word to a known combination of known </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. (1994) &apos;Part-of-speech tagging with neural networks.&apos; In Proceedings of COLING, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
<author>Y Singer</author>
</authors>
<title>Part-of-speech Tagging Using a Variable Context Markov Model</title>
<date>1994</date>
<booktitle>Proceedings of ACL</booktitle>
<location>Las Cruces, New Mexico</location>
<marker>Schiitze, Singer, 1994</marker>
<rawString>Schiitze, H., and Y. Singer. (1994) &apos;Part-of-speech Tagging Using a Variable Context Markov Model&apos; Proceedings of ACL 1994, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Skousen</author>
</authors>
<title>Analogical Modeling of Language</title>
<date>1989</date>
<publisher>Kluwer</publisher>
<location>Dordrecht</location>
<contexts>
<context position="5671" citStr="Skousen, 1989">based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen, 1989; Chandler, 1992; Scha, 1992). In computational linguistics (apart from incidental computational work of the linguists referred to earlier), the general approach has only recent</context>
</contexts>
<marker>Skousen, 1989</marker>
<rawString>Skousen, R. (1989). Analogical Modeling of Language. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Sejnowski</author>
<author>C S Rosenberg</author>
</authors>
<title>Parallel networks that learn to pronounce English text</title>
<date>1987</date>
<journal>Complex Systems</journal>
<volume>1</volume>
<pages>145--168</pages>
<contexts>
<context position="17226" citStr="Sejnowski &amp; Rosenberg, 1987">y can be found there, a case representation is constructed for them, and they are retrieved from either the known words case base or the unknown words case base. 4.2 Known Words A windowing approach (Sejnowski &amp; Rosenberg, 1987) was used to represent the tagging task as a classification problem. A case consists of information about a focus word to be tagged, its left and right context, and an associated category (tag) valid </context>
</contexts>
<marker>Sejnowski, Rosenberg, 1987</marker>
<rawString>Sejnowski, T. J., Rosenberg, C. S. (1987). Parallel networks that learn to pronounce English text. Complex Systems, 1, 145-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>D Waltz</author>
</authors>
<title>Toward memory-based reasoning</title>
<date>1986</date>
<journal>Communications of the ACM</journal>
<volume>29</volume>
<pages>1212--1228</pages>
<contexts>
<context position="5489" citStr="Stanfill and Waltz, 1986">ines (from computer vision to robotics), using terminology such as similarity-based, example-based, memory-based, exemplar- based, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990). Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing Skousen,</context>
<context position="36170" citStr="Stanfill &amp; Waltz, 1986"> rb-in and vb-nn. Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g. Stanfill &amp; Waltz, 1986, Cost &amp; Salzberg, 1994). Acknowledgements Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar. Thanks to Antal van </context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>Stanfill, C. and Waltz, D. (1986). &apos;Toward memory-based reasoning.&apos; Communications of the ACM, 29, 1212-1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weiss</author>
<author>C Kulikowski</author>
</authors>
<title>Computer systems that learn. San-Mateo</title>
<date>1991</date>
<publisher>Morgan Kaufmann</publisher>
<contexts>
<context position="25235" citStr="Weiss &amp; Kulikowski, 1991">ts we studied the performance of our system on predicting the category of both known and unknown words. Experimental Set-up The experimental methodology was taken from Machine Learning practice (e.g. Weiss &amp; Kulikowski, 1991): independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) wa</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>Weiss, S. and Kulikowski, C. (1991). Computer systems that learn. San-Mateo: Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>