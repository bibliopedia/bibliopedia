<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="100401">
<algorithm name="ParsCit" version="100401">
<citationList>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process</title>
<date>1972</date>
<journal>Inequalities</journal>
<volume>3</volume>
<contexts>
<context position="4285" citStr="Baum, 1972">s for such commands rather than struggle with mouse menus or dialog boxes during class. 3 The Subject Matter Among topics in natural language processing, the forward-backward or Baum-Welch algorithm (Baum, 1972) is particularly difficult to teach. The algorithm estimates the parameters of a Hidden Markov Model (HMM) by Expectation- Maximization (EM), using dynamic programming to carry out the expectation ste</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. E. Baum. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT: A statistical part-of-speech tagger</title>
<date>2000</date>
<booktitle>In Proc. ofANLP</booktitle>
<location>Seattle</location>
<contexts>
<context position="29440" citStr="Brants, 2000">s and verbs after reestimation. They will evolve to make whatever distinctions help to predict the word sequence. 39Advanced students might also want to read about a modern supervised trigram tagger (Brants, 2000), or the mixed results when one actually trains trigram taggers by EM (Merialdo, 1994). Furthermore, students could check their ice cream output against the spreadsheet, and track down basic bugs by c</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-of-speech tagger. In Proc. ofANLP, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text</title>
<date>1988</date>
<booktitle>Proc. ofANLP</booktitle>
<contexts>
<context position="4624" citStr="Church, 1988">mization (EM), using dynamic programming to carry out the expectation steps efficiently. HMMs have long been central in speech recognition (Rabiner, 1989). Their application to partof-speech tagging (Church, 1988; DeRose, 1988) kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction. The</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proc. ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<contexts>
<context position="4639" citStr="DeRose, 1988"> using dynamic programming to carry out the expectation steps efficiently. HMMs have long been central in speech recognition (Rabiner, 1989). Their application to partof-speech tagging (Church, 1988; DeRose, 1988) kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, word-sense disambiguation, and information extraction. The algorithm is a</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>Steven J. DeRose. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14(1):31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers</title>
<date>2002</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="5367" citStr="Eisner, 2002">ected Bayesian networks and junction trees (Pearl, 1988; Lauritzen, 1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finite- state parameter estimation (Eisner, 2002). Before studying the algorithm, students should first have worked with some if not all of the key ideas in simpler settings. Markov models can be introduced through n-gram models or probabilistic fin</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen L Lauritzen</author>
</authors>
<title>The EM algorithm for graphical association models with missing data. Computational Statistics and Data Analysis</title>
<date>1995</date>
<contexts>
<context position="5227" citStr="Lauritzen, 1995">case of (1) the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected Bayesian networks and junction trees (Pearl, 1988; Lauritzen, 1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finite- state parameter estimation (Eisner, 2002). Before studying the algorithm, students should first have </context>
</contexts>
<marker>Lauritzen, 1995</marker>
<rawString>Steffen L. Lauritzen. 1995. The EM algorithm for graphical association models with missing data. Computational Statistics and Data Analysis, 19:191–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model</title>
<date>1994</date>
<journal>Comp. Ling</journal>
<volume>20</volume>
<contexts>
<context position="29526" citStr="Merialdo, 1994">to predict the word sequence. 39Advanced students might also want to read about a modern supervised trigram tagger (Brants, 2000), or the mixed results when one actually trains trigram taggers by EM (Merialdo, 1994). Furthermore, students could check their ice cream output against the spreadsheet, and track down basic bugs by comparing their intermediate results to the spreadsheet’s. They reported this to be ver</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Comp. Ling., 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
<date>1988</date>
<publisher>Morgan Kaufmann</publisher>
<location>San Mateo, California</location>
<contexts>
<context position="5209" citStr="Pearl, 1988">tive special case of (1) the inside-outside algorithm for estimation of probabilistic context-free grammars; (2) belief propagation for training singly-connected Bayesian networks and junction trees (Pearl, 1988; Lauritzen, 1995); (3) algorithms for learning alignment models such as weighted edit distance; (4) general finite- state parameter estimation (Eisner, 2002). Before studying the algorithm, students </context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition</title>
<date>1989</date>
<booktitle>Proceedings ofIEEE</booktitle>
<volume>77</volume>
<pages>285</pages>
<contexts>
<context position="4566" citStr="Rabiner, 1989">rameters of a Hidden Markov Model (HMM) by Expectation- Maximization (EM), using dynamic programming to carry out the expectation steps efficiently. HMMs have long been central in speech recognition (Rabiner, 1989). Their application to partof-speech tagging (Church, 1988; DeRose, 1988) kicked off the era of statistical NLP, and they have found additional NLP applications to phrase chunking, text segmentation, </context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings ofIEEE, 77(2):257 –285, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>