# Para 0 2
Reversibility in a Constraint and Type based Logic Grammar:
Application to Secondary Predication
# Para 2 1
Palmira Marrafa	Patrick Saint-Dizier
# Para 3 1
FLL - Universidade de Lisboa / 1LTEC	IRIT Universite Paul Sabatier
# Para 4 1
Ava Engo Arantes e Oliveira, 40 (Lte 41) , 8 E â€¢ 118, route de Narbonne
# Para 5 1
P- 1900 LISBOA	F-31062 TOULOUSE cedex France
# Para 6 1
Portugal	e-mail: stdizier@iritirit.fr
# Para 7 1
Abstract
# Para 8 13
In this document, we present a formalism for 
natural language processing which associates type 
construction principles to constraint logic 
programming. We show that it provides more 
uniform, expressive and efficient tools for parsing 
and generating language. Next, we present two 
abstract machines which enable us to design, in a 
symmetric way, a parser and a generator from that 
formalism. This abstract machinery is then 
exemplified by a detailed study of secondary 
predication within the framework of a principled- 
based description of language: Government and 
Binding theory.
# Para 21 1
Introduction
# Para 22 21
Lexical as well as grammatical and discursive 
knowledge required to understand or to produce 
natural language utterances is usually a description 
which is independent of the sentence production or 
comprehension &apos;algorithms&apos;. It comes naturally into 
mind to have a common, shared knowledge base of 
what language is, independently of its potential uses. 
Besides well-known advantages of uniformity and 
transparency, this point of view is very convenient 
for the computer scientist who does not have to 
integrate into his parsers or generators the 
unavoidable updatings required by grammar 
development. The linguistic knowledge is thus 
specified in a declarative way in different modules 
(lexical, grammatical, discursive, ...) and different 
strategies are applied which refer to these data 
(directly for interpreters or via the production of a 
compiled code for compilers). This approach can 
however be realized more or less easily depending on 
the formalisms used to describe language 
phenomena.
# Para 43 1
7 -
# Para 44 10
In this document we introduce new advanced 
tools of the Logic Programming framework and 
show that they contribute to meeting the 
requirements imposed by the manipulation and the 
control of large amounts of data required by both the 
parsing and the generation procedure. We first 
consider logical types which are a declarative and 
easy-to-use tool and formalism which permit a 
grammar writer to encode knowledge in a very 
flexible and principled-based way.
# Para 54 9
In addition to types, we introduce new active 
constraints of the Constraint Logic Programming 
(CLP) framework which allow us to treat and to 
check for consistency of constraints throughout the 
whole generation procedure and not to only evaluate 
them when they are given in the programme or 
grammar. These active constraints are fully 
declarative and can be used by any type of 
parsing/generation process.
# Para 63 9
CLP introduces a greater expressive power 
together with a higher efficiency since the resolution 
of constraints is postponed till they can be properly 
evaluated and since constraints have to be always 
true and consistent with each other. Finally, a feature 
of active constraints is that they are usually 
independent of the way they are processed, they are 
thus strategy independent and can equivalently be 
used for parsing and for generation.
# Para 72 8
To make reversibility concrete in our system, 
we develop in this document two different abstract 
machines based on type construction and constraint 
satisfaction which give the foundations of a parser 
and a generator using the same source of declarative 
linguistic knowledge. The differences between these 
machines exemplifies the &apos;technical&apos; differences one 
may have between parsing and generation processes.
# Para 80 1
2
# Para 81 1
1. A type based description language
# Para 82 3
Three major types of operations are at the basis 
of the typed-based language we have designed for 
language processing, namely:
# Para 85 2
- the expression of type construction to generate 
phrase structures,
# Para 87 2
- the expression of dependencies (either local or 
long-distance) between types,
# Para 89 2
- the expression of well-formedness constraints 
on types.
# Para 91 6
Types here refers to the usual data structures in 
computer science. We now informally present the 
syntax of our type-based language. It is directly 
derived from the syntax of Login (Ait-ICagi and Nasr 
86). The syntactic representation of a structured term 
is called a Nr-term. It consists of:
# Para 97 2
(1) a root symbol, which is a type constructor 
and denotes a class of entities,
# Para 99 4
(2) attribute labels, which are record field 
symbols. Each attribute denotes a function in 
extenso, from the root to the attribute value. The 
attribute value can itself be a reference to a type.
# Para 103 4
(3) coreference constraints among paths of 
labels, which indicate that the corresponding 
attributes denote the same function. They are 
indicated by variables. Here is an example:
# Para 107 1
person( id =&gt; name(first =&gt; string,
# Para 108 3
last =&gt; X: string), 
born =&gt; date(day =&gt; integer, 
month =&gt; monthname,
# Para 111 2
year =&gt; integer), 
father =&gt; person( id =&gt;
# Para 113 1
name(last =&gt; X ))).
# Para 114 9
The root symbol is person; id, born and 
father are three sub-w-terms which have either 
constants or types as values. X indicates a 
coreference. All different type structures are tagged 
by different symbols. Notice also that in the latter 
field only relevant information about person is 
mentioned. Infinite structures can also be specified 
by coreference links. Variables are in capital letters, 
constants in small letters.
# Para 123 1
2. Dealing with constraints
# Para 124 5
We have extended the type description 
framework with active constraints and have given 
them a Constraint Logic Programming (Colmerauer 
90, Jaffar and Lassez 87) interpretation, permitting 
us to have a more principled-based description of 
# Para 129 2
language. The general form of a type is :
Type :- Constraints.
# Para 131 2
We view constraints as part of the type:
(Type :- Constraints)
# Para 133 1
is itself a type, subsumed by Type.
# Para 134 2
The simplest constraint is the precedence 
constraint:
# Para 136 1
precede(X,Y),
# Para 137 13
where X and Y are of type string. This 
constraint imposes that the string X precedes of the 
string Y. When processing a sentence, precedence 
constraints on constituents are stated in the grammar 
rules and possibly at the lexical level. At each stage 
i of the processing, there is a partial order P1(i) on 
the words and structures already processed. At the end 
of the process, precedence constraints give all the 
possible word orderings which meet the constraints 
of the grammar. In the case of parsing, constraints 
imposed by the order of words in the input string 
must be coherent with the precedence results of the 
parse.
# Para 150 2
The next constraint imposes the presence of an 
attribute in a type:
# Para 152 1
has(Attribute, Type)
# Para 153 12
where Attribute is either an attribute label or a 
full pair attribute-value and Type is a reference to a 
given type. This constraint imposes that at some 
stage there is an attribute in Type which is 
subsumed by or equal to Attribute. Informally, (1) 
when incoherence with Attribute is detected or (2) 
when Type is fully constructed, the non-satisfaction 
of has(Attribute,Type) will provoque 
backtracking. This constraint permits us to encode 
thematic role assignment and focus management, and 
also to encode the inclusion of a set of values into 
another.
# Para 165 14
The last class of constraint is mainly related to 
the expression of long-distance relations between 
sentence constituents. Within the framework of 
types, the notion of long-distance is somewhat 
obsolete since there is no ordering relation on 
subtypes in a type (attributes may be written in any 
order). Thus, the notion of long-distance dependency 
will be here formulated as a sub-type co-occurence 
constraint. This constraint emerged from Dislog 
(Saint-Dizier 87, 89). Very briefly, the co-occurence 
of two or more subtypes in a larger type is expressed 
by the constraint: pending(A,B) where A is a 
type specification and B is a list of type 
specifications. Informally, this constraint means that
# Para 179 1
3
# Para 180 5
A originates the pending of the sub-types in B, in 
other terms that A can be used if, somewhere else in 
the main type (corresponding for example to a full 
sentence), all the sub-types in B are used with 
identical substitutions applied to identical variables.
# Para 185 2
3. Processing Language with types 
and constraints
# Para 187 9
We will mainly present here simple, 
motivational examples. A more abstract syntactic 
description will be given in section 6 which will 
more fully motivate our approach. The examples 
given in this text show that our description language 
can accomodate principled-based descriptions of 
language like Government and Binding theory as 
well as lexically and head driven descriptions like in 
the HPSG framework.
# Para 196 2
In the following simple examples, we only 
have two main type constructors:
# Para 198 1
- x0 corresponding to lexical entries,
# Para 199 1
- xp corresponding to phrase structures.
# Para 200 2
Here is the description of the lexical entry 
corresponding to the verb to give:
# Para 202 2
x0( cat =&gt; v, string =&gt; [give] ) :- 
pending(x0(cat =&gt; v),
# Para 204 2
[xp( cat =&gt; n, string =&gt; Si, role =&gt; patient ), 
xp( cat =&gt; p, string =&gt; S2,
# Para 206 1
role =&gt; recipient) ] ),
# Para 207 1
precede([give],S1), precede(S1, S2).
# Para 208 14
This entry says that give is a verb which 
subcategorizes for an np with role patient and a pp 
with role recipient; np and pp are left pending. The 
string Si generated from the np has to precede the 
string S2 generated from the pp. These constraints 
will be treated at the level of the type describing the 
structure of a vp. The whole description x0 
construction and related constraints is the type of 
the verb to give. Let us now consider the 
construction of a vp with an np and a pp 
complements. To the construction of a vp type 
corresponds the generation of a (set of) string(s) 
corresponding to a vp, this is stored in S. We then 
have the following construction:
# Para 222 1
xp( cat .&gt; v, string =&gt; S,
# Para 223 1
const1 =&gt; x0(cat =&gt; v ),
# Para 224 1
const2 =&gt; X : xp(cat =&gt; n),
# Para 225 2
const3 =&gt; Y : xp( cat =&gt; p) ) :-
has(role, X), has(case, X),
# Para 227 1
has(role, Y), has(case, Y).
# Para 228 7
The constraints has(role,X) and has(role,Y) 
impose that the constituents e,onst2 and const3 have 
a role assigned at some level of the type construction 
process. The same situation holds for case. This is a 
simple expression, for example, of the case filter in 
GB theory. Notice that most pending situations are 
satisfied locally, which limits complexity.
# Para 235 2
4. An abstract machine for type 
construction in a parsing process
# Para 237 7
Parsing a sentence is constructing a well-formed 
type describing the sentence structure. We present in 
this section an abstract machine which describes how 
types are constructed. This machine is based on the 
procedural semantics of Prolog but it resembles a 
push-down tree automaton whose stack is updated 
each time a subtype is modified.
# Para 244 6
There are two kinds of type constructors: those 
corresponding to non-terminal structures (such as xp 
and xi in our examples) and those corresponding to 
terminal structures (e.g. x0). We now present a step 
in the construction of a type. It can be decomposed 
into 3 levels:
# Para 250 1
(1) current state
# Para 251 1
c0( ai =&gt; 4, a2 =&gt; t2,	an =&gt; tn),
# Para 252 2
(2) selection in the current programme P of a 
type construction specification:
# Para 254 1
c1( bi =&gt; t&apos;1, ...,bm =&gt; enn )
# Para 255 2
such that t1 subsumes it or unifies with it 
modulo the mgu 01.
# Para 257 1
(3) New state cri+1 : t1 is replaced by :
# Para 258 1
el( bi =&gt; t&apos;i,	bm =&gt; t&apos;m ),
# Para 259 1
with, as a result, the following type:
# Para 260 1
c0(ai =&gt; c1( bi =&gt;	bin -=&gt;	,
# Para 261 1
a2 =&gt; t2, ..., an =&gt; tn) 0i
# Para 262 9
The process goes on and processes t&apos;1. The type 
construction strategy is here similar to Prolog&apos;s 
strategy and computation rule : depth-first and from 
left to right. The main difference at this level with 
SLD-resolution is that only types corresponding to 
non-terminal structures are expanded. Informally, 
when a type ti corresponds to a terminal structure, an 
attempt is made to find a terminal type description 
ej in the programme which is subsumed by or
# Para 271 2
unifies with t. and, if so, a replacement occurs. C. is 
. J
# Para 273 1
oesaid to be m a final state. If t&apos; â€¢ d s not exist,
# Para 274 2
backtracking occurs. The next type description 
immediately to the right of ei is then treated in the
# Para 276 1
4
# Para 277 5
same manner. The type construction process 
successfully ends when all subtypes corresponding to 
terminal symbols are in a final state and it fails if a 
terminal type description tp cannot reach a final 
state. The initial state is :
# Para 282 1
xp( cat =&gt; sentence,
# Para 283 1
string =&gt; [ string,to,parse] ).
# Para 284 2
4.2. Extension of the abstract 
machine to constraints
# Para 286 5
The above abstract machine can be extended in a 
simple way to deal with constraints. Constraint 
resolution mechanisms are similar to usual 
constraint logic programming systems like Prolog 
III. The three above levels become:
# Para 291 3
(1) current state ai represented by the couple: 
&lt; c0( al =&gt; t1, a2 =&gt; t2, ..., an =&gt; tn), S &gt; 
where S is the set of current constraints,
# Para 294 2
(2) selection in the current programme P of a 
type construction specification:
# Para 296 1
c1( bi =&gt; t&apos;1,	bm =&gt; t&apos;m ) :- R.
# Para 297 3
where R is the set of constraints associated to 
cl , and t1 subsumes Or unifies with t&apos;1 modulo the 
mgu
# Para 300 2
(3) New state ai+ I characterized by the 
following couple:
# Para 302 1
&lt; c0( al =&gt; c1( =&gt;	bm =&gt; t&apos;m ) ,
# Para 303 1
a2 =&gt; t2, ..., an =&gt; tn) 0i
# Para 304 2
S uRu subsume(t1,c1( b1 =&gt; t&apos;1, â€¢ , 
bm =&gt; t&apos;m ) ) &gt;
# Para 306 5
with the condition that the new set of 
constraints must be satisfiable with respect to the 
constraint resolution axioms defined for each type of 
constraint and, if not, a backtracking occurs. At this 
level constraints simplifications may also occur.
# Para 311 7
The output of the parsing process may be 
simply a syntactic tree, but it may also be a logical 
formula, similar to the one used and presented in 
section 5. We however think that both processes, 
parsing and generating, need not necessarily 
respectively produce And start from the same abstract 
internal representation.
# Para 318 2
5. An Abstract Machine for 
Language Generation
# Para 320 2
From the above declarative descriptions of 
language construction, an abstract machine for 
# Para 322 16
language generation can also be defined. At the level 
of type construction, generation proceeds by 
monotone increasing restrictions: a phrase structure 
is described by a type constructor linking a set of 
subtypes. This operation introduces a restriction on 
the possible left and right contexts that each of the 
subtypes could potentially have if they were 
independent from each other. The degree of generality 
of the selected type constructor linking those 
subtypes can be subject to various interpretations. 
Finally, generation is guided by the semantic 
representation from which a sentence is uttered. As 
shall be seen, the semantic representation will 
determine the computation rule and the subgoal 
selection procedure. It is thus much more 
deterministic than its parsing process counterpart.
# Para 338 4
Let us now briefly consider the abstract machine 
for language generation. The general technique, that 
we have already exemplified in (Saint-Dizier 89), 
consists in:
# Para 342 3
- (1) writing a formal grammar of the semantic 
representation from which the generation process 
starts,
# Para 345 4
- (2) identifying the phrasal units and the lexical 
units (and intermediate units if necessary) which can 
be associated to the symbols of that formal 
grammar,
# Para 349 6
- (3) associating generation points to these 
symbols (terminal and non-terminal) which will 
generate natural language fragments based on a 
consultation of the grammatical and the lexical 
system (these generation points could be added 
automatically).
# Para 355 3
For example, if the formal grammar of the 
semantic representation of quantified noun phrases 
is:
# Para 358 1
Quant_np --&gt; det([Quant, Np,
# Para 359 1
Rest_of_sentence).
# Para 360 1
Np --&gt; and( Noun, Modifiers ).
# Para 361 4
We then have, for example, and informally, the 
following generation points, where the call 
p(formula, string, corresponding syntactic category) 
is used to process the semantic representation:
# Para 365 2
p(det([Quant,VarI,Np,Rest_of_sent), 
Type ) :-
# Para 367 1
p(Quant, Typel),	p(Np, Type2),
# Para 368 3
generation_point(Typel, Type2, Type3), 
p(Rest_of_sentence, Type4), 
generation_point(Type3, Type4, Type).
# Para 371 1
5
# Para 372 1
p(and(Np,Mod),Type) :-
# Para 373 2
p(Np, Typal), p(Mod,Type2), 
generation_point(Typet , Type2, Type).
# Para 375 3
The relation between a predicate (or an 
argument) and a word is established by a call to a 
lexical entry as follows:
# Para 378 1
p(Predicate, Type) :-
# Para 379 1
Type, has(Type,
# Para 380 1
sem_rept =&gt; Predicate ).
# Para 381 14
Informally, Typel and Type2 are constructed 
from the treatment of the quantifier and the noun 
phrase, they are then combined, in the first rule 
above, by means of the first call to 
generation_point, resulting in Type3. This 
generation point includes the treament of the string 
of words being generated (including the precedence 
constraints on the words generated from lexical 
insertion) and the treatment of more abstract features 
such as category, inflection or semantic 
characteristics. Finally, the second call to 
generation_point integrates Type3 with Type4, the 
latter being the type associated to the remainder of 
the sentence. The result is Type.
# Para 395 9
Generation points support by themselves the 
generation strategy. A model of these generation 
points is given below by means of an abstract 
machine. As can be noticed, calls to generation 
points occur after the parse of the corresponding 
semantic structure. This means that calls to 
generation points will be stacked (by Prolog) and 
will be then unstacked in the reverse order they have 
been stacked: the strategy is then bottom-up.
# Para 404 12
Generation points determine, by means of a call 
to the grammatical system, the resulting syntactic 
category and the way the partial strings of words in 
Typet , Type2 and Type4 are assembled. The way 
types are constructed by generation points is 
modelled by the following abstract machine. At this 
level, we generalize the generation points to take 
into account any number of subtypes, and not only 
two as shown in the examples.We claim that this 
method is general and can be used from most current 
semantic representations (such as, for example, DRT 
or Conceptual Graphs).
# Para 416 4
The abstract machine for language generation 
can be described by its initial state and a step in the 
construction procedure. It has the general form of a 
finite state tree automaton. The initial state is ao, it 
# Para 420 1
is the empty type. Let us now consider a step a1.
# Para 421 1
1. Two cases arise: it is either
# Para 422 2
(a) a set of subtypes from which a more 
general type can be constructed :
# Para 424 2
= (a) (C1, C2, ..., Cn) is an unordered 
sequence of subtypes ;	or
# Para 426 1
(b) it is a single type : a1 = D1
# Para 427 1
2. Type constructor selection:
# Para 428 2
(a) let DC be such that: DC has exactly k 
attributes consti, k n,
# Para 430 1
and llSC is of the form:
# Para 431 1
DC := xp(..., consti =&gt; C&apos;1,	constk =&gt; C&apos;k )
# Para 432 1
and :
# Para 433 1
for all j E [Lk], subsume(C&apos;i, Ci )
# Para 434 5
(notice that the C are not necessarily the jth 
element of the list given in 1 above, notice also that 
the type constructor DC contains the subtypes 
const together with other information like category 
and morphology.)
# Para 439 1
or (b) D&apos; (single type)
# Para 440 1
3. ai +1 = (a) ( DC , Ck+i,	Cn)
# Para 441 1
for all j E [1,k1
# Para 442 1
or (b) (D1, D&apos;).
# Para 443 12
The type constructor DC contains the subtypes 
const together with other information like category 
and morphology. It should be noticed that the 
constructor DC is selected according to a 
subsumption criterion, which is more general and 
powerful than standard unification. It better 
corresponds to the process of incremental generation 
of phrases. The process ends when a type with 
category sentence is reached. This is a terminal state 
in the automaton, more precisely it is the root of the 
tree automaton, since our generation system proceeds 
in a bottom-up fashion.
# Para 455 14
Let us now consider the step 2 above devoted to 
the selection of a type constructor. This selection is 
mainly guided by the generation points given in the 
formal grammar of the semantic representation. They 
indeed select between cases (a) or (b) and in case (a) 
they directly characterize which of the ci will be 
included in the type construction at the current stage. 
Finally, since active constraints associated to type 
descriptions can be executed at any time, the 
constraint resolution mechanisms which maintain 
constraint coherence are independent of the 
generation strategy. In other terms, these 
mechanisms are independent of the way and the order 
constraints are added to the set of active constraints.
# Para 469 1
6
# Para 470 3
The abstract machine which handles types and 
constraints is the following. It is represented by a 
tuple: &lt;type, set of active constraints&gt;.
# Para 473 1
We then have:
# Para 474 1
1.	=
# Para 475 2
(a) &lt; (C1, C2, ..., Ca), S &gt; sequence of 
subtypes Ci and of active constraints S
# Para 477 1
(b) &lt;D1, S&gt;
# Para 478 1
2. Type constructor selection:
# Para 479 2
(a) &lt; DC, R&gt; where R is the set of 
constraints associated to DC and such that:
# Para 481 1
i) same restrictions as above on DC and
# Para 482 1
ii) R is consistent with S
# Para 483 1
(b) &lt; D&apos; , R &gt; (single type)
# Para 484 1
with R consistent with S.
# Para 485 1
3. al	=
# Para 486 2
(a) &lt; ( DC , Ck+i, ..., Ca), (S L.) R L.) 
(subsume(C&apos;i =&gt; Ci )) for all j E [1,k] ) &gt;
# Para 488 1
(1)) &lt; (Di, D&apos;), S u R&gt;
# Para 489 5
At the end of the generation process, the set of 
possible admissible surface sentences can be directly 
derived from the precedence constraints which may 
not be a total order (some words may have different 
positions).
# Para 494 1
6.	An Application to Secondary
# Para 495 1
Predication
# Para 496 11
We now present a more elaborate and 
comprehensive example which will further motivate 
our approach. Secondary predication is described at 
both lexical and syntaciic levels, the intertwining of 
several constraints makes it simpler to describe in a 
fully declarative way. The description is thus 
independent of its use: parsing or generation. This 
gives a good application example of the specification 
and use of our formalism and system for a real 
phenomenon of much importance to natural 
language system designers.
# Para 507 1
6.1 A linguistic approach
# Para 508 7
Secondary Predication is a term used in the 
literature to denote a very productive structural 
relationship in many languages: the relationship 
between a subject and a predicate, the subject being 
assigned a thematic role by that predicate and by an 
obligatory thematic role assigner in the sentence, 
namely the verb. For instance, in (1)
# Para 515 3
(1) Mary drinks the water cold 
the water, the direct object of drinks, is assigned a 
thematic role by this verb and another one by the 
# Para 518 6
adjective cold. Then, water is, at the same time, an 
object for drinks and a subject for cold. In other 
terms, water integrates - as an object - a primary 
predication which corresponds to the whole sentence, 
and - as a subject - a secondary predication which 
corresponds to the sequence the water cold.
# Para 524 1
6.1.1 Object-oriented Predicates
# Para 525 8
Secondary predication is not an uniform or an 
homogeneous phenomenon, neither from the point 
of view of a specific language, nor from a 
crosslinguistic one. We will describe here some of 
the most relevant structural properties and lexical 
constraints of this type of construction in French. 
Let us begin by considering the French sentence 
corresponding to (1):
# Para 533 1
(2) Marie boit l&apos;eau froide.
# Para 534 4
(2) is an ambiguous sentence as can be 
illustrated by the paraphrases below (the English 
translations of the examples are, all of them, literal 
translations):
# Para 538 2
(2) (a)( Marie boit l&apos;eau qui est froide 
(&quot;Mary drinks the water which is cold&quot;)
# Para 540 2
(b) Quand Marie boit l&apos;eau, l&apos;eau est froide. 
(&quot;When Mary drinks the water, the water is cold&quot;)
# Para 542 4
Considering the interpretation in (2)(a), the 
adjective is part of the direct object of the verb, 
which is not the case for the interpretation in (2)(b). 
Then, /&apos;eaufroide can have the structure
# Para 546 1
(3)(a) [NP[NP l&apos;eau] [AP froide]]
# Para 547 1
or the structure:
# Para 548 1
(3)(b) [NP i l&apos;eau][APi froide].
# Para 549 4
In (3)(a) froide is a modifier of eau, while in 
(3)(b) it behaves as a predicate, assigning a 
secondary thematic roleto the NP. The predication 
relationship is expressed by coindexation.
# Para 553 1
Let us now consider the sentence (4):
# Para 554 1
(4) Marie boil l&apos;eau minerale
# Para 555 1
(&quot;Mary drinks the water mineral&quot;)
# Para 556 4
In spite of its superficial structural resemblance 
with the example above, (4) is not ambiguous, the 
interpretation corresponding to the paraphrase (b) 
being not available:
# Para 560 1
(4)(a) Marie boit l&apos;eau qui est minerale
# Para 561 1
(&quot;Mary drinks the water that is mineral&quot;)
# Para 562 1
but :
# Para 563 2
(4)(b) *Quand Marie boit l&apos;eau, l&apos;eau est 
minerale
# Para 565 2
(&quot;When Mary drinks the water, the water is 
mineral&quot;)
# Para 567 2
This means that the possibility of having or not 
having an object-oriented secondary predication
# Para 569 1
7
# Para 570 4
depends on the semantic nature of the adjective. 
Moreover, there also exist semantic co-occurrence 
restrictions between the adjectival predicate and the 
verb:
# Para 574 1
(5) *Marie boil l&apos;eau congelie
# Para 575 1
(&quot;Mary drinks the water frozen&quot;)
# Para 576 5
(5) is excluded because something frozen cannot 
be drunk. Notice that the presence of an adjective in 
sentences like (2) is optional, in opposition to what 
happens in sentences like (6) (for the same 
interpretation of the verb):
# Para 581 1
(6) Marie considere l&apos;eau froide
# Para 582 1
(&quot;Mary considers the water cold&quot;)
# Para 583 1
(6)(a) *Marie considÃ©re l&apos;eau
# Para 584 1
&quot;Mary considers the water&quot;)
# Para 585 10
What we can infer from the fact that (6)(a) is 
ruled out is that: (i) considirer (to consider) does not 
subcategorize for an NP, then froide can not be a 
modifier of l&apos;eau; (ii) if froide is not a modifier of 
l&apos;eau it must be a predicate, but, in this case, we 
dont have the structure presented in (3)(b). In fact, 
l&apos;eau froide behaves like a clausal phrase. It can even 
be replaced by a completive sentence (the semantic 
interpretation remaining the same) as exemplified in 
(6)(b):
# Para 595 2
(6)(b) Marie considere que l&apos;eau est froide 
(&quot;Mary considers that the water is cold&quot;)
# Para 597 9
We have then empirical evidence to analyse 
l&apos;eau froide in (6) as a clause, a &quot;small clause&quot; using 
an usual label in the literature (since the categorial 
status of the small clause is irrelevant for our 
purposes, we will only use the symbol &quot;Sc,&apos; to refer 
to this constituent, assuming the small clauses 
analysis proposed by Stowell (1981) and Stowell 
(1983)). As a consequence, l&apos;eau froide is a 
predication having the structure in (7):
# Para 606 1
(7) [SC[NPi l&apos;eau] [APi ficâ€¢idel]
# Para 607 6
In this case it is the whole predication, and not only 
its subject, which is theta-marked by the verb. 
StrictÂ° sensus, we have not a secondary predication, 
nevertheless, the contrastive analysis remains 
important since the two kinds of structures are 
superficially very similar.
# Para 613 7
As largely assumed in the GB framework 
(Chomsky (1981) and (1986)), predication is 
configurationnaly constrained: subject and predicate 
must be reciprocally m-commanded, that is, all 
maximal projections (phrase levels) dominating one 
of them must dominate the other one. Given this 
condition and the facts we have examined, (8) and 
# Para 620 3
(9) are appropriate representations (we use here X-bar 
notation only when relevant), respectively, for (2) 
and (6):
# Para 623 2
(8) [S [NP Marie] [V&quot; [VIV boit] [NPi l&apos;eau] ] 
[APi froide]]]
# Para 625 2
(9) [S [NP Marie] [V&quot;[VIV considere] [SC [NPi 
l&apos;eau] [APi firoide]]]]]
# Para 627 9
Although attached to different nodes inside V&quot; 
(while in (8) the subject of the secondary 
predication occupies the direct object position and 
its predicate is in a weak adjunction position (in the 
sense of Demonte (1988)), in (9) subject and 
predicate are together in direct object position), the 
predications we have considered so far involve only 
adjacent elements. Let us now examine sentence 
(10):
# Para 636 1
(10) La lessive rend le tinge blanc
# Para 637 3
(The washing makes the clothes white&quot;) 
Similary to what happens in (6), the sentence is 
ruled out if the adjectif is not present:
# Para 640 1
(10)(a) *La lessive rend le tinge
# Para 641 1
(&quot;The washing makes the linge&quot;)
# Para 642 4
With respect to these facts it seems to be 
natural that sentence (10) is structurally identical to 
(9). Nevertheless, (10)(b), which is equivalent to 
(10), does not support this hypothesis:
# Para 646 1
(10)(b) La lessive blanchit le tinge
# Para 647 1
(&quot;The washing whitens the clothes&quot;)
# Para 648 13
As we can observe, blanchit, a verbal predicate, 
can replace the verb - rend - and the adjectival 
predicate - blanc (for the same semantic 
interpretation). Then, rend blanc behaves like a 
single predicate. At the same time, blanc is a 
secondary predicate for linge. Following a proposal 
by Marra&amp; (1983) and (1985) for similar cases in 
Portuguese, we consider rend blanc as a 
discontinuous complex predicate and we express the 
relationship between the two elements that 
constitue it by co-superscription. Therefore, (10) has 
the structure (10)(c),where k indicates the 
discontinuity in the predicate rend-blanc
# Para 661 2
(10)(c) [S [NP La lessive] [V&quot;[V&apos;[Vk rend] [NPi 
le linge] [APki blanc]]]]
# Para 663 1
6.1.2 Subject-oriented predicates
# Para 664 4
Discontinuity can also be an obligatory 
property of a certain kind of secondary predication, 
namely in the case of subject-oriented predicates. 
(11) is an example:
# Para 668 1
(11) Jean dansait triste
# Para 669 1
8
# Para 670 1
(&quot;John dansed sad&quot;)
# Para 671 7
Since Jean is a proper noun, it can not be 
modified. Then triste is necessarily a predicate for 
John. Although the subject of triste is the main 
subject and not an NP in object position as in the 
above sentences, there are semantic co-occurrence 
constraints between the verb and the adjectival 
predicate, as illustrated below:
# Para 678 1
(11)(a) *Jean dansait repenti
# Para 679 1
(&quot;John dansed repented&quot;)
# Para 680 6
Taking into account these constraints and not 
violating the m-command condition refered to above, 
we represent the adjectival predicate as an strong 
adjunction (again in the terms of Demonte (1988)) to 
V&quot;, the syntactic representation of (11) being, then, 
(11) (b):
# Para 686 2
(11)(b) [S[NPi Jean] [V&quot; [V&quot; [V&apos; [V dansait]]] 
[APi triste]]]
# Para 688 3
Notice that continuous and discontinuous 
secondary predications can co-occurre in the same 
sentence:
# Para 691 1
(12)Jean1 boit l&apos;eauj froidei tristei
# Para 692 1
(&quot;John drinks the water cold sad&quot;)
# Para 693 3
It is also interesting to point out that, in certain 
cases, sentences are ;ambiguous with respect to 
continuous and discontinuous secondary predication:
# Para 696 1
(13 )Jeans laisse son amiej tristei j
# Para 697 1
(&quot;John left his girl friend sad&quot;)
# Para 698 6
To summarize, secondary predication can be 
associated to different types of structure and to 
continuous or discontinuous elements. Moreover, 
there are numerous and different semantic co- 
occurrence restrictions of different types affecting 
the lexical items involved.
# Para 704 2
6.2 An Implementation in terms of 
types and constraints
# Para 706 9
We now show how the above examples are 
expressed both at syntactic and lexical levels. The 
full syntactic structures are given under (8), (9), 
(10c) and (1 1b). The structure in (8) says that the AP 
is a sister of the V&apos; (noted in the grammar as V with 
bar level 1) and that the AP is co-indexed with the 
object NP, the co-indexation relation is left pending 
since it is preceded by the V&apos; description. We have 
the following construction:
# Para 715 1
xp( cat =&gt; v, string =&gt; SV,
# Para 716 1
constl =&gt; xl ( cat =&gt; v, string =&gt; $3),
# Para 717 2
const2 =&gt; xp( cat =&gt; a, string =&gt; S4, 
index =&gt; I ) :-
# Para 719 1
pending(xp(cat =&gt; v),[x1( cat =&gt; v, 
# Para 720 2
string =&gt; T, const1 =&gt; x0( cat =&gt; v, 
string =&gt; Si), const2 =&gt; xp( cat =&gt; n,
# Para 722 2
string =&gt; S2, index =&gt; I ) ) j ),
precede(S1,S2), precede(S3,S4).
# Para 724 3
Since the AP is not obligatory (it is a weak 
adjunct), there is nothing said about it in the 
lexicon.
# Para 727 5
Construction (9) introduces a small clause 
(noted here as sc). Since it is not necessarily 
contiguous to the V&apos;, but only dominated by the V&apos;, 
we need a pending constraint. The type construction 
is the following:
# Para 732 1
x1( cat =&gt; v, string =&gt; SV,
# Para 733 3
const1 =&gt; x0( cat =&gt; v, string =&gt; Si ), 
const2 =&gt; xp( cat =&gt; Sc, string =&gt; S2 ) ) :- 
pending(x1 (cat =&gt; v),[x1 ( cat =&gt; Sc,
# Para 736 2
string =&gt; SV, const1 =&gt; xp( cat =&gt; n,
index =&gt; I, string =&gt; S3), const2
# Para 738 2
xp( cat =&gt; a, string =&gt; S4,index =&gt; I ) ) ), 
precede(S1,S2), precede(S3,S4).
# Para 740 3
The lexical entry of the verb (here considerer) 
has a pending constraint for the small clause: the 
verb subcategorizes for a small clause.
# Para 743 4
Construction (10c) introduces a double 
indexation but no long-distance dependency for the 
compound predicate &apos;rend-blanc&apos;. We represent it as 
follows:
# Para 747 1
x1( cat =&gt; v, string =&gt; SV,
# Para 748 1
const1 =&gt; x0( cat =&gt; v,
# Para 749 1
compound_pred =&gt; K,
# Para 750 1
string =&gt; Si),
# Para 751 1
const2 =&gt; xp( cat =&gt; n, index =&gt; I,
# Para 752 1
string =&gt; S2 ),
# Para 753 1
const3 =&gt; xp( cat =&gt; a, index =&gt; I,
# Para 754 2
string =&gt; S3, compound_pred =&gt; K) ) :- 
precede(S1,S2), precede(S2,S3).
# Para 756 7
Finally, the construction given in (11b) 
introduces a long-distance relation between an NP in 
subject position and an AP which is in object 
position. To handle this phenomenon, we have to go 
up to the sentence level, that we will represent here 
for simplicity as s (instead of, for example, COMP). 
The type construction is the following:
# Para 763 1
xp( cat =&gt; s, string =&gt; SV,
# Para 764 1
const1 =&gt; xp( cat =&gt; n, string =&gt; Si,
# Para 765 1
index =&gt; I ),
# Para 766 2
const2 =&gt; xp( cat =&gt; v, string =&gt; S2 ) ) :- 
pending(xp(cat =&gt; s),[xp( cat =&gt; v,
# Para 768 1
9
# Para 769 1
string =&gt; T,
# Para 770 2
const1 =&gt; xp( cat =&gt; v,string =&gt; S3), 
const2 =&gt; xp( cat =&gt; a,
# Para 772 2
string =&gt; S4, index =&gt; I ) ) ), 
precede(S1,S2), precede(S3,S4).
# Para 774 4
At the lexical level, the adjoined AP is not 
mentioned, since it is not syntactically necessary 
(but it might be necessary from a semantic point of 
view, as also for case (8) above).
# Para 778 1
7. Specific features of our approach
# Para 779 13
Our approach can be contrasted mainly with the 
usual systems based on unification grammar (UG) 
formalisms (Shieber, 86), (Emele and Zajac 90). The 
first major difference is that the unification and 
rewriting mechanisms usually associated with UG 
are replaced by a more constraining operation, type 
construction, which always proceeds by sucessive 
restrictions (or monotone increasing specialisation) 
each time a type is further expanded. From that point 
of view, our approach also substantially differs from 
(Alt Kaci and Nasr, 86) who propose a powerful and 
semantically clear mechanism for typed unification 
associated to type inheritance.
# Para 792 8
Next, we have a single operation: type 
construction; we do not have on the one hand 
grammar rules and on the other hand, associated to 
each rule, a set of equations to deal with feature 
values and constraints. The constraints we have 
associated with our types are not of the same nature 
and cannot be compared to the equations of UGs. 
They are moreover a part of the type.
# Para 800 7
Constraints added to types are interpreted within 
the CLP framework, this permits us to have a more 
expressive and powerful constraint system, which is 
also more efficient and simpler to write. Constraint 
satisfaction is not indeed guaranteed at the level they 
are given, but throughout the whole type 
construction process.
# Para 807 9
Our approach is compatible with the current 
principled-based approaches to describing languages. 
This is exemplified in section 4 by the constraints 
on role and case assignments. In a more general way, 
the description language we have presented here is 
particularly appropriate for highly abstract 
descriptions of language, which corresponds to 
several current trends in computational linguistics. 
Our description language is, in the same time, well- 
# Para 816 5
adapted to deal with lexical-based approaches to 
language processing (those approaches like lexicon 
grammars where the lexicon plays a central role) and 
to describe representations developed within lexical 
semantics.
# Para 821 7
Finally, a constraint like pending generalises 
the notion of long-distance dependency to several 
other kinds of dependencies. This generalization is in 
particular a consequence of the fact that type 
structures do not have any ordering on subtypes and 
they cannot, thus, directly express the difference 
between remote and close constituents.
# Para 828 11
Besides these general properties, our approach 
has several interesting properties which are more 
specific to reversibility. First, the common data 
shared by the two processes is all the linguistic data 
which is specified in a declarative way: lexical and 
grammatical. The semantic composition rules are the 
same. In the case of generation, they are translated 
into a parser of the formal grammar of this semantic 
representation. It should be pointed out that the 
parser given in section 5 can be generated 
automatically.
# Para 839 21
Both processes also have a lot of elements in 
common at the procedural level: the type 
construction mechanisms are identical, the major 
difference at this level being the selection rule, 
which is, in the case of generation, guided by the 
semantic form from which the process starts. The 
other difference is that parsing proceeds a priori top- 
down in the case we have exemplified (it could also 
proceed bottom-up). Generation proceeds bottom-up, 
for reasons explained in section 5. From this 
difference it results that the starting type in the case 
of parsing is a general type corresponding to 
sentence whereas there are no starting type in the 
case of generation, the starting points being the 
types corresponding to the predicates appearing in 
the logical formula, which are deduced from an 
operation close to lexical insertion. If the parsing 
process were bottom-up, then the starting types 
would be the same and the subsumption operation 
would also be used instead of the standard 
unification.
# Para 860 5
Finally, and most importantly, the constraint 
system that we have presented is fully independent of 
the strategies used and of the direction of the process: 
generation of parsing. This is a consequence of the 
fact that constraints are evaluted only when there is
# Para 865 1
10
# Para 866 5
sufficient available information to evaluate them and 
also that their coherence with the other constraints is 
checked throughout the whole proof construction 
procedure. The variables which are used by active 
constraints are thus global variables.
# Para 871 1
Conclusion
# Para 872 18
In this document, we have first defined a 
formalism based on types and active constraints of 
the Logic Programming framework and have shown 
that it is well-appropriate to describe language 
constructions. We have in particular illustrated it by 
focussing on second* predication, an important 
phenomenon in language processing. Finally, we 
have shown that our formalism is particularly 
appropriate to be used by a parser and by a generator, 
in a symmetric way, and we have defined for that 
prupose two abstract machines. This work is now 
fully implemented in Sicstus Prolog (which allows 
the writing of constraint resolution mechanism) on a 
Sun workstation. Since constraints are so far meta- 
interpreted, we cannot make real comparisons with 
existing NLP systems. A significant result is 
however the much smaller number of bacictraking 
operations that we have observed.
# Para 890 1
Acknowledgements
# Para 891 4
We thank N. Hathout for his insightful 
comments on this work, which was supported by the 
PRC Communication Homme-Machine and the 
French ministry of research.
# Para 895 1
References
# Para 896 4
Aiit-Kagi, H., Nasr, R., LOGIN: A Logic 
Programming Language with Built-in Inheritance, 
journal of Logic Programming, vol. 3, pp 185-215, 
1986.
# Para 900 2
Chomsky, N., Lectures on Government and 
Binding, Foris, 1981.
# Para 902 2
Chomsky, N., Barriers, Linguistic Inquiry 
monograph no 13, MIT Press, 1986.
# Para 904 2
Colmerauer, A., An Introduction to Prolog III, 
CACM 33-7, 1990.
# Para 906 3
Demonte, V., Remarks on Secondary Predicates: 
C-Command, Extraction and Reanalysis, The 
Linguistic Review 6, pp 1-39, 1988.
# Para 909 2
Emele, M., Zajac, R., Typed Unification 
Grammars, in proc. COLING&apos;90, Helsinki, 1990.
# Para 911 2
Jaffar, J., Lassez, J.L., Constraint Logic 
Programming, Proc. 14th ACM Symposium on
# Para 913 1
Principles of Programming Languages, 1987.
# Para 914 4
Marrafa, P., Teoria das Pequenas Oragoes vs 
Teoria da Predicacao: ControlÂ°, Criterio Tematico e 
Principio de Projeccao, ms, FLL-University of 
Lisbon, 1983.
# Para 918 3
Marrafa, P., A Construcao Transitiva-
Predicativa em Portuguas, FLL-University of 
Lisbon, 1985.
# Para 921 5
Saint-Dizier, P., Contextual Discontinuous 
Grammars, 2nd NLULP, Vancouver 1987 and in: 
Natural Language Understanding and Logic 
Programming II, V. Dahl and P. Saint-Dizier Edts, 
North Holland, 1988.
# Para 926 3
Saint-Dizier, P., A generation Strategy based on 
GB Principles, proc. 2nd European workshop on 
language generation, Edinburgh, 1989.
# Para 929 1
Saint-Dizier, P., Constrained Logic
# Para 930 2
Programming for Natural Language Processing, 
proc. ACL-89, Manchester, 1989.
# Para 932 3
Sheiber, S., An Introduction to Unification- 
Based Approaches to Grammar, CSLI lecture notes 
no 4, Chicago University Press, 1986.
# Para 935 2
Stowell, T., Origins of Phrase Structure, PhD. 
dissertation, MIT, 1981.
# Para 937 2
Stowell, T., Subject across Categories, The 
Linguistic Review 2, pp 285-312, 1983.
# Para 939 1
1].
